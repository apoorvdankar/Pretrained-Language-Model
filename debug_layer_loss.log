2023-03-27 13:30:54,148 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA//TempTinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=30.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='BASE')
2023-03-27 13:31:00,302 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/CoLA//TempTinyBERT_CoLA_4L_312D', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA//TinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=3e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=True, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='BASE')
2023-03-27 13:33:31,297 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA//TempTinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=30.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='BASE')
2023-03-27 13:33:32,499 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 13:33:35,225 device: cuda n_gpu: 1
2023-03-27 13:33:35,314 Writing example 0 of 8551
2023-03-27 13:33:35,314 *** Example ***
2023-03-27 13:33:35,315 guid: train-0
2023-03-27 13:33:35,315 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 13:33:35,315 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 13:33:35,315 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 13:33:35,316 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 13:33:35,316 label: 1
2023-03-27 13:33:35,316 label_id: 1
2023-03-27 13:33:36,429 Writing example 0 of 1043
2023-03-27 13:33:36,429 *** Example ***
2023-03-27 13:33:36,430 guid: dev-0
2023-03-27 13:33:36,430 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 13:33:36,430 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 13:33:36,430 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 13:33:36,430 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 13:33:36,430 label: 1
2023-03-27 13:33:36,430 label_id: 1
2023-03-27 13:33:36,552 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 13:33:36,556 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 13:33:38,455 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 13:33:42,244 loading model...
2023-03-27 13:33:42,305 done!
2023-03-27 13:33:42,305 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 13:33:48,440 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 13:33:48,444 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 13:33:48,669 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 13:33:49,231 loading model...
2023-03-27 13:33:49,248 done!
2023-03-27 13:33:49,248 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 13:33:49,249 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 13:33:49,272 ***** Running training *****
2023-03-27 13:33:49,272   Num examples = 8551
2023-03-27 13:33:49,273   Batch size = 32
2023-03-27 13:33:49,273   Num steps = 8010
2023-03-27 13:33:49,274 n: bert.embeddings.word_embeddings.weight
2023-03-27 13:33:49,274 n: bert.embeddings.position_embeddings.weight
2023-03-27 13:33:49,274 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 13:33:49,275 n: bert.embeddings.LayerNorm.weight
2023-03-27 13:33:49,275 n: bert.embeddings.LayerNorm.bias
2023-03-27 13:33:49,275 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 13:33:49,275 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 13:33:49,276 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 13:33:49,276 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 13:33:49,276 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 13:33:49,276 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 13:33:49,277 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 13:33:49,277 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 13:33:49,277 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 13:33:49,277 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 13:33:49,277 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 13:33:49,278 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 13:33:49,278 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 13:33:49,278 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 13:33:49,278 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 13:33:49,278 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 13:33:49,278 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 13:33:49,279 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 13:33:49,279 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 13:33:49,279 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 13:33:49,279 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 13:33:49,279 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 13:33:49,279 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 13:33:49,280 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 13:33:49,280 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 13:33:49,280 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 13:33:49,280 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 13:33:49,280 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 13:33:49,280 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 13:33:49,280 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 13:33:49,281 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 13:33:49,281 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 13:33:49,281 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 13:33:49,281 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 13:33:49,281 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 13:33:49,281 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 13:33:49,282 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 13:33:49,282 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 13:33:49,282 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 13:33:49,282 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 13:33:49,282 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 13:33:49,282 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 13:33:49,282 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 13:33:49,282 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 13:33:49,282 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 13:33:49,283 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 13:33:49,283 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 13:33:49,283 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 13:33:49,283 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 13:33:49,283 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 13:33:49,283 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 13:33:49,283 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 13:33:49,283 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 13:33:49,284 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 13:33:49,284 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 13:33:49,284 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 13:33:49,284 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 13:33:49,284 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 13:33:49,284 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 13:33:49,284 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 13:33:49,284 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 13:33:49,284 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 13:33:49,285 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 13:33:49,285 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 13:33:49,285 n: bert.pooler.dense.weight
2023-03-27 13:33:49,285 n: bert.pooler.dense.bias
2023-03-27 13:33:49,285 n: classifier.weight
2023-03-27 13:33:49,285 n: classifier.bias
2023-03-27 13:33:49,285 n: fit_dense.weight
2023-03-27 13:33:49,285 n: fit_dense.bias
2023-03-27 13:33:49,285 Total parameters: 14591258
2023-03-27 13:34:04,306 ***** Running evaluation *****
2023-03-27 13:34:04,307   Epoch = 0 iter 49 step
2023-03-27 13:34:04,307   Num examples = 1043
2023-03-27 13:34:04,307   Batch size = 32
2023-03-27 13:34:04,316 ***** Eval results *****
2023-03-27 13:34:04,316   att_loss = 0.5624887608751958
2023-03-27 13:34:04,316   cls_loss = 0.0
2023-03-27 13:34:04,317   global_step = 49
2023-03-27 13:34:04,317   loss = 1.885422227334003
2023-03-27 13:34:04,317   rep_loss = 1.3229334549028047
2023-03-27 13:34:04,324 ***** Save model *****
2023-03-27 13:34:14,589 ***** Running evaluation *****
2023-03-27 13:34:14,589   Epoch = 0 iter 99 step
2023-03-27 13:34:14,589   Num examples = 1043
2023-03-27 13:34:14,590   Batch size = 32
2023-03-27 13:34:14,591 ***** Eval results *****
2023-03-27 13:34:14,592   att_loss = 0.5208015110757616
2023-03-27 13:34:14,592   cls_loss = 0.0
2023-03-27 13:34:14,592   global_step = 99
2023-03-27 13:34:14,592   loss = 1.625842166669441
2023-03-27 13:34:14,592   rep_loss = 1.1050406513792095
2023-03-27 13:34:14,599 ***** Save model *****
2023-03-27 13:34:24,972 ***** Running evaluation *****
2023-03-27 13:34:24,972   Epoch = 0 iter 149 step
2023-03-27 13:34:24,973   Num examples = 1043
2023-03-27 13:34:24,973   Batch size = 32
2023-03-27 13:34:24,974 ***** Eval results *****
2023-03-27 13:34:24,974   att_loss = 0.4964984979405499
2023-03-27 13:34:24,975   cls_loss = 0.0
2023-03-27 13:34:24,975   global_step = 149
2023-03-27 13:34:24,975   loss = 1.5029620956254486
2023-03-27 13:34:24,976   rep_loss = 1.0064635928845245
2023-03-27 13:34:24,983 ***** Save model *****
2023-03-27 13:34:35,434 ***** Running evaluation *****
2023-03-27 13:34:35,435   Epoch = 0 iter 199 step
2023-03-27 13:34:35,435   Num examples = 1043
2023-03-27 13:34:35,435   Batch size = 32
2023-03-27 13:34:35,437 ***** Eval results *****
2023-03-27 13:34:35,437   att_loss = 0.4843230084258707
2023-03-27 13:34:35,437   cls_loss = 0.0
2023-03-27 13:34:35,437   global_step = 199
2023-03-27 13:34:35,437   loss = 1.4326463176976496
2023-03-27 13:34:35,437   rep_loss = 0.9483233037306436
2023-03-27 13:34:35,444 ***** Save model *****
2023-03-27 13:34:45,973 ***** Running evaluation *****
2023-03-27 13:34:45,973   Epoch = 0 iter 249 step
2023-03-27 13:34:45,974   Num examples = 1043
2023-03-27 13:34:45,974   Batch size = 32
2023-03-27 13:34:45,975 ***** Eval results *****
2023-03-27 13:34:45,975   att_loss = 0.47543028882708416
2023-03-27 13:34:45,975   cls_loss = 0.0
2023-03-27 13:34:45,976   global_step = 249
2023-03-27 13:34:45,976   loss = 1.384641918791346
2023-03-27 13:34:45,976   rep_loss = 0.9092116243389237
2023-03-27 13:34:45,983 ***** Save model *****
2023-03-27 13:34:56,555 ***** Running evaluation *****
2023-03-27 13:34:56,555   Epoch = 1 iter 299 step
2023-03-27 13:34:56,555   Num examples = 1043
2023-03-27 13:34:56,555   Batch size = 32
2023-03-27 13:34:56,556 ***** Eval results *****
2023-03-27 13:34:56,556   att_loss = 0.4300664449110627
2023-03-27 13:34:56,556   cls_loss = 0.0
2023-03-27 13:34:56,557   global_step = 299
2023-03-27 13:34:56,557   loss = 1.1647225208580494
2023-03-27 13:34:56,557   rep_loss = 0.7346560824662447
2023-03-27 13:34:56,564 ***** Save model *****
2023-03-27 13:35:07,229 ***** Running evaluation *****
2023-03-27 13:35:07,230   Epoch = 1 iter 349 step
2023-03-27 13:35:07,230   Num examples = 1043
2023-03-27 13:35:07,230   Batch size = 32
2023-03-27 13:35:07,231 ***** Eval results *****
2023-03-27 13:35:07,231   att_loss = 0.4341871356818734
2023-03-27 13:35:07,231   cls_loss = 0.0
2023-03-27 13:35:07,231   global_step = 349
2023-03-27 13:35:07,231   loss = 1.1631340544398239
2023-03-27 13:35:07,232   rep_loss = 0.7289469205751652
2023-03-27 13:35:07,238 ***** Save model *****
2023-03-27 13:35:17,962 ***** Running evaluation *****
2023-03-27 13:35:17,962   Epoch = 1 iter 399 step
2023-03-27 13:35:17,962   Num examples = 1043
2023-03-27 13:35:17,962   Batch size = 32
2023-03-27 13:35:17,964 ***** Eval results *****
2023-03-27 13:35:17,964   att_loss = 0.43267385480981885
2023-03-27 13:35:17,964   cls_loss = 0.0
2023-03-27 13:35:17,964   global_step = 399
2023-03-27 13:35:17,964   loss = 1.1566124359766643
2023-03-27 13:35:17,964   rep_loss = 0.7239385789090936
2023-03-27 13:35:17,972 ***** Save model *****
2023-03-27 13:35:28,715 ***** Running evaluation *****
2023-03-27 13:35:28,715   Epoch = 1 iter 449 step
2023-03-27 13:35:28,715   Num examples = 1043
2023-03-27 13:35:28,715   Batch size = 32
2023-03-27 13:35:28,716 ***** Eval results *****
2023-03-27 13:35:28,717   att_loss = 0.43400396161027005
2023-03-27 13:35:28,717   cls_loss = 0.0
2023-03-27 13:35:28,717   global_step = 449
2023-03-27 13:35:28,717   loss = 1.1545401772299966
2023-03-27 13:35:28,717   rep_loss = 0.7205362136547382
2023-03-27 13:35:28,724 ***** Save model *****
2023-03-27 13:35:39,559 ***** Running evaluation *****
2023-03-27 13:35:39,559   Epoch = 1 iter 499 step
2023-03-27 13:35:39,559   Num examples = 1043
2023-03-27 13:35:39,559   Batch size = 32
2023-03-27 13:35:39,560 ***** Eval results *****
2023-03-27 13:35:39,560   att_loss = 0.4307284517021015
2023-03-27 13:35:39,561   cls_loss = 0.0
2023-03-27 13:35:39,561   global_step = 499
2023-03-27 13:35:39,561   loss = 1.1468715223258938
2023-03-27 13:35:39,561   rep_loss = 0.7161430683115433
2023-03-27 13:35:39,568 ***** Save model *****
2023-03-27 13:35:50,458 ***** Running evaluation *****
2023-03-27 13:35:50,458   Epoch = 2 iter 549 step
2023-03-27 13:35:50,458   Num examples = 1043
2023-03-27 13:35:50,458   Batch size = 32
2023-03-27 13:35:50,459 ***** Eval results *****
2023-03-27 13:35:50,460   att_loss = 0.39662219484647115
2023-03-27 13:35:50,460   cls_loss = 0.0
2023-03-27 13:35:50,460   global_step = 549
2023-03-27 13:35:50,460   loss = 1.0691010157267253
2023-03-27 13:35:50,460   rep_loss = 0.6724788268407186
2023-03-27 13:35:50,462 ***** Save model *****
2023-03-27 13:36:01,283 ***** Running evaluation *****
2023-03-27 13:36:01,283   Epoch = 2 iter 599 step
2023-03-27 13:36:01,283   Num examples = 1043
2023-03-27 13:36:01,283   Batch size = 32
2023-03-27 13:36:01,285 ***** Eval results *****
2023-03-27 13:36:01,285   att_loss = 0.41310284366974465
2023-03-27 13:36:01,285   cls_loss = 0.0
2023-03-27 13:36:01,285   global_step = 599
2023-03-27 13:36:01,285   loss = 1.0961717954048744
2023-03-27 13:36:01,286   rep_loss = 0.6830689430236816
2023-03-27 13:36:01,293 ***** Save model *****
2023-03-27 13:36:12,223 ***** Running evaluation *****
2023-03-27 13:36:12,223   Epoch = 2 iter 649 step
2023-03-27 13:36:12,223   Num examples = 1043
2023-03-27 13:36:12,224   Batch size = 32
2023-03-27 13:36:12,225 ***** Eval results *****
2023-03-27 13:36:12,225   att_loss = 0.40927167768063755
2023-03-27 13:36:12,225   cls_loss = 0.0
2023-03-27 13:36:12,225   global_step = 649
2023-03-27 13:36:12,226   loss = 1.0888966332311216
2023-03-27 13:36:12,226   rep_loss = 0.6796249524406764
2023-03-27 13:36:12,233 ***** Save model *****
2023-03-27 13:36:23,190 ***** Running evaluation *****
2023-03-27 13:36:23,190   Epoch = 2 iter 699 step
2023-03-27 13:36:23,191   Num examples = 1043
2023-03-27 13:36:23,191   Batch size = 32
2023-03-27 13:36:23,192 ***** Eval results *****
2023-03-27 13:36:23,193   att_loss = 0.4119332958351482
2023-03-27 13:36:23,193   cls_loss = 0.0
2023-03-27 13:36:23,193   global_step = 699
2023-03-27 13:36:23,193   loss = 1.0910217075636892
2023-03-27 13:36:23,193   rep_loss = 0.6790884101029598
2023-03-27 13:36:23,201 ***** Save model *****
2023-03-27 13:36:34,164 ***** Running evaluation *****
2023-03-27 13:36:34,165   Epoch = 2 iter 749 step
2023-03-27 13:36:34,165   Num examples = 1043
2023-03-27 13:36:34,165   Batch size = 32
2023-03-27 13:36:34,166 ***** Eval results *****
2023-03-27 13:36:34,166   att_loss = 0.4134378297384395
2023-03-27 13:36:34,166   cls_loss = 0.0
2023-03-27 13:36:34,167   global_step = 749
2023-03-27 13:36:34,167   loss = 1.0912425207537273
2023-03-27 13:36:34,167   rep_loss = 0.6778046899063642
2023-03-27 13:36:34,174 ***** Save model *****
2023-03-27 13:36:45,196 ***** Running evaluation *****
2023-03-27 13:36:45,197   Epoch = 2 iter 799 step
2023-03-27 13:36:45,197   Num examples = 1043
2023-03-27 13:36:45,197   Batch size = 32
2023-03-27 13:36:45,199 ***** Eval results *****
2023-03-27 13:36:45,199   att_loss = 0.4134491615700272
2023-03-27 13:36:45,199   cls_loss = 0.0
2023-03-27 13:36:45,199   global_step = 799
2023-03-27 13:36:45,200   loss = 1.0893800404836547
2023-03-27 13:36:45,200   rep_loss = 0.6759308770017803
2023-03-27 13:36:45,207 ***** Save model *****
2023-03-27 13:36:56,259 ***** Running evaluation *****
2023-03-27 13:36:56,260   Epoch = 3 iter 849 step
2023-03-27 13:36:56,260   Num examples = 1043
2023-03-27 13:36:56,260   Batch size = 32
2023-03-27 13:36:56,261 ***** Eval results *****
2023-03-27 13:36:56,261   att_loss = 0.3948137064774831
2023-03-27 13:36:56,261   cls_loss = 0.0
2023-03-27 13:36:56,261   global_step = 849
2023-03-27 13:36:56,261   loss = 1.0505981830259163
2023-03-27 13:36:56,262   rep_loss = 0.6557844715813795
2023-03-27 13:36:56,269 ***** Save model *****
2023-03-27 13:37:07,345 ***** Running evaluation *****
2023-03-27 13:37:07,345   Epoch = 3 iter 899 step
2023-03-27 13:37:07,345   Num examples = 1043
2023-03-27 13:37:07,346   Batch size = 32
2023-03-27 13:37:07,347 ***** Eval results *****
2023-03-27 13:37:07,347   att_loss = 0.40008256964537564
2023-03-27 13:37:07,347   cls_loss = 0.0
2023-03-27 13:37:07,347   global_step = 899
2023-03-27 13:37:07,348   loss = 1.0546442020912559
2023-03-27 13:37:07,348   rep_loss = 0.6545616345746177
2023-03-27 13:37:07,356 ***** Save model *****
2023-03-27 13:37:18,454 ***** Running evaluation *****
2023-03-27 13:37:18,455   Epoch = 3 iter 949 step
2023-03-27 13:37:18,455   Num examples = 1043
2023-03-27 13:37:18,455   Batch size = 32
2023-03-27 13:37:18,456 ***** Eval results *****
2023-03-27 13:37:18,456   att_loss = 0.403970867395401
2023-03-27 13:37:18,457   cls_loss = 0.0
2023-03-27 13:37:18,457   global_step = 949
2023-03-27 13:37:18,457   loss = 1.059445704560022
2023-03-27 13:37:18,457   rep_loss = 0.6554748407892279
2023-03-27 13:37:18,464 ***** Save model *****
2023-03-27 13:37:29,568 ***** Running evaluation *****
2023-03-27 13:37:29,568   Epoch = 3 iter 999 step
2023-03-27 13:37:29,569   Num examples = 1043
2023-03-27 13:37:29,569   Batch size = 32
2023-03-27 13:37:29,570 ***** Eval results *****
2023-03-27 13:37:29,570   att_loss = 0.4025316471704329
2023-03-27 13:37:29,570   cls_loss = 0.0
2023-03-27 13:37:29,570   global_step = 999
2023-03-27 13:37:29,571   loss = 1.056138980870295
2023-03-27 13:37:29,571   rep_loss = 0.6536073377638152
2023-03-27 13:37:29,578 ***** Save model *****
2023-03-27 13:37:40,690 ***** Running evaluation *****
2023-03-27 13:37:40,690   Epoch = 3 iter 1049 step
2023-03-27 13:37:40,690   Num examples = 1043
2023-03-27 13:37:40,690   Batch size = 32
2023-03-27 13:37:40,692 ***** Eval results *****
2023-03-27 13:37:40,692   att_loss = 0.4021085189475167
2023-03-27 13:37:40,692   cls_loss = 0.0
2023-03-27 13:37:40,692   global_step = 1049
2023-03-27 13:37:40,693   loss = 1.0544886514544487
2023-03-27 13:37:40,693   rep_loss = 0.6523801371935876
2023-03-27 13:37:40,700 ***** Save model *****
2023-03-27 13:37:51,838 ***** Running evaluation *****
2023-03-27 13:37:51,838   Epoch = 4 iter 1099 step
2023-03-27 13:37:51,839   Num examples = 1043
2023-03-27 13:37:51,839   Batch size = 32
2023-03-27 13:37:51,840 ***** Eval results *****
2023-03-27 13:37:51,840   att_loss = 0.4054439740796243
2023-03-27 13:37:51,840   cls_loss = 0.0
2023-03-27 13:37:51,840   global_step = 1099
2023-03-27 13:37:51,841   loss = 1.0505533641384495
2023-03-27 13:37:51,841   rep_loss = 0.6451093842906337
2023-03-27 13:37:51,848 ***** Save model *****
2023-03-27 13:38:03,038 ***** Running evaluation *****
2023-03-27 13:38:03,039   Epoch = 4 iter 1149 step
2023-03-27 13:38:03,039   Num examples = 1043
2023-03-27 13:38:03,039   Batch size = 32
2023-03-27 13:38:03,040 ***** Eval results *****
2023-03-27 13:38:03,040   att_loss = 0.403627230429355
2023-03-27 13:38:03,040   cls_loss = 0.0
2023-03-27 13:38:03,041   global_step = 1149
2023-03-27 13:38:03,041   loss = 1.0472957109227592
2023-03-27 13:38:03,041   rep_loss = 0.6436684801254744
2023-03-27 13:38:03,048 ***** Save model *****
2023-03-27 13:38:14,231 ***** Running evaluation *****
2023-03-27 13:38:14,231   Epoch = 4 iter 1199 step
2023-03-27 13:38:14,231   Num examples = 1043
2023-03-27 13:38:14,232   Batch size = 32
2023-03-27 13:38:14,232 ***** Eval results *****
2023-03-27 13:38:14,233   att_loss = 0.3985099150934292
2023-03-27 13:38:14,233   cls_loss = 0.0
2023-03-27 13:38:14,233   global_step = 1199
2023-03-27 13:38:14,233   loss = 1.039177829527673
2023-03-27 13:38:14,233   rep_loss = 0.6406679135242491
2023-03-27 13:38:14,236 ***** Save model *****
2023-03-27 13:38:25,401 ***** Running evaluation *****
2023-03-27 13:38:25,401   Epoch = 4 iter 1249 step
2023-03-27 13:38:25,401   Num examples = 1043
2023-03-27 13:38:25,401   Batch size = 32
2023-03-27 13:38:25,402 ***** Eval results *****
2023-03-27 13:38:25,402   att_loss = 0.3953841019071927
2023-03-27 13:38:25,402   cls_loss = 0.0
2023-03-27 13:38:25,402   global_step = 1249
2023-03-27 13:38:25,403   loss = 1.0335068396441844
2023-03-27 13:38:25,403   rep_loss = 0.6381227354318397
2023-03-27 13:38:25,410 ***** Save model *****
2023-03-27 13:38:36,603 ***** Running evaluation *****
2023-03-27 13:38:36,604   Epoch = 4 iter 1299 step
2023-03-27 13:38:36,604   Num examples = 1043
2023-03-27 13:38:36,604   Batch size = 32
2023-03-27 13:38:36,606 ***** Eval results *****
2023-03-27 13:38:36,606   att_loss = 0.3963005565977716
2023-03-27 13:38:36,606   cls_loss = 0.0
2023-03-27 13:38:36,606   global_step = 1299
2023-03-27 13:38:36,606   loss = 1.0330461070135042
2023-03-27 13:38:36,606   rep_loss = 0.6367455486095313
2023-03-27 13:38:36,608 ***** Save model *****
2023-03-27 13:38:47,781 ***** Running evaluation *****
2023-03-27 13:38:47,781   Epoch = 5 iter 1349 step
2023-03-27 13:38:47,781   Num examples = 1043
2023-03-27 13:38:47,781   Batch size = 32
2023-03-27 13:38:47,782 ***** Eval results *****
2023-03-27 13:38:47,782   att_loss = 0.3825896169458117
2023-03-27 13:38:47,782   cls_loss = 0.0
2023-03-27 13:38:47,783   global_step = 1349
2023-03-27 13:38:47,783   loss = 1.006858697959355
2023-03-27 13:38:47,783   rep_loss = 0.6242690810135433
2023-03-27 13:38:47,790 ***** Save model *****
2023-03-27 13:38:58,997 ***** Running evaluation *****
2023-03-27 13:38:58,998   Epoch = 5 iter 1399 step
2023-03-27 13:38:58,998   Num examples = 1043
2023-03-27 13:38:58,998   Batch size = 32
2023-03-27 13:38:59,000 ***** Eval results *****
2023-03-27 13:38:59,000   att_loss = 0.39165697526186705
2023-03-27 13:38:59,000   cls_loss = 0.0
2023-03-27 13:38:59,000   global_step = 1399
2023-03-27 13:38:59,000   loss = 1.0195242576301098
2023-03-27 13:38:59,001   rep_loss = 0.627867279574275
2023-03-27 13:38:59,008 ***** Save model *****
2023-03-27 13:39:10,214 ***** Running evaluation *****
2023-03-27 13:39:10,214   Epoch = 5 iter 1449 step
2023-03-27 13:39:10,214   Num examples = 1043
2023-03-27 13:39:10,215   Batch size = 32
2023-03-27 13:39:10,216 ***** Eval results *****
2023-03-27 13:39:10,216   att_loss = 0.3876965087756776
2023-03-27 13:39:10,216   cls_loss = 0.0
2023-03-27 13:39:10,217   global_step = 1449
2023-03-27 13:39:10,217   loss = 1.0139673153559368
2023-03-27 13:39:10,217   rep_loss = 0.6262708081488024
2023-03-27 13:39:10,224 ***** Save model *****
2023-03-27 13:39:21,469 ***** Running evaluation *****
2023-03-27 13:39:21,469   Epoch = 5 iter 1499 step
2023-03-27 13:39:21,470   Num examples = 1043
2023-03-27 13:39:21,470   Batch size = 32
2023-03-27 13:39:21,471 ***** Eval results *****
2023-03-27 13:39:21,472   att_loss = 0.3887402369845204
2023-03-27 13:39:21,472   cls_loss = 0.0
2023-03-27 13:39:21,472   global_step = 1499
2023-03-27 13:39:21,472   loss = 1.0137573829511317
2023-03-27 13:39:21,472   rep_loss = 0.6250171483289905
2023-03-27 13:39:21,479 ***** Save model *****
2023-03-27 13:39:32,738 ***** Running evaluation *****
2023-03-27 13:39:32,738   Epoch = 5 iter 1549 step
2023-03-27 13:39:32,739   Num examples = 1043
2023-03-27 13:39:32,739   Batch size = 32
2023-03-27 13:39:32,740 ***** Eval results *****
2023-03-27 13:39:32,740   att_loss = 0.38855596165233686
2023-03-27 13:39:32,740   cls_loss = 0.0
2023-03-27 13:39:32,740   global_step = 1549
2023-03-27 13:39:32,741   loss = 1.0123969088090914
2023-03-27 13:39:32,741   rep_loss = 0.6238409478530705
2023-03-27 13:39:32,748 ***** Save model *****
2023-03-27 13:39:44,013 ***** Running evaluation *****
2023-03-27 13:39:44,014   Epoch = 5 iter 1599 step
2023-03-27 13:39:44,014   Num examples = 1043
2023-03-27 13:39:44,014   Batch size = 32
2023-03-27 13:39:44,016 ***** Eval results *****
2023-03-27 13:39:44,016   att_loss = 0.3891174128335534
2023-03-27 13:39:44,016   cls_loss = 0.0
2023-03-27 13:39:44,016   global_step = 1599
2023-03-27 13:39:44,016   loss = 1.012273676016114
2023-03-27 13:39:44,016   rep_loss = 0.6231562637469985
2023-03-27 13:39:44,024 ***** Save model *****
2023-03-27 13:39:55,306 ***** Running evaluation *****
2023-03-27 13:39:55,306   Epoch = 6 iter 1649 step
2023-03-27 13:39:55,306   Num examples = 1043
2023-03-27 13:39:55,306   Batch size = 32
2023-03-27 13:39:55,307 ***** Eval results *****
2023-03-27 13:39:55,308   att_loss = 0.37915646903058314
2023-03-27 13:39:55,308   cls_loss = 0.0
2023-03-27 13:39:55,308   global_step = 1649
2023-03-27 13:39:55,308   loss = 0.9920509089814856
2023-03-27 13:39:55,308   rep_loss = 0.6128944424872703
2023-03-27 13:39:55,316 ***** Save model *****
2023-03-27 13:40:06,592 ***** Running evaluation *****
2023-03-27 13:40:06,593   Epoch = 6 iter 1699 step
2023-03-27 13:40:06,593   Num examples = 1043
2023-03-27 13:40:06,593   Batch size = 32
2023-03-27 13:40:06,594 ***** Eval results *****
2023-03-27 13:40:06,595   att_loss = 0.38268627548955153
2023-03-27 13:40:06,595   cls_loss = 0.0
2023-03-27 13:40:06,595   global_step = 1699
2023-03-27 13:40:06,595   loss = 0.9963785729457423
2023-03-27 13:40:06,595   rep_loss = 0.6136922959199885
2023-03-27 13:40:06,597 ***** Save model *****
2023-03-27 13:40:17,855 ***** Running evaluation *****
2023-03-27 13:40:17,855   Epoch = 6 iter 1749 step
2023-03-27 13:40:17,855   Num examples = 1043
2023-03-27 13:40:17,855   Batch size = 32
2023-03-27 13:40:17,856 ***** Eval results *****
2023-03-27 13:40:17,857   att_loss = 0.3831973888841616
2023-03-27 13:40:17,857   cls_loss = 0.0
2023-03-27 13:40:17,857   global_step = 1749
2023-03-27 13:40:17,857   loss = 0.9965345859527588
2023-03-27 13:40:17,857   rep_loss = 0.6133371928111225
2023-03-27 13:40:17,865 ***** Save model *****
2023-03-27 13:40:29,173 ***** Running evaluation *****
2023-03-27 13:40:29,173   Epoch = 6 iter 1799 step
2023-03-27 13:40:29,173   Num examples = 1043
2023-03-27 13:40:29,173   Batch size = 32
2023-03-27 13:40:29,174 ***** Eval results *****
2023-03-27 13:40:29,175   att_loss = 0.3837708675014186
2023-03-27 13:40:29,175   cls_loss = 0.0
2023-03-27 13:40:29,175   global_step = 1799
2023-03-27 13:40:29,175   loss = 0.9967658495539942
2023-03-27 13:40:29,175   rep_loss = 0.6129949791782399
2023-03-27 13:40:29,182 ***** Save model *****
2023-03-27 13:40:40,494 ***** Running evaluation *****
2023-03-27 13:40:40,494   Epoch = 6 iter 1849 step
2023-03-27 13:40:40,494   Num examples = 1043
2023-03-27 13:40:40,494   Batch size = 32
2023-03-27 13:40:40,495 ***** Eval results *****
2023-03-27 13:40:40,496   att_loss = 0.3837572080403687
2023-03-27 13:40:40,496   cls_loss = 0.0
2023-03-27 13:40:40,496   global_step = 1849
2023-03-27 13:40:40,496   loss = 0.995730897916956
2023-03-27 13:40:40,496   rep_loss = 0.6119736879460724
2023-03-27 13:40:40,503 ***** Save model *****
2023-03-27 13:40:51,795 ***** Running evaluation *****
2023-03-27 13:40:51,796   Epoch = 7 iter 1899 step
2023-03-27 13:40:51,796   Num examples = 1043
2023-03-27 13:40:51,796   Batch size = 32
2023-03-27 13:40:51,798 ***** Eval results *****
2023-03-27 13:40:51,798   att_loss = 0.3680061727762222
2023-03-27 13:40:51,798   cls_loss = 0.0
2023-03-27 13:40:51,798   global_step = 1899
2023-03-27 13:40:51,799   loss = 0.9695807774861653
2023-03-27 13:40:51,799   rep_loss = 0.6015746057033539
2023-03-27 13:40:51,806 ***** Save model *****
2023-03-27 13:41:03,093 ***** Running evaluation *****
2023-03-27 13:41:03,093   Epoch = 7 iter 1949 step
2023-03-27 13:41:03,094   Num examples = 1043
2023-03-27 13:41:03,094   Batch size = 32
2023-03-27 13:41:03,095 ***** Eval results *****
2023-03-27 13:41:03,095   att_loss = 0.37605895958840846
2023-03-27 13:41:03,095   cls_loss = 0.0
2023-03-27 13:41:03,095   global_step = 1949
2023-03-27 13:41:03,095   loss = 0.9791359029710293
2023-03-27 13:41:03,095   rep_loss = 0.603076945245266
2023-03-27 13:41:03,098 ***** Save model *****
2023-03-27 13:41:14,399 ***** Running evaluation *****
2023-03-27 13:41:14,400   Epoch = 7 iter 1999 step
2023-03-27 13:41:14,400   Num examples = 1043
2023-03-27 13:41:14,400   Batch size = 32
2023-03-27 13:41:14,401 ***** Eval results *****
2023-03-27 13:41:14,402   att_loss = 0.3776113028709705
2023-03-27 13:41:14,402   cls_loss = 0.0
2023-03-27 13:41:14,402   global_step = 1999
2023-03-27 13:41:14,402   loss = 0.9810894507628221
2023-03-27 13:41:14,403   rep_loss = 0.6034781492673433
2023-03-27 13:41:14,410 ***** Save model *****
2023-03-27 13:41:25,741 ***** Running evaluation *****
2023-03-27 13:41:25,742   Epoch = 7 iter 2049 step
2023-03-27 13:41:25,742   Num examples = 1043
2023-03-27 13:41:25,742   Batch size = 32
2023-03-27 13:41:25,743 ***** Eval results *****
2023-03-27 13:41:25,743   att_loss = 0.37888545162147946
2023-03-27 13:41:25,743   cls_loss = 0.0
2023-03-27 13:41:25,743   global_step = 2049
2023-03-27 13:41:25,744   loss = 0.9825270805093977
2023-03-27 13:41:25,744   rep_loss = 0.6036416288879183
2023-03-27 13:41:25,751 ***** Save model *****
2023-03-27 13:41:37,065 ***** Running evaluation *****
2023-03-27 13:41:37,065   Epoch = 7 iter 2099 step
2023-03-27 13:41:37,065   Num examples = 1043
2023-03-27 13:41:37,065   Batch size = 32
2023-03-27 13:41:37,066 ***** Eval results *****
2023-03-27 13:41:37,066   att_loss = 0.37885430895763894
2023-03-27 13:41:37,066   cls_loss = 0.0
2023-03-27 13:41:37,067   global_step = 2099
2023-03-27 13:41:37,067   loss = 0.9826755585877792
2023-03-27 13:41:37,067   rep_loss = 0.6038212475569352
2023-03-27 13:41:37,074 ***** Save model *****
2023-03-27 13:41:48,381 ***** Running evaluation *****
2023-03-27 13:41:48,381   Epoch = 8 iter 2149 step
2023-03-27 13:41:48,381   Num examples = 1043
2023-03-27 13:41:48,381   Batch size = 32
2023-03-27 13:41:48,382 ***** Eval results *****
2023-03-27 13:41:48,382   att_loss = 0.37249561227284944
2023-03-27 13:41:48,382   cls_loss = 0.0
2023-03-27 13:41:48,383   global_step = 2149
2023-03-27 13:41:48,383   loss = 0.9767011495736929
2023-03-27 13:41:48,383   rep_loss = 0.6042055441783025
2023-03-27 13:41:48,390 ***** Save model *****
2023-03-27 13:41:59,682 ***** Running evaluation *****
2023-03-27 13:41:59,682   Epoch = 8 iter 2199 step
2023-03-27 13:41:59,682   Num examples = 1043
2023-03-27 13:41:59,682   Batch size = 32
2023-03-27 13:41:59,684 ***** Eval results *****
2023-03-27 13:41:59,684   att_loss = 0.3739604065342555
2023-03-27 13:41:59,685   cls_loss = 0.0
2023-03-27 13:41:59,685   global_step = 2199
2023-03-27 13:41:59,685   loss = 0.9735859604108901
2023-03-27 13:41:59,685   rep_loss = 0.5996255600263202
2023-03-27 13:41:59,693 ***** Save model *****
2023-03-27 13:42:10,986 ***** Running evaluation *****
2023-03-27 13:42:10,986   Epoch = 8 iter 2249 step
2023-03-27 13:42:10,986   Num examples = 1043
2023-03-27 13:42:10,986   Batch size = 32
2023-03-27 13:42:10,987 ***** Eval results *****
2023-03-27 13:42:10,988   att_loss = 0.37495181439197167
2023-03-27 13:42:10,988   cls_loss = 0.0
2023-03-27 13:42:10,988   global_step = 2249
2023-03-27 13:42:10,988   loss = 0.9739686002773521
2023-03-27 13:42:10,988   rep_loss = 0.5990167914238651
2023-03-27 13:42:10,996 ***** Save model *****
2023-03-27 13:42:22,317 ***** Running evaluation *****
2023-03-27 13:42:22,318   Epoch = 8 iter 2299 step
2023-03-27 13:42:22,318   Num examples = 1043
2023-03-27 13:42:22,318   Batch size = 32
2023-03-27 13:42:22,320 ***** Eval results *****
2023-03-27 13:42:22,320   att_loss = 0.37278810436008897
2023-03-27 13:42:22,320   cls_loss = 0.0
2023-03-27 13:42:22,320   global_step = 2299
2023-03-27 13:42:22,321   loss = 0.9706643439509386
2023-03-27 13:42:22,321   rep_loss = 0.5978762434304126
2023-03-27 13:42:22,328 ***** Save model *****
2023-03-27 13:42:33,589 ***** Running evaluation *****
2023-03-27 13:42:33,589   Epoch = 8 iter 2349 step
2023-03-27 13:42:33,589   Num examples = 1043
2023-03-27 13:42:33,589   Batch size = 32
2023-03-27 13:42:33,590 ***** Eval results *****
2023-03-27 13:42:33,590   att_loss = 0.37329492952342325
2023-03-27 13:42:33,591   cls_loss = 0.0
2023-03-27 13:42:33,591   global_step = 2349
2023-03-27 13:42:33,591   loss = 0.9703382127721545
2023-03-27 13:42:33,591   rep_loss = 0.5970432870264905
2023-03-27 13:42:33,598 ***** Save model *****
2023-03-27 13:42:44,812 ***** Running evaluation *****
2023-03-27 13:42:44,813   Epoch = 8 iter 2399 step
2023-03-27 13:42:44,813   Num examples = 1043
2023-03-27 13:42:44,813   Batch size = 32
2023-03-27 13:42:44,814 ***** Eval results *****
2023-03-27 13:42:44,814   att_loss = 0.3753601510941756
2023-03-27 13:42:44,814   cls_loss = 0.0
2023-03-27 13:42:44,815   global_step = 2399
2023-03-27 13:42:44,815   loss = 0.9723713835382642
2023-03-27 13:42:44,815   rep_loss = 0.5970112352770091
2023-03-27 13:42:44,822 ***** Save model *****
2023-03-27 13:42:56,076 ***** Running evaluation *****
2023-03-27 13:42:56,076   Epoch = 9 iter 2449 step
2023-03-27 13:42:56,076   Num examples = 1043
2023-03-27 13:42:56,077   Batch size = 32
2023-03-27 13:42:56,077 ***** Eval results *****
2023-03-27 13:42:56,077   att_loss = 0.37606995844322705
2023-03-27 13:42:56,078   cls_loss = 0.0
2023-03-27 13:42:56,078   global_step = 2449
2023-03-27 13:42:56,078   loss = 0.970309818568437
2023-03-27 13:42:56,078   rep_loss = 0.5942398607730865
2023-03-27 13:42:56,085 ***** Save model *****
2023-03-27 13:43:07,331 ***** Running evaluation *****
2023-03-27 13:43:07,331   Epoch = 9 iter 2499 step
2023-03-27 13:43:07,331   Num examples = 1043
2023-03-27 13:43:07,331   Batch size = 32
2023-03-27 13:43:07,333 ***** Eval results *****
2023-03-27 13:43:07,333   att_loss = 0.377351771419247
2023-03-27 13:43:07,333   cls_loss = 0.0
2023-03-27 13:43:07,333   global_step = 2499
2023-03-27 13:43:07,333   loss = 0.9706456822653612
2023-03-27 13:43:07,334   rep_loss = 0.5932939114669958
2023-03-27 13:43:07,340 ***** Save model *****
2023-03-27 13:43:18,584 ***** Running evaluation *****
2023-03-27 13:43:18,585   Epoch = 9 iter 2549 step
2023-03-27 13:43:18,585   Num examples = 1043
2023-03-27 13:43:18,585   Batch size = 32
2023-03-27 13:43:18,586 ***** Eval results *****
2023-03-27 13:43:18,586   att_loss = 0.37518155554386035
2023-03-27 13:43:18,587   cls_loss = 0.0
2023-03-27 13:43:18,587   global_step = 2549
2023-03-27 13:43:18,587   loss = 0.967600991872892
2023-03-27 13:43:18,587   rep_loss = 0.5924194369414081
2023-03-27 13:43:18,595 ***** Save model *****
2023-03-27 13:43:29,856 ***** Running evaluation *****
2023-03-27 13:43:29,856   Epoch = 9 iter 2599 step
2023-03-27 13:43:29,856   Num examples = 1043
2023-03-27 13:43:29,856   Batch size = 32
2023-03-27 13:43:29,858 ***** Eval results *****
2023-03-27 13:43:29,858   att_loss = 0.37535735827927685
2023-03-27 13:43:29,858   cls_loss = 0.0
2023-03-27 13:43:29,858   global_step = 2599
2023-03-27 13:43:29,858   loss = 0.9676433208645606
2023-03-27 13:43:29,858   rep_loss = 0.5922859615209152
2023-03-27 13:43:29,865 ***** Save model *****
2023-03-27 13:43:41,137 ***** Running evaluation *****
2023-03-27 13:43:41,138   Epoch = 9 iter 2649 step
2023-03-27 13:43:41,138   Num examples = 1043
2023-03-27 13:43:41,138   Batch size = 32
2023-03-27 13:43:41,139 ***** Eval results *****
2023-03-27 13:43:41,139   att_loss = 0.3735832095873065
2023-03-27 13:43:41,140   cls_loss = 0.0
2023-03-27 13:43:41,140   global_step = 2649
2023-03-27 13:43:41,140   loss = 0.9649801624984276
2023-03-27 13:43:41,140   rep_loss = 0.5913969506093157
2023-03-27 13:43:41,147 ***** Save model *****
2023-03-27 13:43:52,376 ***** Running evaluation *****
2023-03-27 13:43:52,377   Epoch = 10 iter 2699 step
2023-03-27 13:43:52,377   Num examples = 1043
2023-03-27 13:43:52,377   Batch size = 32
2023-03-27 13:43:52,378 ***** Eval results *****
2023-03-27 13:43:52,378   att_loss = 0.3778485018631508
2023-03-27 13:43:52,378   cls_loss = 0.0
2023-03-27 13:43:52,378   global_step = 2699
2023-03-27 13:43:52,378   loss = 0.9691038830526943
2023-03-27 13:43:52,379   rep_loss = 0.5912553709128807
2023-03-27 13:43:52,386 ***** Save model *****
2023-03-27 13:44:03,728 ***** Running evaluation *****
2023-03-27 13:44:03,729   Epoch = 10 iter 2749 step
2023-03-27 13:44:03,729   Num examples = 1043
2023-03-27 13:44:03,729   Batch size = 32
2023-03-27 13:44:03,730 ***** Eval results *****
2023-03-27 13:44:03,730   att_loss = 0.3741782605648041
2023-03-27 13:44:03,731   cls_loss = 0.0
2023-03-27 13:44:03,731   global_step = 2749
2023-03-27 13:44:03,731   loss = 0.9635008875327774
2023-03-27 13:44:03,731   rep_loss = 0.5893226243272612
2023-03-27 13:44:03,739 ***** Save model *****
2023-03-27 13:44:14,984 ***** Running evaluation *****
2023-03-27 13:44:14,984   Epoch = 10 iter 2799 step
2023-03-27 13:44:14,984   Num examples = 1043
2023-03-27 13:44:14,984   Batch size = 32
2023-03-27 13:44:14,986 ***** Eval results *****
2023-03-27 13:44:14,986   att_loss = 0.37086535816968874
2023-03-27 13:44:14,986   cls_loss = 0.0
2023-03-27 13:44:14,986   global_step = 2799
2023-03-27 13:44:14,987   loss = 0.9589860162069631
2023-03-27 13:44:14,987   rep_loss = 0.5881206568821457
2023-03-27 13:44:14,994 ***** Save model *****
2023-03-27 13:44:26,231 ***** Running evaluation *****
2023-03-27 13:44:26,231   Epoch = 10 iter 2849 step
2023-03-27 13:44:26,231   Num examples = 1043
2023-03-27 13:44:26,232   Batch size = 32
2023-03-27 13:44:26,233 ***** Eval results *****
2023-03-27 13:44:26,233   att_loss = 0.37269004746522316
2023-03-27 13:44:26,233   cls_loss = 0.0
2023-03-27 13:44:26,233   global_step = 2849
2023-03-27 13:44:26,234   loss = 0.9606597107215966
2023-03-27 13:44:26,234   rep_loss = 0.5879696627568932
2023-03-27 13:44:26,241 ***** Save model *****
2023-03-27 13:44:37,494 ***** Running evaluation *****
2023-03-27 13:44:37,495   Epoch = 10 iter 2899 step
2023-03-27 13:44:37,495   Num examples = 1043
2023-03-27 13:44:37,495   Batch size = 32
2023-03-27 13:44:37,497 ***** Eval results *****
2023-03-27 13:44:37,497   att_loss = 0.3725128065810974
2023-03-27 13:44:37,497   cls_loss = 0.0
2023-03-27 13:44:37,497   global_step = 2899
2023-03-27 13:44:37,497   loss = 0.9593727932226189
2023-03-27 13:44:37,498   rep_loss = 0.5868599872922272
2023-03-27 13:44:37,505 ***** Save model *****
2023-03-27 13:44:48,786 ***** Running evaluation *****
2023-03-27 13:44:48,786   Epoch = 11 iter 2949 step
2023-03-27 13:44:48,786   Num examples = 1043
2023-03-27 13:44:48,787   Batch size = 32
2023-03-27 13:44:48,788 ***** Eval results *****
2023-03-27 13:44:48,788   att_loss = 0.37356120596329373
2023-03-27 13:44:48,788   cls_loss = 0.0
2023-03-27 13:44:48,788   global_step = 2949
2023-03-27 13:44:48,789   loss = 0.9578048288822174
2023-03-27 13:44:48,789   rep_loss = 0.5842436204353968
2023-03-27 13:44:48,796 ***** Save model *****
2023-03-27 13:45:00,054 ***** Running evaluation *****
2023-03-27 13:45:00,055   Epoch = 11 iter 2999 step
2023-03-27 13:45:00,055   Num examples = 1043
2023-03-27 13:45:00,055   Batch size = 32
2023-03-27 13:45:00,056 ***** Eval results *****
2023-03-27 13:45:00,056   att_loss = 0.3657850844244803
2023-03-27 13:45:00,056   cls_loss = 0.0
2023-03-27 13:45:00,057   global_step = 2999
2023-03-27 13:45:00,057   loss = 0.9461709030212895
2023-03-27 13:45:00,057   rep_loss = 0.5803858195581744
2023-03-27 13:45:00,064 ***** Save model *****
2023-03-27 13:45:11,311 ***** Running evaluation *****
2023-03-27 13:45:11,311   Epoch = 11 iter 3049 step
2023-03-27 13:45:11,311   Num examples = 1043
2023-03-27 13:45:11,311   Batch size = 32
2023-03-27 13:45:11,312 ***** Eval results *****
2023-03-27 13:45:11,313   att_loss = 0.3677858928484576
2023-03-27 13:45:11,313   cls_loss = 0.0
2023-03-27 13:45:11,313   global_step = 3049
2023-03-27 13:45:11,313   loss = 0.9492979012429714
2023-03-27 13:45:11,313   rep_loss = 0.5815120099910668
2023-03-27 13:45:11,321 ***** Save model *****
2023-03-27 13:45:22,584 ***** Running evaluation *****
2023-03-27 13:45:22,584   Epoch = 11 iter 3099 step
2023-03-27 13:45:22,584   Num examples = 1043
2023-03-27 13:45:22,584   Batch size = 32
2023-03-27 13:45:22,586 ***** Eval results *****
2023-03-27 13:45:22,586   att_loss = 0.3670987071078501
2023-03-27 13:45:22,586   cls_loss = 0.0
2023-03-27 13:45:22,586   global_step = 3099
2023-03-27 13:45:22,586   loss = 0.9486219058802099
2023-03-27 13:45:22,586   rep_loss = 0.5815231987723598
2023-03-27 13:45:22,593 ***** Save model *****
2023-03-27 13:45:33,837 ***** Running evaluation *****
2023-03-27 13:45:33,837   Epoch = 11 iter 3149 step
2023-03-27 13:45:33,838   Num examples = 1043
2023-03-27 13:45:33,838   Batch size = 32
2023-03-27 13:45:33,839 ***** Eval results *****
2023-03-27 13:45:33,839   att_loss = 0.3682414715020162
2023-03-27 13:45:33,839   cls_loss = 0.0
2023-03-27 13:45:33,839   global_step = 3149
2023-03-27 13:45:33,840   loss = 0.9490287838117132
2023-03-27 13:45:33,840   rep_loss = 0.5807873125908509
2023-03-27 13:45:33,847 ***** Save model *****
2023-03-27 13:45:45,103 ***** Running evaluation *****
2023-03-27 13:45:45,103   Epoch = 11 iter 3199 step
2023-03-27 13:45:45,103   Num examples = 1043
2023-03-27 13:45:45,103   Batch size = 32
2023-03-27 13:45:45,105 ***** Eval results *****
2023-03-27 13:45:45,105   att_loss = 0.3688863188255834
2023-03-27 13:45:45,105   cls_loss = 0.0
2023-03-27 13:45:45,105   global_step = 3199
2023-03-27 13:45:45,106   loss = 0.9498974917499163
2023-03-27 13:45:45,106   rep_loss = 0.5810111726968343
2023-03-27 13:45:45,113 ***** Save model *****
2023-03-27 13:45:56,385 ***** Running evaluation *****
2023-03-27 13:45:56,386   Epoch = 12 iter 3249 step
2023-03-27 13:45:56,386   Num examples = 1043
2023-03-27 13:45:56,386   Batch size = 32
2023-03-27 13:45:56,387 ***** Eval results *****
2023-03-27 13:45:56,388   att_loss = 0.36401362816492716
2023-03-27 13:45:56,388   cls_loss = 0.0
2023-03-27 13:45:56,388   global_step = 3249
2023-03-27 13:45:56,388   loss = 0.9390530043178135
2023-03-27 13:45:56,388   rep_loss = 0.5750393761528863
2023-03-27 13:45:56,395 ***** Save model *****
2023-03-27 13:46:07,672 ***** Running evaluation *****
2023-03-27 13:46:07,672   Epoch = 12 iter 3299 step
2023-03-27 13:46:07,672   Num examples = 1043
2023-03-27 13:46:07,673   Batch size = 32
2023-03-27 13:46:07,674 ***** Eval results *****
2023-03-27 13:46:07,674   att_loss = 0.36827822735435084
2023-03-27 13:46:07,674   cls_loss = 0.0
2023-03-27 13:46:07,675   global_step = 3299
2023-03-27 13:46:07,675   loss = 0.9449168286825481
2023-03-27 13:46:07,675   rep_loss = 0.5766386032104492
2023-03-27 13:46:07,682 ***** Save model *****
2023-03-27 13:46:18,948 ***** Running evaluation *****
2023-03-27 13:46:18,949   Epoch = 12 iter 3349 step
2023-03-27 13:46:18,949   Num examples = 1043
2023-03-27 13:46:18,949   Batch size = 32
2023-03-27 13:46:18,950 ***** Eval results *****
2023-03-27 13:46:18,950   att_loss = 0.3675343817677991
2023-03-27 13:46:18,950   cls_loss = 0.0
2023-03-27 13:46:18,950   global_step = 3349
2023-03-27 13:46:18,951   loss = 0.9441294793424935
2023-03-27 13:46:18,951   rep_loss = 0.57659510045216
2023-03-27 13:46:18,958 ***** Save model *****
2023-03-27 13:46:30,193 ***** Running evaluation *****
2023-03-27 13:46:30,193   Epoch = 12 iter 3399 step
2023-03-27 13:46:30,194   Num examples = 1043
2023-03-27 13:46:30,194   Batch size = 32
2023-03-27 13:46:30,195 ***** Eval results *****
2023-03-27 13:46:30,196   att_loss = 0.3665121289399954
2023-03-27 13:46:30,196   cls_loss = 0.0
2023-03-27 13:46:30,196   global_step = 3399
2023-03-27 13:46:30,196   loss = 0.9435237578856639
2023-03-27 13:46:30,196   rep_loss = 0.5770116310853225
2023-03-27 13:46:30,203 ***** Save model *****
2023-03-27 13:46:41,440 ***** Running evaluation *****
2023-03-27 13:46:41,440   Epoch = 12 iter 3449 step
2023-03-27 13:46:41,440   Num examples = 1043
2023-03-27 13:46:41,441   Batch size = 32
2023-03-27 13:46:41,442 ***** Eval results *****
2023-03-27 13:46:41,442   att_loss = 0.36674884423917653
2023-03-27 13:46:41,442   cls_loss = 0.0
2023-03-27 13:46:41,442   global_step = 3449
2023-03-27 13:46:41,443   loss = 0.9436628682272775
2023-03-27 13:46:41,443   rep_loss = 0.5769140245963116
2023-03-27 13:46:41,450 ***** Save model *****
2023-03-27 13:46:52,693 ***** Running evaluation *****
2023-03-27 13:46:52,694   Epoch = 13 iter 3499 step
2023-03-27 13:46:52,694   Num examples = 1043
2023-03-27 13:46:52,694   Batch size = 32
2023-03-27 13:46:52,695 ***** Eval results *****
2023-03-27 13:46:52,695   att_loss = 0.3755521146314485
2023-03-27 13:46:52,696   cls_loss = 0.0
2023-03-27 13:46:52,696   global_step = 3499
2023-03-27 13:46:52,696   loss = 0.9527277094977242
2023-03-27 13:46:52,696   rep_loss = 0.5771755959306445
2023-03-27 13:46:52,703 ***** Save model *****
2023-03-27 13:47:03,972 ***** Running evaluation *****
2023-03-27 13:47:03,973   Epoch = 13 iter 3549 step
2023-03-27 13:47:03,973   Num examples = 1043
2023-03-27 13:47:03,973   Batch size = 32
2023-03-27 13:47:03,974 ***** Eval results *****
2023-03-27 13:47:03,975   att_loss = 0.3685899510597571
2023-03-27 13:47:03,975   cls_loss = 0.0
2023-03-27 13:47:03,975   global_step = 3549
2023-03-27 13:47:03,975   loss = 0.9436863699020483
2023-03-27 13:47:03,975   rep_loss = 0.5750964184602102
2023-03-27 13:47:03,982 ***** Save model *****
2023-03-27 13:47:15,246 ***** Running evaluation *****
2023-03-27 13:47:15,247   Epoch = 13 iter 3599 step
2023-03-27 13:47:15,247   Num examples = 1043
2023-03-27 13:47:15,247   Batch size = 32
2023-03-27 13:47:15,248 ***** Eval results *****
2023-03-27 13:47:15,248   att_loss = 0.36805658671073616
2023-03-27 13:47:15,248   cls_loss = 0.0
2023-03-27 13:47:15,248   global_step = 3599
2023-03-27 13:47:15,248   loss = 0.9426065948791802
2023-03-27 13:47:15,248   rep_loss = 0.5745500093325973
2023-03-27 13:47:15,255 ***** Save model *****
2023-03-27 13:47:26,534 ***** Running evaluation *****
2023-03-27 13:47:26,535   Epoch = 13 iter 3649 step
2023-03-27 13:47:26,535   Num examples = 1043
2023-03-27 13:47:26,535   Batch size = 32
2023-03-27 13:47:26,536 ***** Eval results *****
2023-03-27 13:47:26,536   att_loss = 0.36668392245689135
2023-03-27 13:47:26,537   cls_loss = 0.0
2023-03-27 13:47:26,537   global_step = 3649
2023-03-27 13:47:26,537   loss = 0.9407992028118519
2023-03-27 13:47:26,537   rep_loss = 0.5741152813595333
2023-03-27 13:47:26,544 ***** Save model *****
2023-03-27 13:47:37,763 ***** Running evaluation *****
2023-03-27 13:47:37,764   Epoch = 13 iter 3699 step
2023-03-27 13:47:37,764   Num examples = 1043
2023-03-27 13:47:37,764   Batch size = 32
2023-03-27 13:47:37,766 ***** Eval results *****
2023-03-27 13:47:37,766   att_loss = 0.36648582902393845
2023-03-27 13:47:37,766   cls_loss = 0.0
2023-03-27 13:47:37,766   global_step = 3699
2023-03-27 13:47:37,767   loss = 0.9406257520119349
2023-03-27 13:47:37,767   rep_loss = 0.5741399236415562
2023-03-27 13:47:37,774 ***** Save model *****
2023-03-27 13:47:49,045 ***** Running evaluation *****
2023-03-27 13:47:49,045   Epoch = 14 iter 3749 step
2023-03-27 13:47:49,045   Num examples = 1043
2023-03-27 13:47:49,045   Batch size = 32
2023-03-27 13:47:49,046 ***** Eval results *****
2023-03-27 13:47:49,046   att_loss = 0.3633784218267961
2023-03-27 13:47:49,046   cls_loss = 0.0
2023-03-27 13:47:49,047   global_step = 3749
2023-03-27 13:47:49,047   loss = 0.9308598854325034
2023-03-27 13:47:49,047   rep_loss = 0.5674814527684992
2023-03-27 13:47:49,054 ***** Save model *****
2023-03-27 13:48:00,330 ***** Running evaluation *****
2023-03-27 13:48:00,331   Epoch = 14 iter 3799 step
2023-03-27 13:48:00,331   Num examples = 1043
2023-03-27 13:48:00,331   Batch size = 32
2023-03-27 13:48:00,332 ***** Eval results *****
2023-03-27 13:48:00,333   att_loss = 0.361314805804706
2023-03-27 13:48:00,333   cls_loss = 0.0
2023-03-27 13:48:00,333   global_step = 3799
2023-03-27 13:48:00,333   loss = 0.9305757938838396
2023-03-27 13:48:00,334   rep_loss = 0.569260986124883
2023-03-27 13:48:00,341 ***** Save model *****
2023-03-27 13:48:11,634 ***** Running evaluation *****
2023-03-27 13:48:11,634   Epoch = 14 iter 3849 step
2023-03-27 13:48:11,634   Num examples = 1043
2023-03-27 13:48:11,634   Batch size = 32
2023-03-27 13:48:11,635 ***** Eval results *****
2023-03-27 13:48:11,636   att_loss = 0.35830378935143753
2023-03-27 13:48:11,636   cls_loss = 0.0
2023-03-27 13:48:11,636   global_step = 3849
2023-03-27 13:48:11,636   loss = 0.9264817753353635
2023-03-27 13:48:11,636   rep_loss = 0.5681779841045002
2023-03-27 13:48:11,644 ***** Save model *****
2023-03-27 13:48:22,874 ***** Running evaluation *****
2023-03-27 13:48:22,874   Epoch = 14 iter 3899 step
2023-03-27 13:48:22,874   Num examples = 1043
2023-03-27 13:48:22,875   Batch size = 32
2023-03-27 13:48:22,875 ***** Eval results *****
2023-03-27 13:48:22,876   att_loss = 0.3631305705686534
2023-03-27 13:48:22,876   cls_loss = 0.0
2023-03-27 13:48:22,876   global_step = 3899
2023-03-27 13:48:22,876   loss = 0.9324258373390814
2023-03-27 13:48:22,876   rep_loss = 0.5692952652895673
2023-03-27 13:48:22,886 ***** Save model *****
2023-03-27 13:48:34,123 ***** Running evaluation *****
2023-03-27 13:48:34,123   Epoch = 14 iter 3949 step
2023-03-27 13:48:34,124   Num examples = 1043
2023-03-27 13:48:34,124   Batch size = 32
2023-03-27 13:48:34,124 ***** Eval results *****
2023-03-27 13:48:34,125   att_loss = 0.3654983689152234
2023-03-27 13:48:34,125   cls_loss = 0.0
2023-03-27 13:48:34,125   global_step = 3949
2023-03-27 13:48:34,125   loss = 0.936089210883136
2023-03-27 13:48:34,125   rep_loss = 0.57059084097921
2023-03-27 13:48:34,127 ***** Save model *****
2023-03-27 13:48:45,398 ***** Running evaluation *****
2023-03-27 13:48:45,399   Epoch = 14 iter 3999 step
2023-03-27 13:48:45,399   Num examples = 1043
2023-03-27 13:48:45,399   Batch size = 32
2023-03-27 13:48:45,400 ***** Eval results *****
2023-03-27 13:48:45,401   att_loss = 0.3648476863272802
2023-03-27 13:48:45,401   cls_loss = 0.0
2023-03-27 13:48:45,401   global_step = 3999
2023-03-27 13:48:45,401   loss = 0.934935813205909
2023-03-27 13:48:45,401   rep_loss = 0.5700881257367774
2023-03-27 13:48:45,403 ***** Save model *****
2023-03-27 13:48:56,684 ***** Running evaluation *****
2023-03-27 13:48:56,685   Epoch = 15 iter 4049 step
2023-03-27 13:48:56,685   Num examples = 1043
2023-03-27 13:48:56,685   Batch size = 32
2023-03-27 13:48:56,686 ***** Eval results *****
2023-03-27 13:48:56,686   att_loss = 0.35664178363301535
2023-03-27 13:48:56,687   cls_loss = 0.0
2023-03-27 13:48:56,687   global_step = 4049
2023-03-27 13:48:56,687   loss = 0.9225755794481798
2023-03-27 13:48:56,687   rep_loss = 0.5659337964924899
2023-03-27 13:48:56,694 ***** Save model *****
2023-03-27 13:49:07,970 ***** Running evaluation *****
2023-03-27 13:49:07,974   Epoch = 15 iter 4099 step
2023-03-27 13:49:07,974   Num examples = 1043
2023-03-27 13:49:07,974   Batch size = 32
2023-03-27 13:49:07,977 ***** Eval results *****
2023-03-27 13:49:07,977   att_loss = 0.35876276994005163
2023-03-27 13:49:07,977   cls_loss = 0.0
2023-03-27 13:49:07,977   global_step = 4099
2023-03-27 13:49:07,978   loss = 0.9252404944693788
2023-03-27 13:49:07,978   rep_loss = 0.5664777248463733
2023-03-27 13:49:07,980 ***** Save model *****
2023-03-27 13:49:19,232 ***** Running evaluation *****
2023-03-27 13:49:19,232   Epoch = 15 iter 4149 step
2023-03-27 13:49:19,232   Num examples = 1043
2023-03-27 13:49:19,232   Batch size = 32
2023-03-27 13:49:19,233 ***** Eval results *****
2023-03-27 13:49:19,234   att_loss = 0.3592638565848271
2023-03-27 13:49:19,234   cls_loss = 0.0
2023-03-27 13:49:19,234   global_step = 4149
2023-03-27 13:49:19,234   loss = 0.9258616541822752
2023-03-27 13:49:19,234   rep_loss = 0.5665977973904874
2023-03-27 13:49:19,236 ***** Save model *****
2023-03-27 13:49:30,468 ***** Running evaluation *****
2023-03-27 13:49:30,468   Epoch = 15 iter 4199 step
2023-03-27 13:49:30,469   Num examples = 1043
2023-03-27 13:49:30,469   Batch size = 32
2023-03-27 13:49:30,469 ***** Eval results *****
2023-03-27 13:49:30,470   att_loss = 0.3603966518775704
2023-03-27 13:49:30,470   cls_loss = 0.0
2023-03-27 13:49:30,470   global_step = 4199
2023-03-27 13:49:30,470   loss = 0.927643975338985
2023-03-27 13:49:30,470   rep_loss = 0.5672473225396933
2023-03-27 13:49:30,477 ***** Save model *****
2023-03-27 13:49:41,731 ***** Running evaluation *****
2023-03-27 13:49:41,731   Epoch = 15 iter 4249 step
2023-03-27 13:49:41,731   Num examples = 1043
2023-03-27 13:49:41,731   Batch size = 32
2023-03-27 13:49:41,732 ***** Eval results *****
2023-03-27 13:49:41,733   att_loss = 0.36146229884175
2023-03-27 13:49:41,733   cls_loss = 0.0
2023-03-27 13:49:41,733   global_step = 4249
2023-03-27 13:49:41,733   loss = 0.9286112028067229
2023-03-27 13:49:41,733   rep_loss = 0.5671489033542696
2023-03-27 13:49:41,740 ***** Save model *****
2023-03-27 13:49:53,020 ***** Running evaluation *****
2023-03-27 13:49:53,020   Epoch = 16 iter 4299 step
2023-03-27 13:49:53,020   Num examples = 1043
2023-03-27 13:49:53,020   Batch size = 32
2023-03-27 13:49:53,022 ***** Eval results *****
2023-03-27 13:49:53,022   att_loss = 0.35486351008768435
2023-03-27 13:49:53,023   cls_loss = 0.0
2023-03-27 13:49:53,023   global_step = 4299
2023-03-27 13:49:53,023   loss = 0.9176156587070889
2023-03-27 13:49:53,023   rep_loss = 0.5627521497231943
2023-03-27 13:49:53,030 ***** Save model *****
2023-03-27 13:50:04,297 ***** Running evaluation *****
2023-03-27 13:50:04,297   Epoch = 16 iter 4349 step
2023-03-27 13:50:04,297   Num examples = 1043
2023-03-27 13:50:04,297   Batch size = 32
2023-03-27 13:50:04,298 ***** Eval results *****
2023-03-27 13:50:04,298   att_loss = 0.35528324718599197
2023-03-27 13:50:04,298   cls_loss = 0.0
2023-03-27 13:50:04,299   global_step = 4349
2023-03-27 13:50:04,299   loss = 0.9197121874078528
2023-03-27 13:50:04,299   rep_loss = 0.5644289394477745
2023-03-27 13:50:04,301 ***** Save model *****
2023-03-27 13:50:15,579 ***** Running evaluation *****
2023-03-27 13:50:15,579   Epoch = 16 iter 4399 step
2023-03-27 13:50:15,580   Num examples = 1043
2023-03-27 13:50:15,580   Batch size = 32
2023-03-27 13:50:15,581 ***** Eval results *****
2023-03-27 13:50:15,581   att_loss = 0.3597015470970334
2023-03-27 13:50:15,582   cls_loss = 0.0
2023-03-27 13:50:15,582   global_step = 4399
2023-03-27 13:50:15,582   loss = 0.9241072460422366
2023-03-27 13:50:15,582   rep_loss = 0.5644056970678916
2023-03-27 13:50:15,584 ***** Save model *****
2023-03-27 13:50:26,812 ***** Running evaluation *****
2023-03-27 13:50:26,812   Epoch = 16 iter 4449 step
2023-03-27 13:50:26,812   Num examples = 1043
2023-03-27 13:50:26,812   Batch size = 32
2023-03-27 13:50:26,814 ***** Eval results *****
2023-03-27 13:50:26,814   att_loss = 0.36183081255794247
2023-03-27 13:50:26,814   cls_loss = 0.0
2023-03-27 13:50:26,814   global_step = 4449
2023-03-27 13:50:26,815   loss = 0.9269797970346139
2023-03-27 13:50:26,815   rep_loss = 0.5651489826245496
2023-03-27 13:50:26,822 ***** Save model *****
2023-03-27 13:50:38,088 ***** Running evaluation *****
2023-03-27 13:50:38,088   Epoch = 16 iter 4499 step
2023-03-27 13:50:38,088   Num examples = 1043
2023-03-27 13:50:38,089   Batch size = 32
2023-03-27 13:50:38,090 ***** Eval results *****
2023-03-27 13:50:38,090   att_loss = 0.3611216550356491
2023-03-27 13:50:38,090   cls_loss = 0.0
2023-03-27 13:50:38,090   global_step = 4499
2023-03-27 13:50:38,090   loss = 0.9258502708657723
2023-03-27 13:50:38,090   rep_loss = 0.5647286139920945
2023-03-27 13:50:38,097 ***** Save model *****
2023-03-27 13:50:49,380 ***** Running evaluation *****
2023-03-27 13:50:49,380   Epoch = 17 iter 4549 step
2023-03-27 13:50:49,380   Num examples = 1043
2023-03-27 13:50:49,381   Batch size = 32
2023-03-27 13:50:49,381 ***** Eval results *****
2023-03-27 13:50:49,382   att_loss = 0.3545015811920166
2023-03-27 13:50:49,382   cls_loss = 0.0
2023-03-27 13:50:49,382   global_step = 4549
2023-03-27 13:50:49,382   loss = 0.9135738670825958
2023-03-27 13:50:49,382   rep_loss = 0.5590722799301148
2023-03-27 13:50:49,390 ***** Save model *****
2023-03-27 13:51:00,642 ***** Running evaluation *****
2023-03-27 13:51:00,642   Epoch = 17 iter 4599 step
2023-03-27 13:51:00,642   Num examples = 1043
2023-03-27 13:51:00,642   Batch size = 32
2023-03-27 13:51:00,644 ***** Eval results *****
2023-03-27 13:51:00,644   att_loss = 0.3561781495809555
2023-03-27 13:51:00,644   cls_loss = 0.0
2023-03-27 13:51:00,644   global_step = 4599
2023-03-27 13:51:00,644   loss = 0.9163094222545624
2023-03-27 13:51:00,644   rep_loss = 0.5601312726736069
2023-03-27 13:51:00,652 ***** Save model *****
2023-03-27 13:51:11,893 ***** Running evaluation *****
2023-03-27 13:51:11,894   Epoch = 17 iter 4649 step
2023-03-27 13:51:11,894   Num examples = 1043
2023-03-27 13:51:11,894   Batch size = 32
2023-03-27 13:51:11,895 ***** Eval results *****
2023-03-27 13:51:11,895   att_loss = 0.3577201648191972
2023-03-27 13:51:11,895   cls_loss = 0.0
2023-03-27 13:51:11,896   global_step = 4649
2023-03-27 13:51:11,896   loss = 0.9184086160226301
2023-03-27 13:51:11,896   rep_loss = 0.5606884517452934
2023-03-27 13:51:11,904 ***** Save model *****
2023-03-27 13:51:23,139 ***** Running evaluation *****
2023-03-27 13:51:23,140   Epoch = 17 iter 4699 step
2023-03-27 13:51:23,140   Num examples = 1043
2023-03-27 13:51:23,140   Batch size = 32
2023-03-27 13:51:23,141 ***** Eval results *****
2023-03-27 13:51:23,141   att_loss = 0.35805344898253677
2023-03-27 13:51:23,142   cls_loss = 0.0
2023-03-27 13:51:23,142   global_step = 4699
2023-03-27 13:51:23,142   loss = 0.9187954448163509
2023-03-27 13:51:23,142   rep_loss = 0.5607419952750206
2023-03-27 13:51:23,150 ***** Save model *****
2023-03-27 13:51:34,389 ***** Running evaluation *****
2023-03-27 13:51:34,389   Epoch = 17 iter 4749 step
2023-03-27 13:51:34,389   Num examples = 1043
2023-03-27 13:51:34,389   Batch size = 32
2023-03-27 13:51:34,390 ***** Eval results *****
2023-03-27 13:51:34,390   att_loss = 0.3596189423685982
2023-03-27 13:51:34,390   cls_loss = 0.0
2023-03-27 13:51:34,391   global_step = 4749
2023-03-27 13:51:34,391   loss = 0.9208216635953812
2023-03-27 13:51:34,391   rep_loss = 0.5612027210848672
2023-03-27 13:51:34,398 ***** Save model *****
2023-03-27 13:51:45,645 ***** Running evaluation *****
2023-03-27 13:51:45,645   Epoch = 17 iter 4799 step
2023-03-27 13:51:45,645   Num examples = 1043
2023-03-27 13:51:45,645   Batch size = 32
2023-03-27 13:51:45,646 ***** Eval results *****
2023-03-27 13:51:45,646   att_loss = 0.35991380902437065
2023-03-27 13:51:45,647   cls_loss = 0.0
2023-03-27 13:51:45,647   global_step = 4799
2023-03-27 13:51:45,647   loss = 0.921135202050209
2023-03-27 13:51:45,647   rep_loss = 0.5612213925673412
2023-03-27 13:51:45,649 ***** Save model *****
2023-03-27 13:51:56,909 ***** Running evaluation *****
2023-03-27 13:51:56,909   Epoch = 18 iter 4849 step
2023-03-27 13:51:56,909   Num examples = 1043
2023-03-27 13:51:56,909   Batch size = 32
2023-03-27 13:51:56,910 ***** Eval results *****
2023-03-27 13:51:56,911   att_loss = 0.3606119744999464
2023-03-27 13:51:56,911   cls_loss = 0.0
2023-03-27 13:51:56,911   global_step = 4849
2023-03-27 13:51:56,911   loss = 0.9198890952176826
2023-03-27 13:51:56,911   rep_loss = 0.5592771158661953
2023-03-27 13:51:56,918 ***** Save model *****
2023-03-27 13:52:08,171 ***** Running evaluation *****
2023-03-27 13:52:08,171   Epoch = 18 iter 4899 step
2023-03-27 13:52:08,171   Num examples = 1043
2023-03-27 13:52:08,171   Batch size = 32
2023-03-27 13:52:08,173 ***** Eval results *****
2023-03-27 13:52:08,173   att_loss = 0.3603455571718113
2023-03-27 13:52:08,173   cls_loss = 0.0
2023-03-27 13:52:08,173   global_step = 4899
2023-03-27 13:52:08,173   loss = 0.9191303362128556
2023-03-27 13:52:08,173   rep_loss = 0.5587847777592239
2023-03-27 13:52:08,176 ***** Save model *****
2023-03-27 13:52:19,385 ***** Running evaluation *****
2023-03-27 13:52:19,385   Epoch = 18 iter 4949 step
2023-03-27 13:52:19,386   Num examples = 1043
2023-03-27 13:52:19,386   Batch size = 32
2023-03-27 13:52:19,387 ***** Eval results *****
2023-03-27 13:52:19,387   att_loss = 0.3577866810601908
2023-03-27 13:52:19,387   cls_loss = 0.0
2023-03-27 13:52:19,387   global_step = 4949
2023-03-27 13:52:19,387   loss = 0.9164324377800201
2023-03-27 13:52:19,387   rep_loss = 0.5586457565114215
2023-03-27 13:52:19,394 ***** Save model *****
2023-03-27 13:52:30,666 ***** Running evaluation *****
2023-03-27 13:52:30,667   Epoch = 18 iter 4999 step
2023-03-27 13:52:30,667   Num examples = 1043
2023-03-27 13:52:30,667   Batch size = 32
2023-03-27 13:52:30,668 ***** Eval results *****
2023-03-27 13:52:30,668   att_loss = 0.3584821424953678
2023-03-27 13:52:30,668   cls_loss = 0.0
2023-03-27 13:52:30,668   global_step = 4999
2023-03-27 13:52:30,669   loss = 0.91769359179729
2023-03-27 13:52:30,669   rep_loss = 0.5592114499195869
2023-03-27 13:52:30,671 ***** Save model *****
2023-03-27 13:52:41,923 ***** Running evaluation *****
2023-03-27 13:52:41,924   Epoch = 18 iter 5049 step
2023-03-27 13:52:41,924   Num examples = 1043
2023-03-27 13:52:41,924   Batch size = 32
2023-03-27 13:52:41,925 ***** Eval results *****
2023-03-27 13:52:41,925   att_loss = 0.35883482297261554
2023-03-27 13:52:41,925   cls_loss = 0.0
2023-03-27 13:52:41,925   global_step = 5049
2023-03-27 13:52:41,925   loss = 0.918224887838089
2023-03-27 13:52:41,925   rep_loss = 0.55939006511076
2023-03-27 13:52:41,927 ***** Save model *****
2023-03-27 13:52:53,170 ***** Running evaluation *****
2023-03-27 13:52:53,170   Epoch = 19 iter 5099 step
2023-03-27 13:52:53,170   Num examples = 1043
2023-03-27 13:52:53,171   Batch size = 32
2023-03-27 13:52:53,171 ***** Eval results *****
2023-03-27 13:52:53,172   att_loss = 0.3549548742862848
2023-03-27 13:52:53,172   cls_loss = 0.0
2023-03-27 13:52:53,172   global_step = 5099
2023-03-27 13:52:53,172   loss = 0.9101639137818263
2023-03-27 13:52:53,172   rep_loss = 0.555209036056812
2023-03-27 13:52:53,178 ***** Save model *****
2023-03-27 13:53:04,453 ***** Running evaluation *****
2023-03-27 13:53:04,453   Epoch = 19 iter 5149 step
2023-03-27 13:53:04,453   Num examples = 1043
2023-03-27 13:53:04,453   Batch size = 32
2023-03-27 13:53:04,454 ***** Eval results *****
2023-03-27 13:53:04,454   att_loss = 0.3505826604209448
2023-03-27 13:53:04,454   cls_loss = 0.0
2023-03-27 13:53:04,455   global_step = 5149
2023-03-27 13:53:04,455   loss = 0.9039634732823623
2023-03-27 13:53:04,455   rep_loss = 0.5533808140378249
2023-03-27 13:53:04,457 ***** Save model *****
2023-03-27 13:53:15,693 ***** Running evaluation *****
2023-03-27 13:53:15,693   Epoch = 19 iter 5199 step
2023-03-27 13:53:15,693   Num examples = 1043
2023-03-27 13:53:15,693   Batch size = 32
2023-03-27 13:53:15,694 ***** Eval results *****
2023-03-27 13:53:15,695   att_loss = 0.3531617914873456
2023-03-27 13:53:15,695   cls_loss = 0.0
2023-03-27 13:53:15,695   global_step = 5199
2023-03-27 13:53:15,695   loss = 0.9071955879529318
2023-03-27 13:53:15,695   rep_loss = 0.5540337974116916
2023-03-27 13:53:15,701 ***** Save model *****
2023-03-27 13:53:26,933 ***** Running evaluation *****
2023-03-27 13:53:26,933   Epoch = 19 iter 5249 step
2023-03-27 13:53:26,933   Num examples = 1043
2023-03-27 13:53:26,933   Batch size = 32
2023-03-27 13:53:26,934 ***** Eval results *****
2023-03-27 13:53:26,935   att_loss = 0.35502941127527843
2023-03-27 13:53:26,935   cls_loss = 0.0
2023-03-27 13:53:26,935   global_step = 5249
2023-03-27 13:53:26,935   loss = 0.9098794375630942
2023-03-27 13:53:26,935   rep_loss = 0.554850025949153
2023-03-27 13:53:26,937 ***** Save model *****
2023-03-27 13:53:38,181 ***** Running evaluation *****
2023-03-27 13:53:38,182   Epoch = 19 iter 5299 step
2023-03-27 13:53:38,182   Num examples = 1043
2023-03-27 13:53:38,182   Batch size = 32
2023-03-27 13:53:38,183 ***** Eval results *****
2023-03-27 13:53:38,184   att_loss = 0.3566045816493245
2023-03-27 13:53:38,184   cls_loss = 0.0
2023-03-27 13:53:38,184   global_step = 5299
2023-03-27 13:53:38,184   loss = 0.912560475874791
2023-03-27 13:53:38,184   rep_loss = 0.5559558936979918
2023-03-27 13:53:38,191 ***** Save model *****
2023-03-27 13:53:49,460 ***** Running evaluation *****
2023-03-27 13:53:49,460   Epoch = 20 iter 5349 step
2023-03-27 13:53:49,460   Num examples = 1043
2023-03-27 13:53:49,461   Batch size = 32
2023-03-27 13:53:49,461 ***** Eval results *****
2023-03-27 13:53:49,462   att_loss = 0.35213108857472736
2023-03-27 13:53:49,462   cls_loss = 0.0
2023-03-27 13:53:49,462   global_step = 5349
2023-03-27 13:53:49,462   loss = 0.9054079982969496
2023-03-27 13:53:49,462   rep_loss = 0.5532769097222222
2023-03-27 13:53:49,469 ***** Save model *****
2023-03-27 13:54:00,741 ***** Running evaluation *****
2023-03-27 13:54:00,741   Epoch = 20 iter 5399 step
2023-03-27 13:54:00,742   Num examples = 1043
2023-03-27 13:54:00,742   Batch size = 32
2023-03-27 13:54:00,743 ***** Eval results *****
2023-03-27 13:54:00,743   att_loss = 0.35217027593467193
2023-03-27 13:54:00,743   cls_loss = 0.0
2023-03-27 13:54:00,743   global_step = 5399
2023-03-27 13:54:00,743   loss = 0.9058200282565618
2023-03-27 13:54:00,743   rep_loss = 0.5536497508065176
2023-03-27 13:54:00,750 ***** Save model *****
2023-03-27 13:54:11,957 ***** Running evaluation *****
2023-03-27 13:54:11,957   Epoch = 20 iter 5449 step
2023-03-27 13:54:11,957   Num examples = 1043
2023-03-27 13:54:11,957   Batch size = 32
2023-03-27 13:54:11,958 ***** Eval results *****
2023-03-27 13:54:11,958   att_loss = 0.3531615444279592
2023-03-27 13:54:11,958   cls_loss = 0.0
2023-03-27 13:54:11,959   global_step = 5449
2023-03-27 13:54:11,959   loss = 0.9075280766968333
2023-03-27 13:54:11,959   rep_loss = 0.5543665306283794
2023-03-27 13:54:11,966 ***** Save model *****
2023-03-27 13:54:23,206 ***** Running evaluation *****
2023-03-27 13:54:23,206   Epoch = 20 iter 5499 step
2023-03-27 13:54:23,206   Num examples = 1043
2023-03-27 13:54:23,206   Batch size = 32
2023-03-27 13:54:23,208 ***** Eval results *****
2023-03-27 13:54:23,208   att_loss = 0.35326949436709565
2023-03-27 13:54:23,208   cls_loss = 0.0
2023-03-27 13:54:23,208   global_step = 5499
2023-03-27 13:54:23,208   loss = 0.9076275461874668
2023-03-27 13:54:23,209   rep_loss = 0.5543580497585753
2023-03-27 13:54:23,216 ***** Save model *****
2023-03-27 13:54:34,473 ***** Running evaluation *****
2023-03-27 13:54:34,473   Epoch = 20 iter 5549 step
2023-03-27 13:54:34,473   Num examples = 1043
2023-03-27 13:54:34,473   Batch size = 32
2023-03-27 13:54:34,475 ***** Eval results *****
2023-03-27 13:54:34,475   att_loss = 0.3555484535306264
2023-03-27 13:54:34,475   cls_loss = 0.0
2023-03-27 13:54:34,475   global_step = 5549
2023-03-27 13:54:34,475   loss = 0.9104477138610548
2023-03-27 13:54:34,475   rep_loss = 0.5548992596174541
2023-03-27 13:54:34,482 ***** Save model *****
2023-03-27 13:54:45,736 ***** Running evaluation *****
2023-03-27 13:54:45,737   Epoch = 20 iter 5599 step
2023-03-27 13:54:45,737   Num examples = 1043
2023-03-27 13:54:45,737   Batch size = 32
2023-03-27 13:54:45,738 ***** Eval results *****
2023-03-27 13:54:45,738   att_loss = 0.355227511476826
2023-03-27 13:54:45,739   cls_loss = 0.0
2023-03-27 13:54:45,739   global_step = 5599
2023-03-27 13:54:45,739   loss = 0.9099252502430359
2023-03-27 13:54:45,739   rep_loss = 0.554697737730608
2023-03-27 13:54:45,746 ***** Save model *****
2023-03-27 13:54:56,939 ***** Running evaluation *****
2023-03-27 13:54:56,940   Epoch = 21 iter 5649 step
2023-03-27 13:54:56,940   Num examples = 1043
2023-03-27 13:54:56,940   Batch size = 32
2023-03-27 13:54:56,941 ***** Eval results *****
2023-03-27 13:54:56,941   att_loss = 0.3574288771266029
2023-03-27 13:54:56,942   cls_loss = 0.0
2023-03-27 13:54:56,942   global_step = 5649
2023-03-27 13:54:56,942   loss = 0.9101725660619282
2023-03-27 13:54:56,942   rep_loss = 0.5527436903544835
2023-03-27 13:54:56,950 ***** Save model *****
2023-03-27 13:55:08,200 ***** Running evaluation *****
2023-03-27 13:55:08,200   Epoch = 21 iter 5699 step
2023-03-27 13:55:08,200   Num examples = 1043
2023-03-27 13:55:08,200   Batch size = 32
2023-03-27 13:55:08,201 ***** Eval results *****
2023-03-27 13:55:08,202   att_loss = 0.3562379683489385
2023-03-27 13:55:08,202   cls_loss = 0.0
2023-03-27 13:55:08,202   global_step = 5699
2023-03-27 13:55:08,202   loss = 0.9104550454927527
2023-03-27 13:55:08,202   rep_loss = 0.5542170742283696
2023-03-27 13:55:08,209 ***** Save model *****
2023-03-27 13:55:19,460 ***** Running evaluation *****
2023-03-27 13:55:19,460   Epoch = 21 iter 5749 step
2023-03-27 13:55:19,460   Num examples = 1043
2023-03-27 13:55:19,460   Batch size = 32
2023-03-27 13:55:19,461 ***** Eval results *****
2023-03-27 13:55:19,461   att_loss = 0.3538544465538482
2023-03-27 13:55:19,462   cls_loss = 0.0
2023-03-27 13:55:19,462   global_step = 5749
2023-03-27 13:55:19,462   loss = 0.906833461892437
2023-03-27 13:55:19,462   rep_loss = 0.552979010931203
2023-03-27 13:55:19,469 ***** Save model *****
2023-03-27 13:55:30,726 ***** Running evaluation *****
2023-03-27 13:55:30,726   Epoch = 21 iter 5799 step
2023-03-27 13:55:30,726   Num examples = 1043
2023-03-27 13:55:30,726   Batch size = 32
2023-03-27 13:55:30,728 ***** Eval results *****
2023-03-27 13:55:30,728   att_loss = 0.35456382436677814
2023-03-27 13:55:30,728   cls_loss = 0.0
2023-03-27 13:55:30,728   global_step = 5799
2023-03-27 13:55:30,728   loss = 0.9079253058880568
2023-03-27 13:55:30,728   rep_loss = 0.5533614785720905
2023-03-27 13:55:30,736 ***** Save model *****
2023-03-27 13:55:41,975 ***** Running evaluation *****
2023-03-27 13:55:41,975   Epoch = 21 iter 5849 step
2023-03-27 13:55:41,975   Num examples = 1043
2023-03-27 13:55:41,975   Batch size = 32
2023-03-27 13:55:41,976 ***** Eval results *****
2023-03-27 13:55:41,977   att_loss = 0.35501208063984707
2023-03-27 13:55:41,977   cls_loss = 0.0
2023-03-27 13:55:41,977   global_step = 5849
2023-03-27 13:55:41,978   loss = 0.9084060982731749
2023-03-27 13:55:41,978   rep_loss = 0.5533940154166261
2023-03-27 13:55:41,985 ***** Save model *****
2023-03-27 13:55:53,266 ***** Running evaluation *****
2023-03-27 13:55:53,266   Epoch = 22 iter 5899 step
2023-03-27 13:55:53,266   Num examples = 1043
2023-03-27 13:55:53,266   Batch size = 32
2023-03-27 13:55:53,268 ***** Eval results *****
2023-03-27 13:55:53,268   att_loss = 0.34554816484451295
2023-03-27 13:55:53,268   cls_loss = 0.0
2023-03-27 13:55:53,268   global_step = 5899
2023-03-27 13:55:53,268   loss = 0.8960385489463806
2023-03-27 13:55:53,268   rep_loss = 0.5504903864860534
2023-03-27 13:55:53,271 ***** Save model *****
2023-03-27 13:56:04,533 ***** Running evaluation *****
2023-03-27 13:56:04,533   Epoch = 22 iter 5949 step
2023-03-27 13:56:04,533   Num examples = 1043
2023-03-27 13:56:04,533   Batch size = 32
2023-03-27 13:56:04,534 ***** Eval results *****
2023-03-27 13:56:04,534   att_loss = 0.35059054374694826
2023-03-27 13:56:04,535   cls_loss = 0.0
2023-03-27 13:56:04,535   global_step = 5949
2023-03-27 13:56:04,535   loss = 0.8990005882581075
2023-03-27 13:56:04,535   rep_loss = 0.5484100453058879
2023-03-27 13:56:04,542 ***** Save model *****
2023-03-27 13:56:15,827 ***** Running evaluation *****
2023-03-27 13:56:15,828   Epoch = 22 iter 5999 step
2023-03-27 13:56:15,828   Num examples = 1043
2023-03-27 13:56:15,828   Batch size = 32
2023-03-27 13:56:15,829 ***** Eval results *****
2023-03-27 13:56:15,829   att_loss = 0.35119980430603026
2023-03-27 13:56:15,829   cls_loss = 0.0
2023-03-27 13:56:15,829   global_step = 5999
2023-03-27 13:56:15,829   loss = 0.9007394161224366
2023-03-27 13:56:15,829   rep_loss = 0.5495396127700806
2023-03-27 13:56:15,836 ***** Save model *****
2023-03-27 13:56:27,092 ***** Running evaluation *****
2023-03-27 13:56:27,093   Epoch = 22 iter 6049 step
2023-03-27 13:56:27,093   Num examples = 1043
2023-03-27 13:56:27,093   Batch size = 32
2023-03-27 13:56:27,094 ***** Eval results *****
2023-03-27 13:56:27,094   att_loss = 0.3527162730693817
2023-03-27 13:56:27,095   cls_loss = 0.0
2023-03-27 13:56:27,095   global_step = 6049
2023-03-27 13:56:27,095   loss = 0.9033918823514666
2023-03-27 13:56:27,095   rep_loss = 0.5506756104741778
2023-03-27 13:56:27,102 ***** Save model *****
2023-03-27 13:56:38,389 ***** Running evaluation *****
2023-03-27 13:56:38,389   Epoch = 22 iter 6099 step
2023-03-27 13:56:38,389   Num examples = 1043
2023-03-27 13:56:38,389   Batch size = 32
2023-03-27 13:56:38,391 ***** Eval results *****
2023-03-27 13:56:38,391   att_loss = 0.354451904296875
2023-03-27 13:56:38,391   cls_loss = 0.0
2023-03-27 13:56:38,392   global_step = 6099
2023-03-27 13:56:38,392   loss = 0.9052893543243408
2023-03-27 13:56:38,392   rep_loss = 0.5508374508221944
2023-03-27 13:56:38,399 ***** Save model *****
2023-03-27 13:56:49,668 ***** Running evaluation *****
2023-03-27 13:56:49,668   Epoch = 23 iter 6149 step
2023-03-27 13:56:49,668   Num examples = 1043
2023-03-27 13:56:49,669   Batch size = 32
2023-03-27 13:56:49,670 ***** Eval results *****
2023-03-27 13:56:49,670   att_loss = 0.3573148846626282
2023-03-27 13:56:49,670   cls_loss = 0.0
2023-03-27 13:56:49,670   global_step = 6149
2023-03-27 13:56:49,671   loss = 0.9072972163558006
2023-03-27 13:56:49,671   rep_loss = 0.549982339143753
2023-03-27 13:56:49,678 ***** Save model *****
2023-03-27 13:57:00,981 ***** Running evaluation *****
2023-03-27 13:57:00,982   Epoch = 23 iter 6199 step
2023-03-27 13:57:00,982   Num examples = 1043
2023-03-27 13:57:00,982   Batch size = 32
2023-03-27 13:57:00,983 ***** Eval results *****
2023-03-27 13:57:00,983   att_loss = 0.35313930079854766
2023-03-27 13:57:00,984   cls_loss = 0.0
2023-03-27 13:57:00,984   global_step = 6199
2023-03-27 13:57:00,984   loss = 0.9040112392655735
2023-03-27 13:57:00,984   rep_loss = 0.5508719405223583
2023-03-27 13:57:00,986 ***** Save model *****
2023-03-27 13:57:12,257 ***** Running evaluation *****
2023-03-27 13:57:12,258   Epoch = 23 iter 6249 step
2023-03-27 13:57:12,258   Num examples = 1043
2023-03-27 13:57:12,258   Batch size = 32
2023-03-27 13:57:12,259 ***** Eval results *****
2023-03-27 13:57:12,259   att_loss = 0.3556817000110944
2023-03-27 13:57:12,259   cls_loss = 0.0
2023-03-27 13:57:12,259   global_step = 6249
2023-03-27 13:57:12,259   loss = 0.9062765875348339
2023-03-27 13:57:12,260   rep_loss = 0.5505948883515818
2023-03-27 13:57:12,267 ***** Save model *****
2023-03-27 13:57:23,529 ***** Running evaluation *****
2023-03-27 13:57:23,529   Epoch = 23 iter 6299 step
2023-03-27 13:57:23,529   Num examples = 1043
2023-03-27 13:57:23,529   Batch size = 32
2023-03-27 13:57:23,531 ***** Eval results *****
2023-03-27 13:57:23,531   att_loss = 0.35452684363986875
2023-03-27 13:57:23,531   cls_loss = 0.0
2023-03-27 13:57:23,531   global_step = 6299
2023-03-27 13:57:23,531   loss = 0.9047295998168897
2023-03-27 13:57:23,532   rep_loss = 0.5502027563656433
2023-03-27 13:57:23,539 ***** Save model *****
2023-03-27 13:57:34,827 ***** Running evaluation *****
2023-03-27 13:57:34,827   Epoch = 23 iter 6349 step
2023-03-27 13:57:34,827   Num examples = 1043
2023-03-27 13:57:34,828   Batch size = 32
2023-03-27 13:57:34,829 ***** Eval results *****
2023-03-27 13:57:34,829   att_loss = 0.3544137552380562
2023-03-27 13:57:34,829   cls_loss = 0.0
2023-03-27 13:57:34,829   global_step = 6349
2023-03-27 13:57:34,829   loss = 0.9041727494734985
2023-03-27 13:57:34,829   rep_loss = 0.5497589953816854
2023-03-27 13:57:34,837 ***** Save model *****
2023-03-27 13:57:46,072 ***** Running evaluation *****
2023-03-27 13:57:46,072   Epoch = 23 iter 6399 step
2023-03-27 13:57:46,073   Num examples = 1043
2023-03-27 13:57:46,073   Batch size = 32
2023-03-27 13:57:46,074 ***** Eval results *****
2023-03-27 13:57:46,074   att_loss = 0.3542936869369921
2023-03-27 13:57:46,074   cls_loss = 0.0
2023-03-27 13:57:46,075   global_step = 6399
2023-03-27 13:57:46,075   loss = 0.9043235298275023
2023-03-27 13:57:46,075   rep_loss = 0.5500298435835875
2023-03-27 13:57:46,082 ***** Save model *****
2023-03-27 13:57:57,341 ***** Running evaluation *****
2023-03-27 13:57:57,341   Epoch = 24 iter 6449 step
2023-03-27 13:57:57,341   Num examples = 1043
2023-03-27 13:57:57,342   Batch size = 32
2023-03-27 13:57:57,343 ***** Eval results *****
2023-03-27 13:57:57,343   att_loss = 0.35174437412401527
2023-03-27 13:57:57,343   cls_loss = 0.0
2023-03-27 13:57:57,343   global_step = 6449
2023-03-27 13:57:57,344   loss = 0.8973404372610697
2023-03-27 13:57:57,344   rep_loss = 0.5455960704059135
2023-03-27 13:57:57,351 ***** Save model *****
2023-03-27 13:58:08,610 ***** Running evaluation *****
2023-03-27 13:58:08,610   Epoch = 24 iter 6499 step
2023-03-27 13:58:08,610   Num examples = 1043
2023-03-27 13:58:08,610   Batch size = 32
2023-03-27 13:58:08,612 ***** Eval results *****
2023-03-27 13:58:08,612   att_loss = 0.35144626275523677
2023-03-27 13:58:08,612   cls_loss = 0.0
2023-03-27 13:58:08,612   global_step = 6499
2023-03-27 13:58:08,613   loss = 0.8977541399526072
2023-03-27 13:58:08,613   rep_loss = 0.5463078801448529
2023-03-27 13:58:08,615 ***** Save model *****
2023-03-27 13:58:19,873 ***** Running evaluation *****
2023-03-27 13:58:19,873   Epoch = 24 iter 6549 step
2023-03-27 13:58:19,873   Num examples = 1043
2023-03-27 13:58:19,873   Batch size = 32
2023-03-27 13:58:19,874 ***** Eval results *****
2023-03-27 13:58:19,874   att_loss = 0.3500900942805811
2023-03-27 13:58:19,874   cls_loss = 0.0
2023-03-27 13:58:19,874   global_step = 6549
2023-03-27 13:58:19,875   loss = 0.896411621401496
2023-03-27 13:58:19,875   rep_loss = 0.5463215286004628
2023-03-27 13:58:19,882 ***** Save model *****
2023-03-27 13:58:31,163 ***** Running evaluation *****
2023-03-27 13:58:31,164   Epoch = 24 iter 6599 step
2023-03-27 13:58:31,164   Num examples = 1043
2023-03-27 13:58:31,164   Batch size = 32
2023-03-27 13:58:31,165 ***** Eval results *****
2023-03-27 13:58:31,165   att_loss = 0.35255671109204517
2023-03-27 13:58:31,166   cls_loss = 0.0
2023-03-27 13:58:31,166   global_step = 6599
2023-03-27 13:58:31,166   loss = 0.8998240772342183
2023-03-27 13:58:31,166   rep_loss = 0.5472673661421731
2023-03-27 13:58:31,174 ***** Save model *****
2023-03-27 13:58:42,438 ***** Running evaluation *****
2023-03-27 13:58:42,439   Epoch = 24 iter 6649 step
2023-03-27 13:58:42,439   Num examples = 1043
2023-03-27 13:58:42,439   Batch size = 32
2023-03-27 13:58:42,440 ***** Eval results *****
2023-03-27 13:58:42,440   att_loss = 0.3523872902779164
2023-03-27 13:58:42,441   cls_loss = 0.0
2023-03-27 13:58:42,441   global_step = 6649
2023-03-27 13:58:42,441   loss = 0.8997482486780254
2023-03-27 13:58:42,441   rep_loss = 0.5473609588947533
2023-03-27 13:58:42,449 ***** Save model *****
2023-03-27 13:58:53,702 ***** Running evaluation *****
2023-03-27 13:58:53,702   Epoch = 25 iter 6699 step
2023-03-27 13:58:53,702   Num examples = 1043
2023-03-27 13:58:53,702   Batch size = 32
2023-03-27 13:58:53,704 ***** Eval results *****
2023-03-27 13:58:53,704   att_loss = 0.3481614577273528
2023-03-27 13:58:53,704   cls_loss = 0.0
2023-03-27 13:58:53,704   global_step = 6699
2023-03-27 13:58:53,704   loss = 0.8953962425390879
2023-03-27 13:58:53,704   rep_loss = 0.5472347910205523
2023-03-27 13:58:53,711 ***** Save model *****
2023-03-27 13:59:04,978 ***** Running evaluation *****
2023-03-27 13:59:04,978   Epoch = 25 iter 6749 step
2023-03-27 13:59:04,978   Num examples = 1043
2023-03-27 13:59:04,978   Batch size = 32
2023-03-27 13:59:04,979 ***** Eval results *****
2023-03-27 13:59:04,980   att_loss = 0.35218436935463465
2023-03-27 13:59:04,980   cls_loss = 0.0
2023-03-27 13:59:04,980   global_step = 6749
2023-03-27 13:59:04,980   loss = 0.8996054505979693
2023-03-27 13:59:04,981   rep_loss = 0.5474210856734095
2023-03-27 13:59:04,988 ***** Save model *****
2023-03-27 13:59:16,249 ***** Running evaluation *****
2023-03-27 13:59:16,249   Epoch = 25 iter 6799 step
2023-03-27 13:59:16,249   Num examples = 1043
2023-03-27 13:59:16,249   Batch size = 32
2023-03-27 13:59:16,250 ***** Eval results *****
2023-03-27 13:59:16,251   att_loss = 0.3513663043418238
2023-03-27 13:59:16,251   cls_loss = 0.0
2023-03-27 13:59:16,251   global_step = 6799
2023-03-27 13:59:16,251   loss = 0.8973875656243293
2023-03-27 13:59:16,251   rep_loss = 0.5460212644069425
2023-03-27 13:59:16,258 ***** Save model *****
2023-03-27 13:59:27,521 ***** Running evaluation *****
2023-03-27 13:59:27,521   Epoch = 25 iter 6849 step
2023-03-27 13:59:27,521   Num examples = 1043
2023-03-27 13:59:27,522   Batch size = 32
2023-03-27 13:59:27,523 ***** Eval results *****
2023-03-27 13:59:27,523   att_loss = 0.35155741185292433
2023-03-27 13:59:27,523   cls_loss = 0.0
2023-03-27 13:59:27,523   global_step = 6849
2023-03-27 13:59:27,524   loss = 0.8972644641481596
2023-03-27 13:59:27,524   rep_loss = 0.5457070555495119
2023-03-27 13:59:27,531 ***** Save model *****
2023-03-27 13:59:38,794 ***** Running evaluation *****
2023-03-27 13:59:38,794   Epoch = 25 iter 6899 step
2023-03-27 13:59:38,794   Num examples = 1043
2023-03-27 13:59:38,794   Batch size = 32
2023-03-27 13:59:38,795 ***** Eval results *****
2023-03-27 13:59:38,795   att_loss = 0.35101380571722984
2023-03-27 13:59:38,795   cls_loss = 0.0
2023-03-27 13:59:38,796   global_step = 6899
2023-03-27 13:59:38,796   loss = 0.896530303039721
2023-03-27 13:59:38,796   rep_loss = 0.5455164994512286
2023-03-27 13:59:38,803 ***** Save model *****
2023-03-27 13:59:50,050 ***** Running evaluation *****
2023-03-27 13:59:50,050   Epoch = 26 iter 6949 step
2023-03-27 13:59:50,050   Num examples = 1043
2023-03-27 13:59:50,050   Batch size = 32
2023-03-27 13:59:50,051 ***** Eval results *****
2023-03-27 13:59:50,052   att_loss = 0.3517813895429884
2023-03-27 13:59:50,052   cls_loss = 0.0
2023-03-27 13:59:50,052   global_step = 6949
2023-03-27 13:59:50,052   loss = 0.8914167199816022
2023-03-27 13:59:50,052   rep_loss = 0.5396353346960885
2023-03-27 13:59:50,059 ***** Save model *****
2023-03-27 14:00:01,318 ***** Running evaluation *****
2023-03-27 14:00:01,319   Epoch = 26 iter 6999 step
2023-03-27 14:00:01,319   Num examples = 1043
2023-03-27 14:00:01,319   Batch size = 32
2023-03-27 14:00:01,320 ***** Eval results *****
2023-03-27 14:00:01,321   att_loss = 0.3519183512319598
2023-03-27 14:00:01,321   cls_loss = 0.0
2023-03-27 14:00:01,321   global_step = 6999
2023-03-27 14:00:01,321   loss = 0.8969227180146334
2023-03-27 14:00:01,321   rep_loss = 0.5450043657369781
2023-03-27 14:00:01,323 ***** Save model *****
2023-03-27 14:00:12,589 ***** Running evaluation *****
2023-03-27 14:00:12,590   Epoch = 26 iter 7049 step
2023-03-27 14:00:12,590   Num examples = 1043
2023-03-27 14:00:12,590   Batch size = 32
2023-03-27 14:00:12,591 ***** Eval results *****
2023-03-27 14:00:12,591   att_loss = 0.35151463830582447
2023-03-27 14:00:12,591   cls_loss = 0.0
2023-03-27 14:00:12,591   global_step = 7049
2023-03-27 14:00:12,592   loss = 0.895769428984027
2023-03-27 14:00:12,592   rep_loss = 0.5442547903996762
2023-03-27 14:00:12,599 ***** Save model *****
2023-03-27 14:00:23,876 ***** Running evaluation *****
2023-03-27 14:00:23,877   Epoch = 26 iter 7099 step
2023-03-27 14:00:23,877   Num examples = 1043
2023-03-27 14:00:23,877   Batch size = 32
2023-03-27 14:00:23,878 ***** Eval results *****
2023-03-27 14:00:23,878   att_loss = 0.35170709802086947
2023-03-27 14:00:23,878   cls_loss = 0.0
2023-03-27 14:00:23,878   global_step = 7099
2023-03-27 14:00:23,879   loss = 0.8958577980661089
2023-03-27 14:00:23,879   rep_loss = 0.5441506994757682
2023-03-27 14:00:23,881 ***** Save model *****
2023-03-27 14:00:35,136 ***** Running evaluation *****
2023-03-27 14:00:35,136   Epoch = 26 iter 7149 step
2023-03-27 14:00:35,136   Num examples = 1043
2023-03-27 14:00:35,137   Batch size = 32
2023-03-27 14:00:35,137 ***** Eval results *****
2023-03-27 14:00:35,138   att_loss = 0.34997836547197353
2023-03-27 14:00:35,138   cls_loss = 0.0
2023-03-27 14:00:35,138   global_step = 7149
2023-03-27 14:00:35,138   loss = 0.8941319083821946
2023-03-27 14:00:35,139   rep_loss = 0.5441535420463857
2023-03-27 14:00:35,146 ***** Save model *****
2023-03-27 14:00:46,392 ***** Running evaluation *****
2023-03-27 14:00:46,392   Epoch = 26 iter 7199 step
2023-03-27 14:00:46,392   Num examples = 1043
2023-03-27 14:00:46,393   Batch size = 32
2023-03-27 14:00:46,393 ***** Eval results *****
2023-03-27 14:00:46,394   att_loss = 0.35059102743516174
2023-03-27 14:00:46,394   cls_loss = 0.0
2023-03-27 14:00:46,394   global_step = 7199
2023-03-27 14:00:46,394   loss = 0.894721011707291
2023-03-27 14:00:46,394   rep_loss = 0.5441299829965436
2023-03-27 14:00:46,401 ***** Save model *****
2023-03-27 14:00:57,675 ***** Running evaluation *****
2023-03-27 14:00:57,675   Epoch = 27 iter 7249 step
2023-03-27 14:00:57,676   Num examples = 1043
2023-03-27 14:00:57,676   Batch size = 32
2023-03-27 14:00:57,677 ***** Eval results *****
2023-03-27 14:00:57,677   att_loss = 0.35017412155866623
2023-03-27 14:00:57,677   cls_loss = 0.0
2023-03-27 14:00:57,678   global_step = 7249
2023-03-27 14:00:57,678   loss = 0.8936395853757858
2023-03-27 14:00:57,678   rep_loss = 0.5434654653072357
2023-03-27 14:00:57,680 ***** Save model *****
2023-03-27 14:01:08,931 ***** Running evaluation *****
2023-03-27 14:01:08,931   Epoch = 27 iter 7299 step
2023-03-27 14:01:08,931   Num examples = 1043
2023-03-27 14:01:08,931   Batch size = 32
2023-03-27 14:01:08,933 ***** Eval results *****
2023-03-27 14:01:08,933   att_loss = 0.35182080500655705
2023-03-27 14:01:08,933   cls_loss = 0.0
2023-03-27 14:01:08,933   global_step = 7299
2023-03-27 14:01:08,933   loss = 0.8960857576794095
2023-03-27 14:01:08,933   rep_loss = 0.5442649549908108
2023-03-27 14:01:08,940 ***** Save model *****
2023-03-27 14:01:20,206 ***** Running evaluation *****
2023-03-27 14:01:20,206   Epoch = 27 iter 7349 step
2023-03-27 14:01:20,206   Num examples = 1043
2023-03-27 14:01:20,206   Batch size = 32
2023-03-27 14:01:20,207 ***** Eval results *****
2023-03-27 14:01:20,208   att_loss = 0.35082505877528875
2023-03-27 14:01:20,208   cls_loss = 0.0
2023-03-27 14:01:20,208   global_step = 7349
2023-03-27 14:01:20,208   loss = 0.8936547377279827
2023-03-27 14:01:20,209   rep_loss = 0.5428296817200524
2023-03-27 14:01:20,210 ***** Save model *****
2023-03-27 14:01:31,466 ***** Running evaluation *****
2023-03-27 14:01:31,466   Epoch = 27 iter 7399 step
2023-03-27 14:01:31,466   Num examples = 1043
2023-03-27 14:01:31,467   Batch size = 32
2023-03-27 14:01:31,468 ***** Eval results *****
2023-03-27 14:01:31,468   att_loss = 0.35163068096888694
2023-03-27 14:01:31,469   cls_loss = 0.0
2023-03-27 14:01:31,469   global_step = 7399
2023-03-27 14:01:31,469   loss = 0.8952247585120954
2023-03-27 14:01:31,469   rep_loss = 0.5435940798960234
2023-03-27 14:01:31,476 ***** Save model *****
2023-03-27 14:01:42,694 ***** Running evaluation *****
2023-03-27 14:01:42,695   Epoch = 27 iter 7449 step
2023-03-27 14:01:42,695   Num examples = 1043
2023-03-27 14:01:42,695   Batch size = 32
2023-03-27 14:01:42,696 ***** Eval results *****
2023-03-27 14:01:42,696   att_loss = 0.3507948748767376
2023-03-27 14:01:42,697   cls_loss = 0.0
2023-03-27 14:01:42,697   global_step = 7449
2023-03-27 14:01:42,697   loss = 0.8940333264569441
2023-03-27 14:01:42,697   rep_loss = 0.5432384538153807
2023-03-27 14:01:42,705 ***** Save model *****
2023-03-27 14:01:53,962 ***** Running evaluation *****
2023-03-27 14:01:53,962   Epoch = 28 iter 7499 step
2023-03-27 14:01:53,963   Num examples = 1043
2023-03-27 14:01:53,963   Batch size = 32
2023-03-27 14:01:53,964 ***** Eval results *****
2023-03-27 14:01:53,964   att_loss = 0.3523081320783366
2023-03-27 14:01:53,964   cls_loss = 0.0
2023-03-27 14:01:53,964   global_step = 7499
2023-03-27 14:01:53,964   loss = 0.8935459774473439
2023-03-27 14:01:53,964   rep_loss = 0.5412378440732541
2023-03-27 14:01:53,971 ***** Save model *****
2023-03-27 14:02:05,239 ***** Running evaluation *****
2023-03-27 14:02:05,239   Epoch = 28 iter 7549 step
2023-03-27 14:02:05,239   Num examples = 1043
2023-03-27 14:02:05,239   Batch size = 32
2023-03-27 14:02:05,240 ***** Eval results *****
2023-03-27 14:02:05,241   att_loss = 0.35173274392951026
2023-03-27 14:02:05,241   cls_loss = 0.0
2023-03-27 14:02:05,241   global_step = 7549
2023-03-27 14:02:05,241   loss = 0.8945626771613343
2023-03-27 14:02:05,242   rep_loss = 0.542829934048326
2023-03-27 14:02:05,249 ***** Save model *****
2023-03-27 14:02:16,520 ***** Running evaluation *****
2023-03-27 14:02:16,521   Epoch = 28 iter 7599 step
2023-03-27 14:02:16,521   Num examples = 1043
2023-03-27 14:02:16,521   Batch size = 32
2023-03-27 14:02:16,522 ***** Eval results *****
2023-03-27 14:02:16,522   att_loss = 0.3524727230149556
2023-03-27 14:02:16,522   cls_loss = 0.0
2023-03-27 14:02:16,523   global_step = 7599
2023-03-27 14:02:16,523   loss = 0.8957376799932341
2023-03-27 14:02:16,523   rep_loss = 0.5432649569782785
2023-03-27 14:02:16,530 ***** Save model *****
2023-03-27 14:02:27,787 ***** Running evaluation *****
2023-03-27 14:02:27,787   Epoch = 28 iter 7649 step
2023-03-27 14:02:27,787   Num examples = 1043
2023-03-27 14:02:27,787   Batch size = 32
2023-03-27 14:02:27,789 ***** Eval results *****
2023-03-27 14:02:27,789   att_loss = 0.3525202777344367
2023-03-27 14:02:27,789   cls_loss = 0.0
2023-03-27 14:02:27,789   global_step = 7649
2023-03-27 14:02:27,790   loss = 0.8958187430580228
2023-03-27 14:02:27,790   rep_loss = 0.5432984660126571
2023-03-27 14:02:27,797 ***** Save model *****
2023-03-27 14:02:39,016 ***** Running evaluation *****
2023-03-27 14:02:39,016   Epoch = 28 iter 7699 step
2023-03-27 14:02:39,016   Num examples = 1043
2023-03-27 14:02:39,016   Batch size = 32
2023-03-27 14:02:39,018 ***** Eval results *****
2023-03-27 14:02:39,018   att_loss = 0.35001271655741295
2023-03-27 14:02:39,018   cls_loss = 0.0
2023-03-27 14:02:39,019   global_step = 7699
2023-03-27 14:02:39,019   loss = 0.8923889703280188
2023-03-27 14:02:39,019   rep_loss = 0.5423762547061047
2023-03-27 14:02:39,026 ***** Save model *****
2023-03-27 14:02:50,263 ***** Running evaluation *****
2023-03-27 14:02:50,263   Epoch = 29 iter 7749 step
2023-03-27 14:02:50,264   Num examples = 1043
2023-03-27 14:02:50,264   Batch size = 32
2023-03-27 14:02:50,265 ***** Eval results *****
2023-03-27 14:02:50,265   att_loss = 0.33887263139088947
2023-03-27 14:02:50,265   cls_loss = 0.0
2023-03-27 14:02:50,265   global_step = 7749
2023-03-27 14:02:50,265   loss = 0.8726019958655039
2023-03-27 14:02:50,265   rep_loss = 0.5337293644746145
2023-03-27 14:02:50,268 ***** Save model *****
2023-03-27 14:03:01,514 ***** Running evaluation *****
2023-03-27 14:03:01,515   Epoch = 29 iter 7799 step
2023-03-27 14:03:01,515   Num examples = 1043
2023-03-27 14:03:01,515   Batch size = 32
2023-03-27 14:03:01,516 ***** Eval results *****
2023-03-27 14:03:01,516   att_loss = 0.34361826202699114
2023-03-27 14:03:01,516   cls_loss = 0.0
2023-03-27 14:03:01,516   global_step = 7799
2023-03-27 14:03:01,516   loss = 0.8812104921255793
2023-03-27 14:03:01,516   rep_loss = 0.5375922322273254
2023-03-27 14:03:01,523 ***** Save model *****
2023-03-27 14:03:12,783 ***** Running evaluation *****
2023-03-27 14:03:12,783   Epoch = 29 iter 7849 step
2023-03-27 14:03:12,783   Num examples = 1043
2023-03-27 14:03:12,783   Batch size = 32
2023-03-27 14:03:12,784 ***** Eval results *****
2023-03-27 14:03:12,785   att_loss = 0.34778975906237114
2023-03-27 14:03:12,785   cls_loss = 0.0
2023-03-27 14:03:12,785   global_step = 7849
2023-03-27 14:03:12,785   loss = 0.8872956605452411
2023-03-27 14:03:12,785   rep_loss = 0.5395059040132558
2023-03-27 14:03:12,792 ***** Save model *****
2023-03-27 14:03:24,064 ***** Running evaluation *****
2023-03-27 14:03:24,065   Epoch = 29 iter 7899 step
2023-03-27 14:03:24,065   Num examples = 1043
2023-03-27 14:03:24,065   Batch size = 32
2023-03-27 14:03:24,066 ***** Eval results *****
2023-03-27 14:03:24,066   att_loss = 0.3484852079015512
2023-03-27 14:03:24,066   cls_loss = 0.0
2023-03-27 14:03:24,067   global_step = 7899
2023-03-27 14:03:24,067   loss = 0.888262444581741
2023-03-27 14:03:24,067   rep_loss = 0.5397772391637167
2023-03-27 14:03:24,074 ***** Save model *****
2023-03-27 14:03:35,288 ***** Running evaluation *****
2023-03-27 14:03:35,288   Epoch = 29 iter 7949 step
2023-03-27 14:03:35,288   Num examples = 1043
2023-03-27 14:03:35,288   Batch size = 32
2023-03-27 14:03:35,289 ***** Eval results *****
2023-03-27 14:03:35,289   att_loss = 0.3486705598900619
2023-03-27 14:03:35,290   cls_loss = 0.0
2023-03-27 14:03:35,290   global_step = 7949
2023-03-27 14:03:35,290   loss = 0.8891933574838545
2023-03-27 14:03:35,290   rep_loss = 0.5405227993298503
2023-03-27 14:03:35,292 ***** Save model *****
2023-03-27 14:03:46,558 ***** Running evaluation *****
2023-03-27 14:03:46,558   Epoch = 29 iter 7999 step
2023-03-27 14:03:46,558   Num examples = 1043
2023-03-27 14:03:46,558   Batch size = 32
2023-03-27 14:03:46,560 ***** Eval results *****
2023-03-27 14:03:46,560   att_loss = 0.34720711526460946
2023-03-27 14:03:46,560   cls_loss = 0.0
2023-03-27 14:03:46,561   global_step = 7999
2023-03-27 14:03:46,561   loss = 0.8872488944325596
2023-03-27 14:03:46,561   rep_loss = 0.540041780564934
2023-03-27 14:03:46,568 ***** Save model *****
2023-03-27 14:04:11,926 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/CoLA//TempTinyBERT_CoLA_4L_312D', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA//TinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=3e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=True, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='BASE')
2023-03-27 14:04:13,206 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 14:04:15,751 device: cuda n_gpu: 1
2023-03-27 14:10:02,600 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/BASE_COMPARE/TempTinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=30.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='BASE_COMPARE')
2023-03-27 14:10:04,038 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 14:10:06,584 device: cuda n_gpu: 1
2023-03-27 14:10:06,631 Writing example 0 of 8551
2023-03-27 14:10:06,632 *** Example ***
2023-03-27 14:10:06,632 guid: train-0
2023-03-27 14:10:06,632 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 14:10:06,632 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:10:06,633 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:10:06,633 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:10:06,633 label: 1
2023-03-27 14:10:06,633 label_id: 1
2023-03-27 14:10:07,649 Writing example 0 of 1043
2023-03-27 14:10:07,649 *** Example ***
2023-03-27 14:10:07,649 guid: dev-0
2023-03-27 14:10:07,650 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 14:10:07,650 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:10:07,650 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:10:07,650 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:10:07,650 label: 1
2023-03-27 14:10:07,650 label_id: 1
2023-03-27 14:10:07,770 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 14:10:07,771 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 14:10:09,544 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 14:10:09,739 loading model...
2023-03-27 14:10:09,845 done!
2023-03-27 14:10:09,845 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 14:10:10,874 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 14:10:10,875 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 14:10:11,100 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 14:10:11,125 loading model...
2023-03-27 14:10:11,132 done!
2023-03-27 14:10:11,133 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 14:10:11,133 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 14:10:11,144 ***** Running training *****
2023-03-27 14:10:11,145   Num examples = 8551
2023-03-27 14:10:11,145   Batch size = 32
2023-03-27 14:10:11,145   Num steps = 8010
2023-03-27 14:10:11,145 n: bert.embeddings.word_embeddings.weight
2023-03-27 14:10:11,146 n: bert.embeddings.position_embeddings.weight
2023-03-27 14:10:11,146 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 14:10:11,146 n: bert.embeddings.LayerNorm.weight
2023-03-27 14:10:11,146 n: bert.embeddings.LayerNorm.bias
2023-03-27 14:10:11,146 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 14:10:11,146 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 14:10:11,146 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 14:10:11,147 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 14:10:11,147 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 14:10:11,147 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 14:10:11,147 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 14:10:11,147 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 14:10:11,147 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 14:10:11,147 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 14:10:11,147 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 14:10:11,148 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 14:10:11,148 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 14:10:11,148 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 14:10:11,148 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 14:10:11,148 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 14:10:11,148 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 14:10:11,148 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 14:10:11,148 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 14:10:11,149 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 14:10:11,149 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 14:10:11,149 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 14:10:11,149 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 14:10:11,149 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 14:10:11,149 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 14:10:11,149 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 14:10:11,149 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 14:10:11,149 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 14:10:11,149 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 14:10:11,150 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 14:10:11,150 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 14:10:11,150 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 14:10:11,150 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 14:10:11,150 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 14:10:11,150 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 14:10:11,150 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 14:10:11,150 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 14:10:11,150 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 14:10:11,150 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 14:10:11,151 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 14:10:11,151 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 14:10:11,151 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 14:10:11,151 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 14:10:11,151 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 14:10:11,151 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 14:10:11,151 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 14:10:11,151 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 14:10:11,151 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 14:10:11,151 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 14:10:11,152 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 14:10:11,152 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 14:10:11,152 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 14:10:11,152 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 14:10:11,152 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 14:10:11,152 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 14:10:11,152 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 14:10:11,152 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 14:10:11,152 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 14:10:11,152 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 14:10:11,152 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 14:10:11,153 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 14:10:11,153 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 14:10:11,153 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 14:10:11,153 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 14:10:11,153 n: bert.pooler.dense.weight
2023-03-27 14:10:11,153 n: bert.pooler.dense.bias
2023-03-27 14:10:11,153 n: classifier.weight
2023-03-27 14:10:11,153 n: classifier.bias
2023-03-27 14:10:11,153 n: fit_dense.weight
2023-03-27 14:10:11,153 n: fit_dense.bias
2023-03-27 14:10:11,154 Total parameters: 14591258
2023-03-27 14:10:21,314 ***** Running evaluation *****
2023-03-27 14:10:21,314   Epoch = 0 iter 49 step
2023-03-27 14:10:21,314   Num examples = 1043
2023-03-27 14:10:21,314   Batch size = 32
2023-03-27 14:10:21,322 ***** Eval results *****
2023-03-27 14:10:21,323   att_loss = 0.5624887608751958
2023-03-27 14:10:21,323   cls_loss = 0.0
2023-03-27 14:10:21,324   global_step = 49
2023-03-27 14:10:21,324   loss = 1.885422227334003
2023-03-27 14:10:21,324   rep_loss = 1.3229334549028047
2023-03-27 14:10:21,332 ***** Save model *****
2023-03-27 14:10:31,689 ***** Running evaluation *****
2023-03-27 14:10:31,690   Epoch = 0 iter 99 step
2023-03-27 14:10:31,690   Num examples = 1043
2023-03-27 14:10:31,690   Batch size = 32
2023-03-27 14:10:31,692 ***** Eval results *****
2023-03-27 14:10:31,692   att_loss = 0.5208015110757616
2023-03-27 14:10:31,692   cls_loss = 0.0
2023-03-27 14:10:31,692   global_step = 99
2023-03-27 14:10:31,692   loss = 1.625842166669441
2023-03-27 14:10:31,693   rep_loss = 1.1050406513792095
2023-03-27 14:10:31,700 ***** Save model *****
2023-03-27 14:10:42,147 ***** Running evaluation *****
2023-03-27 14:10:42,148   Epoch = 0 iter 149 step
2023-03-27 14:10:42,148   Num examples = 1043
2023-03-27 14:10:42,148   Batch size = 32
2023-03-27 14:10:42,150 ***** Eval results *****
2023-03-27 14:10:42,150   att_loss = 0.4964984979405499
2023-03-27 14:10:42,150   cls_loss = 0.0
2023-03-27 14:10:42,151   global_step = 149
2023-03-27 14:10:42,151   loss = 1.5029620956254486
2023-03-27 14:10:42,151   rep_loss = 1.0064635928845245
2023-03-27 14:10:42,160 ***** Save model *****
2023-03-27 14:10:52,727 ***** Running evaluation *****
2023-03-27 14:10:52,728   Epoch = 0 iter 199 step
2023-03-27 14:10:52,728   Num examples = 1043
2023-03-27 14:10:52,728   Batch size = 32
2023-03-27 14:10:52,730 ***** Eval results *****
2023-03-27 14:10:52,730   att_loss = 0.4843230084258707
2023-03-27 14:10:52,730   cls_loss = 0.0
2023-03-27 14:10:52,730   global_step = 199
2023-03-27 14:10:52,731   loss = 1.4326463176976496
2023-03-27 14:10:52,731   rep_loss = 0.9483233037306436
2023-03-27 14:10:52,738 ***** Save model *****
2023-03-27 14:11:03,393 ***** Running evaluation *****
2023-03-27 14:11:03,394   Epoch = 0 iter 249 step
2023-03-27 14:11:03,394   Num examples = 1043
2023-03-27 14:11:03,394   Batch size = 32
2023-03-27 14:11:03,395 ***** Eval results *****
2023-03-27 14:11:03,395   att_loss = 0.47543028882708416
2023-03-27 14:11:03,396   cls_loss = 0.0
2023-03-27 14:11:03,396   global_step = 249
2023-03-27 14:11:03,396   loss = 1.384641918791346
2023-03-27 14:11:03,396   rep_loss = 0.9092116243389237
2023-03-27 14:11:03,404 ***** Save model *****
2023-03-27 14:11:14,139 ***** Running evaluation *****
2023-03-27 14:11:14,140   Epoch = 1 iter 299 step
2023-03-27 14:11:14,140   Num examples = 1043
2023-03-27 14:11:14,140   Batch size = 32
2023-03-27 14:11:14,142 ***** Eval results *****
2023-03-27 14:11:14,142   att_loss = 0.4300664449110627
2023-03-27 14:11:14,142   cls_loss = 0.0
2023-03-27 14:11:14,142   global_step = 299
2023-03-27 14:11:14,143   loss = 1.1647225208580494
2023-03-27 14:11:14,143   rep_loss = 0.7346560824662447
2023-03-27 14:11:14,150 ***** Save model *****
2023-03-27 14:11:24,980 ***** Running evaluation *****
2023-03-27 14:11:24,980   Epoch = 1 iter 349 step
2023-03-27 14:11:24,981   Num examples = 1043
2023-03-27 14:11:24,981   Batch size = 32
2023-03-27 14:11:24,982 ***** Eval results *****
2023-03-27 14:11:24,982   att_loss = 0.4341871356818734
2023-03-27 14:11:24,982   cls_loss = 0.0
2023-03-27 14:11:24,983   global_step = 349
2023-03-27 14:11:24,983   loss = 1.1631340544398239
2023-03-27 14:11:24,983   rep_loss = 0.7289469205751652
2023-03-27 14:11:24,990 ***** Save model *****
2023-03-27 14:11:35,892 ***** Running evaluation *****
2023-03-27 14:11:35,892   Epoch = 1 iter 399 step
2023-03-27 14:11:35,892   Num examples = 1043
2023-03-27 14:11:35,892   Batch size = 32
2023-03-27 14:11:35,893 ***** Eval results *****
2023-03-27 14:11:35,894   att_loss = 0.43267385480981885
2023-03-27 14:11:35,894   cls_loss = 0.0
2023-03-27 14:11:35,894   global_step = 399
2023-03-27 14:11:35,894   loss = 1.1566124359766643
2023-03-27 14:11:35,894   rep_loss = 0.7239385789090936
2023-03-27 14:11:35,902 ***** Save model *****
2023-03-27 14:11:46,874 ***** Running evaluation *****
2023-03-27 14:11:46,874   Epoch = 1 iter 449 step
2023-03-27 14:11:46,874   Num examples = 1043
2023-03-27 14:11:46,874   Batch size = 32
2023-03-27 14:11:46,875 ***** Eval results *****
2023-03-27 14:11:46,876   att_loss = 0.43400396161027005
2023-03-27 14:11:46,876   cls_loss = 0.0
2023-03-27 14:11:46,876   global_step = 449
2023-03-27 14:11:46,876   loss = 1.1545401772299966
2023-03-27 14:11:46,876   rep_loss = 0.7205362136547382
2023-03-27 14:11:46,883 ***** Save model *****
2023-03-27 14:11:57,896 ***** Running evaluation *****
2023-03-27 14:11:57,896   Epoch = 1 iter 499 step
2023-03-27 14:11:57,896   Num examples = 1043
2023-03-27 14:11:57,897   Batch size = 32
2023-03-27 14:11:57,898 ***** Eval results *****
2023-03-27 14:11:57,898   att_loss = 0.4307284517021015
2023-03-27 14:11:57,898   cls_loss = 0.0
2023-03-27 14:11:57,898   global_step = 499
2023-03-27 14:11:57,899   loss = 1.1468715223258938
2023-03-27 14:11:57,899   rep_loss = 0.7161430683115433
2023-03-27 14:11:57,906 ***** Save model *****
2023-03-27 14:12:08,954 ***** Running evaluation *****
2023-03-27 14:12:08,955   Epoch = 2 iter 549 step
2023-03-27 14:12:08,955   Num examples = 1043
2023-03-27 14:12:08,955   Batch size = 32
2023-03-27 14:12:08,956 ***** Eval results *****
2023-03-27 14:12:08,956   att_loss = 0.39662219484647115
2023-03-27 14:12:08,956   cls_loss = 0.0
2023-03-27 14:12:08,956   global_step = 549
2023-03-27 14:12:08,956   loss = 1.0691010157267253
2023-03-27 14:12:08,956   rep_loss = 0.6724788268407186
2023-03-27 14:12:08,963 ***** Save model *****
2023-03-27 14:12:20,036 ***** Running evaluation *****
2023-03-27 14:12:20,036   Epoch = 2 iter 599 step
2023-03-27 14:12:20,036   Num examples = 1043
2023-03-27 14:12:20,036   Batch size = 32
2023-03-27 14:12:20,038 ***** Eval results *****
2023-03-27 14:12:20,038   att_loss = 0.41310284366974465
2023-03-27 14:12:20,039   cls_loss = 0.0
2023-03-27 14:12:20,039   global_step = 599
2023-03-27 14:12:20,039   loss = 1.0961717954048744
2023-03-27 14:12:20,039   rep_loss = 0.6830689430236816
2023-03-27 14:12:20,046 ***** Save model *****
2023-03-27 14:12:31,162 ***** Running evaluation *****
2023-03-27 14:12:31,162   Epoch = 2 iter 649 step
2023-03-27 14:12:31,163   Num examples = 1043
2023-03-27 14:12:31,163   Batch size = 32
2023-03-27 14:12:31,164 ***** Eval results *****
2023-03-27 14:12:31,164   att_loss = 0.40927167768063755
2023-03-27 14:12:31,164   cls_loss = 0.0
2023-03-27 14:12:31,164   global_step = 649
2023-03-27 14:12:31,164   loss = 1.0888966332311216
2023-03-27 14:12:31,164   rep_loss = 0.6796249524406764
2023-03-27 14:12:31,171 ***** Save model *****
2023-03-27 14:12:42,317 ***** Running evaluation *****
2023-03-27 14:12:42,317   Epoch = 2 iter 699 step
2023-03-27 14:12:42,317   Num examples = 1043
2023-03-27 14:12:42,317   Batch size = 32
2023-03-27 14:12:42,318 ***** Eval results *****
2023-03-27 14:12:42,319   att_loss = 0.4119332958351482
2023-03-27 14:12:42,319   cls_loss = 0.0
2023-03-27 14:12:42,319   global_step = 699
2023-03-27 14:12:42,319   loss = 1.0910217075636892
2023-03-27 14:12:42,319   rep_loss = 0.6790884101029598
2023-03-27 14:12:42,326 ***** Save model *****
2023-03-27 14:12:53,469 ***** Running evaluation *****
2023-03-27 14:12:53,469   Epoch = 2 iter 749 step
2023-03-27 14:12:53,469   Num examples = 1043
2023-03-27 14:12:53,470   Batch size = 32
2023-03-27 14:12:53,471 ***** Eval results *****
2023-03-27 14:12:53,471   att_loss = 0.4134378297384395
2023-03-27 14:12:53,472   cls_loss = 0.0
2023-03-27 14:12:53,472   global_step = 749
2023-03-27 14:12:53,472   loss = 1.0912425207537273
2023-03-27 14:12:53,473   rep_loss = 0.6778046899063642
2023-03-27 14:12:53,480 ***** Save model *****
2023-03-27 14:13:04,651 ***** Running evaluation *****
2023-03-27 14:13:04,651   Epoch = 2 iter 799 step
2023-03-27 14:13:04,651   Num examples = 1043
2023-03-27 14:13:04,651   Batch size = 32
2023-03-27 14:13:04,652 ***** Eval results *****
2023-03-27 14:13:04,653   att_loss = 0.4134491615700272
2023-03-27 14:13:04,653   cls_loss = 0.0
2023-03-27 14:13:04,653   global_step = 799
2023-03-27 14:13:04,653   loss = 1.0893800404836547
2023-03-27 14:13:04,653   rep_loss = 0.6759308770017803
2023-03-27 14:13:04,660 ***** Save model *****
2023-03-27 14:13:15,867 ***** Running evaluation *****
2023-03-27 14:13:15,867   Epoch = 3 iter 849 step
2023-03-27 14:13:15,868   Num examples = 1043
2023-03-27 14:13:15,868   Batch size = 32
2023-03-27 14:13:15,869 ***** Eval results *****
2023-03-27 14:13:15,869   att_loss = 0.3948137064774831
2023-03-27 14:13:15,869   cls_loss = 0.0
2023-03-27 14:13:15,869   global_step = 849
2023-03-27 14:13:15,869   loss = 1.0505981830259163
2023-03-27 14:13:15,870   rep_loss = 0.6557844715813795
2023-03-27 14:13:15,877 ***** Save model *****
2023-03-27 14:13:27,103 ***** Running evaluation *****
2023-03-27 14:13:27,103   Epoch = 3 iter 899 step
2023-03-27 14:13:27,104   Num examples = 1043
2023-03-27 14:13:27,104   Batch size = 32
2023-03-27 14:13:27,105 ***** Eval results *****
2023-03-27 14:13:27,105   att_loss = 0.40008256964537564
2023-03-27 14:13:27,106   cls_loss = 0.0
2023-03-27 14:13:27,106   global_step = 899
2023-03-27 14:13:27,106   loss = 1.0546442020912559
2023-03-27 14:13:27,106   rep_loss = 0.6545616345746177
2023-03-27 14:13:27,113 ***** Save model *****
2023-03-27 14:13:38,323 ***** Running evaluation *****
2023-03-27 14:13:38,323   Epoch = 3 iter 949 step
2023-03-27 14:13:38,323   Num examples = 1043
2023-03-27 14:13:38,324   Batch size = 32
2023-03-27 14:13:38,325 ***** Eval results *****
2023-03-27 14:13:38,325   att_loss = 0.403970867395401
2023-03-27 14:13:38,325   cls_loss = 0.0
2023-03-27 14:13:38,325   global_step = 949
2023-03-27 14:13:38,325   loss = 1.059445704560022
2023-03-27 14:13:38,325   rep_loss = 0.6554748407892279
2023-03-27 14:13:38,333 ***** Save model *****
2023-03-27 14:13:49,535 ***** Running evaluation *****
2023-03-27 14:13:49,535   Epoch = 3 iter 999 step
2023-03-27 14:13:49,535   Num examples = 1043
2023-03-27 14:13:49,535   Batch size = 32
2023-03-27 14:13:49,537 ***** Eval results *****
2023-03-27 14:13:49,537   att_loss = 0.4025316471704329
2023-03-27 14:13:49,537   cls_loss = 0.0
2023-03-27 14:13:49,537   global_step = 999
2023-03-27 14:13:49,537   loss = 1.056138980870295
2023-03-27 14:13:49,537   rep_loss = 0.6536073377638152
2023-03-27 14:13:49,545 ***** Save model *****
2023-03-27 14:14:00,833 ***** Running evaluation *****
2023-03-27 14:14:00,833   Epoch = 3 iter 1049 step
2023-03-27 14:14:00,833   Num examples = 1043
2023-03-27 14:14:00,833   Batch size = 32
2023-03-27 14:14:00,835 ***** Eval results *****
2023-03-27 14:14:00,835   att_loss = 0.4021085189475167
2023-03-27 14:14:00,835   cls_loss = 0.0
2023-03-27 14:14:00,835   global_step = 1049
2023-03-27 14:14:00,835   loss = 1.0544886514544487
2023-03-27 14:14:00,835   rep_loss = 0.6523801371935876
2023-03-27 14:14:00,838 ***** Save model *****
2023-03-27 14:14:12,092 ***** Running evaluation *****
2023-03-27 14:14:12,092   Epoch = 4 iter 1099 step
2023-03-27 14:14:12,092   Num examples = 1043
2023-03-27 14:14:12,092   Batch size = 32
2023-03-27 14:14:12,093 ***** Eval results *****
2023-03-27 14:14:12,094   att_loss = 0.4054439740796243
2023-03-27 14:14:12,094   cls_loss = 0.0
2023-03-27 14:14:12,094   global_step = 1099
2023-03-27 14:14:12,094   loss = 1.0505533641384495
2023-03-27 14:14:12,095   rep_loss = 0.6451093842906337
2023-03-27 14:14:12,102 ***** Save model *****
2023-03-27 14:14:23,397 ***** Running evaluation *****
2023-03-27 14:14:23,397   Epoch = 4 iter 1149 step
2023-03-27 14:14:23,397   Num examples = 1043
2023-03-27 14:14:23,397   Batch size = 32
2023-03-27 14:14:23,399 ***** Eval results *****
2023-03-27 14:14:23,399   att_loss = 0.403627230429355
2023-03-27 14:14:23,399   cls_loss = 0.0
2023-03-27 14:14:23,399   global_step = 1149
2023-03-27 14:14:23,400   loss = 1.0472957109227592
2023-03-27 14:14:23,400   rep_loss = 0.6436684801254744
2023-03-27 14:14:23,407 ***** Save model *****
2023-03-27 14:14:34,683 ***** Running evaluation *****
2023-03-27 14:14:34,683   Epoch = 4 iter 1199 step
2023-03-27 14:14:34,683   Num examples = 1043
2023-03-27 14:14:34,684   Batch size = 32
2023-03-27 14:14:34,685 ***** Eval results *****
2023-03-27 14:14:34,685   att_loss = 0.3985099150934292
2023-03-27 14:14:34,686   cls_loss = 0.0
2023-03-27 14:14:34,686   global_step = 1199
2023-03-27 14:14:34,686   loss = 1.039177829527673
2023-03-27 14:14:34,686   rep_loss = 0.6406679135242491
2023-03-27 14:14:34,693 ***** Save model *****
2023-03-27 14:14:45,985 ***** Running evaluation *****
2023-03-27 14:14:45,986   Epoch = 4 iter 1249 step
2023-03-27 14:14:45,986   Num examples = 1043
2023-03-27 14:14:45,986   Batch size = 32
2023-03-27 14:14:45,987 ***** Eval results *****
2023-03-27 14:14:45,987   att_loss = 0.3953841019071927
2023-03-27 14:14:45,988   cls_loss = 0.0
2023-03-27 14:14:45,988   global_step = 1249
2023-03-27 14:14:45,988   loss = 1.0335068396441844
2023-03-27 14:14:45,988   rep_loss = 0.6381227354318397
2023-03-27 14:14:45,995 ***** Save model *****
2023-03-27 14:14:57,312 ***** Running evaluation *****
2023-03-27 14:14:57,312   Epoch = 4 iter 1299 step
2023-03-27 14:14:57,313   Num examples = 1043
2023-03-27 14:14:57,313   Batch size = 32
2023-03-27 14:14:57,314 ***** Eval results *****
2023-03-27 14:14:57,314   att_loss = 0.3963005565977716
2023-03-27 14:14:57,314   cls_loss = 0.0
2023-03-27 14:14:57,315   global_step = 1299
2023-03-27 14:14:57,315   loss = 1.0330461070135042
2023-03-27 14:14:57,315   rep_loss = 0.6367455486095313
2023-03-27 14:14:57,317 ***** Save model *****
2023-03-27 14:15:08,627 ***** Running evaluation *****
2023-03-27 14:15:08,628   Epoch = 5 iter 1349 step
2023-03-27 14:15:08,628   Num examples = 1043
2023-03-27 14:15:08,628   Batch size = 32
2023-03-27 14:15:08,629 ***** Eval results *****
2023-03-27 14:15:08,630   att_loss = 0.3825896169458117
2023-03-27 14:15:08,630   cls_loss = 0.0
2023-03-27 14:15:08,630   global_step = 1349
2023-03-27 14:15:08,630   loss = 1.006858697959355
2023-03-27 14:15:08,631   rep_loss = 0.6242690810135433
2023-03-27 14:15:08,638 ***** Save model *****
2023-03-27 14:15:19,982 ***** Running evaluation *****
2023-03-27 14:15:19,982   Epoch = 5 iter 1399 step
2023-03-27 14:15:19,983   Num examples = 1043
2023-03-27 14:15:19,983   Batch size = 32
2023-03-27 14:15:19,984 ***** Eval results *****
2023-03-27 14:15:19,984   att_loss = 0.39165697526186705
2023-03-27 14:15:19,985   cls_loss = 0.0
2023-03-27 14:15:19,985   global_step = 1399
2023-03-27 14:15:19,985   loss = 1.0195242576301098
2023-03-27 14:15:19,985   rep_loss = 0.627867279574275
2023-03-27 14:15:19,993 ***** Save model *****
2023-03-27 14:15:31,324 ***** Running evaluation *****
2023-03-27 14:15:31,325   Epoch = 5 iter 1449 step
2023-03-27 14:15:31,325   Num examples = 1043
2023-03-27 14:15:31,325   Batch size = 32
2023-03-27 14:15:31,326 ***** Eval results *****
2023-03-27 14:15:31,326   att_loss = 0.3876965087756776
2023-03-27 14:15:31,327   cls_loss = 0.0
2023-03-27 14:15:31,327   global_step = 1449
2023-03-27 14:15:31,327   loss = 1.0139673153559368
2023-03-27 14:15:31,327   rep_loss = 0.6262708081488024
2023-03-27 14:15:31,335 ***** Save model *****
2023-03-27 14:15:42,667 ***** Running evaluation *****
2023-03-27 14:15:42,668   Epoch = 5 iter 1499 step
2023-03-27 14:15:42,668   Num examples = 1043
2023-03-27 14:15:42,668   Batch size = 32
2023-03-27 14:15:42,670 ***** Eval results *****
2023-03-27 14:15:42,670   att_loss = 0.3887402369845204
2023-03-27 14:15:42,670   cls_loss = 0.0
2023-03-27 14:15:42,671   global_step = 1499
2023-03-27 14:15:42,671   loss = 1.0137573829511317
2023-03-27 14:15:42,671   rep_loss = 0.6250171483289905
2023-03-27 14:15:42,678 ***** Save model *****
2023-03-27 14:15:53,956 ***** Running evaluation *****
2023-03-27 14:15:53,956   Epoch = 5 iter 1549 step
2023-03-27 14:15:53,956   Num examples = 1043
2023-03-27 14:15:53,956   Batch size = 32
2023-03-27 14:15:53,957 ***** Eval results *****
2023-03-27 14:15:53,957   att_loss = 0.38855596165233686
2023-03-27 14:15:53,957   cls_loss = 0.0
2023-03-27 14:15:53,958   global_step = 1549
2023-03-27 14:15:53,958   loss = 1.0123969088090914
2023-03-27 14:15:53,958   rep_loss = 0.6238409478530705
2023-03-27 14:15:53,965 ***** Save model *****
2023-03-27 14:16:05,255 ***** Running evaluation *****
2023-03-27 14:16:05,256   Epoch = 5 iter 1599 step
2023-03-27 14:16:05,256   Num examples = 1043
2023-03-27 14:16:05,256   Batch size = 32
2023-03-27 14:16:05,257 ***** Eval results *****
2023-03-27 14:16:05,257   att_loss = 0.3891174128335534
2023-03-27 14:16:05,257   cls_loss = 0.0
2023-03-27 14:16:05,257   global_step = 1599
2023-03-27 14:16:05,258   loss = 1.012273676016114
2023-03-27 14:16:05,258   rep_loss = 0.6231562637469985
2023-03-27 14:16:05,265 ***** Save model *****
2023-03-27 14:16:16,545 ***** Running evaluation *****
2023-03-27 14:16:16,546   Epoch = 6 iter 1649 step
2023-03-27 14:16:16,546   Num examples = 1043
2023-03-27 14:16:16,546   Batch size = 32
2023-03-27 14:16:16,547 ***** Eval results *****
2023-03-27 14:16:16,548   att_loss = 0.37915646903058314
2023-03-27 14:16:16,548   cls_loss = 0.0
2023-03-27 14:16:16,548   global_step = 1649
2023-03-27 14:16:16,548   loss = 0.9920509089814856
2023-03-27 14:16:16,549   rep_loss = 0.6128944424872703
2023-03-27 14:16:16,556 ***** Save model *****
2023-03-27 14:16:27,848 ***** Running evaluation *****
2023-03-27 14:16:27,848   Epoch = 6 iter 1699 step
2023-03-27 14:16:27,848   Num examples = 1043
2023-03-27 14:16:27,849   Batch size = 32
2023-03-27 14:16:27,850 ***** Eval results *****
2023-03-27 14:16:27,850   att_loss = 0.38268627548955153
2023-03-27 14:16:27,850   cls_loss = 0.0
2023-03-27 14:16:27,850   global_step = 1699
2023-03-27 14:16:27,850   loss = 0.9963785729457423
2023-03-27 14:16:27,850   rep_loss = 0.6136922959199885
2023-03-27 14:16:27,857 ***** Save model *****
2023-03-27 14:16:39,103 ***** Running evaluation *****
2023-03-27 14:16:39,103   Epoch = 6 iter 1749 step
2023-03-27 14:16:39,103   Num examples = 1043
2023-03-27 14:16:39,103   Batch size = 32
2023-03-27 14:16:39,104 ***** Eval results *****
2023-03-27 14:16:39,104   att_loss = 0.3831973888841616
2023-03-27 14:16:39,104   cls_loss = 0.0
2023-03-27 14:16:39,104   global_step = 1749
2023-03-27 14:16:39,104   loss = 0.9965345859527588
2023-03-27 14:16:39,105   rep_loss = 0.6133371928111225
2023-03-27 14:16:39,111 ***** Save model *****
2023-03-27 14:16:50,379 ***** Running evaluation *****
2023-03-27 14:16:50,379   Epoch = 6 iter 1799 step
2023-03-27 14:16:50,379   Num examples = 1043
2023-03-27 14:16:50,380   Batch size = 32
2023-03-27 14:16:50,381 ***** Eval results *****
2023-03-27 14:16:50,381   att_loss = 0.3837708675014186
2023-03-27 14:16:50,382   cls_loss = 0.0
2023-03-27 14:16:50,382   global_step = 1799
2023-03-27 14:16:50,382   loss = 0.9967658495539942
2023-03-27 14:16:50,382   rep_loss = 0.6129949791782399
2023-03-27 14:16:50,389 ***** Save model *****
2023-03-27 14:17:01,657 ***** Running evaluation *****
2023-03-27 14:17:01,657   Epoch = 6 iter 1849 step
2023-03-27 14:17:01,657   Num examples = 1043
2023-03-27 14:17:01,658   Batch size = 32
2023-03-27 14:17:01,658 ***** Eval results *****
2023-03-27 14:17:01,658   att_loss = 0.3837572080403687
2023-03-27 14:17:01,659   cls_loss = 0.0
2023-03-27 14:17:01,659   global_step = 1849
2023-03-27 14:17:01,659   loss = 0.995730897916956
2023-03-27 14:17:01,659   rep_loss = 0.6119736879460724
2023-03-27 14:17:01,661 ***** Save model *****
2023-03-27 14:17:12,941 ***** Running evaluation *****
2023-03-27 14:17:12,941   Epoch = 7 iter 1899 step
2023-03-27 14:17:12,941   Num examples = 1043
2023-03-27 14:17:12,941   Batch size = 32
2023-03-27 14:17:12,943 ***** Eval results *****
2023-03-27 14:17:12,943   att_loss = 0.3680061727762222
2023-03-27 14:17:12,943   cls_loss = 0.0
2023-03-27 14:17:12,943   global_step = 1899
2023-03-27 14:17:12,943   loss = 0.9695807774861653
2023-03-27 14:17:12,944   rep_loss = 0.6015746057033539
2023-03-27 14:17:12,951 ***** Save model *****
2023-03-27 14:17:24,205 ***** Running evaluation *****
2023-03-27 14:17:24,205   Epoch = 7 iter 1949 step
2023-03-27 14:17:24,206   Num examples = 1043
2023-03-27 14:17:24,206   Batch size = 32
2023-03-27 14:17:24,207 ***** Eval results *****
2023-03-27 14:17:24,208   att_loss = 0.37605895958840846
2023-03-27 14:17:24,208   cls_loss = 0.0
2023-03-27 14:17:24,208   global_step = 1949
2023-03-27 14:17:24,208   loss = 0.9791359029710293
2023-03-27 14:17:24,208   rep_loss = 0.603076945245266
2023-03-27 14:17:24,210 ***** Save model *****
2023-03-27 14:17:35,426 ***** Running evaluation *****
2023-03-27 14:17:35,427   Epoch = 7 iter 1999 step
2023-03-27 14:17:35,427   Num examples = 1043
2023-03-27 14:17:35,427   Batch size = 32
2023-03-27 14:17:35,428 ***** Eval results *****
2023-03-27 14:17:35,428   att_loss = 0.3776113028709705
2023-03-27 14:17:35,428   cls_loss = 0.0
2023-03-27 14:17:35,428   global_step = 1999
2023-03-27 14:17:35,429   loss = 0.9810894507628221
2023-03-27 14:17:35,429   rep_loss = 0.6034781492673433
2023-03-27 14:17:35,436 ***** Save model *****
2023-03-27 14:17:46,668 ***** Running evaluation *****
2023-03-27 14:17:46,668   Epoch = 7 iter 2049 step
2023-03-27 14:17:46,668   Num examples = 1043
2023-03-27 14:17:46,668   Batch size = 32
2023-03-27 14:17:46,669 ***** Eval results *****
2023-03-27 14:17:46,669   att_loss = 0.37888545162147946
2023-03-27 14:17:46,669   cls_loss = 0.0
2023-03-27 14:17:46,670   global_step = 2049
2023-03-27 14:17:46,670   loss = 0.9825270805093977
2023-03-27 14:17:46,670   rep_loss = 0.6036416288879183
2023-03-27 14:17:46,676 ***** Save model *****
2023-03-27 14:17:57,877 ***** Running evaluation *****
2023-03-27 14:17:57,877   Epoch = 7 iter 2099 step
2023-03-27 14:17:57,877   Num examples = 1043
2023-03-27 14:17:57,878   Batch size = 32
2023-03-27 14:17:57,879 ***** Eval results *****
2023-03-27 14:17:57,879   att_loss = 0.37885430895763894
2023-03-27 14:17:57,879   cls_loss = 0.0
2023-03-27 14:17:57,879   global_step = 2099
2023-03-27 14:17:57,879   loss = 0.9826755585877792
2023-03-27 14:17:57,880   rep_loss = 0.6038212475569352
2023-03-27 14:17:57,881 ***** Save model *****
2023-03-27 14:18:09,118 ***** Running evaluation *****
2023-03-27 14:18:09,119   Epoch = 8 iter 2149 step
2023-03-27 14:18:09,119   Num examples = 1043
2023-03-27 14:18:09,119   Batch size = 32
2023-03-27 14:18:09,120 ***** Eval results *****
2023-03-27 14:18:09,120   att_loss = 0.37249561227284944
2023-03-27 14:18:09,120   cls_loss = 0.0
2023-03-27 14:18:09,121   global_step = 2149
2023-03-27 14:18:09,121   loss = 0.9767011495736929
2023-03-27 14:18:09,121   rep_loss = 0.6042055441783025
2023-03-27 14:18:09,123 ***** Save model *****
2023-03-27 14:18:20,381 ***** Running evaluation *****
2023-03-27 14:18:20,381   Epoch = 8 iter 2199 step
2023-03-27 14:18:20,381   Num examples = 1043
2023-03-27 14:18:20,382   Batch size = 32
2023-03-27 14:18:20,383 ***** Eval results *****
2023-03-27 14:18:20,383   att_loss = 0.3739604065342555
2023-03-27 14:18:20,383   cls_loss = 0.0
2023-03-27 14:18:20,383   global_step = 2199
2023-03-27 14:18:20,384   loss = 0.9735859604108901
2023-03-27 14:18:20,384   rep_loss = 0.5996255600263202
2023-03-27 14:18:20,391 ***** Save model *****
2023-03-27 14:18:31,666 ***** Running evaluation *****
2023-03-27 14:18:31,666   Epoch = 8 iter 2249 step
2023-03-27 14:18:31,666   Num examples = 1043
2023-03-27 14:18:31,666   Batch size = 32
2023-03-27 14:18:31,668 ***** Eval results *****
2023-03-27 14:18:31,668   att_loss = 0.37495181439197167
2023-03-27 14:18:31,668   cls_loss = 0.0
2023-03-27 14:18:31,668   global_step = 2249
2023-03-27 14:18:31,669   loss = 0.9739686002773521
2023-03-27 14:18:31,669   rep_loss = 0.5990167914238651
2023-03-27 14:18:31,676 ***** Save model *****
2023-03-27 14:18:42,929 ***** Running evaluation *****
2023-03-27 14:18:42,930   Epoch = 8 iter 2299 step
2023-03-27 14:18:42,930   Num examples = 1043
2023-03-27 14:18:42,930   Batch size = 32
2023-03-27 14:18:42,932 ***** Eval results *****
2023-03-27 14:18:42,932   att_loss = 0.37278810436008897
2023-03-27 14:18:42,932   cls_loss = 0.0
2023-03-27 14:18:42,933   global_step = 2299
2023-03-27 14:18:42,933   loss = 0.9706643439509386
2023-03-27 14:18:42,933   rep_loss = 0.5978762434304126
2023-03-27 14:18:42,935 ***** Save model *****
2023-03-27 14:18:54,135 ***** Running evaluation *****
2023-03-27 14:18:54,135   Epoch = 8 iter 2349 step
2023-03-27 14:18:54,136   Num examples = 1043
2023-03-27 14:18:54,136   Batch size = 32
2023-03-27 14:18:54,137 ***** Eval results *****
2023-03-27 14:18:54,137   att_loss = 0.37329492952342325
2023-03-27 14:18:54,137   cls_loss = 0.0
2023-03-27 14:18:54,137   global_step = 2349
2023-03-27 14:18:54,137   loss = 0.9703382127721545
2023-03-27 14:18:54,137   rep_loss = 0.5970432870264905
2023-03-27 14:18:54,144 ***** Save model *****
2023-03-27 14:19:05,401 ***** Running evaluation *****
2023-03-27 14:19:05,401   Epoch = 8 iter 2399 step
2023-03-27 14:19:05,401   Num examples = 1043
2023-03-27 14:19:05,401   Batch size = 32
2023-03-27 14:19:05,403 ***** Eval results *****
2023-03-27 14:19:05,403   att_loss = 0.3753601510941756
2023-03-27 14:19:05,403   cls_loss = 0.0
2023-03-27 14:19:05,404   global_step = 2399
2023-03-27 14:19:05,404   loss = 0.9723713835382642
2023-03-27 14:19:05,404   rep_loss = 0.5970112352770091
2023-03-27 14:19:05,411 ***** Save model *****
2023-03-27 14:19:16,672 ***** Running evaluation *****
2023-03-27 14:19:16,673   Epoch = 9 iter 2449 step
2023-03-27 14:19:16,673   Num examples = 1043
2023-03-27 14:19:16,673   Batch size = 32
2023-03-27 14:19:16,674 ***** Eval results *****
2023-03-27 14:19:16,674   att_loss = 0.37606995844322705
2023-03-27 14:19:16,674   cls_loss = 0.0
2023-03-27 14:19:16,675   global_step = 2449
2023-03-27 14:19:16,675   loss = 0.970309818568437
2023-03-27 14:19:16,675   rep_loss = 0.5942398607730865
2023-03-27 14:19:16,677 ***** Save model *****
2023-03-27 14:19:27,959 ***** Running evaluation *****
2023-03-27 14:19:27,960   Epoch = 9 iter 2499 step
2023-03-27 14:19:27,960   Num examples = 1043
2023-03-27 14:19:27,960   Batch size = 32
2023-03-27 14:19:27,961 ***** Eval results *****
2023-03-27 14:19:27,961   att_loss = 0.377351771419247
2023-03-27 14:19:27,961   cls_loss = 0.0
2023-03-27 14:19:27,961   global_step = 2499
2023-03-27 14:19:27,961   loss = 0.9706456822653612
2023-03-27 14:19:27,962   rep_loss = 0.5932939114669958
2023-03-27 14:19:27,963 ***** Save model *****
2023-03-27 14:19:39,202 ***** Running evaluation *****
2023-03-27 14:19:39,202   Epoch = 9 iter 2549 step
2023-03-27 14:19:39,202   Num examples = 1043
2023-03-27 14:19:39,203   Batch size = 32
2023-03-27 14:19:39,204 ***** Eval results *****
2023-03-27 14:19:39,204   att_loss = 0.37518155554386035
2023-03-27 14:19:39,205   cls_loss = 0.0
2023-03-27 14:19:39,205   global_step = 2549
2023-03-27 14:19:39,205   loss = 0.967600991872892
2023-03-27 14:19:39,205   rep_loss = 0.5924194369414081
2023-03-27 14:19:39,213 ***** Save model *****
2023-03-27 14:19:50,436 ***** Running evaluation *****
2023-03-27 14:19:50,436   Epoch = 9 iter 2599 step
2023-03-27 14:19:50,436   Num examples = 1043
2023-03-27 14:19:50,436   Batch size = 32
2023-03-27 14:19:50,437 ***** Eval results *****
2023-03-27 14:19:50,438   att_loss = 0.37535735827927685
2023-03-27 14:19:50,438   cls_loss = 0.0
2023-03-27 14:19:50,438   global_step = 2599
2023-03-27 14:19:50,438   loss = 0.9676433208645606
2023-03-27 14:19:50,438   rep_loss = 0.5922859615209152
2023-03-27 14:19:50,445 ***** Save model *****
2023-03-27 14:20:01,699 ***** Running evaluation *****
2023-03-27 14:20:01,699   Epoch = 9 iter 2649 step
2023-03-27 14:20:01,699   Num examples = 1043
2023-03-27 14:20:01,699   Batch size = 32
2023-03-27 14:20:01,700 ***** Eval results *****
2023-03-27 14:20:01,700   att_loss = 0.3735832095873065
2023-03-27 14:20:01,701   cls_loss = 0.0
2023-03-27 14:20:01,701   global_step = 2649
2023-03-27 14:20:01,701   loss = 0.9649801624984276
2023-03-27 14:20:01,701   rep_loss = 0.5913969506093157
2023-03-27 14:20:01,703 ***** Save model *****
2023-03-27 14:20:12,971 ***** Running evaluation *****
2023-03-27 14:20:12,971   Epoch = 10 iter 2699 step
2023-03-27 14:20:12,971   Num examples = 1043
2023-03-27 14:20:12,971   Batch size = 32
2023-03-27 14:20:12,973 ***** Eval results *****
2023-03-27 14:20:12,973   att_loss = 0.3778485018631508
2023-03-27 14:20:12,974   cls_loss = 0.0
2023-03-27 14:20:12,974   global_step = 2699
2023-03-27 14:20:12,974   loss = 0.9691038830526943
2023-03-27 14:20:12,974   rep_loss = 0.5912553709128807
2023-03-27 14:20:12,981 ***** Save model *****
2023-03-27 14:20:24,231 ***** Running evaluation *****
2023-03-27 14:20:24,231   Epoch = 10 iter 2749 step
2023-03-27 14:20:24,231   Num examples = 1043
2023-03-27 14:20:24,231   Batch size = 32
2023-03-27 14:20:24,232 ***** Eval results *****
2023-03-27 14:20:24,233   att_loss = 0.3741782605648041
2023-03-27 14:20:24,233   cls_loss = 0.0
2023-03-27 14:20:24,233   global_step = 2749
2023-03-27 14:20:24,233   loss = 0.9635008875327774
2023-03-27 14:20:24,233   rep_loss = 0.5893226243272612
2023-03-27 14:20:24,241 ***** Save model *****
2023-03-27 14:20:35,482 ***** Running evaluation *****
2023-03-27 14:20:35,482   Epoch = 10 iter 2799 step
2023-03-27 14:20:35,482   Num examples = 1043
2023-03-27 14:20:35,483   Batch size = 32
2023-03-27 14:20:35,484 ***** Eval results *****
2023-03-27 14:20:35,484   att_loss = 0.37086535816968874
2023-03-27 14:20:35,484   cls_loss = 0.0
2023-03-27 14:20:35,484   global_step = 2799
2023-03-27 14:20:35,485   loss = 0.9589860162069631
2023-03-27 14:20:35,485   rep_loss = 0.5881206568821457
2023-03-27 14:20:35,492 ***** Save model *****
2023-03-27 14:20:46,737 ***** Running evaluation *****
2023-03-27 14:20:46,738   Epoch = 10 iter 2849 step
2023-03-27 14:20:46,738   Num examples = 1043
2023-03-27 14:20:46,739   Batch size = 32
2023-03-27 14:20:46,740 ***** Eval results *****
2023-03-27 14:20:46,740   att_loss = 0.37269004746522316
2023-03-27 14:20:46,740   cls_loss = 0.0
2023-03-27 14:20:46,740   global_step = 2849
2023-03-27 14:20:46,740   loss = 0.9606597107215966
2023-03-27 14:20:46,741   rep_loss = 0.5879696627568932
2023-03-27 14:20:46,748 ***** Save model *****
2023-03-27 14:20:58,001 ***** Running evaluation *****
2023-03-27 14:20:58,001   Epoch = 10 iter 2899 step
2023-03-27 14:20:58,001   Num examples = 1043
2023-03-27 14:20:58,001   Batch size = 32
2023-03-27 14:20:58,003 ***** Eval results *****
2023-03-27 14:20:58,003   att_loss = 0.3725128065810974
2023-03-27 14:20:58,003   cls_loss = 0.0
2023-03-27 14:20:58,003   global_step = 2899
2023-03-27 14:20:58,003   loss = 0.9593727932226189
2023-03-27 14:20:58,003   rep_loss = 0.5868599872922272
2023-03-27 14:20:58,005 ***** Save model *****
2023-03-27 14:21:09,254 ***** Running evaluation *****
2023-03-27 14:21:09,254   Epoch = 11 iter 2949 step
2023-03-27 14:21:09,254   Num examples = 1043
2023-03-27 14:21:09,254   Batch size = 32
2023-03-27 14:21:09,255 ***** Eval results *****
2023-03-27 14:21:09,255   att_loss = 0.37356120596329373
2023-03-27 14:21:09,256   cls_loss = 0.0
2023-03-27 14:21:09,256   global_step = 2949
2023-03-27 14:21:09,256   loss = 0.9578048288822174
2023-03-27 14:21:09,256   rep_loss = 0.5842436204353968
2023-03-27 14:21:09,263 ***** Save model *****
2023-03-27 14:21:20,536 ***** Running evaluation *****
2023-03-27 14:21:20,536   Epoch = 11 iter 2999 step
2023-03-27 14:21:20,536   Num examples = 1043
2023-03-27 14:21:20,537   Batch size = 32
2023-03-27 14:21:20,539 ***** Eval results *****
2023-03-27 14:21:20,539   att_loss = 0.3657850844244803
2023-03-27 14:21:20,539   cls_loss = 0.0
2023-03-27 14:21:20,539   global_step = 2999
2023-03-27 14:21:20,540   loss = 0.9461709030212895
2023-03-27 14:21:20,540   rep_loss = 0.5803858195581744
2023-03-27 14:21:20,542 ***** Save model *****
2023-03-27 14:21:31,828 ***** Running evaluation *****
2023-03-27 14:21:31,828   Epoch = 11 iter 3049 step
2023-03-27 14:21:31,829   Num examples = 1043
2023-03-27 14:21:31,829   Batch size = 32
2023-03-27 14:21:31,830 ***** Eval results *****
2023-03-27 14:21:31,830   att_loss = 0.3677858928484576
2023-03-27 14:21:31,830   cls_loss = 0.0
2023-03-27 14:21:31,830   global_step = 3049
2023-03-27 14:21:31,831   loss = 0.9492979012429714
2023-03-27 14:21:31,831   rep_loss = 0.5815120099910668
2023-03-27 14:21:31,838 ***** Save model *****
2023-03-27 14:21:43,092 ***** Running evaluation *****
2023-03-27 14:21:43,092   Epoch = 11 iter 3099 step
2023-03-27 14:21:43,093   Num examples = 1043
2023-03-27 14:21:43,093   Batch size = 32
2023-03-27 14:21:43,094 ***** Eval results *****
2023-03-27 14:21:43,094   att_loss = 0.3670987071078501
2023-03-27 14:21:43,094   cls_loss = 0.0
2023-03-27 14:21:43,094   global_step = 3099
2023-03-27 14:21:43,095   loss = 0.9486219058802099
2023-03-27 14:21:43,095   rep_loss = 0.5815231987723598
2023-03-27 14:21:43,102 ***** Save model *****
2023-03-27 14:21:54,352 ***** Running evaluation *****
2023-03-27 14:21:54,353   Epoch = 11 iter 3149 step
2023-03-27 14:21:54,353   Num examples = 1043
2023-03-27 14:21:54,353   Batch size = 32
2023-03-27 14:21:54,354 ***** Eval results *****
2023-03-27 14:21:54,355   att_loss = 0.3682414715020162
2023-03-27 14:21:54,355   cls_loss = 0.0
2023-03-27 14:21:54,355   global_step = 3149
2023-03-27 14:21:54,355   loss = 0.9490287838117132
2023-03-27 14:21:54,356   rep_loss = 0.5807873125908509
2023-03-27 14:21:54,363 ***** Save model *****
2023-03-27 14:22:05,629 ***** Running evaluation *****
2023-03-27 14:22:05,629   Epoch = 11 iter 3199 step
2023-03-27 14:22:05,629   Num examples = 1043
2023-03-27 14:22:05,629   Batch size = 32
2023-03-27 14:22:05,631 ***** Eval results *****
2023-03-27 14:22:05,631   att_loss = 0.3688863188255834
2023-03-27 14:22:05,631   cls_loss = 0.0
2023-03-27 14:22:05,631   global_step = 3199
2023-03-27 14:22:05,632   loss = 0.9498974917499163
2023-03-27 14:22:05,632   rep_loss = 0.5810111726968343
2023-03-27 14:22:05,639 ***** Save model *****
2023-03-27 14:22:16,908 ***** Running evaluation *****
2023-03-27 14:22:16,908   Epoch = 12 iter 3249 step
2023-03-27 14:22:16,909   Num examples = 1043
2023-03-27 14:22:16,909   Batch size = 32
2023-03-27 14:22:16,910 ***** Eval results *****
2023-03-27 14:22:16,910   att_loss = 0.36401362816492716
2023-03-27 14:22:16,910   cls_loss = 0.0
2023-03-27 14:22:16,911   global_step = 3249
2023-03-27 14:22:16,911   loss = 0.9390530043178135
2023-03-27 14:22:16,911   rep_loss = 0.5750393761528863
2023-03-27 14:22:16,918 ***** Save model *****
2023-03-27 14:22:28,188 ***** Running evaluation *****
2023-03-27 14:22:28,189   Epoch = 12 iter 3299 step
2023-03-27 14:22:28,189   Num examples = 1043
2023-03-27 14:22:28,189   Batch size = 32
2023-03-27 14:22:28,191 ***** Eval results *****
2023-03-27 14:22:28,191   att_loss = 0.36827822735435084
2023-03-27 14:22:28,191   cls_loss = 0.0
2023-03-27 14:22:28,191   global_step = 3299
2023-03-27 14:22:28,192   loss = 0.9449168286825481
2023-03-27 14:22:28,192   rep_loss = 0.5766386032104492
2023-03-27 14:22:28,199 ***** Save model *****
2023-03-27 14:22:39,523 ***** Running evaluation *****
2023-03-27 14:22:39,524   Epoch = 12 iter 3349 step
2023-03-27 14:22:39,524   Num examples = 1043
2023-03-27 14:22:39,524   Batch size = 32
2023-03-27 14:22:39,525 ***** Eval results *****
2023-03-27 14:22:39,525   att_loss = 0.3675343817677991
2023-03-27 14:22:39,525   cls_loss = 0.0
2023-03-27 14:22:39,526   global_step = 3349
2023-03-27 14:22:39,526   loss = 0.9441294793424935
2023-03-27 14:22:39,526   rep_loss = 0.57659510045216
2023-03-27 14:22:39,534 ***** Save model *****
2023-03-27 14:22:50,776 ***** Running evaluation *****
2023-03-27 14:22:50,776   Epoch = 12 iter 3399 step
2023-03-27 14:22:50,776   Num examples = 1043
2023-03-27 14:22:50,777   Batch size = 32
2023-03-27 14:22:50,777 ***** Eval results *****
2023-03-27 14:22:50,778   att_loss = 0.3665121289399954
2023-03-27 14:22:50,778   cls_loss = 0.0
2023-03-27 14:22:50,778   global_step = 3399
2023-03-27 14:22:50,778   loss = 0.9435237578856639
2023-03-27 14:22:50,778   rep_loss = 0.5770116310853225
2023-03-27 14:22:50,780 ***** Save model *****
2023-03-27 14:23:02,047 ***** Running evaluation *****
2023-03-27 14:23:02,047   Epoch = 12 iter 3449 step
2023-03-27 14:23:02,048   Num examples = 1043
2023-03-27 14:23:02,048   Batch size = 32
2023-03-27 14:23:02,049 ***** Eval results *****
2023-03-27 14:23:02,049   att_loss = 0.36674884423917653
2023-03-27 14:23:02,049   cls_loss = 0.0
2023-03-27 14:23:02,050   global_step = 3449
2023-03-27 14:23:02,050   loss = 0.9436628682272775
2023-03-27 14:23:02,050   rep_loss = 0.5769140245963116
2023-03-27 14:23:02,053 ***** Save model *****
2023-03-27 14:23:13,286 ***** Running evaluation *****
2023-03-27 14:23:13,287   Epoch = 13 iter 3499 step
2023-03-27 14:23:13,287   Num examples = 1043
2023-03-27 14:23:13,287   Batch size = 32
2023-03-27 14:23:13,288 ***** Eval results *****
2023-03-27 14:23:13,289   att_loss = 0.3755521146314485
2023-03-27 14:23:13,289   cls_loss = 0.0
2023-03-27 14:23:13,289   global_step = 3499
2023-03-27 14:23:13,289   loss = 0.9527277094977242
2023-03-27 14:23:13,290   rep_loss = 0.5771755959306445
2023-03-27 14:23:13,297 ***** Save model *****
2023-03-27 14:23:24,556 ***** Running evaluation *****
2023-03-27 14:23:24,556   Epoch = 13 iter 3549 step
2023-03-27 14:23:24,556   Num examples = 1043
2023-03-27 14:23:24,556   Batch size = 32
2023-03-27 14:23:24,557 ***** Eval results *****
2023-03-27 14:23:24,557   att_loss = 0.3685899510597571
2023-03-27 14:23:24,558   cls_loss = 0.0
2023-03-27 14:23:24,558   global_step = 3549
2023-03-27 14:23:24,558   loss = 0.9436863699020483
2023-03-27 14:23:24,558   rep_loss = 0.5750964184602102
2023-03-27 14:23:24,565 ***** Save model *****
2023-03-27 14:23:35,818 ***** Running evaluation *****
2023-03-27 14:23:35,818   Epoch = 13 iter 3599 step
2023-03-27 14:23:35,818   Num examples = 1043
2023-03-27 14:23:35,819   Batch size = 32
2023-03-27 14:23:35,820 ***** Eval results *****
2023-03-27 14:23:35,820   att_loss = 0.36805658671073616
2023-03-27 14:23:35,821   cls_loss = 0.0
2023-03-27 14:23:35,821   global_step = 3599
2023-03-27 14:23:35,821   loss = 0.9426065948791802
2023-03-27 14:23:35,821   rep_loss = 0.5745500093325973
2023-03-27 14:23:35,823 ***** Save model *****
2023-03-27 14:23:47,062 ***** Running evaluation *****
2023-03-27 14:23:47,062   Epoch = 13 iter 3649 step
2023-03-27 14:23:47,062   Num examples = 1043
2023-03-27 14:23:47,063   Batch size = 32
2023-03-27 14:23:47,064 ***** Eval results *****
2023-03-27 14:23:47,064   att_loss = 0.36668392245689135
2023-03-27 14:23:47,064   cls_loss = 0.0
2023-03-27 14:23:47,064   global_step = 3649
2023-03-27 14:23:47,064   loss = 0.9407992028118519
2023-03-27 14:23:47,064   rep_loss = 0.5741152813595333
2023-03-27 14:23:47,066 ***** Save model *****
2023-03-27 14:23:58,316 ***** Running evaluation *****
2023-03-27 14:23:58,317   Epoch = 13 iter 3699 step
2023-03-27 14:23:58,317   Num examples = 1043
2023-03-27 14:23:58,317   Batch size = 32
2023-03-27 14:23:58,318 ***** Eval results *****
2023-03-27 14:23:58,318   att_loss = 0.36648582902393845
2023-03-27 14:23:58,318   cls_loss = 0.0
2023-03-27 14:23:58,319   global_step = 3699
2023-03-27 14:23:58,319   loss = 0.9406257520119349
2023-03-27 14:23:58,319   rep_loss = 0.5741399236415562
2023-03-27 14:23:58,326 ***** Save model *****
2023-03-27 14:24:09,578 ***** Running evaluation *****
2023-03-27 14:24:09,578   Epoch = 14 iter 3749 step
2023-03-27 14:24:09,578   Num examples = 1043
2023-03-27 14:24:09,579   Batch size = 32
2023-03-27 14:24:09,580 ***** Eval results *****
2023-03-27 14:24:09,580   att_loss = 0.3633784218267961
2023-03-27 14:24:09,580   cls_loss = 0.0
2023-03-27 14:24:09,581   global_step = 3749
2023-03-27 14:24:09,581   loss = 0.9308598854325034
2023-03-27 14:24:09,581   rep_loss = 0.5674814527684992
2023-03-27 14:24:09,588 ***** Save model *****
2023-03-27 14:24:20,823 ***** Running evaluation *****
2023-03-27 14:24:20,823   Epoch = 14 iter 3799 step
2023-03-27 14:24:20,823   Num examples = 1043
2023-03-27 14:24:20,823   Batch size = 32
2023-03-27 14:24:20,824 ***** Eval results *****
2023-03-27 14:24:20,825   att_loss = 0.361314805804706
2023-03-27 14:24:20,825   cls_loss = 0.0
2023-03-27 14:24:20,825   global_step = 3799
2023-03-27 14:24:20,825   loss = 0.9305757938838396
2023-03-27 14:24:20,826   rep_loss = 0.569260986124883
2023-03-27 14:24:20,833 ***** Save model *****
2023-03-27 14:24:32,076 ***** Running evaluation *****
2023-03-27 14:24:32,076   Epoch = 14 iter 3849 step
2023-03-27 14:24:32,077   Num examples = 1043
2023-03-27 14:24:32,077   Batch size = 32
2023-03-27 14:24:32,078 ***** Eval results *****
2023-03-27 14:24:32,078   att_loss = 0.35830378935143753
2023-03-27 14:24:32,078   cls_loss = 0.0
2023-03-27 14:24:32,079   global_step = 3849
2023-03-27 14:24:32,079   loss = 0.9264817753353635
2023-03-27 14:24:32,079   rep_loss = 0.5681779841045002
2023-03-27 14:24:32,086 ***** Save model *****
2023-03-27 14:24:43,353 ***** Running evaluation *****
2023-03-27 14:24:43,353   Epoch = 14 iter 3899 step
2023-03-27 14:24:43,353   Num examples = 1043
2023-03-27 14:24:43,354   Batch size = 32
2023-03-27 14:24:43,355 ***** Eval results *****
2023-03-27 14:24:43,355   att_loss = 0.3631305705686534
2023-03-27 14:24:43,356   cls_loss = 0.0
2023-03-27 14:24:43,356   global_step = 3899
2023-03-27 14:24:43,356   loss = 0.9324258373390814
2023-03-27 14:24:43,356   rep_loss = 0.5692952652895673
2023-03-27 14:24:43,358 ***** Save model *****
2023-03-27 14:24:54,633 ***** Running evaluation *****
2023-03-27 14:24:54,633   Epoch = 14 iter 3949 step
2023-03-27 14:24:54,634   Num examples = 1043
2023-03-27 14:24:54,634   Batch size = 32
2023-03-27 14:24:54,635 ***** Eval results *****
2023-03-27 14:24:54,635   att_loss = 0.3654983689152234
2023-03-27 14:24:54,635   cls_loss = 0.0
2023-03-27 14:24:54,636   global_step = 3949
2023-03-27 14:24:54,636   loss = 0.936089210883136
2023-03-27 14:24:54,636   rep_loss = 0.57059084097921
2023-03-27 14:24:54,643 ***** Save model *****
2023-03-27 14:25:05,878 ***** Running evaluation *****
2023-03-27 14:25:05,879   Epoch = 14 iter 3999 step
2023-03-27 14:25:05,879   Num examples = 1043
2023-03-27 14:25:05,879   Batch size = 32
2023-03-27 14:25:05,880 ***** Eval results *****
2023-03-27 14:25:05,880   att_loss = 0.3648476863272802
2023-03-27 14:25:05,880   cls_loss = 0.0
2023-03-27 14:25:05,880   global_step = 3999
2023-03-27 14:25:05,880   loss = 0.934935813205909
2023-03-27 14:25:05,880   rep_loss = 0.5700881257367774
2023-03-27 14:25:05,888 ***** Save model *****
2023-03-27 14:25:17,119 ***** Running evaluation *****
2023-03-27 14:25:17,119   Epoch = 15 iter 4049 step
2023-03-27 14:25:17,120   Num examples = 1043
2023-03-27 14:25:17,120   Batch size = 32
2023-03-27 14:25:17,121 ***** Eval results *****
2023-03-27 14:25:17,121   att_loss = 0.35664178363301535
2023-03-27 14:25:17,122   cls_loss = 0.0
2023-03-27 14:25:17,122   global_step = 4049
2023-03-27 14:25:17,122   loss = 0.9225755794481798
2023-03-27 14:25:17,122   rep_loss = 0.5659337964924899
2023-03-27 14:25:17,129 ***** Save model *****
2023-03-27 14:25:28,345 ***** Running evaluation *****
2023-03-27 14:25:28,345   Epoch = 15 iter 4099 step
2023-03-27 14:25:28,345   Num examples = 1043
2023-03-27 14:25:28,345   Batch size = 32
2023-03-27 14:25:28,347 ***** Eval results *****
2023-03-27 14:25:28,347   att_loss = 0.35876276994005163
2023-03-27 14:25:28,347   cls_loss = 0.0
2023-03-27 14:25:28,347   global_step = 4099
2023-03-27 14:25:28,347   loss = 0.9252404944693788
2023-03-27 14:25:28,348   rep_loss = 0.5664777248463733
2023-03-27 14:25:28,350 ***** Save model *****
2023-03-27 14:25:39,611 ***** Running evaluation *****
2023-03-27 14:25:39,611   Epoch = 15 iter 4149 step
2023-03-27 14:25:39,611   Num examples = 1043
2023-03-27 14:25:39,611   Batch size = 32
2023-03-27 14:25:39,612 ***** Eval results *****
2023-03-27 14:25:39,612   att_loss = 0.3592638565848271
2023-03-27 14:25:39,613   cls_loss = 0.0
2023-03-27 14:25:39,613   global_step = 4149
2023-03-27 14:25:39,613   loss = 0.9258616541822752
2023-03-27 14:25:39,613   rep_loss = 0.5665977973904874
2023-03-27 14:25:39,620 ***** Save model *****
2023-03-27 14:25:50,870 ***** Running evaluation *****
2023-03-27 14:25:50,871   Epoch = 15 iter 4199 step
2023-03-27 14:25:50,871   Num examples = 1043
2023-03-27 14:25:50,871   Batch size = 32
2023-03-27 14:25:50,873 ***** Eval results *****
2023-03-27 14:25:50,873   att_loss = 0.3603966518775704
2023-03-27 14:25:50,873   cls_loss = 0.0
2023-03-27 14:25:50,873   global_step = 4199
2023-03-27 14:25:50,873   loss = 0.927643975338985
2023-03-27 14:25:50,873   rep_loss = 0.5672473225396933
2023-03-27 14:25:50,880 ***** Save model *****
2023-03-27 14:26:02,133 ***** Running evaluation *****
2023-03-27 14:26:02,133   Epoch = 15 iter 4249 step
2023-03-27 14:26:02,133   Num examples = 1043
2023-03-27 14:26:02,133   Batch size = 32
2023-03-27 14:26:02,134 ***** Eval results *****
2023-03-27 14:26:02,134   att_loss = 0.36146229884175
2023-03-27 14:26:02,134   cls_loss = 0.0
2023-03-27 14:26:02,135   global_step = 4249
2023-03-27 14:26:02,135   loss = 0.9286112028067229
2023-03-27 14:26:02,135   rep_loss = 0.5671489033542696
2023-03-27 14:26:02,137 ***** Save model *****
2023-03-27 14:26:13,395 ***** Running evaluation *****
2023-03-27 14:26:13,396   Epoch = 16 iter 4299 step
2023-03-27 14:26:13,396   Num examples = 1043
2023-03-27 14:26:13,396   Batch size = 32
2023-03-27 14:26:13,397 ***** Eval results *****
2023-03-27 14:26:13,398   att_loss = 0.35486351008768435
2023-03-27 14:26:13,398   cls_loss = 0.0
2023-03-27 14:26:13,398   global_step = 4299
2023-03-27 14:26:13,398   loss = 0.9176156587070889
2023-03-27 14:26:13,398   rep_loss = 0.5627521497231943
2023-03-27 14:26:13,405 ***** Save model *****
2023-03-27 14:26:24,668 ***** Running evaluation *****
2023-03-27 14:26:24,668   Epoch = 16 iter 4349 step
2023-03-27 14:26:24,668   Num examples = 1043
2023-03-27 14:26:24,668   Batch size = 32
2023-03-27 14:26:24,670 ***** Eval results *****
2023-03-27 14:26:24,670   att_loss = 0.35528324718599197
2023-03-27 14:26:24,670   cls_loss = 0.0
2023-03-27 14:26:24,671   global_step = 4349
2023-03-27 14:26:24,671   loss = 0.9197121874078528
2023-03-27 14:26:24,671   rep_loss = 0.5644289394477745
2023-03-27 14:26:24,673 ***** Save model *****
2023-03-27 14:26:35,919 ***** Running evaluation *****
2023-03-27 14:26:35,920   Epoch = 16 iter 4399 step
2023-03-27 14:26:35,920   Num examples = 1043
2023-03-27 14:26:35,920   Batch size = 32
2023-03-27 14:26:35,921 ***** Eval results *****
2023-03-27 14:26:35,921   att_loss = 0.3597015470970334
2023-03-27 14:26:35,922   cls_loss = 0.0
2023-03-27 14:26:35,922   global_step = 4399
2023-03-27 14:26:35,922   loss = 0.9241072460422366
2023-03-27 14:26:35,922   rep_loss = 0.5644056970678916
2023-03-27 14:26:35,929 ***** Save model *****
2023-03-27 14:26:47,188 ***** Running evaluation *****
2023-03-27 14:26:47,189   Epoch = 16 iter 4449 step
2023-03-27 14:26:47,189   Num examples = 1043
2023-03-27 14:26:47,189   Batch size = 32
2023-03-27 14:26:47,190 ***** Eval results *****
2023-03-27 14:26:47,190   att_loss = 0.36183081255794247
2023-03-27 14:26:47,191   cls_loss = 0.0
2023-03-27 14:26:47,191   global_step = 4449
2023-03-27 14:26:47,191   loss = 0.9269797970346139
2023-03-27 14:26:47,191   rep_loss = 0.5651489826245496
2023-03-27 14:26:47,199 ***** Save model *****
2023-03-27 14:26:58,460 ***** Running evaluation *****
2023-03-27 14:26:58,461   Epoch = 16 iter 4499 step
2023-03-27 14:26:58,461   Num examples = 1043
2023-03-27 14:26:58,461   Batch size = 32
2023-03-27 14:26:58,462 ***** Eval results *****
2023-03-27 14:26:58,463   att_loss = 0.3611216550356491
2023-03-27 14:26:58,463   cls_loss = 0.0
2023-03-27 14:26:58,463   global_step = 4499
2023-03-27 14:26:58,463   loss = 0.9258502708657723
2023-03-27 14:26:58,463   rep_loss = 0.5647286139920945
2023-03-27 14:26:58,470 ***** Save model *****
2023-03-27 14:27:09,734 ***** Running evaluation *****
2023-03-27 14:27:09,735   Epoch = 17 iter 4549 step
2023-03-27 14:27:09,735   Num examples = 1043
2023-03-27 14:27:09,735   Batch size = 32
2023-03-27 14:27:09,736 ***** Eval results *****
2023-03-27 14:27:09,736   att_loss = 0.3545015811920166
2023-03-27 14:27:09,737   cls_loss = 0.0
2023-03-27 14:27:09,737   global_step = 4549
2023-03-27 14:27:09,737   loss = 0.9135738670825958
2023-03-27 14:27:09,737   rep_loss = 0.5590722799301148
2023-03-27 14:27:09,745 ***** Save model *****
2023-03-27 14:27:20,983 ***** Running evaluation *****
2023-03-27 14:27:20,984   Epoch = 17 iter 4599 step
2023-03-27 14:27:20,984   Num examples = 1043
2023-03-27 14:27:20,984   Batch size = 32
2023-03-27 14:27:20,985 ***** Eval results *****
2023-03-27 14:27:20,985   att_loss = 0.3561781495809555
2023-03-27 14:27:20,985   cls_loss = 0.0
2023-03-27 14:27:20,986   global_step = 4599
2023-03-27 14:27:20,986   loss = 0.9163094222545624
2023-03-27 14:27:20,986   rep_loss = 0.5601312726736069
2023-03-27 14:27:20,993 ***** Save model *****
2023-03-27 14:27:32,275 ***** Running evaluation *****
2023-03-27 14:27:32,276   Epoch = 17 iter 4649 step
2023-03-27 14:27:32,276   Num examples = 1043
2023-03-27 14:27:32,276   Batch size = 32
2023-03-27 14:27:32,277 ***** Eval results *****
2023-03-27 14:27:32,277   att_loss = 0.3577201648191972
2023-03-27 14:27:32,278   cls_loss = 0.0
2023-03-27 14:27:32,278   global_step = 4649
2023-03-27 14:27:32,278   loss = 0.9184086160226301
2023-03-27 14:27:32,278   rep_loss = 0.5606884517452934
2023-03-27 14:27:32,280 ***** Save model *****
2023-03-27 14:27:43,545 ***** Running evaluation *****
2023-03-27 14:27:43,545   Epoch = 17 iter 4699 step
2023-03-27 14:27:43,545   Num examples = 1043
2023-03-27 14:27:43,545   Batch size = 32
2023-03-27 14:27:43,546 ***** Eval results *****
2023-03-27 14:27:43,546   att_loss = 0.35805344898253677
2023-03-27 14:27:43,547   cls_loss = 0.0
2023-03-27 14:27:43,547   global_step = 4699
2023-03-27 14:27:43,547   loss = 0.9187954448163509
2023-03-27 14:27:43,547   rep_loss = 0.5607419952750206
2023-03-27 14:27:43,554 ***** Save model *****
2023-03-27 14:27:54,833 ***** Running evaluation *****
2023-03-27 14:27:54,833   Epoch = 17 iter 4749 step
2023-03-27 14:27:54,833   Num examples = 1043
2023-03-27 14:27:54,833   Batch size = 32
2023-03-27 14:27:54,834 ***** Eval results *****
2023-03-27 14:27:54,834   att_loss = 0.3596189423685982
2023-03-27 14:27:54,834   cls_loss = 0.0
2023-03-27 14:27:54,835   global_step = 4749
2023-03-27 14:27:54,835   loss = 0.9208216635953812
2023-03-27 14:27:54,835   rep_loss = 0.5612027210848672
2023-03-27 14:27:54,842 ***** Save model *****
2023-03-27 14:28:06,102 ***** Running evaluation *****
2023-03-27 14:28:06,102   Epoch = 17 iter 4799 step
2023-03-27 14:28:06,102   Num examples = 1043
2023-03-27 14:28:06,102   Batch size = 32
2023-03-27 14:28:06,104 ***** Eval results *****
2023-03-27 14:28:06,104   att_loss = 0.35991380902437065
2023-03-27 14:28:06,105   cls_loss = 0.0
2023-03-27 14:28:06,105   global_step = 4799
2023-03-27 14:28:06,105   loss = 0.921135202050209
2023-03-27 14:28:06,105   rep_loss = 0.5612213925673412
2023-03-27 14:28:06,113 ***** Save model *****
2023-03-27 14:28:17,353 ***** Running evaluation *****
2023-03-27 14:28:17,353   Epoch = 18 iter 4849 step
2023-03-27 14:28:17,353   Num examples = 1043
2023-03-27 14:28:17,353   Batch size = 32
2023-03-27 14:28:17,354 ***** Eval results *****
2023-03-27 14:28:17,355   att_loss = 0.3606119744999464
2023-03-27 14:28:17,355   cls_loss = 0.0
2023-03-27 14:28:17,355   global_step = 4849
2023-03-27 14:28:17,355   loss = 0.9198890952176826
2023-03-27 14:28:17,355   rep_loss = 0.5592771158661953
2023-03-27 14:28:17,363 ***** Save model *****
2023-03-27 14:28:28,650 ***** Running evaluation *****
2023-03-27 14:28:28,651   Epoch = 18 iter 4899 step
2023-03-27 14:28:28,651   Num examples = 1043
2023-03-27 14:28:28,651   Batch size = 32
2023-03-27 14:28:28,652 ***** Eval results *****
2023-03-27 14:28:28,652   att_loss = 0.3603455571718113
2023-03-27 14:28:28,652   cls_loss = 0.0
2023-03-27 14:28:28,652   global_step = 4899
2023-03-27 14:28:28,652   loss = 0.9191303362128556
2023-03-27 14:28:28,652   rep_loss = 0.5587847777592239
2023-03-27 14:28:28,659 ***** Save model *****
2023-03-27 14:28:39,937 ***** Running evaluation *****
2023-03-27 14:28:39,937   Epoch = 18 iter 4949 step
2023-03-27 14:28:39,937   Num examples = 1043
2023-03-27 14:28:39,938   Batch size = 32
2023-03-27 14:28:39,939 ***** Eval results *****
2023-03-27 14:28:39,939   att_loss = 0.3577866810601908
2023-03-27 14:28:39,939   cls_loss = 0.0
2023-03-27 14:28:39,939   global_step = 4949
2023-03-27 14:28:39,939   loss = 0.9164324377800201
2023-03-27 14:28:39,939   rep_loss = 0.5586457565114215
2023-03-27 14:28:39,947 ***** Save model *****
2023-03-27 14:28:51,217 ***** Running evaluation *****
2023-03-27 14:28:51,218   Epoch = 18 iter 4999 step
2023-03-27 14:28:51,218   Num examples = 1043
2023-03-27 14:28:51,218   Batch size = 32
2023-03-27 14:28:51,219 ***** Eval results *****
2023-03-27 14:28:51,220   att_loss = 0.3584821424953678
2023-03-27 14:28:51,220   cls_loss = 0.0
2023-03-27 14:28:51,220   global_step = 4999
2023-03-27 14:28:51,220   loss = 0.91769359179729
2023-03-27 14:28:51,220   rep_loss = 0.5592114499195869
2023-03-27 14:28:51,228 ***** Save model *****
2023-03-27 14:29:02,497 ***** Running evaluation *****
2023-03-27 14:29:02,497   Epoch = 18 iter 5049 step
2023-03-27 14:29:02,497   Num examples = 1043
2023-03-27 14:29:02,497   Batch size = 32
2023-03-27 14:29:02,499 ***** Eval results *****
2023-03-27 14:29:02,499   att_loss = 0.35883482297261554
2023-03-27 14:29:02,499   cls_loss = 0.0
2023-03-27 14:29:02,499   global_step = 5049
2023-03-27 14:29:02,500   loss = 0.918224887838089
2023-03-27 14:29:02,500   rep_loss = 0.55939006511076
2023-03-27 14:29:02,507 ***** Save model *****
2023-03-27 14:29:13,755 ***** Running evaluation *****
2023-03-27 14:29:13,756   Epoch = 19 iter 5099 step
2023-03-27 14:29:13,756   Num examples = 1043
2023-03-27 14:29:13,756   Batch size = 32
2023-03-27 14:29:13,758 ***** Eval results *****
2023-03-27 14:29:13,758   att_loss = 0.3549548742862848
2023-03-27 14:29:13,759   cls_loss = 0.0
2023-03-27 14:29:13,759   global_step = 5099
2023-03-27 14:29:13,759   loss = 0.9101639137818263
2023-03-27 14:29:13,759   rep_loss = 0.555209036056812
2023-03-27 14:29:13,767 ***** Save model *****
2023-03-27 14:29:25,044 ***** Running evaluation *****
2023-03-27 14:29:25,045   Epoch = 19 iter 5149 step
2023-03-27 14:29:25,045   Num examples = 1043
2023-03-27 14:29:25,045   Batch size = 32
2023-03-27 14:29:25,046 ***** Eval results *****
2023-03-27 14:29:25,046   att_loss = 0.3505826604209448
2023-03-27 14:29:25,046   cls_loss = 0.0
2023-03-27 14:29:25,046   global_step = 5149
2023-03-27 14:29:25,046   loss = 0.9039634732823623
2023-03-27 14:29:25,047   rep_loss = 0.5533808140378249
2023-03-27 14:29:25,049 ***** Save model *****
2023-03-27 14:29:36,340 ***** Running evaluation *****
2023-03-27 14:29:36,341   Epoch = 19 iter 5199 step
2023-03-27 14:29:36,341   Num examples = 1043
2023-03-27 14:29:36,341   Batch size = 32
2023-03-27 14:29:36,342 ***** Eval results *****
2023-03-27 14:29:36,342   att_loss = 0.3531617914873456
2023-03-27 14:29:36,343   cls_loss = 0.0
2023-03-27 14:29:36,343   global_step = 5199
2023-03-27 14:29:36,343   loss = 0.9071955879529318
2023-03-27 14:29:36,343   rep_loss = 0.5540337974116916
2023-03-27 14:29:36,350 ***** Save model *****
2023-03-27 14:29:47,635 ***** Running evaluation *****
2023-03-27 14:29:47,635   Epoch = 19 iter 5249 step
2023-03-27 14:29:47,635   Num examples = 1043
2023-03-27 14:29:47,636   Batch size = 32
2023-03-27 14:29:47,637 ***** Eval results *****
2023-03-27 14:29:47,637   att_loss = 0.35502941127527843
2023-03-27 14:29:47,638   cls_loss = 0.0
2023-03-27 14:29:47,638   global_step = 5249
2023-03-27 14:29:47,638   loss = 0.9098794375630942
2023-03-27 14:29:47,638   rep_loss = 0.554850025949153
2023-03-27 14:29:47,646 ***** Save model *****
2023-03-27 14:29:58,965 ***** Running evaluation *****
2023-03-27 14:29:58,965   Epoch = 19 iter 5299 step
2023-03-27 14:29:58,965   Num examples = 1043
2023-03-27 14:29:58,965   Batch size = 32
2023-03-27 14:29:58,966 ***** Eval results *****
2023-03-27 14:29:58,966   att_loss = 0.3566045816493245
2023-03-27 14:29:58,967   cls_loss = 0.0
2023-03-27 14:29:58,967   global_step = 5299
2023-03-27 14:29:58,967   loss = 0.912560475874791
2023-03-27 14:29:58,967   rep_loss = 0.5559558936979918
2023-03-27 14:29:58,974 ***** Save model *****
2023-03-27 14:30:10,242 ***** Running evaluation *****
2023-03-27 14:30:10,243   Epoch = 20 iter 5349 step
2023-03-27 14:30:10,243   Num examples = 1043
2023-03-27 14:30:10,243   Batch size = 32
2023-03-27 14:30:10,244 ***** Eval results *****
2023-03-27 14:30:10,244   att_loss = 0.35213108857472736
2023-03-27 14:30:10,244   cls_loss = 0.0
2023-03-27 14:30:10,244   global_step = 5349
2023-03-27 14:30:10,244   loss = 0.9054079982969496
2023-03-27 14:30:10,244   rep_loss = 0.5532769097222222
2023-03-27 14:30:10,249 ***** Save model *****
2023-03-27 14:30:21,501 ***** Running evaluation *****
2023-03-27 14:30:21,501   Epoch = 20 iter 5399 step
2023-03-27 14:30:21,501   Num examples = 1043
2023-03-27 14:30:21,501   Batch size = 32
2023-03-27 14:30:21,503 ***** Eval results *****
2023-03-27 14:30:21,503   att_loss = 0.35217027593467193
2023-03-27 14:30:21,503   cls_loss = 0.0
2023-03-27 14:30:21,504   global_step = 5399
2023-03-27 14:30:21,504   loss = 0.9058200282565618
2023-03-27 14:30:21,504   rep_loss = 0.5536497508065176
2023-03-27 14:30:21,510 ***** Save model *****
2023-03-27 14:30:32,785 ***** Running evaluation *****
2023-03-27 14:30:32,786   Epoch = 20 iter 5449 step
2023-03-27 14:30:32,786   Num examples = 1043
2023-03-27 14:30:32,786   Batch size = 32
2023-03-27 14:30:32,787 ***** Eval results *****
2023-03-27 14:30:32,787   att_loss = 0.3531615444279592
2023-03-27 14:30:32,787   cls_loss = 0.0
2023-03-27 14:30:32,787   global_step = 5449
2023-03-27 14:30:32,788   loss = 0.9075280766968333
2023-03-27 14:30:32,788   rep_loss = 0.5543665306283794
2023-03-27 14:30:32,795 ***** Save model *****
2023-03-27 14:30:44,053 ***** Running evaluation *****
2023-03-27 14:30:44,053   Epoch = 20 iter 5499 step
2023-03-27 14:30:44,054   Num examples = 1043
2023-03-27 14:30:44,054   Batch size = 32
2023-03-27 14:30:44,055 ***** Eval results *****
2023-03-27 14:30:44,055   att_loss = 0.35326949436709565
2023-03-27 14:30:44,056   cls_loss = 0.0
2023-03-27 14:30:44,056   global_step = 5499
2023-03-27 14:30:44,056   loss = 0.9076275461874668
2023-03-27 14:30:44,056   rep_loss = 0.5543580497585753
2023-03-27 14:30:44,063 ***** Save model *****
2023-03-27 14:30:55,337 ***** Running evaluation *****
2023-03-27 14:30:55,340   Epoch = 20 iter 5549 step
2023-03-27 14:30:55,340   Num examples = 1043
2023-03-27 14:30:55,341   Batch size = 32
2023-03-27 14:30:55,342 ***** Eval results *****
2023-03-27 14:30:55,342   att_loss = 0.3555484535306264
2023-03-27 14:30:55,342   cls_loss = 0.0
2023-03-27 14:30:55,343   global_step = 5549
2023-03-27 14:30:55,343   loss = 0.9104477138610548
2023-03-27 14:30:55,343   rep_loss = 0.5548992596174541
2023-03-27 14:30:55,351 ***** Save model *****
2023-03-27 14:31:06,594 ***** Running evaluation *****
2023-03-27 14:31:06,594   Epoch = 20 iter 5599 step
2023-03-27 14:31:06,594   Num examples = 1043
2023-03-27 14:31:06,594   Batch size = 32
2023-03-27 14:31:06,596 ***** Eval results *****
2023-03-27 14:31:06,596   att_loss = 0.355227511476826
2023-03-27 14:31:06,596   cls_loss = 0.0
2023-03-27 14:31:06,596   global_step = 5599
2023-03-27 14:31:06,596   loss = 0.9099252502430359
2023-03-27 14:31:06,596   rep_loss = 0.554697737730608
2023-03-27 14:31:06,598 ***** Save model *****
2023-03-27 14:31:17,858 ***** Running evaluation *****
2023-03-27 14:31:17,859   Epoch = 21 iter 5649 step
2023-03-27 14:31:17,859   Num examples = 1043
2023-03-27 14:31:17,859   Batch size = 32
2023-03-27 14:31:17,860 ***** Eval results *****
2023-03-27 14:31:17,860   att_loss = 0.3574288771266029
2023-03-27 14:31:17,860   cls_loss = 0.0
2023-03-27 14:31:17,860   global_step = 5649
2023-03-27 14:31:17,861   loss = 0.9101725660619282
2023-03-27 14:31:17,861   rep_loss = 0.5527436903544835
2023-03-27 14:31:17,868 ***** Save model *****
2023-03-27 14:31:29,147 ***** Running evaluation *****
2023-03-27 14:31:29,148   Epoch = 21 iter 5699 step
2023-03-27 14:31:29,148   Num examples = 1043
2023-03-27 14:31:29,148   Batch size = 32
2023-03-27 14:31:29,150 ***** Eval results *****
2023-03-27 14:31:29,150   att_loss = 0.3562379683489385
2023-03-27 14:31:29,150   cls_loss = 0.0
2023-03-27 14:31:29,151   global_step = 5699
2023-03-27 14:31:29,151   loss = 0.9104550454927527
2023-03-27 14:31:29,151   rep_loss = 0.5542170742283696
2023-03-27 14:31:29,158 ***** Save model *****
2023-03-27 14:31:40,429 ***** Running evaluation *****
2023-03-27 14:31:40,430   Epoch = 21 iter 5749 step
2023-03-27 14:31:40,430   Num examples = 1043
2023-03-27 14:31:40,430   Batch size = 32
2023-03-27 14:31:40,431 ***** Eval results *****
2023-03-27 14:31:40,431   att_loss = 0.3538544465538482
2023-03-27 14:31:40,432   cls_loss = 0.0
2023-03-27 14:31:40,432   global_step = 5749
2023-03-27 14:31:40,432   loss = 0.906833461892437
2023-03-27 14:31:40,432   rep_loss = 0.552979010931203
2023-03-27 14:31:40,439 ***** Save model *****
2023-03-27 14:31:51,709 ***** Running evaluation *****
2023-03-27 14:31:51,710   Epoch = 21 iter 5799 step
2023-03-27 14:31:51,710   Num examples = 1043
2023-03-27 14:31:51,710   Batch size = 32
2023-03-27 14:31:51,711 ***** Eval results *****
2023-03-27 14:31:51,711   att_loss = 0.35456382436677814
2023-03-27 14:31:51,711   cls_loss = 0.0
2023-03-27 14:31:51,712   global_step = 5799
2023-03-27 14:31:51,712   loss = 0.9079253058880568
2023-03-27 14:31:51,712   rep_loss = 0.5533614785720905
2023-03-27 14:31:51,719 ***** Save model *****
2023-03-27 14:32:02,991 ***** Running evaluation *****
2023-03-27 14:32:02,991   Epoch = 21 iter 5849 step
2023-03-27 14:32:02,991   Num examples = 1043
2023-03-27 14:32:02,991   Batch size = 32
2023-03-27 14:32:02,992 ***** Eval results *****
2023-03-27 14:32:02,993   att_loss = 0.35501208063984707
2023-03-27 14:32:02,993   cls_loss = 0.0
2023-03-27 14:32:02,993   global_step = 5849
2023-03-27 14:32:02,993   loss = 0.9084060982731749
2023-03-27 14:32:02,993   rep_loss = 0.5533940154166261
2023-03-27 14:32:03,000 ***** Save model *****
2023-03-27 14:32:14,291 ***** Running evaluation *****
2023-03-27 14:32:14,291   Epoch = 22 iter 5899 step
2023-03-27 14:32:14,292   Num examples = 1043
2023-03-27 14:32:14,292   Batch size = 32
2023-03-27 14:32:14,293 ***** Eval results *****
2023-03-27 14:32:14,293   att_loss = 0.34554816484451295
2023-03-27 14:32:14,294   cls_loss = 0.0
2023-03-27 14:32:14,294   global_step = 5899
2023-03-27 14:32:14,294   loss = 0.8960385489463806
2023-03-27 14:32:14,294   rep_loss = 0.5504903864860534
2023-03-27 14:32:14,302 ***** Save model *****
2023-03-27 14:32:25,593 ***** Running evaluation *****
2023-03-27 14:32:25,594   Epoch = 22 iter 5949 step
2023-03-27 14:32:25,594   Num examples = 1043
2023-03-27 14:32:25,595   Batch size = 32
2023-03-27 14:32:25,596 ***** Eval results *****
2023-03-27 14:32:25,596   att_loss = 0.35059054374694826
2023-03-27 14:32:25,596   cls_loss = 0.0
2023-03-27 14:32:25,597   global_step = 5949
2023-03-27 14:32:25,597   loss = 0.8990005882581075
2023-03-27 14:32:25,597   rep_loss = 0.5484100453058879
2023-03-27 14:32:25,605 ***** Save model *****
2023-03-27 14:32:36,874 ***** Running evaluation *****
2023-03-27 14:32:36,874   Epoch = 22 iter 5999 step
2023-03-27 14:32:36,874   Num examples = 1043
2023-03-27 14:32:36,874   Batch size = 32
2023-03-27 14:32:36,876 ***** Eval results *****
2023-03-27 14:32:36,877   att_loss = 0.35119980430603026
2023-03-27 14:32:36,877   cls_loss = 0.0
2023-03-27 14:32:36,877   global_step = 5999
2023-03-27 14:32:36,877   loss = 0.9007394161224366
2023-03-27 14:32:36,877   rep_loss = 0.5495396127700806
2023-03-27 14:32:36,880 ***** Save model *****
2023-03-27 14:32:48,143 ***** Running evaluation *****
2023-03-27 14:32:48,144   Epoch = 22 iter 6049 step
2023-03-27 14:32:48,144   Num examples = 1043
2023-03-27 14:32:48,144   Batch size = 32
2023-03-27 14:32:48,145 ***** Eval results *****
2023-03-27 14:32:48,145   att_loss = 0.3527162730693817
2023-03-27 14:32:48,146   cls_loss = 0.0
2023-03-27 14:32:48,146   global_step = 6049
2023-03-27 14:32:48,146   loss = 0.9033918823514666
2023-03-27 14:32:48,146   rep_loss = 0.5506756104741778
2023-03-27 14:32:48,148 ***** Save model *****
2023-03-27 14:32:59,388 ***** Running evaluation *****
2023-03-27 14:32:59,388   Epoch = 22 iter 6099 step
2023-03-27 14:32:59,388   Num examples = 1043
2023-03-27 14:32:59,388   Batch size = 32
2023-03-27 14:32:59,389 ***** Eval results *****
2023-03-27 14:32:59,390   att_loss = 0.354451904296875
2023-03-27 14:32:59,390   cls_loss = 0.0
2023-03-27 14:32:59,390   global_step = 6099
2023-03-27 14:32:59,390   loss = 0.9052893543243408
2023-03-27 14:32:59,391   rep_loss = 0.5508374508221944
2023-03-27 14:32:59,398 ***** Save model *****
2023-03-27 14:33:10,695 ***** Running evaluation *****
2023-03-27 14:33:10,695   Epoch = 23 iter 6149 step
2023-03-27 14:33:10,696   Num examples = 1043
2023-03-27 14:33:10,696   Batch size = 32
2023-03-27 14:33:10,697 ***** Eval results *****
2023-03-27 14:33:10,697   att_loss = 0.3573148846626282
2023-03-27 14:33:10,698   cls_loss = 0.0
2023-03-27 14:33:10,698   global_step = 6149
2023-03-27 14:33:10,698   loss = 0.9072972163558006
2023-03-27 14:33:10,698   rep_loss = 0.549982339143753
2023-03-27 14:33:10,705 ***** Save model *****
2023-03-27 14:33:21,985 ***** Running evaluation *****
2023-03-27 14:33:21,985   Epoch = 23 iter 6199 step
2023-03-27 14:33:21,985   Num examples = 1043
2023-03-27 14:33:21,985   Batch size = 32
2023-03-27 14:33:21,986 ***** Eval results *****
2023-03-27 14:33:21,987   att_loss = 0.35313930079854766
2023-03-27 14:33:21,987   cls_loss = 0.0
2023-03-27 14:33:21,987   global_step = 6199
2023-03-27 14:33:21,987   loss = 0.9040112392655735
2023-03-27 14:33:21,987   rep_loss = 0.5508719405223583
2023-03-27 14:33:21,994 ***** Save model *****
2023-03-27 14:33:33,263 ***** Running evaluation *****
2023-03-27 14:33:33,264   Epoch = 23 iter 6249 step
2023-03-27 14:33:33,264   Num examples = 1043
2023-03-27 14:33:33,264   Batch size = 32
2023-03-27 14:33:33,265 ***** Eval results *****
2023-03-27 14:33:33,265   att_loss = 0.3556817000110944
2023-03-27 14:33:33,265   cls_loss = 0.0
2023-03-27 14:33:33,266   global_step = 6249
2023-03-27 14:33:33,266   loss = 0.9062765875348339
2023-03-27 14:33:33,266   rep_loss = 0.5505948883515818
2023-03-27 14:33:33,273 ***** Save model *****
2023-03-27 14:33:44,571 ***** Running evaluation *****
2023-03-27 14:33:44,571   Epoch = 23 iter 6299 step
2023-03-27 14:33:44,572   Num examples = 1043
2023-03-27 14:33:44,572   Batch size = 32
2023-03-27 14:33:44,573 ***** Eval results *****
2023-03-27 14:33:44,574   att_loss = 0.35452684363986875
2023-03-27 14:33:44,574   cls_loss = 0.0
2023-03-27 14:33:44,574   global_step = 6299
2023-03-27 14:33:44,574   loss = 0.9047295998168897
2023-03-27 14:33:44,575   rep_loss = 0.5502027563656433
2023-03-27 14:33:44,577 ***** Save model *****
2023-03-27 14:33:55,839 ***** Running evaluation *****
2023-03-27 14:33:55,840   Epoch = 23 iter 6349 step
2023-03-27 14:33:55,840   Num examples = 1043
2023-03-27 14:33:55,840   Batch size = 32
2023-03-27 14:33:55,841 ***** Eval results *****
2023-03-27 14:33:55,842   att_loss = 0.3544137552380562
2023-03-27 14:33:55,842   cls_loss = 0.0
2023-03-27 14:33:55,842   global_step = 6349
2023-03-27 14:33:55,842   loss = 0.9041727494734985
2023-03-27 14:33:55,842   rep_loss = 0.5497589953816854
2023-03-27 14:33:55,850 ***** Save model *****
2023-03-27 14:34:07,142 ***** Running evaluation *****
2023-03-27 14:34:07,143   Epoch = 23 iter 6399 step
2023-03-27 14:34:07,143   Num examples = 1043
2023-03-27 14:34:07,143   Batch size = 32
2023-03-27 14:34:07,144 ***** Eval results *****
2023-03-27 14:34:07,144   att_loss = 0.3542936869369921
2023-03-27 14:34:07,144   cls_loss = 0.0
2023-03-27 14:34:07,144   global_step = 6399
2023-03-27 14:34:07,144   loss = 0.9043235298275023
2023-03-27 14:34:07,144   rep_loss = 0.5500298435835875
2023-03-27 14:34:07,151 ***** Save model *****
2023-03-27 14:34:18,429 ***** Running evaluation *****
2023-03-27 14:34:18,429   Epoch = 24 iter 6449 step
2023-03-27 14:34:18,429   Num examples = 1043
2023-03-27 14:34:18,429   Batch size = 32
2023-03-27 14:34:18,431 ***** Eval results *****
2023-03-27 14:34:18,431   att_loss = 0.35174437412401527
2023-03-27 14:34:18,431   cls_loss = 0.0
2023-03-27 14:34:18,431   global_step = 6449
2023-03-27 14:34:18,432   loss = 0.8973404372610697
2023-03-27 14:34:18,432   rep_loss = 0.5455960704059135
2023-03-27 14:34:18,439 ***** Save model *****
2023-03-27 14:34:29,704 ***** Running evaluation *****
2023-03-27 14:34:29,704   Epoch = 24 iter 6499 step
2023-03-27 14:34:29,705   Num examples = 1043
2023-03-27 14:34:29,705   Batch size = 32
2023-03-27 14:34:29,706 ***** Eval results *****
2023-03-27 14:34:29,706   att_loss = 0.35144626275523677
2023-03-27 14:34:29,707   cls_loss = 0.0
2023-03-27 14:34:29,707   global_step = 6499
2023-03-27 14:34:29,707   loss = 0.8977541399526072
2023-03-27 14:34:29,707   rep_loss = 0.5463078801448529
2023-03-27 14:34:29,715 ***** Save model *****
2023-03-27 14:34:40,964 ***** Running evaluation *****
2023-03-27 14:34:40,964   Epoch = 24 iter 6549 step
2023-03-27 14:34:40,964   Num examples = 1043
2023-03-27 14:34:40,964   Batch size = 32
2023-03-27 14:34:40,965 ***** Eval results *****
2023-03-27 14:34:40,966   att_loss = 0.3500900942805811
2023-03-27 14:34:40,966   cls_loss = 0.0
2023-03-27 14:34:40,966   global_step = 6549
2023-03-27 14:34:40,966   loss = 0.896411621401496
2023-03-27 14:34:40,966   rep_loss = 0.5463215286004628
2023-03-27 14:34:40,969 ***** Save model *****
2023-03-27 14:34:52,253 ***** Running evaluation *****
2023-03-27 14:34:52,253   Epoch = 24 iter 6599 step
2023-03-27 14:34:52,253   Num examples = 1043
2023-03-27 14:34:52,253   Batch size = 32
2023-03-27 14:34:52,255 ***** Eval results *****
2023-03-27 14:34:52,255   att_loss = 0.35255671109204517
2023-03-27 14:34:52,255   cls_loss = 0.0
2023-03-27 14:34:52,255   global_step = 6599
2023-03-27 14:34:52,255   loss = 0.8998240772342183
2023-03-27 14:34:52,255   rep_loss = 0.5472673661421731
2023-03-27 14:34:52,262 ***** Save model *****
2023-03-27 14:35:03,552 ***** Running evaluation *****
2023-03-27 14:35:03,552   Epoch = 24 iter 6649 step
2023-03-27 14:35:03,553   Num examples = 1043
2023-03-27 14:35:03,553   Batch size = 32
2023-03-27 14:35:03,554 ***** Eval results *****
2023-03-27 14:35:03,554   att_loss = 0.3523872902779164
2023-03-27 14:35:03,554   cls_loss = 0.0
2023-03-27 14:35:03,554   global_step = 6649
2023-03-27 14:35:03,554   loss = 0.8997482486780254
2023-03-27 14:35:03,554   rep_loss = 0.5473609588947533
2023-03-27 14:35:03,560 ***** Save model *****
2023-03-27 14:35:14,809 ***** Running evaluation *****
2023-03-27 14:35:14,809   Epoch = 25 iter 6699 step
2023-03-27 14:35:14,809   Num examples = 1043
2023-03-27 14:35:14,809   Batch size = 32
2023-03-27 14:35:14,810 ***** Eval results *****
2023-03-27 14:35:14,810   att_loss = 0.3481614577273528
2023-03-27 14:35:14,810   cls_loss = 0.0
2023-03-27 14:35:14,811   global_step = 6699
2023-03-27 14:35:14,811   loss = 0.8953962425390879
2023-03-27 14:35:14,811   rep_loss = 0.5472347910205523
2023-03-27 14:35:14,818 ***** Save model *****
2023-03-27 14:35:26,123 ***** Running evaluation *****
2023-03-27 14:35:26,123   Epoch = 25 iter 6749 step
2023-03-27 14:35:26,123   Num examples = 1043
2023-03-27 14:35:26,123   Batch size = 32
2023-03-27 14:35:26,125 ***** Eval results *****
2023-03-27 14:35:26,125   att_loss = 0.35218436935463465
2023-03-27 14:35:26,125   cls_loss = 0.0
2023-03-27 14:35:26,125   global_step = 6749
2023-03-27 14:35:26,125   loss = 0.8996054505979693
2023-03-27 14:35:26,126   rep_loss = 0.5474210856734095
2023-03-27 14:35:26,128 ***** Save model *****
2023-03-27 14:35:37,390 ***** Running evaluation *****
2023-03-27 14:35:37,391   Epoch = 25 iter 6799 step
2023-03-27 14:35:37,391   Num examples = 1043
2023-03-27 14:35:37,391   Batch size = 32
2023-03-27 14:35:37,392 ***** Eval results *****
2023-03-27 14:35:37,392   att_loss = 0.3513663043418238
2023-03-27 14:35:37,393   cls_loss = 0.0
2023-03-27 14:35:37,393   global_step = 6799
2023-03-27 14:35:37,393   loss = 0.8973875656243293
2023-03-27 14:35:37,393   rep_loss = 0.5460212644069425
2023-03-27 14:35:37,395 ***** Save model *****
2023-03-27 14:35:48,661 ***** Running evaluation *****
2023-03-27 14:35:48,661   Epoch = 25 iter 6849 step
2023-03-27 14:35:48,662   Num examples = 1043
2023-03-27 14:35:48,662   Batch size = 32
2023-03-27 14:35:48,663 ***** Eval results *****
2023-03-27 14:35:48,663   att_loss = 0.35155741185292433
2023-03-27 14:35:48,663   cls_loss = 0.0
2023-03-27 14:35:48,664   global_step = 6849
2023-03-27 14:35:48,664   loss = 0.8972644641481596
2023-03-27 14:35:48,664   rep_loss = 0.5457070555495119
2023-03-27 14:35:48,671 ***** Save model *****
2023-03-27 14:35:59,884 ***** Running evaluation *****
2023-03-27 14:35:59,884   Epoch = 25 iter 6899 step
2023-03-27 14:35:59,885   Num examples = 1043
2023-03-27 14:35:59,885   Batch size = 32
2023-03-27 14:35:59,886 ***** Eval results *****
2023-03-27 14:35:59,887   att_loss = 0.35101380571722984
2023-03-27 14:35:59,887   cls_loss = 0.0
2023-03-27 14:35:59,887   global_step = 6899
2023-03-27 14:35:59,887   loss = 0.896530303039721
2023-03-27 14:35:59,887   rep_loss = 0.5455164994512286
2023-03-27 14:35:59,895 ***** Save model *****
2023-03-27 14:36:11,173 ***** Running evaluation *****
2023-03-27 14:36:11,173   Epoch = 26 iter 6949 step
2023-03-27 14:36:11,173   Num examples = 1043
2023-03-27 14:36:11,173   Batch size = 32
2023-03-27 14:36:11,174 ***** Eval results *****
2023-03-27 14:36:11,175   att_loss = 0.3517813895429884
2023-03-27 14:36:11,175   cls_loss = 0.0
2023-03-27 14:36:11,175   global_step = 6949
2023-03-27 14:36:11,175   loss = 0.8914167199816022
2023-03-27 14:36:11,175   rep_loss = 0.5396353346960885
2023-03-27 14:36:11,183 ***** Save model *****
2023-03-27 14:36:22,454 ***** Running evaluation *****
2023-03-27 14:36:22,454   Epoch = 26 iter 6999 step
2023-03-27 14:36:22,454   Num examples = 1043
2023-03-27 14:36:22,454   Batch size = 32
2023-03-27 14:36:22,455 ***** Eval results *****
2023-03-27 14:36:22,456   att_loss = 0.3519183512319598
2023-03-27 14:36:22,456   cls_loss = 0.0
2023-03-27 14:36:22,456   global_step = 6999
2023-03-27 14:36:22,456   loss = 0.8969227180146334
2023-03-27 14:36:22,456   rep_loss = 0.5450043657369781
2023-03-27 14:36:22,464 ***** Save model *****
2023-03-27 14:36:33,729 ***** Running evaluation *****
2023-03-27 14:36:33,729   Epoch = 26 iter 7049 step
2023-03-27 14:36:33,729   Num examples = 1043
2023-03-27 14:36:33,730   Batch size = 32
2023-03-27 14:36:33,731 ***** Eval results *****
2023-03-27 14:36:33,731   att_loss = 0.35151463830582447
2023-03-27 14:36:33,731   cls_loss = 0.0
2023-03-27 14:36:33,731   global_step = 7049
2023-03-27 14:36:33,732   loss = 0.895769428984027
2023-03-27 14:36:33,732   rep_loss = 0.5442547903996762
2023-03-27 14:36:33,734 ***** Save model *****
2023-03-27 14:36:45,009 ***** Running evaluation *****
2023-03-27 14:36:45,009   Epoch = 26 iter 7099 step
2023-03-27 14:36:45,010   Num examples = 1043
2023-03-27 14:36:45,010   Batch size = 32
2023-03-27 14:36:45,011 ***** Eval results *****
2023-03-27 14:36:45,011   att_loss = 0.35170709802086947
2023-03-27 14:36:45,012   cls_loss = 0.0
2023-03-27 14:36:45,012   global_step = 7099
2023-03-27 14:36:45,012   loss = 0.8958577980661089
2023-03-27 14:36:45,012   rep_loss = 0.5441506994757682
2023-03-27 14:36:45,019 ***** Save model *****
2023-03-27 14:36:56,299 ***** Running evaluation *****
2023-03-27 14:36:56,300   Epoch = 26 iter 7149 step
2023-03-27 14:36:56,300   Num examples = 1043
2023-03-27 14:36:56,300   Batch size = 32
2023-03-27 14:36:56,301 ***** Eval results *****
2023-03-27 14:36:56,301   att_loss = 0.34997836547197353
2023-03-27 14:36:56,302   cls_loss = 0.0
2023-03-27 14:36:56,302   global_step = 7149
2023-03-27 14:36:56,302   loss = 0.8941319083821946
2023-03-27 14:36:56,302   rep_loss = 0.5441535420463857
2023-03-27 14:36:56,309 ***** Save model *****
2023-03-27 14:37:07,592 ***** Running evaluation *****
2023-03-27 14:37:07,593   Epoch = 26 iter 7199 step
2023-03-27 14:37:07,593   Num examples = 1043
2023-03-27 14:37:07,594   Batch size = 32
2023-03-27 14:37:07,596 ***** Eval results *****
2023-03-27 14:37:07,596   att_loss = 0.35059102743516174
2023-03-27 14:37:07,596   cls_loss = 0.0
2023-03-27 14:37:07,597   global_step = 7199
2023-03-27 14:37:07,597   loss = 0.894721011707291
2023-03-27 14:37:07,597   rep_loss = 0.5441299829965436
2023-03-27 14:37:07,605 ***** Save model *****
2023-03-27 14:37:18,877 ***** Running evaluation *****
2023-03-27 14:37:18,877   Epoch = 27 iter 7249 step
2023-03-27 14:37:18,878   Num examples = 1043
2023-03-27 14:37:18,878   Batch size = 32
2023-03-27 14:37:18,879 ***** Eval results *****
2023-03-27 14:37:18,879   att_loss = 0.35017412155866623
2023-03-27 14:37:18,879   cls_loss = 0.0
2023-03-27 14:37:18,879   global_step = 7249
2023-03-27 14:37:18,879   loss = 0.8936395853757858
2023-03-27 14:37:18,879   rep_loss = 0.5434654653072357
2023-03-27 14:37:18,887 ***** Save model *****
2023-03-27 14:37:30,149 ***** Running evaluation *****
2023-03-27 14:37:30,150   Epoch = 27 iter 7299 step
2023-03-27 14:37:30,150   Num examples = 1043
2023-03-27 14:37:30,150   Batch size = 32
2023-03-27 14:37:30,151 ***** Eval results *****
2023-03-27 14:37:30,151   att_loss = 0.35182080500655705
2023-03-27 14:37:30,152   cls_loss = 0.0
2023-03-27 14:37:30,152   global_step = 7299
2023-03-27 14:37:30,152   loss = 0.8960857576794095
2023-03-27 14:37:30,152   rep_loss = 0.5442649549908108
2023-03-27 14:37:30,159 ***** Save model *****
2023-03-27 14:37:41,422 ***** Running evaluation *****
2023-03-27 14:37:41,422   Epoch = 27 iter 7349 step
2023-03-27 14:37:41,422   Num examples = 1043
2023-03-27 14:37:41,422   Batch size = 32
2023-03-27 14:37:41,424 ***** Eval results *****
2023-03-27 14:37:41,424   att_loss = 0.35082505877528875
2023-03-27 14:37:41,424   cls_loss = 0.0
2023-03-27 14:37:41,424   global_step = 7349
2023-03-27 14:37:41,424   loss = 0.8936547377279827
2023-03-27 14:37:41,424   rep_loss = 0.5428296817200524
2023-03-27 14:37:41,431 ***** Save model *****
2023-03-27 14:37:52,698 ***** Running evaluation *****
2023-03-27 14:37:52,698   Epoch = 27 iter 7399 step
2023-03-27 14:37:52,698   Num examples = 1043
2023-03-27 14:37:52,699   Batch size = 32
2023-03-27 14:37:52,700 ***** Eval results *****
2023-03-27 14:37:52,700   att_loss = 0.35163068096888694
2023-03-27 14:37:52,700   cls_loss = 0.0
2023-03-27 14:37:52,700   global_step = 7399
2023-03-27 14:37:52,700   loss = 0.8952247585120954
2023-03-27 14:37:52,700   rep_loss = 0.5435940798960234
2023-03-27 14:37:52,708 ***** Save model *****
2023-03-27 14:38:03,998 ***** Running evaluation *****
2023-03-27 14:38:03,999   Epoch = 27 iter 7449 step
2023-03-27 14:38:03,999   Num examples = 1043
2023-03-27 14:38:03,999   Batch size = 32
2023-03-27 14:38:04,000 ***** Eval results *****
2023-03-27 14:38:04,000   att_loss = 0.3507948748767376
2023-03-27 14:38:04,001   cls_loss = 0.0
2023-03-27 14:38:04,001   global_step = 7449
2023-03-27 14:38:04,001   loss = 0.8940333264569441
2023-03-27 14:38:04,001   rep_loss = 0.5432384538153807
2023-03-27 14:38:04,009 ***** Save model *****
2023-03-27 14:38:15,311 ***** Running evaluation *****
2023-03-27 14:38:15,311   Epoch = 28 iter 7499 step
2023-03-27 14:38:15,311   Num examples = 1043
2023-03-27 14:38:15,311   Batch size = 32
2023-03-27 14:38:15,313 ***** Eval results *****
2023-03-27 14:38:15,313   att_loss = 0.3523081320783366
2023-03-27 14:38:15,314   cls_loss = 0.0
2023-03-27 14:38:15,314   global_step = 7499
2023-03-27 14:38:15,314   loss = 0.8935459774473439
2023-03-27 14:38:15,314   rep_loss = 0.5412378440732541
2023-03-27 14:38:15,321 ***** Save model *****
2023-03-27 14:38:26,589 ***** Running evaluation *****
2023-03-27 14:38:26,589   Epoch = 28 iter 7549 step
2023-03-27 14:38:26,589   Num examples = 1043
2023-03-27 14:38:26,589   Batch size = 32
2023-03-27 14:38:26,591 ***** Eval results *****
2023-03-27 14:38:26,591   att_loss = 0.35173274392951026
2023-03-27 14:38:26,591   cls_loss = 0.0
2023-03-27 14:38:26,591   global_step = 7549
2023-03-27 14:38:26,592   loss = 0.8945626771613343
2023-03-27 14:38:26,592   rep_loss = 0.542829934048326
2023-03-27 14:38:26,599 ***** Save model *****
2023-03-27 14:38:37,867 ***** Running evaluation *****
2023-03-27 14:38:37,868   Epoch = 28 iter 7599 step
2023-03-27 14:38:37,868   Num examples = 1043
2023-03-27 14:38:37,868   Batch size = 32
2023-03-27 14:38:37,869 ***** Eval results *****
2023-03-27 14:38:37,869   att_loss = 0.3524727230149556
2023-03-27 14:38:37,869   cls_loss = 0.0
2023-03-27 14:38:37,869   global_step = 7599
2023-03-27 14:38:37,869   loss = 0.8957376799932341
2023-03-27 14:38:37,869   rep_loss = 0.5432649569782785
2023-03-27 14:38:37,876 ***** Save model *****
2023-03-27 14:38:49,164 ***** Running evaluation *****
2023-03-27 14:38:49,165   Epoch = 28 iter 7649 step
2023-03-27 14:38:49,165   Num examples = 1043
2023-03-27 14:38:49,166   Batch size = 32
2023-03-27 14:38:49,167 ***** Eval results *****
2023-03-27 14:38:49,168   att_loss = 0.3525202777344367
2023-03-27 14:38:49,168   cls_loss = 0.0
2023-03-27 14:38:49,168   global_step = 7649
2023-03-27 14:38:49,168   loss = 0.8958187430580228
2023-03-27 14:38:49,169   rep_loss = 0.5432984660126571
2023-03-27 14:38:49,176 ***** Save model *****
2023-03-27 14:39:00,459 ***** Running evaluation *****
2023-03-27 14:39:00,460   Epoch = 28 iter 7699 step
2023-03-27 14:39:00,460   Num examples = 1043
2023-03-27 14:39:00,460   Batch size = 32
2023-03-27 14:39:00,461 ***** Eval results *****
2023-03-27 14:39:00,461   att_loss = 0.35001271655741295
2023-03-27 14:39:00,461   cls_loss = 0.0
2023-03-27 14:39:00,461   global_step = 7699
2023-03-27 14:39:00,461   loss = 0.8923889703280188
2023-03-27 14:39:00,462   rep_loss = 0.5423762547061047
2023-03-27 14:39:00,469 ***** Save model *****
2023-03-27 14:39:11,767 ***** Running evaluation *****
2023-03-27 14:39:11,768   Epoch = 29 iter 7749 step
2023-03-27 14:39:11,768   Num examples = 1043
2023-03-27 14:39:11,768   Batch size = 32
2023-03-27 14:39:11,769 ***** Eval results *****
2023-03-27 14:39:11,769   att_loss = 0.33887263139088947
2023-03-27 14:39:11,770   cls_loss = 0.0
2023-03-27 14:39:11,770   global_step = 7749
2023-03-27 14:39:11,770   loss = 0.8726019958655039
2023-03-27 14:39:11,770   rep_loss = 0.5337293644746145
2023-03-27 14:39:11,778 ***** Save model *****
2023-03-27 14:39:23,037 ***** Running evaluation *****
2023-03-27 14:39:23,037   Epoch = 29 iter 7799 step
2023-03-27 14:39:23,037   Num examples = 1043
2023-03-27 14:39:23,037   Batch size = 32
2023-03-27 14:39:23,039 ***** Eval results *****
2023-03-27 14:39:23,039   att_loss = 0.34361826202699114
2023-03-27 14:39:23,040   cls_loss = 0.0
2023-03-27 14:39:23,040   global_step = 7799
2023-03-27 14:39:23,040   loss = 0.8812104921255793
2023-03-27 14:39:23,040   rep_loss = 0.5375922322273254
2023-03-27 14:39:23,048 ***** Save model *****
2023-03-27 14:39:34,335 ***** Running evaluation *****
2023-03-27 14:39:34,335   Epoch = 29 iter 7849 step
2023-03-27 14:39:34,335   Num examples = 1043
2023-03-27 14:39:34,335   Batch size = 32
2023-03-27 14:39:34,336 ***** Eval results *****
2023-03-27 14:39:34,336   att_loss = 0.34778975906237114
2023-03-27 14:39:34,336   cls_loss = 0.0
2023-03-27 14:39:34,337   global_step = 7849
2023-03-27 14:39:34,337   loss = 0.8872956605452411
2023-03-27 14:39:34,337   rep_loss = 0.5395059040132558
2023-03-27 14:39:34,339 ***** Save model *****
2023-03-27 14:39:45,598 ***** Running evaluation *****
2023-03-27 14:39:45,598   Epoch = 29 iter 7899 step
2023-03-27 14:39:45,598   Num examples = 1043
2023-03-27 14:39:45,599   Batch size = 32
2023-03-27 14:39:45,600 ***** Eval results *****
2023-03-27 14:39:45,600   att_loss = 0.3484852079015512
2023-03-27 14:39:45,600   cls_loss = 0.0
2023-03-27 14:39:45,600   global_step = 7899
2023-03-27 14:39:45,600   loss = 0.888262444581741
2023-03-27 14:39:45,601   rep_loss = 0.5397772391637167
2023-03-27 14:39:45,608 ***** Save model *****
2023-03-27 14:39:56,899 ***** Running evaluation *****
2023-03-27 14:39:56,900   Epoch = 29 iter 7949 step
2023-03-27 14:39:56,900   Num examples = 1043
2023-03-27 14:39:56,900   Batch size = 32
2023-03-27 14:39:56,901 ***** Eval results *****
2023-03-27 14:39:56,901   att_loss = 0.3486705598900619
2023-03-27 14:39:56,901   cls_loss = 0.0
2023-03-27 14:39:56,902   global_step = 7949
2023-03-27 14:39:56,902   loss = 0.8891933574838545
2023-03-27 14:39:56,902   rep_loss = 0.5405227993298503
2023-03-27 14:39:56,909 ***** Save model *****
2023-03-27 14:40:08,178 ***** Running evaluation *****
2023-03-27 14:40:08,178   Epoch = 29 iter 7999 step
2023-03-27 14:40:08,179   Num examples = 1043
2023-03-27 14:40:08,179   Batch size = 32
2023-03-27 14:40:08,180 ***** Eval results *****
2023-03-27 14:40:08,180   att_loss = 0.34720711526460946
2023-03-27 14:40:08,181   cls_loss = 0.0
2023-03-27 14:40:08,181   global_step = 7999
2023-03-27 14:40:08,181   loss = 0.8872488944325596
2023-03-27 14:40:08,182   rep_loss = 0.540041780564934
2023-03-27 14:40:08,184 ***** Save model *****
2023-03-27 14:40:31,855 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/CoLA/BASE_COMPARE/TempTinyBERT_CoLA_4L_312D', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/BASE_COMPARE/TinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=3e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=True, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='BASE_COMPARE')
2023-03-27 14:40:33,000 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 14:40:35,621 device: cuda n_gpu: 1
2023-03-27 14:56:08,244 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/CoLA/BASE_COMPARE/TempTinyBERT_CoLA_4L_312D_5e-05_32', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/BASE_COMPARE/TinyBERT_CoLA_4L_312D_5e-05_32', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=3e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=True, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='BASE_COMPARE')
2023-03-27 14:56:09,379 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 14:56:11,882 device: cuda n_gpu: 1
2023-03-27 14:56:11,979 Writing example 0 of 8551
2023-03-27 14:56:11,980 *** Example ***
2023-03-27 14:56:11,980 guid: train-0
2023-03-27 14:56:11,980 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 14:56:11,980 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:56:11,980 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:56:11,980 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:56:11,981 label: 1
2023-03-27 14:56:11,981 label_id: 1
2023-03-27 14:56:12,948 Writing example 0 of 1043
2023-03-27 14:56:12,949 *** Example ***
2023-03-27 14:56:12,949 guid: dev-0
2023-03-27 14:56:12,949 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 14:56:12,949 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:56:12,949 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:56:12,949 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 14:56:12,950 label: 1
2023-03-27 14:56:12,950 label_id: 1
2023-03-27 14:56:13,069 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 14:56:13,070 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 14:56:14,840 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 14:56:15,016 loading model...
2023-03-27 14:56:15,110 done!
2023-03-27 14:56:15,111 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 14:56:16,154 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/BASE_COMPARE/TempTinyBERT_CoLA_4L_312D_5e-05_32
2023-03-27 14:56:16,155 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 14:56:16,381 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/BASE_COMPARE/TempTinyBERT_CoLA_4L_312D_5e-05_32/pytorch_model.bin
2023-03-27 14:56:16,403 loading model...
2023-03-27 14:56:16,411 done!
2023-03-27 14:56:16,422 ***** Running training *****
2023-03-27 14:56:16,423   Num examples = 8551
2023-03-27 14:56:16,423   Batch size = 32
2023-03-27 14:56:16,423   Num steps = 801
2023-03-27 14:56:16,423 n: bert.embeddings.word_embeddings.weight
2023-03-27 14:56:16,424 n: bert.embeddings.position_embeddings.weight
2023-03-27 14:56:16,424 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 14:56:16,424 n: bert.embeddings.LayerNorm.weight
2023-03-27 14:56:16,424 n: bert.embeddings.LayerNorm.bias
2023-03-27 14:56:16,424 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 14:56:16,424 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 14:56:16,425 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 14:56:16,425 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 14:56:16,425 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 14:56:16,425 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 14:56:16,425 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 14:56:16,425 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 14:56:16,425 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 14:56:16,425 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 14:56:16,426 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 14:56:16,426 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 14:56:16,426 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 14:56:16,426 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 14:56:16,426 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 14:56:16,426 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 14:56:16,426 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 14:56:16,426 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 14:56:16,426 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 14:56:16,427 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 14:56:16,427 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 14:56:16,427 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 14:56:16,427 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 14:56:16,427 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 14:56:16,427 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 14:56:16,427 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 14:56:16,428 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 14:56:16,428 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 14:56:16,428 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 14:56:16,428 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 14:56:16,428 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 14:56:16,428 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 14:56:16,428 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 14:56:16,428 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 14:56:16,428 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 14:56:16,428 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 14:56:16,429 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 14:56:16,429 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 14:56:16,429 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 14:56:16,429 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 14:56:16,429 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 14:56:16,429 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 14:56:16,429 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 14:56:16,429 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 14:56:16,429 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 14:56:16,429 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 14:56:16,430 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 14:56:16,430 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 14:56:16,430 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 14:56:16,430 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 14:56:16,430 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 14:56:16,430 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 14:56:16,430 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 14:56:16,430 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 14:56:16,430 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 14:56:16,430 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 14:56:16,431 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 14:56:16,431 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 14:56:16,431 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 14:56:16,431 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 14:56:16,431 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 14:56:16,431 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 14:56:16,431 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 14:56:16,431 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 14:56:16,431 n: bert.pooler.dense.weight
2023-03-27 14:56:16,431 n: bert.pooler.dense.bias
2023-03-27 14:56:16,432 n: classifier.weight
2023-03-27 14:56:16,432 n: classifier.bias
2023-03-27 14:56:16,432 n: fit_dense.weight
2023-03-27 14:56:16,432 n: fit_dense.bias
2023-03-27 14:56:16,432 Total parameters: 14591258
2023-03-27 14:56:26,092 ***** Running evaluation *****
2023-03-27 14:56:26,092   Epoch = 0 iter 49 step
2023-03-27 14:56:26,093   Num examples = 1043
2023-03-27 14:56:26,093   Batch size = 32
2023-03-27 14:56:26,531 ***** Eval results *****
2023-03-27 14:56:26,531   acc = 0.7277085330776606
2023-03-27 14:56:26,532   att_loss = 0.0
2023-03-27 14:56:26,532   cls_loss = 0.30911190290840307
2023-03-27 14:56:26,532   eval_loss = 0.570376576799335
2023-03-27 14:56:26,532   global_step = 49
2023-03-27 14:56:26,532   loss = 0.30911190290840307
2023-03-27 14:56:26,532   mcc = 0.3053388692306635
2023-03-27 14:56:26,532   rep_loss = 0.0
2023-03-27 14:56:26,534 ***** Save model *****
2023-03-27 14:56:36,342 ***** Running evaluation *****
2023-03-27 14:56:36,342   Epoch = 0 iter 99 step
2023-03-27 14:56:36,342   Num examples = 1043
2023-03-27 14:56:36,342   Batch size = 32
2023-03-27 14:56:36,773 ***** Eval results *****
2023-03-27 14:56:36,774   acc = 0.7190795781399808
2023-03-27 14:56:36,774   att_loss = 0.0
2023-03-27 14:56:36,774   cls_loss = 0.30134249064657426
2023-03-27 14:56:36,774   eval_loss = 0.5687761857654109
2023-03-27 14:56:36,775   global_step = 99
2023-03-27 14:56:36,775   loss = 0.30134249064657426
2023-03-27 14:56:36,775   mcc = 0.23122488432210597
2023-03-27 14:56:36,775   rep_loss = 0.0
2023-03-27 14:56:46,006 ***** Running evaluation *****
2023-03-27 14:56:46,006   Epoch = 0 iter 149 step
2023-03-27 14:56:46,007   Num examples = 1043
2023-03-27 14:56:46,007   Batch size = 32
2023-03-27 14:56:46,442 ***** Eval results *****
2023-03-27 14:56:46,442   acc = 0.7037392138063279
2023-03-27 14:56:46,442   att_loss = 0.0
2023-03-27 14:56:46,442   cls_loss = 0.2942354991131981
2023-03-27 14:56:46,443   eval_loss = 0.5930602171204307
2023-03-27 14:56:46,443   global_step = 149
2023-03-27 14:56:46,443   loss = 0.2942354991131981
2023-03-27 14:56:46,443   mcc = 0.31008867974053894
2023-03-27 14:56:46,443   rep_loss = 0.0
2023-03-27 14:56:46,450 ***** Save model *****
2023-03-27 14:56:56,424 ***** Running evaluation *****
2023-03-27 14:56:56,424   Epoch = 0 iter 199 step
2023-03-27 14:56:56,424   Num examples = 1043
2023-03-27 14:56:56,424   Batch size = 32
2023-03-27 14:56:56,863 ***** Eval results *****
2023-03-27 14:56:56,863   acc = 0.7277085330776606
2023-03-27 14:56:56,863   att_loss = 0.0
2023-03-27 14:56:56,863   cls_loss = 0.29126956145368027
2023-03-27 14:56:56,864   eval_loss = 0.5636538429693743
2023-03-27 14:56:56,864   global_step = 199
2023-03-27 14:56:56,864   loss = 0.29126956145368027
2023-03-27 14:56:56,864   mcc = 0.3238801944300833
2023-03-27 14:56:56,864   rep_loss = 0.0
2023-03-27 14:56:56,866 ***** Save model *****
2023-03-27 14:57:06,936 ***** Running evaluation *****
2023-03-27 14:57:06,936   Epoch = 0 iter 249 step
2023-03-27 14:57:06,936   Num examples = 1043
2023-03-27 14:57:06,936   Batch size = 32
2023-03-27 14:57:07,377 ***** Eval results *****
2023-03-27 14:57:07,377   acc = 0.7334611697027804
2023-03-27 14:57:07,377   att_loss = 0.0
2023-03-27 14:57:07,377   cls_loss = 0.28815502131799137
2023-03-27 14:57:07,378   eval_loss = 0.5498982216372634
2023-03-27 14:57:07,378   global_step = 249
2023-03-27 14:57:07,378   loss = 0.28815502131799137
2023-03-27 14:57:07,378   mcc = 0.31765323876865625
2023-03-27 14:57:07,378   rep_loss = 0.0
2023-03-27 14:57:16,861 ***** Running evaluation *****
2023-03-27 14:57:16,861   Epoch = 1 iter 299 step
2023-03-27 14:57:16,861   Num examples = 1043
2023-03-27 14:57:16,861   Batch size = 32
2023-03-27 14:57:17,306 ***** Eval results *****
2023-03-27 14:57:17,306   acc = 0.7411313518696069
2023-03-27 14:57:17,306   att_loss = 0.0
2023-03-27 14:57:17,306   cls_loss = 0.27480138232931495
2023-03-27 14:57:17,306   eval_loss = 0.548909663250952
2023-03-27 14:57:17,307   global_step = 299
2023-03-27 14:57:17,307   loss = 0.27480138232931495
2023-03-27 14:57:17,307   mcc = 0.32301000206570946
2023-03-27 14:57:17,307   rep_loss = 0.0
2023-03-27 14:57:26,842 ***** Running evaluation *****
2023-03-27 14:57:26,842   Epoch = 1 iter 349 step
2023-03-27 14:57:26,842   Num examples = 1043
2023-03-27 14:57:26,842   Batch size = 32
2023-03-27 14:57:27,287 ***** Eval results *****
2023-03-27 14:57:27,287   acc = 0.7363374880153404
2023-03-27 14:57:27,287   att_loss = 0.0
2023-03-27 14:57:27,288   cls_loss = 0.2731961269931095
2023-03-27 14:57:27,288   eval_loss = 0.5590108768506483
2023-03-27 14:57:27,288   global_step = 349
2023-03-27 14:57:27,288   loss = 0.2731961269931095
2023-03-27 14:57:27,288   mcc = 0.3144288006990196
2023-03-27 14:57:27,288   rep_loss = 0.0
2023-03-27 14:57:36,906 ***** Running evaluation *****
2023-03-27 14:57:36,907   Epoch = 1 iter 399 step
2023-03-27 14:57:36,907   Num examples = 1043
2023-03-27 14:57:36,907   Batch size = 32
2023-03-27 14:57:37,352 ***** Eval results *****
2023-03-27 14:57:37,352   acc = 0.7372962607861937
2023-03-27 14:57:37,352   att_loss = 0.0
2023-03-27 14:57:37,353   cls_loss = 0.27367669434258435
2023-03-27 14:57:37,353   eval_loss = 0.5579707956675327
2023-03-27 14:57:37,353   global_step = 399
2023-03-27 14:57:37,353   loss = 0.27367669434258435
2023-03-27 14:57:37,353   mcc = 0.32668035465644646
2023-03-27 14:57:37,353   rep_loss = 0.0
2023-03-27 14:57:37,360 ***** Save model *****
2023-03-27 14:57:47,691 ***** Running evaluation *****
2023-03-27 14:57:47,691   Epoch = 1 iter 449 step
2023-03-27 14:57:47,692   Num examples = 1043
2023-03-27 14:57:47,692   Batch size = 32
2023-03-27 14:57:48,137 ***** Eval results *****
2023-03-27 14:57:48,138   acc = 0.7344199424736337
2023-03-27 14:57:48,138   att_loss = 0.0
2023-03-27 14:57:48,138   cls_loss = 0.27326735133653157
2023-03-27 14:57:48,138   eval_loss = 0.5477382965160139
2023-03-27 14:57:48,138   global_step = 449
2023-03-27 14:57:48,138   loss = 0.27326735133653157
2023-03-27 14:57:48,138   mcc = 0.30426441210435545
2023-03-27 14:57:48,139   rep_loss = 0.0
2023-03-27 14:57:57,859 ***** Running evaluation *****
2023-03-27 14:57:57,859   Epoch = 1 iter 499 step
2023-03-27 14:57:57,859   Num examples = 1043
2023-03-27 14:57:57,859   Batch size = 32
2023-03-27 14:57:58,306 ***** Eval results *****
2023-03-27 14:57:58,306   acc = 0.7363374880153404
2023-03-27 14:57:58,306   att_loss = 0.0
2023-03-27 14:57:58,306   cls_loss = 0.2732744624131712
2023-03-27 14:57:58,306   eval_loss = 0.5519915214090636
2023-03-27 14:57:58,306   global_step = 499
2023-03-27 14:57:58,307   loss = 0.2732744624131712
2023-03-27 14:57:58,307   mcc = 0.32517421747458736
2023-03-27 14:57:58,307   rep_loss = 0.0
2023-03-27 14:58:08,082 ***** Running evaluation *****
2023-03-27 14:58:08,082   Epoch = 2 iter 549 step
2023-03-27 14:58:08,082   Num examples = 1043
2023-03-27 14:58:08,083   Batch size = 32
2023-03-27 14:58:08,530 ***** Eval results *****
2023-03-27 14:58:08,530   acc = 0.7325023969319271
2023-03-27 14:58:08,530   att_loss = 0.0
2023-03-27 14:58:08,530   cls_loss = 0.2702888379494349
2023-03-27 14:58:08,531   eval_loss = 0.5516266877000983
2023-03-27 14:58:08,531   global_step = 549
2023-03-27 14:58:08,531   loss = 0.2702888379494349
2023-03-27 14:58:08,531   mcc = 0.3256334538187928
2023-03-27 14:58:08,531   rep_loss = 0.0
2023-03-27 14:58:18,353 ***** Running evaluation *****
2023-03-27 14:58:18,354   Epoch = 2 iter 599 step
2023-03-27 14:58:18,354   Num examples = 1043
2023-03-27 14:58:18,354   Batch size = 32
2023-03-27 14:58:18,803 ***** Eval results *****
2023-03-27 14:58:18,803   acc = 0.7277085330776606
2023-03-27 14:58:18,804   att_loss = 0.0
2023-03-27 14:58:18,804   cls_loss = 0.26967574678934536
2023-03-27 14:58:18,804   eval_loss = 0.5511483224955472
2023-03-27 14:58:18,804   global_step = 599
2023-03-27 14:58:18,804   loss = 0.26967574678934536
2023-03-27 14:58:18,804   mcc = 0.3144063704156539
2023-03-27 14:58:18,804   rep_loss = 0.0
2023-03-27 14:58:28,661 ***** Running evaluation *****
2023-03-27 14:58:28,661   Epoch = 2 iter 649 step
2023-03-27 14:58:28,661   Num examples = 1043
2023-03-27 14:58:28,661   Batch size = 32
2023-03-27 14:58:29,110 ***** Eval results *****
2023-03-27 14:58:29,111   acc = 0.7219558964525408
2023-03-27 14:58:29,111   att_loss = 0.0
2023-03-27 14:58:29,111   cls_loss = 0.26988406388655956
2023-03-27 14:58:29,111   eval_loss = 0.5578407871000695
2023-03-27 14:58:29,111   global_step = 649
2023-03-27 14:58:29,112   loss = 0.26988406388655956
2023-03-27 14:58:29,112   mcc = 0.3062480298225157
2023-03-27 14:58:29,112   rep_loss = 0.0
2023-03-27 14:58:38,998 ***** Running evaluation *****
2023-03-27 14:58:38,998   Epoch = 2 iter 699 step
2023-03-27 14:58:38,998   Num examples = 1043
2023-03-27 14:58:38,999   Batch size = 32
2023-03-27 14:58:39,447 ***** Eval results *****
2023-03-27 14:58:39,448   acc = 0.7286673058485139
2023-03-27 14:58:39,448   att_loss = 0.0
2023-03-27 14:58:39,448   cls_loss = 0.27069000114094127
2023-03-27 14:58:39,448   eval_loss = 0.5526806468313391
2023-03-27 14:58:39,448   global_step = 699
2023-03-27 14:58:39,449   loss = 0.27069000114094127
2023-03-27 14:58:39,449   mcc = 0.30837420944189176
2023-03-27 14:58:39,449   rep_loss = 0.0
2023-03-27 14:58:49,366 ***** Running evaluation *****
2023-03-27 14:58:49,367   Epoch = 2 iter 749 step
2023-03-27 14:58:49,367   Num examples = 1043
2023-03-27 14:58:49,367   Batch size = 32
2023-03-27 14:58:49,816 ***** Eval results *****
2023-03-27 14:58:49,816   acc = 0.7296260786193672
2023-03-27 14:58:49,817   att_loss = 0.0
2023-03-27 14:58:49,817   cls_loss = 0.2702580578105394
2023-03-27 14:58:49,817   eval_loss = 0.5516445690935309
2023-03-27 14:58:49,817   global_step = 749
2023-03-27 14:58:49,817   loss = 0.2702580578105394
2023-03-27 14:58:49,817   mcc = 0.3131528264872866
2023-03-27 14:58:49,818   rep_loss = 0.0
2023-03-27 14:58:59,782 ***** Running evaluation *****
2023-03-27 14:58:59,782   Epoch = 2 iter 799 step
2023-03-27 14:58:59,783   Num examples = 1043
2023-03-27 14:58:59,783   Batch size = 32
2023-03-27 14:59:00,233 ***** Eval results *****
2023-03-27 14:59:00,233   acc = 0.7296260786193672
2023-03-27 14:59:00,233   att_loss = 0.0
2023-03-27 14:59:00,233   cls_loss = 0.2698961450243896
2023-03-27 14:59:00,233   eval_loss = 0.5511026156671119
2023-03-27 14:59:00,234   global_step = 799
2023-03-27 14:59:00,234   loss = 0.2698961450243896
2023-03-27 14:59:00,234   mcc = 0.31403563513642563
2023-03-27 14:59:00,234   rep_loss = 0.0
2023-03-27 15:46:13,286 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_BASE/TempTinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=30.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='KL_ATTN_BASE', kl_attn=True)
2023-03-27 15:46:14,467 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 15:46:17,093 device: cuda n_gpu: 1
2023-03-27 15:46:17,200 Writing example 0 of 8551
2023-03-27 15:46:17,200 *** Example ***
2023-03-27 15:46:17,200 guid: train-0
2023-03-27 15:46:17,200 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 15:46:17,200 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:46:17,201 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:46:17,201 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:46:17,201 label: 1
2023-03-27 15:46:17,201 label_id: 1
2023-03-27 15:46:18,159 Writing example 0 of 1043
2023-03-27 15:46:18,159 *** Example ***
2023-03-27 15:46:18,159 guid: dev-0
2023-03-27 15:46:18,159 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 15:46:18,159 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:46:18,159 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:46:18,160 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:46:18,160 label: 1
2023-03-27 15:46:18,160 label_id: 1
2023-03-27 15:46:18,278 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 15:46:18,279 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 15:46:20,048 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 15:46:20,222 loading model...
2023-03-27 15:46:20,273 done!
2023-03-27 15:46:20,274 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 15:46:21,299 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 15:46:21,301 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 15:46:21,527 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 15:46:21,549 loading model...
2023-03-27 15:46:21,557 done!
2023-03-27 15:46:21,557 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 15:46:21,558 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 15:46:21,570 ***** Running training *****
2023-03-27 15:46:21,571   Num examples = 8551
2023-03-27 15:46:21,571   Batch size = 32
2023-03-27 15:46:21,571   Num steps = 8010
2023-03-27 15:46:21,571 n: bert.embeddings.word_embeddings.weight
2023-03-27 15:46:21,571 n: bert.embeddings.position_embeddings.weight
2023-03-27 15:46:21,572 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 15:46:21,572 n: bert.embeddings.LayerNorm.weight
2023-03-27 15:46:21,572 n: bert.embeddings.LayerNorm.bias
2023-03-27 15:46:21,572 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 15:46:21,572 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 15:46:21,572 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 15:46:21,572 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 15:46:21,572 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 15:46:21,573 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 15:46:21,573 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 15:46:21,573 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 15:46:21,573 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 15:46:21,573 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 15:46:21,573 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 15:46:21,573 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 15:46:21,573 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 15:46:21,574 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 15:46:21,574 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 15:46:21,574 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 15:46:21,574 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 15:46:21,574 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 15:46:21,574 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 15:46:21,574 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 15:46:21,574 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 15:46:21,575 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 15:46:21,575 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 15:46:21,575 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 15:46:21,575 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 15:46:21,575 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 15:46:21,575 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 15:46:21,575 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 15:46:21,575 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 15:46:21,575 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 15:46:21,575 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 15:46:21,576 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 15:46:21,576 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 15:46:21,576 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 15:46:21,576 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 15:46:21,576 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 15:46:21,576 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 15:46:21,576 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 15:46:21,576 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 15:46:21,576 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 15:46:21,577 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 15:46:21,577 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 15:46:21,577 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 15:46:21,577 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 15:46:21,577 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 15:46:21,577 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 15:46:21,577 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 15:46:21,577 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 15:46:21,577 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 15:46:21,577 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 15:46:21,578 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 15:46:21,578 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 15:46:21,578 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 15:46:21,578 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 15:46:21,578 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 15:46:21,578 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 15:46:21,578 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 15:46:21,578 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 15:46:21,578 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 15:46:21,578 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 15:46:21,579 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 15:46:21,579 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 15:46:21,579 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 15:46:21,579 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 15:46:21,579 n: bert.pooler.dense.weight
2023-03-27 15:46:21,579 n: bert.pooler.dense.bias
2023-03-27 15:46:21,579 n: classifier.weight
2023-03-27 15:46:21,579 n: classifier.bias
2023-03-27 15:46:21,579 n: fit_dense.weight
2023-03-27 15:46:21,579 n: fit_dense.bias
2023-03-27 15:46:21,580 Total parameters: 14591258
2023-03-27 15:51:16,066 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_BASE/TempTinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=30.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='KL_ATTN_BASE', kl_attn=True)
2023-03-27 15:51:17,299 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 15:51:19,780 device: cuda n_gpu: 1
2023-03-27 15:51:19,879 Writing example 0 of 8551
2023-03-27 15:51:19,879 *** Example ***
2023-03-27 15:51:19,879 guid: train-0
2023-03-27 15:51:19,880 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 15:51:19,880 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:51:19,880 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:51:19,880 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:51:19,880 label: 1
2023-03-27 15:51:19,880 label_id: 1
2023-03-27 15:51:20,843 Writing example 0 of 1043
2023-03-27 15:51:20,844 *** Example ***
2023-03-27 15:51:20,844 guid: dev-0
2023-03-27 15:51:20,844 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 15:51:20,844 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:51:20,844 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:51:20,844 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 15:51:20,844 label: 1
2023-03-27 15:51:20,845 label_id: 1
2023-03-27 15:51:20,963 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 15:51:20,964 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 15:51:22,740 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 15:51:22,913 loading model...
2023-03-27 15:51:23,021 done!
2023-03-27 15:51:23,021 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 15:51:24,049 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 15:51:24,051 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 15:51:24,276 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 15:51:24,297 loading model...
2023-03-27 15:51:24,305 done!
2023-03-27 15:51:24,305 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 15:51:24,306 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 15:51:24,317 ***** Running training *****
2023-03-27 15:51:24,317   Num examples = 8551
2023-03-27 15:51:24,318   Batch size = 32
2023-03-27 15:51:24,318   Num steps = 8010
2023-03-27 15:51:24,318 n: bert.embeddings.word_embeddings.weight
2023-03-27 15:51:24,318 n: bert.embeddings.position_embeddings.weight
2023-03-27 15:51:24,318 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 15:51:24,319 n: bert.embeddings.LayerNorm.weight
2023-03-27 15:51:24,319 n: bert.embeddings.LayerNorm.bias
2023-03-27 15:51:24,319 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 15:51:24,319 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 15:51:24,319 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 15:51:24,319 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 15:51:24,319 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 15:51:24,319 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 15:51:24,320 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 15:51:24,320 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 15:51:24,320 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 15:51:24,320 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 15:51:24,320 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 15:51:24,320 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 15:51:24,320 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 15:51:24,320 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 15:51:24,320 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 15:51:24,320 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 15:51:24,321 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 15:51:24,321 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 15:51:24,321 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 15:51:24,321 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 15:51:24,321 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 15:51:24,321 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 15:51:24,321 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 15:51:24,321 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 15:51:24,321 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 15:51:24,321 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 15:51:24,321 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 15:51:24,322 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 15:51:24,322 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 15:51:24,322 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 15:51:24,322 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 15:51:24,322 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 15:51:24,322 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 15:51:24,322 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 15:51:24,322 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 15:51:24,322 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 15:51:24,322 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 15:51:24,323 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 15:51:24,323 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 15:51:24,323 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 15:51:24,323 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 15:51:24,323 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 15:51:24,323 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 15:51:24,323 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 15:51:24,323 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 15:51:24,323 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 15:51:24,323 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 15:51:24,324 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 15:51:24,324 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 15:51:24,324 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 15:51:24,324 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 15:51:24,324 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 15:51:24,324 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 15:51:24,324 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 15:51:24,324 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 15:51:24,324 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 15:51:24,324 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 15:51:24,325 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 15:51:24,325 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 15:51:24,325 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 15:51:24,325 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 15:51:24,325 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 15:51:24,325 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 15:51:24,325 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 15:51:24,325 n: bert.pooler.dense.weight
2023-03-27 15:51:24,325 n: bert.pooler.dense.bias
2023-03-27 15:51:24,325 n: classifier.weight
2023-03-27 15:51:24,325 n: classifier.bias
2023-03-27 15:51:24,326 n: fit_dense.weight
2023-03-27 15:51:24,326 n: fit_dense.bias
2023-03-27 15:51:24,326 Total parameters: 14591258
2023-03-27 15:51:34,597 ***** Running evaluation *****
2023-03-27 15:51:34,597   Epoch = 0 iter 49 step
2023-03-27 15:51:34,598   Num examples = 1043
2023-03-27 15:51:34,598   Batch size = 32
2023-03-27 15:51:34,606 ***** Eval results *****
2023-03-27 15:51:34,606   att_loss = 0.007929856261732628
2023-03-27 15:51:34,606   cls_loss = 0.0
2023-03-27 15:51:34,606   global_step = 49
2023-03-27 15:51:34,606   loss = 1.245269631852909
2023-03-27 15:51:34,606   rep_loss = 1.2373397739566103
2023-03-27 15:51:34,614 ***** Save model *****
2023-03-27 15:51:44,991 ***** Running evaluation *****
2023-03-27 15:51:44,991   Epoch = 0 iter 99 step
2023-03-27 15:51:44,991   Num examples = 1043
2023-03-27 15:51:44,991   Batch size = 32
2023-03-27 15:51:44,993 ***** Eval results *****
2023-03-27 15:51:44,993   att_loss = 0.007061318320344494
2023-03-27 15:51:44,993   cls_loss = 0.0
2023-03-27 15:51:44,994   global_step = 99
2023-03-27 15:51:44,994   loss = 1.0594653956817859
2023-03-27 15:51:44,994   rep_loss = 1.0524040755599435
2023-03-27 15:51:45,001 ***** Save model *****
2023-03-27 15:51:55,521 ***** Running evaluation *****
2023-03-27 15:51:55,521   Epoch = 0 iter 149 step
2023-03-27 15:51:55,522   Num examples = 1043
2023-03-27 15:51:55,522   Batch size = 32
2023-03-27 15:51:55,523 ***** Eval results *****
2023-03-27 15:51:55,523   att_loss = 0.0066383128716991655
2023-03-27 15:51:55,523   cls_loss = 0.0
2023-03-27 15:51:55,524   global_step = 149
2023-03-27 15:51:55,524   loss = 0.9723860213420535
2023-03-27 15:51:55,524   rep_loss = 0.9657477072421337
2023-03-27 15:51:55,531 ***** Save model *****
2023-03-27 15:52:06,140 ***** Running evaluation *****
2023-03-27 15:52:06,141   Epoch = 0 iter 199 step
2023-03-27 15:52:06,141   Num examples = 1043
2023-03-27 15:52:06,141   Batch size = 32
2023-03-27 15:52:06,142 ***** Eval results *****
2023-03-27 15:52:06,142   att_loss = 0.006427235435694456
2023-03-27 15:52:06,142   cls_loss = 0.0
2023-03-27 15:52:06,142   global_step = 199
2023-03-27 15:52:06,142   loss = 0.919778054084011
2023-03-27 15:52:06,142   rep_loss = 0.9133508172466527
2023-03-27 15:52:06,149 ***** Save model *****
2023-03-27 15:52:16,853 ***** Running evaluation *****
2023-03-27 15:52:16,853   Epoch = 0 iter 249 step
2023-03-27 15:52:16,853   Num examples = 1043
2023-03-27 15:52:16,854   Batch size = 32
2023-03-27 15:52:16,855 ***** Eval results *****
2023-03-27 15:52:16,855   att_loss = 0.006286794811591925
2023-03-27 15:52:16,855   cls_loss = 0.0
2023-03-27 15:52:16,855   global_step = 249
2023-03-27 15:52:16,855   loss = 0.8843349807233696
2023-03-27 15:52:16,856   rep_loss = 0.8780481844063265
2023-03-27 15:52:16,863 ***** Save model *****
2023-03-27 15:52:27,610 ***** Running evaluation *****
2023-03-27 15:52:27,610   Epoch = 1 iter 299 step
2023-03-27 15:52:27,610   Num examples = 1043
2023-03-27 15:52:27,610   Batch size = 32
2023-03-27 15:52:27,612 ***** Eval results *****
2023-03-27 15:52:27,613   att_loss = 0.0056104044342646375
2023-03-27 15:52:27,613   cls_loss = 0.0
2023-03-27 15:52:27,613   global_step = 299
2023-03-27 15:52:27,613   loss = 0.7250259518623352
2023-03-27 15:52:27,613   rep_loss = 0.7194155473262072
2023-03-27 15:52:27,620 ***** Save model *****
2023-03-27 15:52:38,438 ***** Running evaluation *****
2023-03-27 15:52:38,438   Epoch = 1 iter 349 step
2023-03-27 15:52:38,439   Num examples = 1043
2023-03-27 15:52:38,439   Batch size = 32
2023-03-27 15:52:38,440 ***** Eval results *****
2023-03-27 15:52:38,440   att_loss = 0.0056372748903657606
2023-03-27 15:52:38,440   cls_loss = 0.0
2023-03-27 15:52:38,441   global_step = 349
2023-03-27 15:52:38,441   loss = 0.7207967798884322
2023-03-27 15:52:38,441   rep_loss = 0.7151595041519259
2023-03-27 15:52:38,448 ***** Save model *****
2023-03-27 15:52:49,289 ***** Running evaluation *****
2023-03-27 15:52:49,290   Epoch = 1 iter 399 step
2023-03-27 15:52:49,290   Num examples = 1043
2023-03-27 15:52:49,290   Batch size = 32
2023-03-27 15:52:49,291 ***** Eval results *****
2023-03-27 15:52:49,291   att_loss = 0.005621290102488164
2023-03-27 15:52:49,291   cls_loss = 0.0
2023-03-27 15:52:49,292   global_step = 399
2023-03-27 15:52:49,292   loss = 0.7164950691389315
2023-03-27 15:52:49,292   rep_loss = 0.7108737781192317
2023-03-27 15:52:49,299 ***** Save model *****
2023-03-27 15:53:00,209 ***** Running evaluation *****
2023-03-27 15:53:00,210   Epoch = 1 iter 449 step
2023-03-27 15:53:00,210   Num examples = 1043
2023-03-27 15:53:00,210   Batch size = 32
2023-03-27 15:53:00,211 ***** Eval results *****
2023-03-27 15:53:00,212   att_loss = 0.005618599831935141
2023-03-27 15:53:00,212   cls_loss = 0.0
2023-03-27 15:53:00,212   global_step = 449
2023-03-27 15:53:00,212   loss = 0.7138931757801181
2023-03-27 15:53:00,213   rep_loss = 0.7082745747251825
2023-03-27 15:53:00,220 ***** Save model *****
2023-03-27 15:53:11,249 ***** Running evaluation *****
2023-03-27 15:53:11,249   Epoch = 1 iter 499 step
2023-03-27 15:53:11,250   Num examples = 1043
2023-03-27 15:53:11,250   Batch size = 32
2023-03-27 15:53:11,251 ***** Eval results *****
2023-03-27 15:53:11,251   att_loss = 0.005580351340728973
2023-03-27 15:53:11,251   cls_loss = 0.0
2023-03-27 15:53:11,251   global_step = 499
2023-03-27 15:53:11,251   loss = 0.7098711893476289
2023-03-27 15:53:11,251   rep_loss = 0.7042908373064009
2023-03-27 15:53:11,253 ***** Save model *****
2023-03-27 15:53:22,282 ***** Running evaluation *****
2023-03-27 15:53:22,282   Epoch = 2 iter 549 step
2023-03-27 15:53:22,282   Num examples = 1043
2023-03-27 15:53:22,282   Batch size = 32
2023-03-27 15:53:22,283 ***** Eval results *****
2023-03-27 15:53:22,283   att_loss = 0.005269136062512795
2023-03-27 15:53:22,284   cls_loss = 0.0
2023-03-27 15:53:22,284   global_step = 549
2023-03-27 15:53:22,284   loss = 0.6732939998308818
2023-03-27 15:53:22,284   rep_loss = 0.6680248578389486
2023-03-27 15:53:22,286 ***** Save model *****
2023-03-27 15:53:33,337 ***** Running evaluation *****
2023-03-27 15:53:33,337   Epoch = 2 iter 599 step
2023-03-27 15:53:33,338   Num examples = 1043
2023-03-27 15:53:33,338   Batch size = 32
2023-03-27 15:53:33,339 ***** Eval results *****
2023-03-27 15:53:33,340   att_loss = 0.005434327622732291
2023-03-27 15:53:33,340   cls_loss = 0.0
2023-03-27 15:53:33,340   global_step = 599
2023-03-27 15:53:33,340   loss = 0.6851946995808528
2023-03-27 15:53:33,340   rep_loss = 0.6797603708047133
2023-03-27 15:53:33,347 ***** Save model *****
2023-03-27 15:53:44,453 ***** Running evaluation *****
2023-03-27 15:53:44,454   Epoch = 2 iter 649 step
2023-03-27 15:53:44,454   Num examples = 1043
2023-03-27 15:53:44,454   Batch size = 32
2023-03-27 15:53:44,455 ***** Eval results *****
2023-03-27 15:53:44,456   att_loss = 0.005415243930790735
2023-03-27 15:53:44,456   cls_loss = 0.0
2023-03-27 15:53:44,456   global_step = 649
2023-03-27 15:53:44,456   loss = 0.6818554810855699
2023-03-27 15:53:44,456   rep_loss = 0.6764402363611304
2023-03-27 15:53:44,463 ***** Save model *****
2023-03-27 15:53:55,589 ***** Running evaluation *****
2023-03-27 15:53:55,589   Epoch = 2 iter 699 step
2023-03-27 15:53:55,589   Num examples = 1043
2023-03-27 15:53:55,589   Batch size = 32
2023-03-27 15:53:55,590 ***** Eval results *****
2023-03-27 15:53:55,591   att_loss = 0.005435206227456078
2023-03-27 15:53:55,591   cls_loss = 0.0
2023-03-27 15:53:55,591   global_step = 699
2023-03-27 15:53:55,591   loss = 0.6813172712470545
2023-03-27 15:53:55,591   rep_loss = 0.6758820645736926
2023-03-27 15:53:55,599 ***** Save model *****
2023-03-27 15:54:06,752 ***** Running evaluation *****
2023-03-27 15:54:06,753   Epoch = 2 iter 749 step
2023-03-27 15:54:06,753   Num examples = 1043
2023-03-27 15:54:06,753   Batch size = 32
2023-03-27 15:54:06,754 ***** Eval results *****
2023-03-27 15:54:06,755   att_loss = 0.005448844755978085
2023-03-27 15:54:06,755   cls_loss = 0.0
2023-03-27 15:54:06,755   global_step = 749
2023-03-27 15:54:06,755   loss = 0.6801346712334212
2023-03-27 15:54:06,756   rep_loss = 0.6746858253035435
2023-03-27 15:54:06,764 ***** Save model *****
2023-03-27 15:54:17,925 ***** Running evaluation *****
2023-03-27 15:54:17,926   Epoch = 2 iter 799 step
2023-03-27 15:54:17,926   Num examples = 1043
2023-03-27 15:54:17,926   Batch size = 32
2023-03-27 15:54:17,927 ***** Eval results *****
2023-03-27 15:54:17,927   att_loss = 0.00544942775389777
2023-03-27 15:54:17,927   cls_loss = 0.0
2023-03-27 15:54:17,927   global_step = 799
2023-03-27 15:54:17,927   loss = 0.6781911098732138
2023-03-27 15:54:17,928   rep_loss = 0.6727416818996645
2023-03-27 15:54:17,930 ***** Save model *****
2023-03-27 15:54:29,109 ***** Running evaluation *****
2023-03-27 15:54:29,110   Epoch = 3 iter 849 step
2023-03-27 15:54:29,110   Num examples = 1043
2023-03-27 15:54:29,110   Batch size = 32
2023-03-27 15:54:29,111 ***** Eval results *****
2023-03-27 15:54:29,111   att_loss = 0.005375787023998176
2023-03-27 15:54:29,111   cls_loss = 0.0
2023-03-27 15:54:29,112   global_step = 849
2023-03-27 15:54:29,112   loss = 0.6625601947307587
2023-03-27 15:54:29,112   rep_loss = 0.6571844120820364
2023-03-27 15:54:29,114 ***** Save model *****
2023-03-27 15:54:40,345 ***** Running evaluation *****
2023-03-27 15:54:40,346   Epoch = 3 iter 899 step
2023-03-27 15:54:40,346   Num examples = 1043
2023-03-27 15:54:40,346   Batch size = 32
2023-03-27 15:54:40,348 ***** Eval results *****
2023-03-27 15:54:40,348   att_loss = 0.005411924314400067
2023-03-27 15:54:40,348   cls_loss = 0.0
2023-03-27 15:54:40,348   global_step = 899
2023-03-27 15:54:40,348   loss = 0.6610446231705802
2023-03-27 15:54:40,349   rep_loss = 0.6556327032799624
2023-03-27 15:54:40,356 ***** Save model *****
2023-03-27 15:54:51,549 ***** Running evaluation *****
2023-03-27 15:54:51,549   Epoch = 3 iter 949 step
2023-03-27 15:54:51,549   Num examples = 1043
2023-03-27 15:54:51,550   Batch size = 32
2023-03-27 15:54:51,551 ***** Eval results *****
2023-03-27 15:54:51,551   att_loss = 0.005424012341914144
2023-03-27 15:54:51,551   cls_loss = 0.0
2023-03-27 15:54:51,551   global_step = 949
2023-03-27 15:54:51,551   loss = 0.6604770885126011
2023-03-27 15:54:51,552   rep_loss = 0.6550530787255313
2023-03-27 15:54:51,559 ***** Save model *****
2023-03-27 15:55:02,787 ***** Running evaluation *****
2023-03-27 15:55:02,787   Epoch = 3 iter 999 step
2023-03-27 15:55:02,787   Num examples = 1043
2023-03-27 15:55:02,788   Batch size = 32
2023-03-27 15:55:02,788 ***** Eval results *****
2023-03-27 15:55:02,789   att_loss = 0.005414582589039147
2023-03-27 15:55:02,789   cls_loss = 0.0
2023-03-27 15:55:02,789   global_step = 999
2023-03-27 15:55:02,789   loss = 0.6585073955733367
2023-03-27 15:55:02,789   rep_loss = 0.6530928151174025
2023-03-27 15:55:02,791 ***** Save model *****
2023-03-27 15:55:14,046 ***** Running evaluation *****
2023-03-27 15:55:14,046   Epoch = 3 iter 1049 step
2023-03-27 15:55:14,046   Num examples = 1043
2023-03-27 15:55:14,046   Batch size = 32
2023-03-27 15:55:14,047 ***** Eval results *****
2023-03-27 15:55:14,048   att_loss = 0.005414632117120369
2023-03-27 15:55:14,048   cls_loss = 0.0
2023-03-27 15:55:14,048   global_step = 1049
2023-03-27 15:55:14,048   loss = 0.6572313676438024
2023-03-27 15:55:14,049   rep_loss = 0.6518167370269375
2023-03-27 15:55:14,051 ***** Save model *****
2023-03-27 15:55:25,343 ***** Running evaluation *****
2023-03-27 15:55:25,343   Epoch = 4 iter 1099 step
2023-03-27 15:55:25,343   Num examples = 1043
2023-03-27 15:55:25,343   Batch size = 32
2023-03-27 15:55:25,344 ***** Eval results *****
2023-03-27 15:55:25,345   att_loss = 0.00540884590196994
2023-03-27 15:55:25,345   cls_loss = 0.0
2023-03-27 15:55:25,345   global_step = 1099
2023-03-27 15:55:25,345   loss = 0.6496991495932302
2023-03-27 15:55:25,345   rep_loss = 0.6442903030303216
2023-03-27 15:55:25,352 ***** Save model *****
2023-03-27 15:55:36,642 ***** Running evaluation *****
2023-03-27 15:55:36,642   Epoch = 4 iter 1149 step
2023-03-27 15:55:36,642   Num examples = 1043
2023-03-27 15:55:36,642   Batch size = 32
2023-03-27 15:55:36,643 ***** Eval results *****
2023-03-27 15:55:36,644   att_loss = 0.005411969681764826
2023-03-27 15:55:36,644   cls_loss = 0.0
2023-03-27 15:55:36,644   global_step = 1149
2023-03-27 15:55:36,644   loss = 0.6476600795616339
2023-03-27 15:55:36,644   rep_loss = 0.6422481110066544
2023-03-27 15:55:36,652 ***** Save model *****
2023-03-27 15:55:48,045 ***** Running evaluation *****
2023-03-27 15:55:48,045   Epoch = 4 iter 1199 step
2023-03-27 15:55:48,046   Num examples = 1043
2023-03-27 15:55:48,046   Batch size = 32
2023-03-27 15:55:48,048 ***** Eval results *****
2023-03-27 15:55:48,048   att_loss = 0.005374491232525529
2023-03-27 15:55:48,048   cls_loss = 0.0
2023-03-27 15:55:48,049   global_step = 1199
2023-03-27 15:55:48,049   loss = 0.6443468127541869
2023-03-27 15:55:48,049   rep_loss = 0.6389723224494294
2023-03-27 15:55:48,056 ***** Save model *****
2023-03-27 15:55:59,402 ***** Running evaluation *****
2023-03-27 15:55:59,402   Epoch = 4 iter 1249 step
2023-03-27 15:55:59,402   Num examples = 1043
2023-03-27 15:55:59,403   Batch size = 32
2023-03-27 15:55:59,404 ***** Eval results *****
2023-03-27 15:55:59,404   att_loss = 0.005357520909496269
2023-03-27 15:55:59,404   cls_loss = 0.0
2023-03-27 15:55:59,404   global_step = 1249
2023-03-27 15:55:59,404   loss = 0.6422173479643974
2023-03-27 15:55:59,404   rep_loss = 0.6368598272787274
2023-03-27 15:55:59,411 ***** Save model *****
2023-03-27 15:56:10,791 ***** Running evaluation *****
2023-03-27 15:56:10,792   Epoch = 4 iter 1299 step
2023-03-27 15:56:10,792   Num examples = 1043
2023-03-27 15:56:10,792   Batch size = 32
2023-03-27 15:56:10,793 ***** Eval results *****
2023-03-27 15:56:10,793   att_loss = 0.005359175695646635
2023-03-27 15:56:10,794   cls_loss = 0.0
2023-03-27 15:56:10,794   global_step = 1299
2023-03-27 15:56:10,794   loss = 0.6407558572240722
2023-03-27 15:56:10,794   rep_loss = 0.6353966807906246
2023-03-27 15:56:10,802 ***** Save model *****
2023-03-27 15:56:22,176 ***** Running evaluation *****
2023-03-27 15:56:22,177   Epoch = 5 iter 1349 step
2023-03-27 15:56:22,177   Num examples = 1043
2023-03-27 15:56:22,177   Batch size = 32
2023-03-27 15:56:22,178 ***** Eval results *****
2023-03-27 15:56:22,178   att_loss = 0.0052720885058598855
2023-03-27 15:56:22,178   cls_loss = 0.0
2023-03-27 15:56:22,178   global_step = 1349
2023-03-27 15:56:22,178   loss = 0.6263261267117092
2023-03-27 15:56:22,178   rep_loss = 0.6210540320192065
2023-03-27 15:56:22,185 ***** Save model *****
2023-03-27 15:56:33,569 ***** Running evaluation *****
2023-03-27 15:56:33,569   Epoch = 5 iter 1399 step
2023-03-27 15:56:33,569   Num examples = 1043
2023-03-27 15:56:33,569   Batch size = 32
2023-03-27 15:56:33,570 ***** Eval results *****
2023-03-27 15:56:33,570   att_loss = 0.005303948651999235
2023-03-27 15:56:33,571   cls_loss = 0.0
2023-03-27 15:56:33,571   global_step = 1399
2023-03-27 15:56:33,571   loss = 0.6284945784136653
2023-03-27 15:56:33,571   rep_loss = 0.6231906302273273
2023-03-27 15:56:33,578 ***** Save model *****
2023-03-27 15:56:44,952 ***** Running evaluation *****
2023-03-27 15:56:44,952   Epoch = 5 iter 1449 step
2023-03-27 15:56:44,952   Num examples = 1043
2023-03-27 15:56:44,952   Batch size = 32
2023-03-27 15:56:44,953 ***** Eval results *****
2023-03-27 15:56:44,954   att_loss = 0.005279991563344211
2023-03-27 15:56:44,954   cls_loss = 0.0
2023-03-27 15:56:44,954   global_step = 1449
2023-03-27 15:56:44,954   loss = 0.6283843961724064
2023-03-27 15:56:44,954   rep_loss = 0.623104404984859
2023-03-27 15:56:44,962 ***** Save model *****
2023-03-27 15:56:56,358 ***** Running evaluation *****
2023-03-27 15:56:56,359   Epoch = 5 iter 1499 step
2023-03-27 15:56:56,359   Num examples = 1043
2023-03-27 15:56:56,359   Batch size = 32
2023-03-27 15:56:56,361 ***** Eval results *****
2023-03-27 15:56:56,361   att_loss = 0.005285135964385983
2023-03-27 15:56:56,361   cls_loss = 0.0
2023-03-27 15:56:56,361   global_step = 1499
2023-03-27 15:56:56,361   loss = 0.6273402117374467
2023-03-27 15:56:56,361   rep_loss = 0.6220550762444008
2023-03-27 15:56:56,369 ***** Save model *****
2023-03-27 15:57:07,772 ***** Running evaluation *****
2023-03-27 15:57:07,772   Epoch = 5 iter 1549 step
2023-03-27 15:57:07,772   Num examples = 1043
2023-03-27 15:57:07,772   Batch size = 32
2023-03-27 15:57:07,773 ***** Eval results *****
2023-03-27 15:57:07,773   att_loss = 0.005294967475390741
2023-03-27 15:57:07,774   cls_loss = 0.0
2023-03-27 15:57:07,774   global_step = 1549
2023-03-27 15:57:07,774   loss = 0.6267069583741304
2023-03-27 15:57:07,774   rep_loss = 0.6214119922891955
2023-03-27 15:57:07,781 ***** Save model *****
2023-03-27 15:57:19,188 ***** Running evaluation *****
2023-03-27 15:57:19,188   Epoch = 5 iter 1599 step
2023-03-27 15:57:19,188   Num examples = 1043
2023-03-27 15:57:19,188   Batch size = 32
2023-03-27 15:57:19,189 ***** Eval results *****
2023-03-27 15:57:19,190   att_loss = 0.005292023174027263
2023-03-27 15:57:19,190   cls_loss = 0.0
2023-03-27 15:57:19,190   global_step = 1599
2023-03-27 15:57:19,190   loss = 0.6260446707407633
2023-03-27 15:57:19,190   rep_loss = 0.6207526495510881
2023-03-27 15:57:19,198 ***** Save model *****
2023-03-27 15:57:30,607 ***** Running evaluation *****
2023-03-27 15:57:30,607   Epoch = 6 iter 1649 step
2023-03-27 15:57:30,607   Num examples = 1043
2023-03-27 15:57:30,608   Batch size = 32
2023-03-27 15:57:30,609 ***** Eval results *****
2023-03-27 15:57:30,609   att_loss = 0.005231481530961204
2023-03-27 15:57:30,609   cls_loss = 0.0
2023-03-27 15:57:30,610   global_step = 1649
2023-03-27 15:57:30,610   loss = 0.6157555389911571
2023-03-27 15:57:30,610   rep_loss = 0.61052405834198
2023-03-27 15:57:30,617 ***** Save model *****
2023-03-27 15:57:42,036 ***** Running evaluation *****
2023-03-27 15:57:42,036   Epoch = 6 iter 1699 step
2023-03-27 15:57:42,036   Num examples = 1043
2023-03-27 15:57:42,036   Batch size = 32
2023-03-27 15:57:42,037 ***** Eval results *****
2023-03-27 15:57:42,038   att_loss = 0.005251210941405026
2023-03-27 15:57:42,038   cls_loss = 0.0
2023-03-27 15:57:42,038   global_step = 1699
2023-03-27 15:57:42,038   loss = 0.6175035103080199
2023-03-27 15:57:42,038   rep_loss = 0.6122523005475703
2023-03-27 15:57:42,046 ***** Save model *****
2023-03-27 15:57:53,457 ***** Running evaluation *****
2023-03-27 15:57:53,457   Epoch = 6 iter 1749 step
2023-03-27 15:57:53,457   Num examples = 1043
2023-03-27 15:57:53,458   Batch size = 32
2023-03-27 15:57:53,458 ***** Eval results *****
2023-03-27 15:57:53,458   att_loss = 0.005251997137809692
2023-03-27 15:57:53,459   cls_loss = 0.0
2023-03-27 15:57:53,459   global_step = 1749
2023-03-27 15:57:53,459   loss = 0.6163548145975385
2023-03-27 15:57:53,459   rep_loss = 0.6111028186318015
2023-03-27 15:57:53,466 ***** Save model *****
2023-03-27 15:58:04,909 ***** Running evaluation *****
2023-03-27 15:58:04,909   Epoch = 6 iter 1799 step
2023-03-27 15:58:04,909   Num examples = 1043
2023-03-27 15:58:04,909   Batch size = 32
2023-03-27 15:58:04,911 ***** Eval results *****
2023-03-27 15:58:04,911   att_loss = 0.00524895680302442
2023-03-27 15:58:04,912   cls_loss = 0.0
2023-03-27 15:58:04,912   global_step = 1799
2023-03-27 15:58:04,912   loss = 0.615916080281214
2023-03-27 15:58:04,912   rep_loss = 0.6106671246175234
2023-03-27 15:58:04,919 ***** Save model *****
2023-03-27 15:58:16,384 ***** Running evaluation *****
2023-03-27 15:58:16,385   Epoch = 6 iter 1849 step
2023-03-27 15:58:16,385   Num examples = 1043
2023-03-27 15:58:16,385   Batch size = 32
2023-03-27 15:58:16,386 ***** Eval results *****
2023-03-27 15:58:16,386   att_loss = 0.0052419376864969005
2023-03-27 15:58:16,386   cls_loss = 0.0
2023-03-27 15:58:16,386   global_step = 1849
2023-03-27 15:58:16,386   loss = 0.6148072558375988
2023-03-27 15:58:16,387   rep_loss = 0.609565319078654
2023-03-27 15:58:16,393 ***** Save model *****
2023-03-27 15:58:27,831 ***** Running evaluation *****
2023-03-27 15:58:27,832   Epoch = 7 iter 1899 step
2023-03-27 15:58:27,832   Num examples = 1043
2023-03-27 15:58:27,832   Batch size = 32
2023-03-27 15:58:27,833 ***** Eval results *****
2023-03-27 15:58:27,833   att_loss = 0.005113483996440967
2023-03-27 15:58:27,834   cls_loss = 0.0
2023-03-27 15:58:27,834   global_step = 1899
2023-03-27 15:58:27,834   loss = 0.6048370718955993
2023-03-27 15:58:27,834   rep_loss = 0.5997235894203186
2023-03-27 15:58:27,841 ***** Save model *****
2023-03-27 15:58:39,277 ***** Running evaluation *****
2023-03-27 15:58:39,278   Epoch = 7 iter 1949 step
2023-03-27 15:58:39,278   Num examples = 1043
2023-03-27 15:58:39,278   Batch size = 32
2023-03-27 15:58:39,279 ***** Eval results *****
2023-03-27 15:58:39,279   att_loss = 0.005178168066777289
2023-03-27 15:58:39,279   cls_loss = 0.0
2023-03-27 15:58:39,279   global_step = 1949
2023-03-27 15:58:39,279   loss = 0.606122150272131
2023-03-27 15:58:39,279   rep_loss = 0.6009439833462238
2023-03-27 15:58:39,286 ***** Save model *****
2023-03-27 15:58:50,747 ***** Running evaluation *****
2023-03-27 15:58:50,748   Epoch = 7 iter 1999 step
2023-03-27 15:58:50,748   Num examples = 1043
2023-03-27 15:58:50,748   Batch size = 32
2023-03-27 15:58:50,749 ***** Eval results *****
2023-03-27 15:58:50,749   att_loss = 0.0051981591834471775
2023-03-27 15:58:50,750   cls_loss = 0.0
2023-03-27 15:58:50,750   global_step = 1999
2023-03-27 15:58:50,750   loss = 0.6063107155836546
2023-03-27 15:58:50,750   rep_loss = 0.6011125569160168
2023-03-27 15:58:50,757 ***** Save model *****
2023-03-27 15:59:02,198 ***** Running evaluation *****
2023-03-27 15:59:02,199   Epoch = 7 iter 2049 step
2023-03-27 15:59:02,199   Num examples = 1043
2023-03-27 15:59:02,199   Batch size = 32
2023-03-27 15:59:02,200 ***** Eval results *****
2023-03-27 15:59:02,200   att_loss = 0.005191907710913155
2023-03-27 15:59:02,200   cls_loss = 0.0
2023-03-27 15:59:02,200   global_step = 2049
2023-03-27 15:59:02,200   loss = 0.6059514863623513
2023-03-27 15:59:02,201   rep_loss = 0.6007595787445704
2023-03-27 15:59:02,207 ***** Save model *****
2023-03-27 15:59:13,638 ***** Running evaluation *****
2023-03-27 15:59:13,638   Epoch = 7 iter 2099 step
2023-03-27 15:59:13,638   Num examples = 1043
2023-03-27 15:59:13,639   Batch size = 32
2023-03-27 15:59:13,641 ***** Eval results *****
2023-03-27 15:59:13,641   att_loss = 0.005189126968869697
2023-03-27 15:59:13,641   cls_loss = 0.0
2023-03-27 15:59:13,641   global_step = 2099
2023-03-27 15:59:13,642   loss = 0.6058780436930449
2023-03-27 15:59:13,642   rep_loss = 0.600688917481381
2023-03-27 15:59:13,649 ***** Save model *****
2023-03-27 15:59:25,075 ***** Running evaluation *****
2023-03-27 15:59:25,076   Epoch = 8 iter 2149 step
2023-03-27 15:59:25,076   Num examples = 1043
2023-03-27 15:59:25,076   Batch size = 32
2023-03-27 15:59:25,077 ***** Eval results *****
2023-03-27 15:59:25,078   att_loss = 0.005148873843539219
2023-03-27 15:59:25,078   cls_loss = 0.0
2023-03-27 15:59:25,078   global_step = 2149
2023-03-27 15:59:25,078   loss = 0.6057797945462741
2023-03-27 15:59:25,078   rep_loss = 0.6006309206669147
2023-03-27 15:59:25,080 ***** Save model *****
2023-03-27 15:59:36,500 ***** Running evaluation *****
2023-03-27 15:59:36,500   Epoch = 8 iter 2199 step
2023-03-27 15:59:36,500   Num examples = 1043
2023-03-27 15:59:36,501   Batch size = 32
2023-03-27 15:59:36,502 ***** Eval results *****
2023-03-27 15:59:36,502   att_loss = 0.005157732138676303
2023-03-27 15:59:36,502   cls_loss = 0.0
2023-03-27 15:59:36,502   global_step = 2199
2023-03-27 15:59:36,503   loss = 0.6012409092887999
2023-03-27 15:59:36,503   rep_loss = 0.5960831746222481
2023-03-27 15:59:36,510 ***** Save model *****
2023-03-27 15:59:47,920 ***** Running evaluation *****
2023-03-27 15:59:47,921   Epoch = 8 iter 2249 step
2023-03-27 15:59:47,921   Num examples = 1043
2023-03-27 15:59:47,921   Batch size = 32
2023-03-27 15:59:47,922 ***** Eval results *****
2023-03-27 15:59:47,922   att_loss = 0.0051449236781459995
2023-03-27 15:59:47,923   cls_loss = 0.0
2023-03-27 15:59:47,923   global_step = 2249
2023-03-27 15:59:47,923   loss = 0.6004306093781395
2023-03-27 15:59:47,923   rep_loss = 0.5952856862439518
2023-03-27 15:59:47,930 ***** Save model *****
2023-03-27 15:59:59,329 ***** Running evaluation *****
2023-03-27 15:59:59,330   Epoch = 8 iter 2299 step
2023-03-27 15:59:59,330   Num examples = 1043
2023-03-27 15:59:59,330   Batch size = 32
2023-03-27 15:59:59,331 ***** Eval results *****
2023-03-27 15:59:59,331   att_loss = 0.005123766019163322
2023-03-27 15:59:59,332   cls_loss = 0.0
2023-03-27 15:59:59,332   global_step = 2299
2023-03-27 15:59:59,332   loss = 0.5990781601221283
2023-03-27 15:59:59,332   rep_loss = 0.5939543934687515
2023-03-27 15:59:59,339 ***** Save model *****
2023-03-27 16:00:10,742 ***** Running evaluation *****
2023-03-27 16:00:10,742   Epoch = 8 iter 2349 step
2023-03-27 16:00:10,742   Num examples = 1043
2023-03-27 16:00:10,743   Batch size = 32
2023-03-27 16:00:10,743 ***** Eval results *****
2023-03-27 16:00:10,744   att_loss = 0.005126285482582772
2023-03-27 16:00:10,744   cls_loss = 0.0
2023-03-27 16:00:10,744   global_step = 2349
2023-03-27 16:00:10,744   loss = 0.598605367499338
2023-03-27 16:00:10,744   rep_loss = 0.5934790812187911
2023-03-27 16:00:10,746 ***** Save model *****
2023-03-27 16:00:22,121 ***** Running evaluation *****
2023-03-27 16:00:22,122   Epoch = 8 iter 2399 step
2023-03-27 16:00:22,122   Num examples = 1043
2023-03-27 16:00:22,122   Batch size = 32
2023-03-27 16:00:22,124 ***** Eval results *****
2023-03-27 16:00:22,124   att_loss = 0.005134294926766094
2023-03-27 16:00:22,124   cls_loss = 0.0
2023-03-27 16:00:22,124   global_step = 2399
2023-03-27 16:00:22,124   loss = 0.5987042639645334
2023-03-27 16:00:22,124   rep_loss = 0.5935699681365445
2023-03-27 16:00:22,164 ***** Save model *****
2023-03-27 16:00:33,645 ***** Running evaluation *****
2023-03-27 16:00:33,645   Epoch = 9 iter 2449 step
2023-03-27 16:00:33,645   Num examples = 1043
2023-03-27 16:00:33,646   Batch size = 32
2023-03-27 16:00:33,647 ***** Eval results *****
2023-03-27 16:00:33,647   att_loss = 0.00508936315410487
2023-03-27 16:00:33,647   cls_loss = 0.0
2023-03-27 16:00:33,647   global_step = 2449
2023-03-27 16:00:33,648   loss = 0.5938428471917692
2023-03-27 16:00:33,648   rep_loss = 0.5887534812740658
2023-03-27 16:00:33,662 ***** Save model *****
2023-03-27 16:00:44,995 ***** Running evaluation *****
2023-03-27 16:00:44,995   Epoch = 9 iter 2499 step
2023-03-27 16:00:44,995   Num examples = 1043
2023-03-27 16:00:44,996   Batch size = 32
2023-03-27 16:00:44,997 ***** Eval results *****
2023-03-27 16:00:44,997   att_loss = 0.005115043920037958
2023-03-27 16:00:44,997   cls_loss = 0.0
2023-03-27 16:00:44,997   global_step = 2499
2023-03-27 16:00:44,997   loss = 0.5935081901649634
2023-03-27 16:00:44,998   rep_loss = 0.5883931455512842
2023-03-27 16:00:45,004 ***** Save model *****
2023-03-27 16:00:56,392 ***** Running evaluation *****
2023-03-27 16:00:56,393   Epoch = 9 iter 2549 step
2023-03-27 16:00:56,393   Num examples = 1043
2023-03-27 16:00:56,394   Batch size = 32
2023-03-27 16:00:56,395 ***** Eval results *****
2023-03-27 16:00:56,395   att_loss = 0.005118139301847718
2023-03-27 16:00:56,395   cls_loss = 0.0
2023-03-27 16:00:56,396   global_step = 2549
2023-03-27 16:00:56,396   loss = 0.5927470216195877
2023-03-27 16:00:56,396   rep_loss = 0.5876288822252457
2023-03-27 16:00:56,403 ***** Save model *****
2023-03-27 16:01:07,777 ***** Running evaluation *****
2023-03-27 16:01:07,777   Epoch = 9 iter 2599 step
2023-03-27 16:01:07,778   Num examples = 1043
2023-03-27 16:01:07,778   Batch size = 32
2023-03-27 16:01:07,779 ***** Eval results *****
2023-03-27 16:01:07,779   att_loss = 0.005122186397487412
2023-03-27 16:01:07,779   cls_loss = 0.0
2023-03-27 16:01:07,780   global_step = 2599
2023-03-27 16:01:07,780   loss = 0.5929118036007395
2023-03-27 16:01:07,780   rep_loss = 0.5877896170226895
2023-03-27 16:01:07,787 ***** Save model *****
2023-03-27 16:01:19,178 ***** Running evaluation *****
2023-03-27 16:01:19,179   Epoch = 9 iter 2649 step
2023-03-27 16:01:19,179   Num examples = 1043
2023-03-27 16:01:19,179   Batch size = 32
2023-03-27 16:01:19,180 ***** Eval results *****
2023-03-27 16:01:19,180   att_loss = 0.005106445318617956
2023-03-27 16:01:19,181   cls_loss = 0.0
2023-03-27 16:01:19,181   global_step = 2649
2023-03-27 16:01:19,181   loss = 0.5922907334033066
2023-03-27 16:01:19,181   rep_loss = 0.5871842881528343
2023-03-27 16:01:19,188 ***** Save model *****
2023-03-27 16:01:30,572 ***** Running evaluation *****
2023-03-27 16:01:30,573   Epoch = 10 iter 2699 step
2023-03-27 16:01:30,573   Num examples = 1043
2023-03-27 16:01:30,573   Batch size = 32
2023-03-27 16:01:30,575 ***** Eval results *****
2023-03-27 16:01:30,575   att_loss = 0.0051353722489599525
2023-03-27 16:01:30,575   cls_loss = 0.0
2023-03-27 16:01:30,575   global_step = 2699
2023-03-27 16:01:30,575   loss = 0.5927278563894075
2023-03-27 16:01:30,575   rep_loss = 0.5875924866774986
2023-03-27 16:01:30,582 ***** Save model *****
2023-03-27 16:01:41,957 ***** Running evaluation *****
2023-03-27 16:01:41,957   Epoch = 10 iter 2749 step
2023-03-27 16:01:41,957   Num examples = 1043
2023-03-27 16:01:41,957   Batch size = 32
2023-03-27 16:01:41,958 ***** Eval results *****
2023-03-27 16:01:41,959   att_loss = 0.005089004300063169
2023-03-27 16:01:41,959   cls_loss = 0.0
2023-03-27 16:01:41,959   global_step = 2749
2023-03-27 16:01:41,959   loss = 0.5904849579062643
2023-03-27 16:01:41,959   rep_loss = 0.5853959563412244
2023-03-27 16:01:41,966 ***** Save model *****
2023-03-27 16:01:53,356 ***** Running evaluation *****
2023-03-27 16:01:53,356   Epoch = 10 iter 2799 step
2023-03-27 16:01:53,356   Num examples = 1043
2023-03-27 16:01:53,356   Batch size = 32
2023-03-27 16:01:53,357 ***** Eval results *****
2023-03-27 16:01:53,357   att_loss = 0.005070050561064204
2023-03-27 16:01:53,358   cls_loss = 0.0
2023-03-27 16:01:53,358   global_step = 2799
2023-03-27 16:01:53,358   loss = 0.588505699653034
2023-03-27 16:01:53,358   rep_loss = 0.5834356514058372
2023-03-27 16:01:53,361 ***** Save model *****
2023-03-27 16:02:04,738 ***** Running evaluation *****
2023-03-27 16:02:04,738   Epoch = 10 iter 2849 step
2023-03-27 16:02:04,738   Num examples = 1043
2023-03-27 16:02:04,738   Batch size = 32
2023-03-27 16:02:04,739 ***** Eval results *****
2023-03-27 16:02:04,740   att_loss = 0.005092119540519721
2023-03-27 16:02:04,740   cls_loss = 0.0
2023-03-27 16:02:04,740   global_step = 2849
2023-03-27 16:02:04,740   loss = 0.5884215808447513
2023-03-27 16:02:04,740   rep_loss = 0.5833294631382606
2023-03-27 16:02:04,748 ***** Save model *****
2023-03-27 16:02:16,128 ***** Running evaluation *****
2023-03-27 16:02:16,128   Epoch = 10 iter 2899 step
2023-03-27 16:02:16,128   Num examples = 1043
2023-03-27 16:02:16,128   Batch size = 32
2023-03-27 16:02:16,130 ***** Eval results *****
2023-03-27 16:02:16,131   att_loss = 0.005081639669868103
2023-03-27 16:02:16,131   cls_loss = 0.0
2023-03-27 16:02:16,131   global_step = 2899
2023-03-27 16:02:16,131   loss = 0.5872614006288187
2023-03-27 16:02:16,131   rep_loss = 0.5821797616096563
2023-03-27 16:02:16,138 ***** Save model *****
2023-03-27 16:02:27,494 ***** Running evaluation *****
2023-03-27 16:02:27,494   Epoch = 11 iter 2949 step
2023-03-27 16:02:27,494   Num examples = 1043
2023-03-27 16:02:27,495   Batch size = 32
2023-03-27 16:02:27,495 ***** Eval results *****
2023-03-27 16:02:27,496   att_loss = 0.005065099724257986
2023-03-27 16:02:27,496   cls_loss = 0.0
2023-03-27 16:02:27,496   global_step = 2949
2023-03-27 16:02:27,496   loss = 0.5857657591501871
2023-03-27 16:02:27,496   rep_loss = 0.5807006657123566
2023-03-27 16:02:27,503 ***** Save model *****
2023-03-27 16:02:38,900 ***** Running evaluation *****
2023-03-27 16:02:38,900   Epoch = 11 iter 2999 step
2023-03-27 16:02:38,900   Num examples = 1043
2023-03-27 16:02:38,900   Batch size = 32
2023-03-27 16:02:38,902 ***** Eval results *****
2023-03-27 16:02:38,902   att_loss = 0.004998223901155495
2023-03-27 16:02:38,902   cls_loss = 0.0
2023-03-27 16:02:38,903   global_step = 2999
2023-03-27 16:02:38,903   loss = 0.5816128234709462
2023-03-27 16:02:38,903   rep_loss = 0.5766146009968173
2023-03-27 16:02:38,910 ***** Save model *****
2023-03-27 16:02:50,252 ***** Running evaluation *****
2023-03-27 16:02:50,252   Epoch = 11 iter 3049 step
2023-03-27 16:02:50,252   Num examples = 1043
2023-03-27 16:02:50,252   Batch size = 32
2023-03-27 16:02:50,254 ***** Eval results *****
2023-03-27 16:02:50,254   att_loss = 0.0050066018219305465
2023-03-27 16:02:50,254   cls_loss = 0.0
2023-03-27 16:02:50,254   global_step = 3049
2023-03-27 16:02:50,254   loss = 0.5819164679518768
2023-03-27 16:02:50,255   rep_loss = 0.5769098672483649
2023-03-27 16:02:50,261 ***** Save model *****
2023-03-27 16:03:01,626 ***** Running evaluation *****
2023-03-27 16:03:01,626   Epoch = 11 iter 3099 step
2023-03-27 16:03:01,626   Num examples = 1043
2023-03-27 16:03:01,626   Batch size = 32
2023-03-27 16:03:01,628 ***** Eval results *****
2023-03-27 16:03:01,628   att_loss = 0.005004737690022147
2023-03-27 16:03:01,628   cls_loss = 0.0
2023-03-27 16:03:01,628   global_step = 3099
2023-03-27 16:03:01,628   loss = 0.58114046594243
2023-03-27 16:03:01,629   rep_loss = 0.5761357280943129
2023-03-27 16:03:01,636 ***** Save model *****
2023-03-27 16:03:13,023 ***** Running evaluation *****
2023-03-27 16:03:13,023   Epoch = 11 iter 3149 step
2023-03-27 16:03:13,023   Num examples = 1043
2023-03-27 16:03:13,024   Batch size = 32
2023-03-27 16:03:13,025 ***** Eval results *****
2023-03-27 16:03:13,025   att_loss = 0.005011878545255453
2023-03-27 16:03:13,025   cls_loss = 0.0
2023-03-27 16:03:13,025   global_step = 3149
2023-03-27 16:03:13,026   loss = 0.5803038570678459
2023-03-27 16:03:13,026   rep_loss = 0.5752919788630504
2023-03-27 16:03:13,033 ***** Save model *****
2023-03-27 16:03:24,440 ***** Running evaluation *****
2023-03-27 16:03:24,441   Epoch = 11 iter 3199 step
2023-03-27 16:03:24,441   Num examples = 1043
2023-03-27 16:03:24,441   Batch size = 32
2023-03-27 16:03:24,442 ***** Eval results *****
2023-03-27 16:03:24,443   att_loss = 0.0050167323353181355
2023-03-27 16:03:24,443   cls_loss = 0.0
2023-03-27 16:03:24,443   global_step = 3199
2023-03-27 16:03:24,443   loss = 0.5808674649882862
2023-03-27 16:03:24,443   rep_loss = 0.5758507329089041
2023-03-27 16:03:24,451 ***** Save model *****
2023-03-27 16:03:35,867 ***** Running evaluation *****
2023-03-27 16:03:35,868   Epoch = 12 iter 3249 step
2023-03-27 16:03:35,868   Num examples = 1043
2023-03-27 16:03:35,868   Batch size = 32
2023-03-27 16:03:35,869 ***** Eval results *****
2023-03-27 16:03:35,869   att_loss = 0.0049997344716555545
2023-03-27 16:03:35,869   cls_loss = 0.0
2023-03-27 16:03:35,869   global_step = 3249
2023-03-27 16:03:35,869   loss = 0.5745918922954135
2023-03-27 16:03:35,869   rep_loss = 0.5695921566751269
2023-03-27 16:03:35,871 ***** Save model *****
2023-03-27 16:03:47,252 ***** Running evaluation *****
2023-03-27 16:03:47,253   Epoch = 12 iter 3299 step
2023-03-27 16:03:47,253   Num examples = 1043
2023-03-27 16:03:47,254   Batch size = 32
2023-03-27 16:03:47,255 ***** Eval results *****
2023-03-27 16:03:47,256   att_loss = 0.0050235742408978315
2023-03-27 16:03:47,256   cls_loss = 0.0
2023-03-27 16:03:47,256   global_step = 3299
2023-03-27 16:03:47,256   loss = 0.5766076602433857
2023-03-27 16:03:47,256   rep_loss = 0.5715840854142842
2023-03-27 16:03:47,263 ***** Save model *****
2023-03-27 16:03:58,644 ***** Running evaluation *****
2023-03-27 16:03:58,644   Epoch = 12 iter 3349 step
2023-03-27 16:03:58,644   Num examples = 1043
2023-03-27 16:03:58,644   Batch size = 32
2023-03-27 16:03:58,646 ***** Eval results *****
2023-03-27 16:03:58,646   att_loss = 0.005014432311571878
2023-03-27 16:03:58,646   cls_loss = 0.0
2023-03-27 16:03:58,647   global_step = 3349
2023-03-27 16:03:58,647   loss = 0.5766569474647785
2023-03-27 16:03:58,647   rep_loss = 0.5716425135217864
2023-03-27 16:03:58,654 ***** Save model *****
2023-03-27 16:04:10,075 ***** Running evaluation *****
2023-03-27 16:04:10,076   Epoch = 12 iter 3399 step
2023-03-27 16:04:10,076   Num examples = 1043
2023-03-27 16:04:10,076   Batch size = 32
2023-03-27 16:04:10,077 ***** Eval results *****
2023-03-27 16:04:10,077   att_loss = 0.005002168831057274
2023-03-27 16:04:10,077   cls_loss = 0.0
2023-03-27 16:04:10,078   global_step = 3399
2023-03-27 16:04:10,078   loss = 0.576677642418788
2023-03-27 16:04:10,078   rep_loss = 0.5716754729931172
2023-03-27 16:04:10,086 ***** Save model *****
2023-03-27 16:04:21,507 ***** Running evaluation *****
2023-03-27 16:04:21,507   Epoch = 12 iter 3449 step
2023-03-27 16:04:21,507   Num examples = 1043
2023-03-27 16:04:21,507   Batch size = 32
2023-03-27 16:04:21,509 ***** Eval results *****
2023-03-27 16:04:21,509   att_loss = 0.005001461735869549
2023-03-27 16:04:21,509   cls_loss = 0.0
2023-03-27 16:04:21,509   global_step = 3449
2023-03-27 16:04:21,509   loss = 0.5766253714658776
2023-03-27 16:04:21,510   rep_loss = 0.5716239089868507
2023-03-27 16:04:21,517 ***** Save model *****
2023-03-27 16:04:32,897 ***** Running evaluation *****
2023-03-27 16:04:32,898   Epoch = 13 iter 3499 step
2023-03-27 16:04:32,898   Num examples = 1043
2023-03-27 16:04:32,898   Batch size = 32
2023-03-27 16:04:32,899 ***** Eval results *****
2023-03-27 16:04:32,899   att_loss = 0.005058855832820492
2023-03-27 16:04:32,900   cls_loss = 0.0
2023-03-27 16:04:32,900   global_step = 3499
2023-03-27 16:04:32,900   loss = 0.5756687990256718
2023-03-27 16:04:32,900   rep_loss = 0.5706099420785904
2023-03-27 16:04:32,907 ***** Save model *****
2023-03-27 16:04:44,308 ***** Running evaluation *****
2023-03-27 16:04:44,308   Epoch = 13 iter 3549 step
2023-03-27 16:04:44,308   Num examples = 1043
2023-03-27 16:04:44,309   Batch size = 32
2023-03-27 16:04:44,310 ***** Eval results *****
2023-03-27 16:04:44,310   att_loss = 0.004969771127574719
2023-03-27 16:04:44,310   cls_loss = 0.0
2023-03-27 16:04:44,310   global_step = 3549
2023-03-27 16:04:44,311   loss = 0.5735852244572762
2023-03-27 16:04:44,311   rep_loss = 0.5686154518371973
2023-03-27 16:04:44,318 ***** Save model *****
2023-03-27 16:04:55,729 ***** Running evaluation *****
2023-03-27 16:04:55,729   Epoch = 13 iter 3599 step
2023-03-27 16:04:55,729   Num examples = 1043
2023-03-27 16:04:55,729   Batch size = 32
2023-03-27 16:04:55,731 ***** Eval results *****
2023-03-27 16:04:55,731   att_loss = 0.004975617513991892
2023-03-27 16:04:55,732   cls_loss = 0.0
2023-03-27 16:04:55,732   global_step = 3599
2023-03-27 16:04:55,732   loss = 0.573342896066606
2023-03-27 16:04:55,732   rep_loss = 0.5683672768063843
2023-03-27 16:04:55,739 ***** Save model *****
2023-03-27 16:05:07,125 ***** Running evaluation *****
2023-03-27 16:05:07,125   Epoch = 13 iter 3649 step
2023-03-27 16:05:07,126   Num examples = 1043
2023-03-27 16:05:07,126   Batch size = 32
2023-03-27 16:05:07,127 ***** Eval results *****
2023-03-27 16:05:07,127   att_loss = 0.004980227002204301
2023-03-27 16:05:07,128   cls_loss = 0.0
2023-03-27 16:05:07,128   global_step = 3649
2023-03-27 16:05:07,128   loss = 0.5728816282883119
2023-03-27 16:05:07,128   rep_loss = 0.5679014003678654
2023-03-27 16:05:07,130 ***** Save model *****
2023-03-27 16:05:18,523 ***** Running evaluation *****
2023-03-27 16:05:18,523   Epoch = 13 iter 3699 step
2023-03-27 16:05:18,523   Num examples = 1043
2023-03-27 16:05:18,523   Batch size = 32
2023-03-27 16:05:18,524 ***** Eval results *****
2023-03-27 16:05:18,525   att_loss = 0.004976521734262637
2023-03-27 16:05:18,525   cls_loss = 0.0
2023-03-27 16:05:18,525   global_step = 3699
2023-03-27 16:05:18,525   loss = 0.5729063997666041
2023-03-27 16:05:18,526   rep_loss = 0.5679298780466381
2023-03-27 16:05:18,533 ***** Save model *****
2023-03-27 16:05:29,940 ***** Running evaluation *****
2023-03-27 16:05:29,940   Epoch = 14 iter 3749 step
2023-03-27 16:05:29,940   Num examples = 1043
2023-03-27 16:05:29,940   Batch size = 32
2023-03-27 16:05:29,942 ***** Eval results *****
2023-03-27 16:05:29,942   att_loss = 0.004961139226162975
2023-03-27 16:05:29,942   cls_loss = 0.0
2023-03-27 16:05:29,942   global_step = 3749
2023-03-27 16:05:29,942   loss = 0.5670121962373907
2023-03-27 16:05:29,943   rep_loss = 0.5620510578155518
2023-03-27 16:05:29,950 ***** Save model *****
2023-03-27 16:05:41,354 ***** Running evaluation *****
2023-03-27 16:05:41,355   Epoch = 14 iter 3799 step
2023-03-27 16:05:41,355   Num examples = 1043
2023-03-27 16:05:41,355   Batch size = 32
2023-03-27 16:05:41,356 ***** Eval results *****
2023-03-27 16:05:41,356   att_loss = 0.0049489197534982295
2023-03-27 16:05:41,356   cls_loss = 0.0
2023-03-27 16:05:41,356   global_step = 3799
2023-03-27 16:05:41,356   loss = 0.5690726716010297
2023-03-27 16:05:41,357   rep_loss = 0.5641237516872218
2023-03-27 16:05:41,364 ***** Save model *****
2023-03-27 16:05:52,763 ***** Running evaluation *****
2023-03-27 16:05:52,763   Epoch = 14 iter 3849 step
2023-03-27 16:05:52,763   Num examples = 1043
2023-03-27 16:05:52,763   Batch size = 32
2023-03-27 16:05:52,764 ***** Eval results *****
2023-03-27 16:05:52,764   att_loss = 0.004917258342207821
2023-03-27 16:05:52,765   cls_loss = 0.0
2023-03-27 16:05:52,765   global_step = 3849
2023-03-27 16:05:52,765   loss = 0.5681770250603959
2023-03-27 16:05:52,765   rep_loss = 0.5632597675194612
2023-03-27 16:05:52,772 ***** Save model *****
2023-03-27 16:06:04,168 ***** Running evaluation *****
2023-03-27 16:06:04,169   Epoch = 14 iter 3899 step
2023-03-27 16:06:04,169   Num examples = 1043
2023-03-27 16:06:04,169   Batch size = 32
2023-03-27 16:06:04,171 ***** Eval results *****
2023-03-27 16:06:04,171   att_loss = 0.004946982311605482
2023-03-27 16:06:04,171   cls_loss = 0.0
2023-03-27 16:06:04,171   global_step = 3899
2023-03-27 16:06:04,172   loss = 0.5699260049725171
2023-03-27 16:06:04,172   rep_loss = 0.5649790227042962
2023-03-27 16:06:04,179 ***** Save model *****
2023-03-27 16:06:15,617 ***** Running evaluation *****
2023-03-27 16:06:15,617   Epoch = 14 iter 3949 step
2023-03-27 16:06:15,617   Num examples = 1043
2023-03-27 16:06:15,617   Batch size = 32
2023-03-27 16:06:15,619 ***** Eval results *****
2023-03-27 16:06:15,619   att_loss = 0.004961735809495523
2023-03-27 16:06:15,620   cls_loss = 0.0
2023-03-27 16:06:15,620   global_step = 3949
2023-03-27 16:06:15,620   loss = 0.5707725430551863
2023-03-27 16:06:15,620   rep_loss = 0.5658108075083149
2023-03-27 16:06:15,627 ***** Save model *****
2023-03-27 16:06:27,041 ***** Running evaluation *****
2023-03-27 16:06:27,041   Epoch = 14 iter 3999 step
2023-03-27 16:06:27,042   Num examples = 1043
2023-03-27 16:06:27,042   Batch size = 32
2023-03-27 16:06:27,043 ***** Eval results *****
2023-03-27 16:06:27,043   att_loss = 0.00494987706357606
2023-03-27 16:06:27,043   cls_loss = 0.0
2023-03-27 16:06:27,043   global_step = 3999
2023-03-27 16:06:27,043   loss = 0.569825154611434
2023-03-27 16:06:27,043   rep_loss = 0.5648752768377692
2023-03-27 16:06:27,051 ***** Save model *****
2023-03-27 16:06:38,469 ***** Running evaluation *****
2023-03-27 16:06:38,469   Epoch = 15 iter 4049 step
2023-03-27 16:06:38,469   Num examples = 1043
2023-03-27 16:06:38,469   Batch size = 32
2023-03-27 16:06:38,471 ***** Eval results *****
2023-03-27 16:06:38,471   att_loss = 0.004914909908124669
2023-03-27 16:06:38,471   cls_loss = 0.0
2023-03-27 16:06:38,471   global_step = 4049
2023-03-27 16:06:38,472   loss = 0.5625050487843427
2023-03-27 16:06:38,472   rep_loss = 0.5575901351191781
2023-03-27 16:06:38,479 ***** Save model *****
2023-03-27 16:06:49,891 ***** Running evaluation *****
2023-03-27 16:06:49,891   Epoch = 15 iter 4099 step
2023-03-27 16:06:49,891   Num examples = 1043
2023-03-27 16:06:49,891   Batch size = 32
2023-03-27 16:06:49,893 ***** Eval results *****
2023-03-27 16:06:49,893   att_loss = 0.0049155550275711305
2023-03-27 16:06:49,893   cls_loss = 0.0
2023-03-27 16:06:49,893   global_step = 4099
2023-03-27 16:06:49,894   loss = 0.5640470575779042
2023-03-27 16:06:49,894   rep_loss = 0.5591315012028877
2023-03-27 16:06:49,901 ***** Save model *****
2023-03-27 16:07:01,277 ***** Running evaluation *****
2023-03-27 16:07:01,278   Epoch = 15 iter 4149 step
2023-03-27 16:07:01,278   Num examples = 1043
2023-03-27 16:07:01,278   Batch size = 32
2023-03-27 16:07:01,279 ***** Eval results *****
2023-03-27 16:07:01,279   att_loss = 0.00490932646789588
2023-03-27 16:07:01,279   cls_loss = 0.0
2023-03-27 16:07:01,279   global_step = 4149
2023-03-27 16:07:01,279   loss = 0.5650684403048621
2023-03-27 16:07:01,279   rep_loss = 0.5601591128442023
2023-03-27 16:07:01,286 ***** Save model *****
2023-03-27 16:07:12,692 ***** Running evaluation *****
2023-03-27 16:07:12,693   Epoch = 15 iter 4199 step
2023-03-27 16:07:12,693   Num examples = 1043
2023-03-27 16:07:12,693   Batch size = 32
2023-03-27 16:07:12,695 ***** Eval results *****
2023-03-27 16:07:12,695   att_loss = 0.004898754839026897
2023-03-27 16:07:12,695   cls_loss = 0.0
2023-03-27 16:07:12,695   global_step = 4199
2023-03-27 16:07:12,695   loss = 0.5652768418346483
2023-03-27 16:07:12,695   rep_loss = 0.5603780863211327
2023-03-27 16:07:12,703 ***** Save model *****
2023-03-27 16:07:24,105 ***** Running evaluation *****
2023-03-27 16:07:24,105   Epoch = 15 iter 4249 step
2023-03-27 16:07:24,106   Num examples = 1043
2023-03-27 16:07:24,106   Batch size = 32
2023-03-27 16:07:24,107 ***** Eval results *****
2023-03-27 16:07:24,107   att_loss = 0.004906761127936303
2023-03-27 16:07:24,108   cls_loss = 0.0
2023-03-27 16:07:24,108   global_step = 4249
2023-03-27 16:07:24,108   loss = 0.5654658909703865
2023-03-27 16:07:24,108   rep_loss = 0.5605591296172533
2023-03-27 16:07:24,116 ***** Save model *****
2023-03-27 16:07:35,513 ***** Running evaluation *****
2023-03-27 16:07:35,513   Epoch = 16 iter 4299 step
2023-03-27 16:07:35,513   Num examples = 1043
2023-03-27 16:07:35,513   Batch size = 32
2023-03-27 16:07:35,514 ***** Eval results *****
2023-03-27 16:07:35,515   att_loss = 0.004863409032286318
2023-03-27 16:07:35,515   cls_loss = 0.0
2023-03-27 16:07:35,515   global_step = 4299
2023-03-27 16:07:35,515   loss = 0.561107149830571
2023-03-27 16:07:35,516   rep_loss = 0.5562437397462351
2023-03-27 16:07:35,518 ***** Save model *****
2023-03-27 16:07:46,937 ***** Running evaluation *****
2023-03-27 16:07:46,937   Epoch = 16 iter 4349 step
2023-03-27 16:07:46,937   Num examples = 1043
2023-03-27 16:07:46,937   Batch size = 32
2023-03-27 16:07:46,938 ***** Eval results *****
2023-03-27 16:07:46,939   att_loss = 0.0048724646111587425
2023-03-27 16:07:46,939   cls_loss = 0.0
2023-03-27 16:07:46,939   global_step = 4349
2023-03-27 16:07:46,939   loss = 0.563237548648537
2023-03-27 16:07:46,940   rep_loss = 0.558365084134139
2023-03-27 16:07:46,942 ***** Save model *****
2023-03-27 16:07:58,363 ***** Running evaluation *****
2023-03-27 16:07:58,363   Epoch = 16 iter 4399 step
2023-03-27 16:07:58,363   Num examples = 1043
2023-03-27 16:07:58,363   Batch size = 32
2023-03-27 16:07:58,364 ***** Eval results *****
2023-03-27 16:07:58,364   att_loss = 0.004892694789505615
2023-03-27 16:07:58,365   cls_loss = 0.0
2023-03-27 16:07:58,365   global_step = 4399
2023-03-27 16:07:58,365   loss = 0.5627626828321322
2023-03-27 16:07:58,365   rep_loss = 0.557869989102281
2023-03-27 16:07:58,372 ***** Save model *****
2023-03-27 16:08:09,777 ***** Running evaluation *****
2023-03-27 16:08:09,777   Epoch = 16 iter 4449 step
2023-03-27 16:08:09,777   Num examples = 1043
2023-03-27 16:08:09,777   Batch size = 32
2023-03-27 16:08:09,778 ***** Eval results *****
2023-03-27 16:08:09,779   att_loss = 0.004893668016976556
2023-03-27 16:08:09,779   cls_loss = 0.0
2023-03-27 16:08:09,779   global_step = 4449
2023-03-27 16:08:09,779   loss = 0.5633793931222905
2023-03-27 16:08:09,779   rep_loss = 0.5584857261787026
2023-03-27 16:08:09,787 ***** Save model *****
2023-03-27 16:08:21,200 ***** Running evaluation *****
2023-03-27 16:08:21,200   Epoch = 16 iter 4499 step
2023-03-27 16:08:21,200   Num examples = 1043
2023-03-27 16:08:21,200   Batch size = 32
2023-03-27 16:08:21,202 ***** Eval results *****
2023-03-27 16:08:21,203   att_loss = 0.004889415466279973
2023-03-27 16:08:21,203   cls_loss = 0.0
2023-03-27 16:08:21,203   global_step = 4499
2023-03-27 16:08:21,203   loss = 0.5627848049092398
2023-03-27 16:08:21,203   rep_loss = 0.5578953902101726
2023-03-27 16:08:21,210 ***** Save model *****
2023-03-27 16:08:32,630 ***** Running evaluation *****
2023-03-27 16:08:32,630   Epoch = 17 iter 4549 step
2023-03-27 16:08:32,630   Num examples = 1043
2023-03-27 16:08:32,630   Batch size = 32
2023-03-27 16:08:32,632 ***** Eval results *****
2023-03-27 16:08:32,632   att_loss = 0.004815820232033729
2023-03-27 16:08:32,632   cls_loss = 0.0
2023-03-27 16:08:32,633   global_step = 4549
2023-03-27 16:08:32,633   loss = 0.555840653181076
2023-03-27 16:08:32,633   rep_loss = 0.5510248363018035
2023-03-27 16:08:32,640 ***** Save model *****
2023-03-27 16:08:44,070 ***** Running evaluation *****
2023-03-27 16:08:44,070   Epoch = 17 iter 4599 step
2023-03-27 16:08:44,070   Num examples = 1043
2023-03-27 16:08:44,071   Batch size = 32
2023-03-27 16:08:44,071 ***** Eval results *****
2023-03-27 16:08:44,072   att_loss = 0.0048149141327788435
2023-03-27 16:08:44,072   cls_loss = 0.0
2023-03-27 16:08:44,072   global_step = 4599
2023-03-27 16:08:44,072   loss = 0.5567185948292415
2023-03-27 16:08:44,072   rep_loss = 0.5519036819537481
2023-03-27 16:08:44,079 ***** Save model *****
2023-03-27 16:08:55,482 ***** Running evaluation *****
2023-03-27 16:08:55,483   Epoch = 17 iter 4649 step
2023-03-27 16:08:55,483   Num examples = 1043
2023-03-27 16:08:55,483   Batch size = 32
2023-03-27 16:08:55,484 ***** Eval results *****
2023-03-27 16:08:55,484   att_loss = 0.004835114230147817
2023-03-27 16:08:55,485   cls_loss = 0.0
2023-03-27 16:08:55,485   global_step = 4649
2023-03-27 16:08:55,485   loss = 0.558088388226249
2023-03-27 16:08:55,485   rep_loss = 0.5532532735304398
2023-03-27 16:08:55,492 ***** Save model *****
2023-03-27 16:09:06,904 ***** Running evaluation *****
2023-03-27 16:09:06,904   Epoch = 17 iter 4699 step
2023-03-27 16:09:06,904   Num examples = 1043
2023-03-27 16:09:06,904   Batch size = 32
2023-03-27 16:09:06,905 ***** Eval results *****
2023-03-27 16:09:06,905   att_loss = 0.004840222143684514
2023-03-27 16:09:06,905   cls_loss = 0.0
2023-03-27 16:09:06,906   global_step = 4699
2023-03-27 16:09:06,906   loss = 0.558088880032301
2023-03-27 16:09:06,906   rep_loss = 0.5532486561685801
2023-03-27 16:09:06,913 ***** Save model *****
2023-03-27 16:09:18,339 ***** Running evaluation *****
2023-03-27 16:09:18,339   Epoch = 17 iter 4749 step
2023-03-27 16:09:18,339   Num examples = 1043
2023-03-27 16:09:18,339   Batch size = 32
2023-03-27 16:09:18,340 ***** Eval results *****
2023-03-27 16:09:18,341   att_loss = 0.00484624782000624
2023-03-27 16:09:18,341   cls_loss = 0.0
2023-03-27 16:09:18,341   global_step = 4749
2023-03-27 16:09:18,341   loss = 0.5585815705004192
2023-03-27 16:09:18,341   rep_loss = 0.5537353211925143
2023-03-27 16:09:18,348 ***** Save model *****
2023-03-27 16:09:29,713 ***** Running evaluation *****
2023-03-27 16:09:29,713   Epoch = 17 iter 4799 step
2023-03-27 16:09:29,713   Num examples = 1043
2023-03-27 16:09:29,714   Batch size = 32
2023-03-27 16:09:29,716 ***** Eval results *****
2023-03-27 16:09:29,716   att_loss = 0.004852658155589149
2023-03-27 16:09:29,716   cls_loss = 0.0
2023-03-27 16:09:29,717   global_step = 4799
2023-03-27 16:09:29,717   loss = 0.5586071935983804
2023-03-27 16:09:29,717   rep_loss = 0.5537545341711778
2023-03-27 16:09:29,724 ***** Save model *****
2023-03-27 16:09:41,130 ***** Running evaluation *****
2023-03-27 16:09:41,131   Epoch = 18 iter 4849 step
2023-03-27 16:09:41,131   Num examples = 1043
2023-03-27 16:09:41,131   Batch size = 32
2023-03-27 16:09:41,132 ***** Eval results *****
2023-03-27 16:09:41,132   att_loss = 0.0048343597283197004
2023-03-27 16:09:41,132   cls_loss = 0.0
2023-03-27 16:09:41,132   global_step = 4849
2023-03-27 16:09:41,132   loss = 0.556257052477016
2023-03-27 16:09:41,133   rep_loss = 0.5514226902362912
2023-03-27 16:09:41,140 ***** Save model *****
2023-03-27 16:09:52,562 ***** Running evaluation *****
2023-03-27 16:09:52,562   Epoch = 18 iter 4899 step
2023-03-27 16:09:52,562   Num examples = 1043
2023-03-27 16:09:52,562   Batch size = 32
2023-03-27 16:09:52,563 ***** Eval results *****
2023-03-27 16:09:52,564   att_loss = 0.00482895308905231
2023-03-27 16:09:52,564   cls_loss = 0.0
2023-03-27 16:09:52,564   global_step = 4899
2023-03-27 16:09:52,564   loss = 0.5562691156582166
2023-03-27 16:09:52,564   rep_loss = 0.5514401620434176
2023-03-27 16:09:52,572 ***** Save model *****
2023-03-27 16:10:03,998 ***** Running evaluation *****
2023-03-27 16:10:03,998   Epoch = 18 iter 4949 step
2023-03-27 16:10:03,999   Num examples = 1043
2023-03-27 16:10:03,999   Batch size = 32
2023-03-27 16:10:03,999 ***** Eval results *****
2023-03-27 16:10:04,000   att_loss = 0.0048207955506558604
2023-03-27 16:10:04,000   cls_loss = 0.0
2023-03-27 16:10:04,000   global_step = 4949
2023-03-27 16:10:04,000   loss = 0.5565287329100229
2023-03-27 16:10:04,000   rep_loss = 0.5517079363336096
2023-03-27 16:10:04,007 ***** Save model *****
2023-03-27 16:10:15,428 ***** Running evaluation *****
2023-03-27 16:10:15,428   Epoch = 18 iter 4999 step
2023-03-27 16:10:15,428   Num examples = 1043
2023-03-27 16:10:15,428   Batch size = 32
2023-03-27 16:10:15,429 ***** Eval results *****
2023-03-27 16:10:15,430   att_loss = 0.004823025494543691
2023-03-27 16:10:15,430   cls_loss = 0.0
2023-03-27 16:10:15,430   global_step = 4999
2023-03-27 16:10:15,430   loss = 0.5569142368791017
2023-03-27 16:10:15,430   rep_loss = 0.552091210926135
2023-03-27 16:10:15,438 ***** Save model *****
2023-03-27 16:10:26,848 ***** Running evaluation *****
2023-03-27 16:10:26,848   Epoch = 18 iter 5049 step
2023-03-27 16:10:26,849   Num examples = 1043
2023-03-27 16:10:26,849   Batch size = 32
2023-03-27 16:10:26,850 ***** Eval results *****
2023-03-27 16:10:26,850   att_loss = 0.00482795495777525
2023-03-27 16:10:26,850   cls_loss = 0.0
2023-03-27 16:10:26,851   global_step = 5049
2023-03-27 16:10:26,851   loss = 0.5571588061964561
2023-03-27 16:10:26,851   rep_loss = 0.552330851309584
2023-03-27 16:10:26,858 ***** Save model *****
2023-03-27 16:10:38,270 ***** Running evaluation *****
2023-03-27 16:10:38,270   Epoch = 19 iter 5099 step
2023-03-27 16:10:38,270   Num examples = 1043
2023-03-27 16:10:38,271   Batch size = 32
2023-03-27 16:10:38,273 ***** Eval results *****
2023-03-27 16:10:38,273   att_loss = 0.004878170555457473
2023-03-27 16:10:38,273   cls_loss = 0.0
2023-03-27 16:10:38,273   global_step = 5099
2023-03-27 16:10:38,274   loss = 0.5529107268039997
2023-03-27 16:10:38,274   rep_loss = 0.5480325542963468
2023-03-27 16:10:38,281 ***** Save model *****
2023-03-27 16:10:49,698 ***** Running evaluation *****
2023-03-27 16:10:49,698   Epoch = 19 iter 5149 step
2023-03-27 16:10:49,699   Num examples = 1043
2023-03-27 16:10:49,699   Batch size = 32
2023-03-27 16:10:49,700 ***** Eval results *****
2023-03-27 16:10:49,700   att_loss = 0.004788284490228091
2023-03-27 16:10:49,701   cls_loss = 0.0
2023-03-27 16:10:49,701   global_step = 5149
2023-03-27 16:10:49,701   loss = 0.5513913858877985
2023-03-27 16:10:49,701   rep_loss = 0.5466031008645108
2023-03-27 16:10:49,708 ***** Save model *****
2023-03-27 16:11:01,115 ***** Running evaluation *****
2023-03-27 16:11:01,115   Epoch = 19 iter 5199 step
2023-03-27 16:11:01,115   Num examples = 1043
2023-03-27 16:11:01,115   Batch size = 32
2023-03-27 16:11:01,116 ***** Eval results *****
2023-03-27 16:11:01,117   att_loss = 0.004790471948032815
2023-03-27 16:11:01,117   cls_loss = 0.0
2023-03-27 16:11:01,117   global_step = 5199
2023-03-27 16:11:01,117   loss = 0.5516768774342915
2023-03-27 16:11:01,117   rep_loss = 0.5468864053014725
2023-03-27 16:11:01,125 ***** Save model *****
2023-03-27 16:11:12,531 ***** Running evaluation *****
2023-03-27 16:11:12,531   Epoch = 19 iter 5249 step
2023-03-27 16:11:12,531   Num examples = 1043
2023-03-27 16:11:12,532   Batch size = 32
2023-03-27 16:11:12,533 ***** Eval results *****
2023-03-27 16:11:12,533   att_loss = 0.0047995507484301925
2023-03-27 16:11:12,533   cls_loss = 0.0
2023-03-27 16:11:12,533   global_step = 5249
2023-03-27 16:11:12,534   loss = 0.552411347288977
2023-03-27 16:11:12,534   rep_loss = 0.5476117967204615
2023-03-27 16:11:12,541 ***** Save model *****
2023-03-27 16:11:23,969 ***** Running evaluation *****
2023-03-27 16:11:23,969   Epoch = 19 iter 5299 step
2023-03-27 16:11:23,969   Num examples = 1043
2023-03-27 16:11:23,969   Batch size = 32
2023-03-27 16:11:23,971 ***** Eval results *****
2023-03-27 16:11:23,971   att_loss = 0.004802153660066887
2023-03-27 16:11:23,971   cls_loss = 0.0
2023-03-27 16:11:23,971   global_step = 5299
2023-03-27 16:11:23,971   loss = 0.5530820502116617
2023-03-27 16:11:23,972   rep_loss = 0.548279897301598
2023-03-27 16:11:23,979 ***** Save model *****
2023-03-27 16:11:35,404 ***** Running evaluation *****
2023-03-27 16:11:35,405   Epoch = 20 iter 5349 step
2023-03-27 16:11:35,405   Num examples = 1043
2023-03-27 16:11:35,405   Batch size = 32
2023-03-27 16:11:35,406 ***** Eval results *****
2023-03-27 16:11:35,406   att_loss = 0.004801668899340762
2023-03-27 16:11:35,407   cls_loss = 0.0
2023-03-27 16:11:35,407   global_step = 5349
2023-03-27 16:11:35,407   loss = 0.550298187467787
2023-03-27 16:11:35,407   rep_loss = 0.5454965101348029
2023-03-27 16:11:35,409 ***** Save model *****
2023-03-27 16:11:46,802 ***** Running evaluation *****
2023-03-27 16:11:46,802   Epoch = 20 iter 5399 step
2023-03-27 16:11:46,803   Num examples = 1043
2023-03-27 16:11:46,803   Batch size = 32
2023-03-27 16:11:46,805 ***** Eval results *****
2023-03-27 16:11:46,805   att_loss = 0.0047641298260097786
2023-03-27 16:11:46,805   cls_loss = 0.0
2023-03-27 16:11:46,805   global_step = 5399
2023-03-27 16:11:46,806   loss = 0.5501229803440935
2023-03-27 16:11:46,806   rep_loss = 0.5453588477635788
2023-03-27 16:11:46,813 ***** Save model *****
2023-03-27 16:11:58,197 ***** Running evaluation *****
2023-03-27 16:11:58,197   Epoch = 20 iter 5449 step
2023-03-27 16:11:58,197   Num examples = 1043
2023-03-27 16:11:58,197   Batch size = 32
2023-03-27 16:11:58,199 ***** Eval results *****
2023-03-27 16:11:58,199   att_loss = 0.004763040742879614
2023-03-27 16:11:58,199   cls_loss = 0.0
2023-03-27 16:11:58,199   global_step = 5449
2023-03-27 16:11:58,199   loss = 0.5506750892061706
2023-03-27 16:11:58,200   rep_loss = 0.5459120470449466
2023-03-27 16:11:58,207 ***** Save model *****
2023-03-27 16:12:09,597 ***** Running evaluation *****
2023-03-27 16:12:09,597   Epoch = 20 iter 5499 step
2023-03-27 16:12:09,597   Num examples = 1043
2023-03-27 16:12:09,597   Batch size = 32
2023-03-27 16:12:09,599 ***** Eval results *****
2023-03-27 16:12:09,599   att_loss = 0.004760640392185382
2023-03-27 16:12:09,599   cls_loss = 0.0
2023-03-27 16:12:09,599   global_step = 5499
2023-03-27 16:12:09,599   loss = 0.5510007023061596
2023-03-27 16:12:09,600   rep_loss = 0.5462400605843502
2023-03-27 16:12:09,607 ***** Save model *****
2023-03-27 16:12:21,005 ***** Running evaluation *****
2023-03-27 16:12:21,005   Epoch = 20 iter 5549 step
2023-03-27 16:12:21,005   Num examples = 1043
2023-03-27 16:12:21,005   Batch size = 32
2023-03-27 16:12:21,007 ***** Eval results *****
2023-03-27 16:12:21,007   att_loss = 0.00477678784387605
2023-03-27 16:12:21,007   cls_loss = 0.0
2023-03-27 16:12:21,007   global_step = 5549
2023-03-27 16:12:21,007   loss = 0.5510821239799975
2023-03-27 16:12:21,008   rep_loss = 0.5463053344539478
2023-03-27 16:12:21,015 ***** Save model *****
2023-03-27 16:12:32,418 ***** Running evaluation *****
2023-03-27 16:12:32,419   Epoch = 20 iter 5599 step
2023-03-27 16:12:32,419   Num examples = 1043
2023-03-27 16:12:32,419   Batch size = 32
2023-03-27 16:12:32,420 ***** Eval results *****
2023-03-27 16:12:32,420   att_loss = 0.004772861658965874
2023-03-27 16:12:32,420   cls_loss = 0.0
2023-03-27 16:12:32,420   global_step = 5599
2023-03-27 16:12:32,420   loss = 0.5508405149673403
2023-03-27 16:12:32,420   rep_loss = 0.5460676518646447
2023-03-27 16:12:32,427 ***** Save model *****
2023-03-27 16:12:43,828 ***** Running evaluation *****
2023-03-27 16:12:43,828   Epoch = 21 iter 5649 step
2023-03-27 16:12:43,829   Num examples = 1043
2023-03-27 16:12:43,829   Batch size = 32
2023-03-27 16:12:43,830 ***** Eval results *****
2023-03-27 16:12:43,830   att_loss = 0.004768807773611375
2023-03-27 16:12:43,830   cls_loss = 0.0
2023-03-27 16:12:43,830   global_step = 5649
2023-03-27 16:12:43,830   loss = 0.5479653406710852
2023-03-27 16:12:43,830   rep_loss = 0.5431965334074838
2023-03-27 16:12:43,832 ***** Save model *****
2023-03-27 16:12:55,248 ***** Running evaluation *****
2023-03-27 16:12:55,248   Epoch = 21 iter 5699 step
2023-03-27 16:12:55,249   Num examples = 1043
2023-03-27 16:12:55,249   Batch size = 32
2023-03-27 16:12:55,251 ***** Eval results *****
2023-03-27 16:12:55,251   att_loss = 0.004765888286309074
2023-03-27 16:12:55,251   cls_loss = 0.0
2023-03-27 16:12:55,252   global_step = 5699
2023-03-27 16:12:55,252   loss = 0.5495110627101816
2023-03-27 16:12:55,252   rep_loss = 0.5447451757348102
2023-03-27 16:12:55,259 ***** Save model *****
2023-03-27 16:13:06,678 ***** Running evaluation *****
2023-03-27 16:13:06,679   Epoch = 21 iter 5749 step
2023-03-27 16:13:06,679   Num examples = 1043
2023-03-27 16:13:06,679   Batch size = 32
2023-03-27 16:13:06,680 ***** Eval results *****
2023-03-27 16:13:06,681   att_loss = 0.004750438998650078
2023-03-27 16:13:06,681   cls_loss = 0.0
2023-03-27 16:13:06,681   global_step = 5749
2023-03-27 16:13:06,681   loss = 0.5488397034121232
2023-03-27 16:13:06,681   rep_loss = 0.5440892648528999
2023-03-27 16:13:06,684 ***** Save model *****
2023-03-27 16:13:18,110 ***** Running evaluation *****
2023-03-27 16:13:18,110   Epoch = 21 iter 5799 step
2023-03-27 16:13:18,110   Num examples = 1043
2023-03-27 16:13:18,111   Batch size = 32
2023-03-27 16:13:18,112 ***** Eval results *****
2023-03-27 16:13:18,112   att_loss = 0.0047570637228394235
2023-03-27 16:13:18,112   cls_loss = 0.0
2023-03-27 16:13:18,112   global_step = 5799
2023-03-27 16:13:18,113   loss = 0.549134441340963
2023-03-27 16:13:18,113   rep_loss = 0.5443773772567511
2023-03-27 16:13:18,120 ***** Save model *****
2023-03-27 16:13:29,528 ***** Running evaluation *****
2023-03-27 16:13:29,528   Epoch = 21 iter 5849 step
2023-03-27 16:13:29,528   Num examples = 1043
2023-03-27 16:13:29,529   Batch size = 32
2023-03-27 16:13:29,530 ***** Eval results *****
2023-03-27 16:13:29,530   att_loss = 0.004759362258001669
2023-03-27 16:13:29,530   cls_loss = 0.0
2023-03-27 16:13:29,530   global_step = 5849
2023-03-27 16:13:29,530   loss = 0.5493087763628683
2023-03-27 16:13:29,530   rep_loss = 0.5445494129638041
2023-03-27 16:13:29,537 ***** Save model *****
2023-03-27 16:13:40,967 ***** Running evaluation *****
2023-03-27 16:13:40,968   Epoch = 22 iter 5899 step
2023-03-27 16:13:40,968   Num examples = 1043
2023-03-27 16:13:40,968   Batch size = 32
2023-03-27 16:13:40,969 ***** Eval results *****
2023-03-27 16:13:40,969   att_loss = 0.004673370532691479
2023-03-27 16:13:40,969   cls_loss = 0.0
2023-03-27 16:13:40,970   global_step = 5899
2023-03-27 16:13:40,970   loss = 0.5467032957077026
2023-03-27 16:13:40,970   rep_loss = 0.542029926776886
2023-03-27 16:13:40,977 ***** Save model *****
2023-03-27 16:13:52,402 ***** Running evaluation *****
2023-03-27 16:13:52,403   Epoch = 22 iter 5949 step
2023-03-27 16:13:52,403   Num examples = 1043
2023-03-27 16:13:52,403   Batch size = 32
2023-03-27 16:13:52,404 ***** Eval results *****
2023-03-27 16:13:52,404   att_loss = 0.004718842959652345
2023-03-27 16:13:52,404   cls_loss = 0.0
2023-03-27 16:13:52,404   global_step = 5949
2023-03-27 16:13:52,405   loss = 0.5446461153030395
2023-03-27 16:13:52,405   rep_loss = 0.5399272712071737
2023-03-27 16:13:52,412 ***** Save model *****
2023-03-27 16:14:03,839 ***** Running evaluation *****
2023-03-27 16:14:03,839   Epoch = 22 iter 5999 step
2023-03-27 16:14:03,839   Num examples = 1043
2023-03-27 16:14:03,840   Batch size = 32
2023-03-27 16:14:03,841 ***** Eval results *****
2023-03-27 16:14:03,841   att_loss = 0.004716547381132841
2023-03-27 16:14:03,842   cls_loss = 0.0
2023-03-27 16:14:03,842   global_step = 5999
2023-03-27 16:14:03,842   loss = 0.5453640332221985
2023-03-27 16:14:03,842   rep_loss = 0.5406474852561951
2023-03-27 16:14:03,849 ***** Save model *****
2023-03-27 16:14:15,268 ***** Running evaluation *****
2023-03-27 16:14:15,268   Epoch = 22 iter 6049 step
2023-03-27 16:14:15,268   Num examples = 1043
2023-03-27 16:14:15,268   Batch size = 32
2023-03-27 16:14:15,270 ***** Eval results *****
2023-03-27 16:14:15,270   att_loss = 0.00473274813964963
2023-03-27 16:14:15,270   cls_loss = 0.0
2023-03-27 16:14:15,271   global_step = 6049
2023-03-27 16:14:15,271   loss = 0.5463607284000942
2023-03-27 16:14:15,271   rep_loss = 0.5416279806409563
2023-03-27 16:14:15,278 ***** Save model *****
2023-03-27 16:14:26,710 ***** Running evaluation *****
2023-03-27 16:14:26,710   Epoch = 22 iter 6099 step
2023-03-27 16:14:26,710   Num examples = 1043
2023-03-27 16:14:26,710   Batch size = 32
2023-03-27 16:14:26,712 ***** Eval results *****
2023-03-27 16:14:26,712   att_loss = 0.0047466420961750876
2023-03-27 16:14:26,712   cls_loss = 0.0
2023-03-27 16:14:26,712   global_step = 6099
2023-03-27 16:14:26,713   loss = 0.5467107285393609
2023-03-27 16:14:26,713   rep_loss = 0.5419640874862671
2023-03-27 16:14:26,720 ***** Save model *****
2023-03-27 16:14:38,157 ***** Running evaluation *****
2023-03-27 16:14:38,157   Epoch = 23 iter 6149 step
2023-03-27 16:14:38,157   Num examples = 1043
2023-03-27 16:14:38,157   Batch size = 32
2023-03-27 16:14:38,158 ***** Eval results *****
2023-03-27 16:14:38,158   att_loss = 0.004815689928364009
2023-03-27 16:14:38,158   cls_loss = 0.0
2023-03-27 16:14:38,158   global_step = 6149
2023-03-27 16:14:38,159   loss = 0.5435303002595901
2023-03-27 16:14:38,159   rep_loss = 0.5387146100401878
2023-03-27 16:14:38,166 ***** Save model *****
2023-03-27 16:14:49,593 ***** Running evaluation *****
2023-03-27 16:14:49,594   Epoch = 23 iter 6199 step
2023-03-27 16:14:49,594   Num examples = 1043
2023-03-27 16:14:49,594   Batch size = 32
2023-03-27 16:14:49,595 ***** Eval results *****
2023-03-27 16:14:49,595   att_loss = 0.004742914409344566
2023-03-27 16:14:49,596   cls_loss = 0.0
2023-03-27 16:14:49,596   global_step = 6199
2023-03-27 16:14:49,596   loss = 0.5459568171665586
2023-03-27 16:14:49,596   rep_loss = 0.5412139008785116
2023-03-27 16:14:49,603 ***** Save model *****
2023-03-27 16:15:01,031 ***** Running evaluation *****
2023-03-27 16:15:01,032   Epoch = 23 iter 6249 step
2023-03-27 16:15:01,032   Num examples = 1043
2023-03-27 16:15:01,032   Batch size = 32
2023-03-27 16:15:01,033 ***** Eval results *****
2023-03-27 16:15:01,033   att_loss = 0.004741522893137126
2023-03-27 16:15:01,034   cls_loss = 0.0
2023-03-27 16:15:01,034   global_step = 6249
2023-03-27 16:15:01,034   loss = 0.5457223074303733
2023-03-27 16:15:01,034   rep_loss = 0.5409807844294442
2023-03-27 16:15:01,041 ***** Save model *****
2023-03-27 16:15:12,451 ***** Running evaluation *****
2023-03-27 16:15:12,451   Epoch = 23 iter 6299 step
2023-03-27 16:15:12,451   Num examples = 1043
2023-03-27 16:15:12,451   Batch size = 32
2023-03-27 16:15:12,453 ***** Eval results *****
2023-03-27 16:15:12,453   att_loss = 0.0047400734458071525
2023-03-27 16:15:12,454   cls_loss = 0.0
2023-03-27 16:15:12,454   global_step = 6299
2023-03-27 16:15:12,454   loss = 0.5450880599172809
2023-03-27 16:15:12,454   rep_loss = 0.5403479873379574
2023-03-27 16:15:12,461 ***** Save model *****
2023-03-27 16:15:23,881 ***** Running evaluation *****
2023-03-27 16:15:23,881   Epoch = 23 iter 6349 step
2023-03-27 16:15:23,881   Num examples = 1043
2023-03-27 16:15:23,881   Batch size = 32
2023-03-27 16:15:23,882 ***** Eval results *****
2023-03-27 16:15:23,883   att_loss = 0.004748953148149527
2023-03-27 16:15:23,883   cls_loss = 0.0
2023-03-27 16:15:23,883   global_step = 6349
2023-03-27 16:15:23,883   loss = 0.5446418615487906
2023-03-27 16:15:23,883   rep_loss = 0.539892909475244
2023-03-27 16:15:23,891 ***** Save model *****
2023-03-27 16:15:35,322 ***** Running evaluation *****
2023-03-27 16:15:35,323   Epoch = 23 iter 6399 step
2023-03-27 16:15:35,323   Num examples = 1043
2023-03-27 16:15:35,323   Batch size = 32
2023-03-27 16:15:35,324 ***** Eval results *****
2023-03-27 16:15:35,324   att_loss = 0.004750079381489015
2023-03-27 16:15:35,324   cls_loss = 0.0
2023-03-27 16:15:35,324   global_step = 6399
2023-03-27 16:15:35,324   loss = 0.5450342073459034
2023-03-27 16:15:35,324   rep_loss = 0.5402841284986615
2023-03-27 16:15:35,332 ***** Save model *****
2023-03-27 16:15:46,749 ***** Running evaluation *****
2023-03-27 16:15:46,749   Epoch = 24 iter 6449 step
2023-03-27 16:15:46,750   Num examples = 1043
2023-03-27 16:15:46,750   Batch size = 32
2023-03-27 16:15:46,751 ***** Eval results *****
2023-03-27 16:15:46,751   att_loss = 0.004699437844953159
2023-03-27 16:15:46,751   cls_loss = 0.0
2023-03-27 16:15:46,752   global_step = 6449
2023-03-27 16:15:46,752   loss = 0.5414547629472686
2023-03-27 16:15:46,752   rep_loss = 0.5367553263175778
2023-03-27 16:15:46,759 ***** Save model *****
2023-03-27 16:15:58,180 ***** Running evaluation *****
2023-03-27 16:15:58,180   Epoch = 24 iter 6499 step
2023-03-27 16:15:58,180   Num examples = 1043
2023-03-27 16:15:58,180   Batch size = 32
2023-03-27 16:15:58,181 ***** Eval results *****
2023-03-27 16:15:58,182   att_loss = 0.004705017493976341
2023-03-27 16:15:58,182   cls_loss = 0.0
2023-03-27 16:15:58,182   global_step = 6499
2023-03-27 16:15:58,182   loss = 0.5421018869012266
2023-03-27 16:15:58,182   rep_loss = 0.537396871126615
2023-03-27 16:15:58,184 ***** Save model *****
2023-03-27 16:16:09,594 ***** Running evaluation *****
2023-03-27 16:16:09,594   Epoch = 24 iter 6549 step
2023-03-27 16:16:09,594   Num examples = 1043
2023-03-27 16:16:09,594   Batch size = 32
2023-03-27 16:16:09,595 ***** Eval results *****
2023-03-27 16:16:09,595   att_loss = 0.004689235411954264
2023-03-27 16:16:09,596   cls_loss = 0.0
2023-03-27 16:16:09,596   global_step = 6549
2023-03-27 16:16:09,596   loss = 0.5413895760867613
2023-03-27 16:16:09,596   rep_loss = 0.5367003438320566
2023-03-27 16:16:09,603 ***** Save model *****
2023-03-27 16:16:21,025 ***** Running evaluation *****
2023-03-27 16:16:21,026   Epoch = 24 iter 6599 step
2023-03-27 16:16:21,026   Num examples = 1043
2023-03-27 16:16:21,026   Batch size = 32
2023-03-27 16:16:21,028 ***** Eval results *****
2023-03-27 16:16:21,028   att_loss = 0.004703584904855614
2023-03-27 16:16:21,029   cls_loss = 0.0
2023-03-27 16:16:21,029   global_step = 6599
2023-03-27 16:16:21,029   loss = 0.5422113144584976
2023-03-27 16:16:21,029   rep_loss = 0.5375077325012048
2023-03-27 16:16:21,037 ***** Save model *****
2023-03-27 16:16:32,466 ***** Running evaluation *****
2023-03-27 16:16:32,466   Epoch = 24 iter 6649 step
2023-03-27 16:16:32,466   Num examples = 1043
2023-03-27 16:16:32,466   Batch size = 32
2023-03-27 16:16:32,468 ***** Eval results *****
2023-03-27 16:16:32,468   att_loss = 0.004704813463265594
2023-03-27 16:16:32,468   cls_loss = 0.0
2023-03-27 16:16:32,468   global_step = 6649
2023-03-27 16:16:32,468   loss = 0.5421255428761368
2023-03-27 16:16:32,469   rep_loss = 0.5374207325990764
2023-03-27 16:16:32,479 ***** Save model *****
2023-03-27 16:16:43,893 ***** Running evaluation *****
2023-03-27 16:16:43,893   Epoch = 25 iter 6699 step
2023-03-27 16:16:43,893   Num examples = 1043
2023-03-27 16:16:43,893   Batch size = 32
2023-03-27 16:16:43,894 ***** Eval results *****
2023-03-27 16:16:43,894   att_loss = 0.00468528763546298
2023-03-27 16:16:43,895   cls_loss = 0.0
2023-03-27 16:16:43,895   global_step = 6699
2023-03-27 16:16:43,895   loss = 0.541596531867981
2023-03-27 16:16:43,895   rep_loss = 0.5369112516442934
2023-03-27 16:16:43,902 ***** Save model *****
2023-03-27 16:16:55,317 ***** Running evaluation *****
2023-03-27 16:16:55,317   Epoch = 25 iter 6749 step
2023-03-27 16:16:55,317   Num examples = 1043
2023-03-27 16:16:55,317   Batch size = 32
2023-03-27 16:16:55,318 ***** Eval results *****
2023-03-27 16:16:55,319   att_loss = 0.004695227294153458
2023-03-27 16:16:55,319   cls_loss = 0.0
2023-03-27 16:16:55,319   global_step = 6749
2023-03-27 16:16:55,319   loss = 0.541473006074493
2023-03-27 16:16:55,319   rep_loss = 0.5367777798626874
2023-03-27 16:16:55,327 ***** Save model *****
2023-03-27 16:17:06,743 ***** Running evaluation *****
2023-03-27 16:17:06,744   Epoch = 25 iter 6799 step
2023-03-27 16:17:06,744   Num examples = 1043
2023-03-27 16:17:06,744   Batch size = 32
2023-03-27 16:17:06,745 ***** Eval results *****
2023-03-27 16:17:06,745   att_loss = 0.004695176650139112
2023-03-27 16:17:06,745   cls_loss = 0.0
2023-03-27 16:17:06,745   global_step = 6799
2023-03-27 16:17:06,745   loss = 0.540520879049455
2023-03-27 16:17:06,745   rep_loss = 0.5358257029325731
2023-03-27 16:17:06,752 ***** Save model *****
2023-03-27 16:17:18,170 ***** Running evaluation *****
2023-03-27 16:17:18,171   Epoch = 25 iter 6849 step
2023-03-27 16:17:18,171   Num examples = 1043
2023-03-27 16:17:18,171   Batch size = 32
2023-03-27 16:17:18,172 ***** Eval results *****
2023-03-27 16:17:18,172   att_loss = 0.004692747985580187
2023-03-27 16:17:18,172   cls_loss = 0.0
2023-03-27 16:17:18,172   global_step = 6849
2023-03-27 16:17:18,172   loss = 0.5405287591890356
2023-03-27 16:17:18,172   rep_loss = 0.5358360121990072
2023-03-27 16:17:18,179 ***** Save model *****
2023-03-27 16:17:29,592 ***** Running evaluation *****
2023-03-27 16:17:29,592   Epoch = 25 iter 6899 step
2023-03-27 16:17:29,592   Num examples = 1043
2023-03-27 16:17:29,593   Batch size = 32
2023-03-27 16:17:29,595 ***** Eval results *****
2023-03-27 16:17:29,595   att_loss = 0.004691317401011474
2023-03-27 16:17:29,595   cls_loss = 0.0
2023-03-27 16:17:29,595   global_step = 6899
2023-03-27 16:17:29,595   loss = 0.5402462070009538
2023-03-27 16:17:29,595   rep_loss = 0.5355548903878246
2023-03-27 16:17:29,603 ***** Save model *****
2023-03-27 16:17:41,038 ***** Running evaluation *****
2023-03-27 16:17:41,039   Epoch = 26 iter 6949 step
2023-03-27 16:17:41,039   Num examples = 1043
2023-03-27 16:17:41,039   Batch size = 32
2023-03-27 16:17:41,040 ***** Eval results *****
2023-03-27 16:17:41,041   att_loss = 0.004631469292300088
2023-03-27 16:17:41,041   cls_loss = 0.0
2023-03-27 16:17:41,041   global_step = 6949
2023-03-27 16:17:41,041   loss = 0.5341012392725263
2023-03-27 16:17:41,041   rep_loss = 0.5294697795595441
2023-03-27 16:17:41,044 ***** Save model *****
2023-03-27 16:17:52,446 ***** Running evaluation *****
2023-03-27 16:17:52,446   Epoch = 26 iter 6999 step
2023-03-27 16:17:52,447   Num examples = 1043
2023-03-27 16:17:52,447   Batch size = 32
2023-03-27 16:17:52,448 ***** Eval results *****
2023-03-27 16:17:52,448   att_loss = 0.004700303265596168
2023-03-27 16:17:52,448   cls_loss = 0.0
2023-03-27 16:17:52,449   global_step = 6999
2023-03-27 16:17:52,449   loss = 0.5399501574666876
2023-03-27 16:17:52,449   rep_loss = 0.535249858571772
2023-03-27 16:17:52,451 ***** Save model *****
2023-03-27 16:18:03,860 ***** Running evaluation *****
2023-03-27 16:18:03,861   Epoch = 26 iter 7049 step
2023-03-27 16:18:03,861   Num examples = 1043
2023-03-27 16:18:03,861   Batch size = 32
2023-03-27 16:18:03,862 ***** Eval results *****
2023-03-27 16:18:03,862   att_loss = 0.0046919695336685
2023-03-27 16:18:03,862   cls_loss = 0.0
2023-03-27 16:18:03,862   global_step = 7049
2023-03-27 16:18:03,862   loss = 0.5389220959672304
2023-03-27 16:18:03,862   rep_loss = 0.5342301269557989
2023-03-27 16:18:03,869 ***** Save model *****
2023-03-27 16:18:15,296 ***** Running evaluation *****
2023-03-27 16:18:15,297   Epoch = 26 iter 7099 step
2023-03-27 16:18:15,297   Num examples = 1043
2023-03-27 16:18:15,298   Batch size = 32
2023-03-27 16:18:15,299 ***** Eval results *****
2023-03-27 16:18:15,299   att_loss = 0.00468125580483751
2023-03-27 16:18:15,300   cls_loss = 0.0
2023-03-27 16:18:15,300   global_step = 7099
2023-03-27 16:18:15,300   loss = 0.5388795946054398
2023-03-27 16:18:15,300   rep_loss = 0.5341983391980457
2023-03-27 16:18:15,308 ***** Save model *****
2023-03-27 16:18:26,733 ***** Running evaluation *****
2023-03-27 16:18:26,733   Epoch = 26 iter 7149 step
2023-03-27 16:18:26,733   Num examples = 1043
2023-03-27 16:18:26,734   Batch size = 32
2023-03-27 16:18:26,735 ***** Eval results *****
2023-03-27 16:18:26,735   att_loss = 0.004676927634681336
2023-03-27 16:18:26,735   cls_loss = 0.0
2023-03-27 16:18:26,735   global_step = 7149
2023-03-27 16:18:26,736   loss = 0.538698110891425
2023-03-27 16:18:26,736   rep_loss = 0.5340211832005045
2023-03-27 16:18:26,738 ***** Save model *****
2023-03-27 16:18:38,155 ***** Running evaluation *****
2023-03-27 16:18:38,155   Epoch = 26 iter 7199 step
2023-03-27 16:18:38,155   Num examples = 1043
2023-03-27 16:18:38,155   Batch size = 32
2023-03-27 16:18:38,158 ***** Eval results *****
2023-03-27 16:18:38,158   att_loss = 0.004678543204086532
2023-03-27 16:18:38,158   cls_loss = 0.0
2023-03-27 16:18:38,158   global_step = 7199
2023-03-27 16:18:38,158   loss = 0.5387136114014726
2023-03-27 16:18:38,158   rep_loss = 0.5340350685416493
2023-03-27 16:18:38,165 ***** Save model *****
2023-03-27 16:18:49,604 ***** Running evaluation *****
2023-03-27 16:18:49,605   Epoch = 27 iter 7249 step
2023-03-27 16:18:49,605   Num examples = 1043
2023-03-27 16:18:49,605   Batch size = 32
2023-03-27 16:18:49,606 ***** Eval results *****
2023-03-27 16:18:49,607   att_loss = 0.004686865652911365
2023-03-27 16:18:49,607   cls_loss = 0.0
2023-03-27 16:18:49,607   global_step = 7249
2023-03-27 16:18:49,607   loss = 0.5377747237682342
2023-03-27 16:18:49,607   rep_loss = 0.5330878585577011
2023-03-27 16:18:49,614 ***** Save model *****
2023-03-27 16:19:01,023 ***** Running evaluation *****
2023-03-27 16:19:01,023   Epoch = 27 iter 7299 step
2023-03-27 16:19:01,024   Num examples = 1043
2023-03-27 16:19:01,024   Batch size = 32
2023-03-27 16:19:01,025 ***** Eval results *****
2023-03-27 16:19:01,025   att_loss = 0.004673533726276623
2023-03-27 16:19:01,025   cls_loss = 0.0
2023-03-27 16:19:01,025   global_step = 7299
2023-03-27 16:19:01,025   loss = 0.5381995717684428
2023-03-27 16:19:01,026   rep_loss = 0.5335260364744399
2023-03-27 16:19:01,033 ***** Save model *****
2023-03-27 16:19:12,465 ***** Running evaluation *****
2023-03-27 16:19:12,465   Epoch = 27 iter 7349 step
2023-03-27 16:19:12,466   Num examples = 1043
2023-03-27 16:19:12,466   Batch size = 32
2023-03-27 16:19:12,467 ***** Eval results *****
2023-03-27 16:19:12,467   att_loss = 0.004652750305831432
2023-03-27 16:19:12,467   cls_loss = 0.0
2023-03-27 16:19:12,468   global_step = 7349
2023-03-27 16:19:12,468   loss = 0.5369226153407778
2023-03-27 16:19:12,468   rep_loss = 0.5322698627199446
2023-03-27 16:19:12,475 ***** Save model *****
2023-03-27 16:19:23,854 ***** Running evaluation *****
2023-03-27 16:19:23,854   Epoch = 27 iter 7399 step
2023-03-27 16:19:23,854   Num examples = 1043
2023-03-27 16:19:23,854   Batch size = 32
2023-03-27 16:19:23,855 ***** Eval results *****
2023-03-27 16:19:23,856   att_loss = 0.004660771632763116
2023-03-27 16:19:23,856   cls_loss = 0.0
2023-03-27 16:19:23,856   global_step = 7399
2023-03-27 16:19:23,856   loss = 0.5378265763583936
2023-03-27 16:19:23,857   rep_loss = 0.533165803081111
2023-03-27 16:19:23,864 ***** Save model *****
2023-03-27 16:19:35,286 ***** Running evaluation *****
2023-03-27 16:19:35,286   Epoch = 27 iter 7449 step
2023-03-27 16:19:35,286   Num examples = 1043
2023-03-27 16:19:35,287   Batch size = 32
2023-03-27 16:19:35,288 ***** Eval results *****
2023-03-27 16:19:35,288   att_loss = 0.004655395197914913
2023-03-27 16:19:35,288   cls_loss = 0.0
2023-03-27 16:19:35,288   global_step = 7449
2023-03-27 16:19:35,289   loss = 0.5376783686379591
2023-03-27 16:19:35,289   rep_loss = 0.5330229714512825
2023-03-27 16:19:35,296 ***** Save model *****
2023-03-27 16:19:46,709 ***** Running evaluation *****
2023-03-27 16:19:46,710   Epoch = 28 iter 7499 step
2023-03-27 16:19:46,710   Num examples = 1043
2023-03-27 16:19:46,710   Batch size = 32
2023-03-27 16:19:46,712 ***** Eval results *****
2023-03-27 16:19:46,712   att_loss = 0.0047073270923093605
2023-03-27 16:19:46,713   cls_loss = 0.0
2023-03-27 16:19:46,713   global_step = 7499
2023-03-27 16:19:46,713   loss = 0.5360507628192073
2023-03-27 16:19:46,713   rep_loss = 0.5313434341679448
2023-03-27 16:19:46,721 ***** Save model *****
2023-03-27 16:19:58,151 ***** Running evaluation *****
2023-03-27 16:19:58,151   Epoch = 28 iter 7549 step
2023-03-27 16:19:58,151   Num examples = 1043
2023-03-27 16:19:58,152   Batch size = 32
2023-03-27 16:19:58,153 ***** Eval results *****
2023-03-27 16:19:58,153   att_loss = 0.004672106457491444
2023-03-27 16:19:58,153   cls_loss = 0.0
2023-03-27 16:19:58,153   global_step = 7549
2023-03-27 16:19:58,153   loss = 0.5367987825445932
2023-03-27 16:19:58,153   rep_loss = 0.5321266749133803
2023-03-27 16:19:58,160 ***** Save model *****
2023-03-27 16:20:09,585 ***** Running evaluation *****
2023-03-27 16:20:09,586   Epoch = 28 iter 7599 step
2023-03-27 16:20:09,586   Num examples = 1043
2023-03-27 16:20:09,586   Batch size = 32
2023-03-27 16:20:09,587 ***** Eval results *****
2023-03-27 16:20:09,587   att_loss = 0.004661455689891567
2023-03-27 16:20:09,587   cls_loss = 0.0
2023-03-27 16:20:09,587   global_step = 7599
2023-03-27 16:20:09,587   loss = 0.537231838315483
2023-03-27 16:20:09,588   rep_loss = 0.53257038244387
2023-03-27 16:20:09,589 ***** Save model *****
2023-03-27 16:20:21,000 ***** Running evaluation *****
2023-03-27 16:20:21,000   Epoch = 28 iter 7649 step
2023-03-27 16:20:21,000   Num examples = 1043
2023-03-27 16:20:21,000   Batch size = 32
2023-03-27 16:20:21,001 ***** Eval results *****
2023-03-27 16:20:21,002   att_loss = 0.004663825301571905
2023-03-27 16:20:21,002   cls_loss = 0.0
2023-03-27 16:20:21,002   global_step = 7649
2023-03-27 16:20:21,002   loss = 0.5376259719016235
2023-03-27 16:20:21,003   rep_loss = 0.5329621461774573
2023-03-27 16:20:21,005 ***** Save model *****
2023-03-27 16:20:32,418 ***** Running evaluation *****
2023-03-27 16:20:32,418   Epoch = 28 iter 7699 step
2023-03-27 16:20:32,418   Num examples = 1043
2023-03-27 16:20:32,418   Batch size = 32
2023-03-27 16:20:32,419 ***** Eval results *****
2023-03-27 16:20:32,419   att_loss = 0.004655530365108775
2023-03-27 16:20:32,419   cls_loss = 0.0
2023-03-27 16:20:32,420   global_step = 7699
2023-03-27 16:20:32,420   loss = 0.5366751404621142
2023-03-27 16:20:32,420   rep_loss = 0.532019609293061
2023-03-27 16:20:32,427 ***** Save model *****
2023-03-27 16:20:43,830 ***** Running evaluation *****
2023-03-27 16:20:43,831   Epoch = 29 iter 7749 step
2023-03-27 16:20:43,831   Num examples = 1043
2023-03-27 16:20:43,831   Batch size = 32
2023-03-27 16:20:43,832 ***** Eval results *****
2023-03-27 16:20:43,832   att_loss = 0.0045888228341937065
2023-03-27 16:20:43,832   cls_loss = 0.0
2023-03-27 16:20:43,833   global_step = 7749
2023-03-27 16:20:43,833   loss = 0.5280392169952393
2023-03-27 16:20:43,833   rep_loss = 0.5234503845373789
2023-03-27 16:20:43,840 ***** Save model *****
2023-03-27 16:20:55,232 ***** Running evaluation *****
2023-03-27 16:20:55,232   Epoch = 29 iter 7799 step
2023-03-27 16:20:55,232   Num examples = 1043
2023-03-27 16:20:55,232   Batch size = 32
2023-03-27 16:20:55,234 ***** Eval results *****
2023-03-27 16:20:55,234   att_loss = 0.004600731464701572
2023-03-27 16:20:55,234   cls_loss = 0.0
2023-03-27 16:20:55,234   global_step = 7799
2023-03-27 16:20:55,235   loss = 0.5320730294500079
2023-03-27 16:20:55,235   rep_loss = 0.52747229380267
2023-03-27 16:20:55,237 ***** Save model *****
2023-03-27 16:21:06,649 ***** Running evaluation *****
2023-03-27 16:21:06,649   Epoch = 29 iter 7849 step
2023-03-27 16:21:06,649   Num examples = 1043
2023-03-27 16:21:06,649   Batch size = 32
2023-03-27 16:21:06,651 ***** Eval results *****
2023-03-27 16:21:06,651   att_loss = 0.004629532459525848
2023-03-27 16:21:06,651   cls_loss = 0.0
2023-03-27 16:21:06,652   global_step = 7849
2023-03-27 16:21:06,652   loss = 0.5335828751887915
2023-03-27 16:21:06,652   rep_loss = 0.5289533396936813
2023-03-27 16:21:06,654 ***** Save model *****
2023-03-27 16:21:18,086 ***** Running evaluation *****
2023-03-27 16:21:18,086   Epoch = 29 iter 7899 step
2023-03-27 16:21:18,086   Num examples = 1043
2023-03-27 16:21:18,086   Batch size = 32
2023-03-27 16:21:18,087 ***** Eval results *****
2023-03-27 16:21:18,088   att_loss = 0.004633390499899785
2023-03-27 16:21:18,088   cls_loss = 0.0
2023-03-27 16:21:18,088   global_step = 7899
2023-03-27 16:21:18,088   loss = 0.533695270999884
2023-03-27 16:21:18,088   rep_loss = 0.5290618783388382
2023-03-27 16:21:18,096 ***** Save model *****
2023-03-27 16:21:29,532 ***** Running evaluation *****
2023-03-27 16:21:29,533   Epoch = 29 iter 7949 step
2023-03-27 16:21:29,533   Num examples = 1043
2023-03-27 16:21:29,533   Batch size = 32
2023-03-27 16:21:29,534 ***** Eval results *****
2023-03-27 16:21:29,534   att_loss = 0.004628282239280858
2023-03-27 16:21:29,535   cls_loss = 0.0
2023-03-27 16:21:29,535   global_step = 7949
2023-03-27 16:21:29,535   loss = 0.5340700650099411
2023-03-27 16:21:29,535   rep_loss = 0.5294417808356794
2023-03-27 16:21:29,538 ***** Save model *****
2023-03-27 16:21:40,975 ***** Running evaluation *****
2023-03-27 16:21:40,975   Epoch = 29 iter 7999 step
2023-03-27 16:21:40,976   Num examples = 1043
2023-03-27 16:21:40,976   Batch size = 32
2023-03-27 16:21:40,977 ***** Eval results *****
2023-03-27 16:21:40,977   att_loss = 0.0046177895656001056
2023-03-27 16:21:40,977   cls_loss = 0.0
2023-03-27 16:21:40,978   global_step = 7999
2023-03-27 16:21:40,978   loss = 0.5336774664465338
2023-03-27 16:21:40,978   rep_loss = 0.529059674590826
2023-03-27 16:21:40,985 ***** Save model *****
2023-03-27 16:23:46,736 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_BASE/TempTinyBERT_CoLA_4L_312D_5e-05_32', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_BASE/TinyBERT_CoLA_4L_312D_5e-05_32', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=3e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=True, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='KL_ATTN_BASE', kl_attn=False)
2023-03-27 16:23:48,868 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 16:23:51,901 device: cuda n_gpu: 1
2023-03-27 16:23:52,096 Writing example 0 of 8551
2023-03-27 16:23:52,097 *** Example ***
2023-03-27 16:23:52,097 guid: train-0
2023-03-27 16:23:52,097 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 16:23:52,097 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:23:52,097 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:23:52,097 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:23:52,097 label: 1
2023-03-27 16:23:52,098 label_id: 1
2023-03-27 16:23:53,090 Writing example 0 of 1043
2023-03-27 16:23:53,091 *** Example ***
2023-03-27 16:23:53,091 guid: dev-0
2023-03-27 16:23:53,091 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 16:23:53,091 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:23:53,091 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:23:53,091 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:23:53,092 label: 1
2023-03-27 16:23:53,092 label_id: 1
2023-03-27 16:23:53,210 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 16:23:53,212 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 16:23:55,053 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 16:23:59,779 loading model...
2023-03-27 16:23:59,935 done!
2023-03-27 16:23:59,936 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 16:24:05,158 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_BASE/TempTinyBERT_CoLA_4L_312D_5e-05_32
2023-03-27 16:24:05,160 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 16:24:05,386 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_BASE/TempTinyBERT_CoLA_4L_312D_5e-05_32/pytorch_model.bin
2023-03-27 16:24:05,995 loading model...
2023-03-27 16:24:06,008 done!
2023-03-27 16:24:06,023 ***** Running training *****
2023-03-27 16:24:06,023   Num examples = 8551
2023-03-27 16:24:06,024   Batch size = 32
2023-03-27 16:24:06,024   Num steps = 801
2023-03-27 16:24:06,024 n: bert.embeddings.word_embeddings.weight
2023-03-27 16:24:06,024 n: bert.embeddings.position_embeddings.weight
2023-03-27 16:24:06,024 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 16:24:06,025 n: bert.embeddings.LayerNorm.weight
2023-03-27 16:24:06,025 n: bert.embeddings.LayerNorm.bias
2023-03-27 16:24:06,025 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 16:24:06,025 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 16:24:06,025 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 16:24:06,025 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 16:24:06,025 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 16:24:06,025 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 16:24:06,025 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 16:24:06,026 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 16:24:06,026 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 16:24:06,026 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 16:24:06,026 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 16:24:06,026 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 16:24:06,026 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 16:24:06,026 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 16:24:06,026 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 16:24:06,026 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 16:24:06,026 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 16:24:06,027 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 16:24:06,027 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 16:24:06,027 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 16:24:06,027 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 16:24:06,027 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 16:24:06,027 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 16:24:06,027 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 16:24:06,027 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 16:24:06,027 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 16:24:06,027 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 16:24:06,028 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 16:24:06,028 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 16:24:06,028 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 16:24:06,028 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 16:24:06,028 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 16:24:06,028 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 16:24:06,028 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 16:24:06,028 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 16:24:06,028 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 16:24:06,028 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 16:24:06,029 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 16:24:06,029 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 16:24:06,029 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 16:24:06,029 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 16:24:06,029 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 16:24:06,029 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 16:24:06,029 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 16:24:06,029 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 16:24:06,029 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 16:24:06,029 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 16:24:06,030 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 16:24:06,030 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 16:24:06,030 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 16:24:06,030 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 16:24:06,030 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 16:24:06,030 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 16:24:06,030 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 16:24:06,030 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 16:24:06,030 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 16:24:06,030 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 16:24:06,030 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 16:24:06,031 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 16:24:06,031 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 16:24:06,031 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 16:24:06,031 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 16:24:06,031 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 16:24:06,031 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 16:24:06,031 n: bert.pooler.dense.weight
2023-03-27 16:24:06,031 n: bert.pooler.dense.bias
2023-03-27 16:24:06,031 n: classifier.weight
2023-03-27 16:24:06,031 n: classifier.bias
2023-03-27 16:24:06,032 n: fit_dense.weight
2023-03-27 16:24:06,032 n: fit_dense.bias
2023-03-27 16:24:06,032 Total parameters: 14591258
2023-03-27 16:24:23,174 ***** Running evaluation *****
2023-03-27 16:24:23,175   Epoch = 0 iter 49 step
2023-03-27 16:24:23,175   Num examples = 1043
2023-03-27 16:24:23,175   Batch size = 32
2023-03-27 16:24:23,785 ***** Eval results *****
2023-03-27 16:24:23,785   acc = 0.7066155321188878
2023-03-27 16:24:23,786   att_loss = 0.0
2023-03-27 16:24:23,786   cls_loss = 0.3103802331856319
2023-03-27 16:24:23,786   eval_loss = 0.5863235791524252
2023-03-27 16:24:23,787   global_step = 49
2023-03-27 16:24:23,787   loss = 0.3103802331856319
2023-03-27 16:24:23,788   mcc = 0.22552275265291963
2023-03-27 16:24:23,788   rep_loss = 0.0
2023-03-27 16:24:23,795 ***** Save model *****
2023-03-27 16:24:37,527 ***** Running evaluation *****
2023-03-27 16:24:37,528   Epoch = 0 iter 99 step
2023-03-27 16:24:37,528   Num examples = 1043
2023-03-27 16:24:37,528   Batch size = 32
2023-03-27 16:24:38,123 ***** Eval results *****
2023-03-27 16:24:38,123   acc = 0.7114093959731543
2023-03-27 16:24:38,124   att_loss = 0.0
2023-03-27 16:24:38,124   cls_loss = 0.2980859806441297
2023-03-27 16:24:38,124   eval_loss = 0.5904002794713685
2023-03-27 16:24:38,124   global_step = 99
2023-03-27 16:24:38,124   loss = 0.2980859806441297
2023-03-27 16:24:38,124   mcc = 0.23920647786365987
2023-03-27 16:24:38,125   rep_loss = 0.0
2023-03-27 16:24:38,132 ***** Save model *****
2023-03-27 16:24:51,838 ***** Running evaluation *****
2023-03-27 16:24:51,839   Epoch = 0 iter 149 step
2023-03-27 16:24:51,839   Num examples = 1043
2023-03-27 16:24:51,839   Batch size = 32
2023-03-27 16:24:52,435 ***** Eval results *****
2023-03-27 16:24:52,435   acc = 0.6788111217641419
2023-03-27 16:24:52,435   att_loss = 0.0
2023-03-27 16:24:52,435   cls_loss = 0.2918725621780293
2023-03-27 16:24:52,435   eval_loss = 0.601570596297582
2023-03-27 16:24:52,436   global_step = 149
2023-03-27 16:24:52,436   loss = 0.2918725621780293
2023-03-27 16:24:52,436   mcc = 0.21325587021553824
2023-03-27 16:24:52,436   rep_loss = 0.0
2023-03-27 16:25:05,524 ***** Running evaluation *****
2023-03-27 16:25:05,524   Epoch = 0 iter 199 step
2023-03-27 16:25:05,524   Num examples = 1043
2023-03-27 16:25:05,524   Batch size = 32
2023-03-27 16:25:06,123 ***** Eval results *****
2023-03-27 16:25:06,123   acc = 0.6931927133269415
2023-03-27 16:25:06,123   att_loss = 0.0
2023-03-27 16:25:06,123   cls_loss = 0.29033733821993496
2023-03-27 16:25:06,124   eval_loss = 0.5990417825453209
2023-03-27 16:25:06,124   global_step = 199
2023-03-27 16:25:06,124   loss = 0.29033733821993496
2023-03-27 16:25:06,124   mcc = 0.2530776397478349
2023-03-27 16:25:06,124   rep_loss = 0.0
2023-03-27 16:25:06,126 ***** Save model *****
2023-03-27 16:25:19,920 ***** Running evaluation *****
2023-03-27 16:25:19,921   Epoch = 0 iter 249 step
2023-03-27 16:25:19,922   Num examples = 1043
2023-03-27 16:25:19,922   Batch size = 32
2023-03-27 16:25:20,522 ***** Eval results *****
2023-03-27 16:25:20,522   acc = 0.7056567593480345
2023-03-27 16:25:20,522   att_loss = 0.0
2023-03-27 16:25:20,522   cls_loss = 0.2881632454424019
2023-03-27 16:25:20,523   eval_loss = 0.595067585959579
2023-03-27 16:25:20,523   global_step = 249
2023-03-27 16:25:20,523   loss = 0.2881632454424019
2023-03-27 16:25:20,523   mcc = 0.24505616733536942
2023-03-27 16:25:20,523   rep_loss = 0.0
2023-03-27 16:25:33,679 ***** Running evaluation *****
2023-03-27 16:25:33,680   Epoch = 1 iter 299 step
2023-03-27 16:25:33,680   Num examples = 1043
2023-03-27 16:25:33,680   Batch size = 32
2023-03-27 16:25:34,280 ***** Eval results *****
2023-03-27 16:25:34,281   acc = 0.6864813039309684
2023-03-27 16:25:34,281   att_loss = 0.0
2023-03-27 16:25:34,281   cls_loss = 0.2769400039687753
2023-03-27 16:25:34,281   eval_loss = 0.60657240134297
2023-03-27 16:25:34,281   global_step = 299
2023-03-27 16:25:34,281   loss = 0.2769400039687753
2023-03-27 16:25:34,281   mcc = 0.23473441073354398
2023-03-27 16:25:34,281   rep_loss = 0.0
2023-03-27 16:25:47,488 ***** Running evaluation *****
2023-03-27 16:25:47,488   Epoch = 1 iter 349 step
2023-03-27 16:25:47,488   Num examples = 1043
2023-03-27 16:25:47,488   Batch size = 32
2023-03-27 16:25:48,089 ***** Eval results *****
2023-03-27 16:25:48,089   acc = 0.6931927133269415
2023-03-27 16:25:48,089   att_loss = 0.0
2023-03-27 16:25:48,089   cls_loss = 0.2753661449362592
2023-03-27 16:25:48,089   eval_loss = 0.5987993063348712
2023-03-27 16:25:48,090   global_step = 349
2023-03-27 16:25:48,090   loss = 0.2753661449362592
2023-03-27 16:25:48,090   mcc = 0.22280382305801125
2023-03-27 16:25:48,090   rep_loss = 0.0
2023-03-27 16:26:01,311 ***** Running evaluation *****
2023-03-27 16:26:01,311   Epoch = 1 iter 399 step
2023-03-27 16:26:01,311   Num examples = 1043
2023-03-27 16:26:01,311   Batch size = 32
2023-03-27 16:26:01,920 ***** Eval results *****
2023-03-27 16:26:01,921   acc = 0.716203259827421
2023-03-27 16:26:01,921   att_loss = 0.0
2023-03-27 16:26:01,921   cls_loss = 0.27552773103569494
2023-03-27 16:26:01,921   eval_loss = 0.5817531934290221
2023-03-27 16:26:01,921   global_step = 399
2023-03-27 16:26:01,922   loss = 0.27552773103569494
2023-03-27 16:26:01,922   mcc = 0.2711945584758675
2023-03-27 16:26:01,922   rep_loss = 0.0
2023-03-27 16:26:01,933 ***** Save model *****
2023-03-27 16:26:15,836 ***** Running evaluation *****
2023-03-27 16:26:15,837   Epoch = 1 iter 449 step
2023-03-27 16:26:15,837   Num examples = 1043
2023-03-27 16:26:15,837   Batch size = 32
2023-03-27 16:26:16,439 ***** Eval results *****
2023-03-27 16:26:16,440   acc = 0.7085330776605945
2023-03-27 16:26:16,440   att_loss = 0.0
2023-03-27 16:26:16,440   cls_loss = 0.2751357512814658
2023-03-27 16:26:16,440   eval_loss = 0.5813823613253507
2023-03-27 16:26:16,440   global_step = 449
2023-03-27 16:26:16,441   loss = 0.2751357512814658
2023-03-27 16:26:16,441   mcc = 0.26854789956063607
2023-03-27 16:26:16,441   rep_loss = 0.0
2023-03-27 16:26:29,728 ***** Running evaluation *****
2023-03-27 16:26:29,728   Epoch = 1 iter 499 step
2023-03-27 16:26:29,728   Num examples = 1043
2023-03-27 16:26:29,728   Batch size = 32
2023-03-27 16:26:30,331 ***** Eval results *****
2023-03-27 16:26:30,332   acc = 0.7085330776605945
2023-03-27 16:26:30,332   att_loss = 0.0
2023-03-27 16:26:30,332   cls_loss = 0.2749976789386108
2023-03-27 16:26:30,332   eval_loss = 0.5832070356065576
2023-03-27 16:26:30,332   global_step = 499
2023-03-27 16:26:30,332   loss = 0.2749976789386108
2023-03-27 16:26:30,332   mcc = 0.26202033638388644
2023-03-27 16:26:30,332   rep_loss = 0.0
2023-03-27 16:26:43,635 ***** Running evaluation *****
2023-03-27 16:26:43,635   Epoch = 2 iter 549 step
2023-03-27 16:26:43,635   Num examples = 1043
2023-03-27 16:26:43,635   Batch size = 32
2023-03-27 16:26:44,238 ***** Eval results *****
2023-03-27 16:26:44,239   acc = 0.6941514860977949
2023-03-27 16:26:44,239   att_loss = 0.0
2023-03-27 16:26:44,239   cls_loss = 0.27057949006557463
2023-03-27 16:26:44,239   eval_loss = 0.5882140262560411
2023-03-27 16:26:44,239   global_step = 549
2023-03-27 16:26:44,239   loss = 0.27057949006557463
2023-03-27 16:26:44,239   mcc = 0.25120764646171945
2023-03-27 16:26:44,239   rep_loss = 0.0
2023-03-27 16:26:57,537 ***** Running evaluation *****
2023-03-27 16:26:57,538   Epoch = 2 iter 599 step
2023-03-27 16:26:57,538   Num examples = 1043
2023-03-27 16:26:57,538   Batch size = 32
2023-03-27 16:26:58,141 ***** Eval results *****
2023-03-27 16:26:58,141   acc = 0.7229146692233941
2023-03-27 16:26:58,141   att_loss = 0.0
2023-03-27 16:26:58,141   cls_loss = 0.27090857235284954
2023-03-27 16:26:58,141   eval_loss = 0.5756063145218473
2023-03-27 16:26:58,141   global_step = 599
2023-03-27 16:26:58,141   loss = 0.27090857235284954
2023-03-27 16:26:58,142   mcc = 0.29469430172062516
2023-03-27 16:26:58,142   rep_loss = 0.0
2023-03-27 16:26:58,149 ***** Save model *****
2023-03-27 16:27:12,084 ***** Running evaluation *****
2023-03-27 16:27:12,085   Epoch = 2 iter 649 step
2023-03-27 16:27:12,085   Num examples = 1043
2023-03-27 16:27:12,085   Batch size = 32
2023-03-27 16:27:12,688 ***** Eval results *****
2023-03-27 16:27:12,688   acc = 0.7152444870565676
2023-03-27 16:27:12,688   att_loss = 0.0
2023-03-27 16:27:12,688   cls_loss = 0.27064377497071807
2023-03-27 16:27:12,688   eval_loss = 0.57960219726418
2023-03-27 16:27:12,688   global_step = 649
2023-03-27 16:27:12,688   loss = 0.27064377497071807
2023-03-27 16:27:12,688   mcc = 0.28022221613462045
2023-03-27 16:27:12,689   rep_loss = 0.0
2023-03-27 16:27:26,008 ***** Running evaluation *****
2023-03-27 16:27:26,009   Epoch = 2 iter 699 step
2023-03-27 16:27:26,009   Num examples = 1043
2023-03-27 16:27:26,009   Batch size = 32
2023-03-27 16:27:26,613 ***** Eval results *****
2023-03-27 16:27:26,613   acc = 0.7181208053691275
2023-03-27 16:27:26,613   att_loss = 0.0
2023-03-27 16:27:26,613   cls_loss = 0.27110419517213646
2023-03-27 16:27:26,614   eval_loss = 0.577468196551005
2023-03-27 16:27:26,614   global_step = 699
2023-03-27 16:27:26,614   loss = 0.27110419517213646
2023-03-27 16:27:26,614   mcc = 0.28133108710648114
2023-03-27 16:27:26,614   rep_loss = 0.0
2023-03-27 16:27:39,923 ***** Running evaluation *****
2023-03-27 16:27:39,924   Epoch = 2 iter 749 step
2023-03-27 16:27:39,924   Num examples = 1043
2023-03-27 16:27:39,924   Batch size = 32
2023-03-27 16:27:40,529 ***** Eval results *****
2023-03-27 16:27:40,530   acc = 0.7181208053691275
2023-03-27 16:27:40,530   att_loss = 0.0
2023-03-27 16:27:40,530   cls_loss = 0.27059034857639047
2023-03-27 16:27:40,530   eval_loss = 0.5778703039342706
2023-03-27 16:27:40,530   global_step = 749
2023-03-27 16:27:40,531   loss = 0.27059034857639047
2023-03-27 16:27:40,531   mcc = 0.28529203384052887
2023-03-27 16:27:40,531   rep_loss = 0.0
2023-03-27 16:27:53,857 ***** Running evaluation *****
2023-03-27 16:27:53,857   Epoch = 2 iter 799 step
2023-03-27 16:27:53,857   Num examples = 1043
2023-03-27 16:27:53,857   Batch size = 32
2023-03-27 16:27:54,467 ***** Eval results *****
2023-03-27 16:27:54,467   acc = 0.7181208053691275
2023-03-27 16:27:54,467   att_loss = 0.0
2023-03-27 16:27:54,468   cls_loss = 0.27029852096764545
2023-03-27 16:27:54,468   eval_loss = 0.5788202285766602
2023-03-27 16:27:54,468   global_step = 799
2023-03-27 16:27:54,468   loss = 0.27029852096764545
2023-03-27 16:27:54,468   mcc = 0.28829259472164825
2023-03-27 16:27:54,468   rep_loss = 0.0
2023-03-27 16:47:24,387 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_BASE/TempTinyBERT_CoLA_4L_312D_5e-05_32', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=30.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='KL_ATTN_BASE', kl_attn_weight=None)
2023-03-27 16:47:25,563 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 16:47:28,188 device: cuda n_gpu: 1
2023-03-27 16:48:38,648 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_BASE_SWEEP_TEST/TempTinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=30.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='KL_ATTN_BASE_SWEEP_TEST', kl_attn_weight=None)
2023-03-27 16:48:39,803 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 16:48:42,380 device: cuda n_gpu: 1
2023-03-27 16:48:42,485 Writing example 0 of 8551
2023-03-27 16:48:42,485 *** Example ***
2023-03-27 16:48:42,485 guid: train-0
2023-03-27 16:48:42,485 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 16:48:42,485 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:48:42,486 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:48:42,486 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:48:42,486 label: 1
2023-03-27 16:48:42,486 label_id: 1
2023-03-27 16:48:43,441 Writing example 0 of 1043
2023-03-27 16:48:43,441 *** Example ***
2023-03-27 16:48:43,441 guid: dev-0
2023-03-27 16:48:43,441 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 16:48:43,441 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:48:43,441 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:48:43,442 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 16:48:43,442 label: 1
2023-03-27 16:48:43,442 label_id: 1
2023-03-27 16:48:43,559 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 16:48:43,560 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 16:48:45,328 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 16:48:45,501 loading model...
2023-03-27 16:48:45,553 done!
2023-03-27 16:48:45,554 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 16:48:46,603 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 16:48:46,605 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 16:48:46,831 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 16:48:46,853 loading model...
2023-03-27 16:48:46,864 done!
2023-03-27 16:48:46,864 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 16:48:46,864 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 16:48:46,877 ***** Running training *****
2023-03-27 16:48:46,877   Num examples = 8551
2023-03-27 16:48:46,878   Batch size = 32
2023-03-27 16:48:46,878   Num steps = 8010
2023-03-27 16:48:46,878 n: bert.embeddings.word_embeddings.weight
2023-03-27 16:48:46,878 n: bert.embeddings.position_embeddings.weight
2023-03-27 16:48:46,878 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 16:48:46,878 n: bert.embeddings.LayerNorm.weight
2023-03-27 16:48:46,879 n: bert.embeddings.LayerNorm.bias
2023-03-27 16:48:46,879 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 16:48:46,879 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 16:48:46,879 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 16:48:46,879 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 16:48:46,879 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 16:48:46,879 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 16:48:46,879 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 16:48:46,879 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 16:48:46,880 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 16:48:46,880 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 16:48:46,880 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 16:48:46,880 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 16:48:46,880 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 16:48:46,880 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 16:48:46,880 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 16:48:46,880 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 16:48:46,881 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 16:48:46,881 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 16:48:46,881 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 16:48:46,881 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 16:48:46,881 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 16:48:46,881 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 16:48:46,881 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 16:48:46,881 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 16:48:46,881 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 16:48:46,882 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 16:48:46,882 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 16:48:46,882 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 16:48:46,882 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 16:48:46,882 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 16:48:46,882 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 16:48:46,882 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 16:48:46,882 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 16:48:46,882 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 16:48:46,882 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 16:48:46,883 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 16:48:46,883 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 16:48:46,883 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 16:48:46,883 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 16:48:46,883 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 16:48:46,883 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 16:48:46,883 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 16:48:46,883 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 16:48:46,883 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 16:48:46,883 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 16:48:46,884 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 16:48:46,884 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 16:48:46,884 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 16:48:46,884 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 16:48:46,884 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 16:48:46,884 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 16:48:46,884 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 16:48:46,884 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 16:48:46,884 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 16:48:46,884 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 16:48:46,885 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 16:48:46,885 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 16:48:46,885 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 16:48:46,885 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 16:48:46,885 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 16:48:46,885 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 16:48:46,885 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 16:48:46,885 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 16:48:46,885 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 16:48:46,885 n: bert.pooler.dense.weight
2023-03-27 16:48:46,886 n: bert.pooler.dense.bias
2023-03-27 16:48:46,886 n: classifier.weight
2023-03-27 16:48:46,886 n: classifier.bias
2023-03-27 16:48:46,886 n: fit_dense.weight
2023-03-27 16:48:46,886 n: fit_dense.bias
2023-03-27 16:48:46,886 Total parameters: 14591258
2023-03-27 16:48:57,002 ***** Running evaluation *****
2023-03-27 16:48:57,002   Epoch = 0 iter 49 step
2023-03-27 16:48:57,002   Num examples = 1043
2023-03-27 16:48:57,003   Batch size = 32
2023-03-27 16:48:57,005 ***** Eval results *****
2023-03-27 16:48:57,006   att_loss = 0.5624887608751958
2023-03-27 16:48:57,006   cls_loss = 0.0
2023-03-27 16:48:57,006   global_step = 49
2023-03-27 16:48:57,007   loss = 1.885422227334003
2023-03-27 16:48:57,007   rep_loss = 1.3229334549028047
2023-03-27 16:48:57,009 ***** Save model *****
2023-03-27 16:49:07,271 ***** Running evaluation *****
2023-03-27 16:49:07,272   Epoch = 0 iter 99 step
2023-03-27 16:49:07,272   Num examples = 1043
2023-03-27 16:49:07,272   Batch size = 32
2023-03-27 16:49:07,273 ***** Eval results *****
2023-03-27 16:49:07,274   att_loss = 0.5208015110757616
2023-03-27 16:49:07,274   cls_loss = 0.0
2023-03-27 16:49:07,274   global_step = 99
2023-03-27 16:49:07,274   loss = 1.625842166669441
2023-03-27 16:49:07,275   rep_loss = 1.1050406513792095
2023-03-27 16:49:07,282 ***** Save model *****
2023-03-27 16:49:17,653 ***** Running evaluation *****
2023-03-27 16:49:17,653   Epoch = 0 iter 149 step
2023-03-27 16:49:17,653   Num examples = 1043
2023-03-27 16:49:17,654   Batch size = 32
2023-03-27 16:49:17,654 ***** Eval results *****
2023-03-27 16:49:17,655   att_loss = 0.4964984979405499
2023-03-27 16:49:17,655   cls_loss = 0.0
2023-03-27 16:49:17,655   global_step = 149
2023-03-27 16:49:17,655   loss = 1.5029620956254486
2023-03-27 16:49:17,655   rep_loss = 1.0064635928845245
2023-03-27 16:49:17,662 ***** Save model *****
2023-03-27 16:49:28,123 ***** Running evaluation *****
2023-03-27 16:49:28,123   Epoch = 0 iter 199 step
2023-03-27 16:49:28,123   Num examples = 1043
2023-03-27 16:49:28,123   Batch size = 32
2023-03-27 16:49:28,124 ***** Eval results *****
2023-03-27 16:49:28,125   att_loss = 0.4843230084258707
2023-03-27 16:49:28,125   cls_loss = 0.0
2023-03-27 16:49:28,125   global_step = 199
2023-03-27 16:49:28,125   loss = 1.4326463176976496
2023-03-27 16:49:28,125   rep_loss = 0.9483233037306436
2023-03-27 16:49:28,133 ***** Save model *****
2023-03-27 16:49:38,670 ***** Running evaluation *****
2023-03-27 16:49:38,670   Epoch = 0 iter 249 step
2023-03-27 16:49:38,670   Num examples = 1043
2023-03-27 16:49:38,670   Batch size = 32
2023-03-27 16:49:38,671 ***** Eval results *****
2023-03-27 16:49:38,672   att_loss = 0.47543028882708416
2023-03-27 16:49:38,672   cls_loss = 0.0
2023-03-27 16:49:38,672   global_step = 249
2023-03-27 16:49:38,672   loss = 1.384641918791346
2023-03-27 16:49:38,672   rep_loss = 0.9092116243389237
2023-03-27 16:49:38,675 ***** Save model *****
2023-03-27 16:49:49,255 ***** Running evaluation *****
2023-03-27 16:49:49,255   Epoch = 1 iter 299 step
2023-03-27 16:49:49,255   Num examples = 1043
2023-03-27 16:49:49,256   Batch size = 32
2023-03-27 16:49:49,257 ***** Eval results *****
2023-03-27 16:49:49,257   att_loss = 0.4300664449110627
2023-03-27 16:49:49,257   cls_loss = 0.0
2023-03-27 16:49:49,257   global_step = 299
2023-03-27 16:49:49,257   loss = 1.1647225208580494
2023-03-27 16:49:49,258   rep_loss = 0.7346560824662447
2023-03-27 16:49:49,265 ***** Save model *****
2023-03-27 16:49:59,928 ***** Running evaluation *****
2023-03-27 16:49:59,929   Epoch = 1 iter 349 step
2023-03-27 16:49:59,929   Num examples = 1043
2023-03-27 16:49:59,929   Batch size = 32
2023-03-27 16:49:59,930 ***** Eval results *****
2023-03-27 16:49:59,930   att_loss = 0.4341871356818734
2023-03-27 16:49:59,930   cls_loss = 0.0
2023-03-27 16:49:59,931   global_step = 349
2023-03-27 16:49:59,931   loss = 1.1631340544398239
2023-03-27 16:49:59,931   rep_loss = 0.7289469205751652
2023-03-27 16:49:59,938 ***** Save model *****
2023-03-27 16:50:10,617 ***** Running evaluation *****
2023-03-27 16:50:10,618   Epoch = 1 iter 399 step
2023-03-27 16:50:10,618   Num examples = 1043
2023-03-27 16:50:10,618   Batch size = 32
2023-03-27 16:50:10,619 ***** Eval results *****
2023-03-27 16:50:10,619   att_loss = 0.43267385480981885
2023-03-27 16:50:10,619   cls_loss = 0.0
2023-03-27 16:50:10,619   global_step = 399
2023-03-27 16:50:10,619   loss = 1.1566124359766643
2023-03-27 16:50:10,619   rep_loss = 0.7239385789090936
2023-03-27 16:50:10,626 ***** Save model *****
2023-03-27 16:50:21,390 ***** Running evaluation *****
2023-03-27 16:50:21,390   Epoch = 1 iter 449 step
2023-03-27 16:50:21,391   Num examples = 1043
2023-03-27 16:50:21,391   Batch size = 32
2023-03-27 16:50:21,392 ***** Eval results *****
2023-03-27 16:50:21,392   att_loss = 0.43400396161027005
2023-03-27 16:50:21,393   cls_loss = 0.0
2023-03-27 16:50:21,393   global_step = 449
2023-03-27 16:50:21,393   loss = 1.1545401772299966
2023-03-27 16:50:21,393   rep_loss = 0.7205362136547382
2023-03-27 16:50:21,400 ***** Save model *****
2023-03-27 16:50:32,209 ***** Running evaluation *****
2023-03-27 16:50:32,209   Epoch = 1 iter 499 step
2023-03-27 16:50:32,209   Num examples = 1043
2023-03-27 16:50:32,209   Batch size = 32
2023-03-27 16:50:32,211 ***** Eval results *****
2023-03-27 16:50:32,211   att_loss = 0.4307284517021015
2023-03-27 16:50:32,211   cls_loss = 0.0
2023-03-27 16:50:32,211   global_step = 499
2023-03-27 16:50:32,211   loss = 1.1468715223258938
2023-03-27 16:50:32,212   rep_loss = 0.7161430683115433
2023-03-27 16:50:32,219 ***** Save model *****
2023-03-27 16:50:43,080 ***** Running evaluation *****
2023-03-27 16:50:43,080   Epoch = 2 iter 549 step
2023-03-27 16:50:43,080   Num examples = 1043
2023-03-27 16:50:43,080   Batch size = 32
2023-03-27 16:50:43,081 ***** Eval results *****
2023-03-27 16:50:43,082   att_loss = 0.39662219484647115
2023-03-27 16:50:43,082   cls_loss = 0.0
2023-03-27 16:50:43,082   global_step = 549
2023-03-27 16:50:43,082   loss = 1.0691010157267253
2023-03-27 16:50:43,082   rep_loss = 0.6724788268407186
2023-03-27 16:50:43,089 ***** Save model *****
2023-03-27 16:50:53,986 ***** Running evaluation *****
2023-03-27 16:50:53,986   Epoch = 2 iter 599 step
2023-03-27 16:50:53,986   Num examples = 1043
2023-03-27 16:50:53,986   Batch size = 32
2023-03-27 16:50:53,988 ***** Eval results *****
2023-03-27 16:50:53,988   att_loss = 0.41310284366974465
2023-03-27 16:50:53,988   cls_loss = 0.0
2023-03-27 16:50:53,988   global_step = 599
2023-03-27 16:50:53,989   loss = 1.0961717954048744
2023-03-27 16:50:53,989   rep_loss = 0.6830689430236816
2023-03-27 16:50:53,996 ***** Save model *****
2023-03-27 16:51:04,954 ***** Running evaluation *****
2023-03-27 16:51:04,955   Epoch = 2 iter 649 step
2023-03-27 16:51:04,955   Num examples = 1043
2023-03-27 16:51:04,955   Batch size = 32
2023-03-27 16:51:04,956 ***** Eval results *****
2023-03-27 16:51:04,956   att_loss = 0.40927167768063755
2023-03-27 16:51:04,957   cls_loss = 0.0
2023-03-27 16:51:04,957   global_step = 649
2023-03-27 16:51:04,957   loss = 1.0888966332311216
2023-03-27 16:51:04,957   rep_loss = 0.6796249524406764
2023-03-27 16:51:04,965 ***** Save model *****
2023-03-27 16:51:15,943 ***** Running evaluation *****
2023-03-27 16:51:15,943   Epoch = 2 iter 699 step
2023-03-27 16:51:15,943   Num examples = 1043
2023-03-27 16:51:15,943   Batch size = 32
2023-03-27 16:51:15,944 ***** Eval results *****
2023-03-27 16:51:15,945   att_loss = 0.4119332958351482
2023-03-27 16:51:15,945   cls_loss = 0.0
2023-03-27 16:51:15,945   global_step = 699
2023-03-27 16:51:15,945   loss = 1.0910217075636892
2023-03-27 16:51:15,945   rep_loss = 0.6790884101029598
2023-03-27 16:51:15,953 ***** Save model *****
2023-03-27 16:51:26,963 ***** Running evaluation *****
2023-03-27 16:51:26,963   Epoch = 2 iter 749 step
2023-03-27 16:51:26,963   Num examples = 1043
2023-03-27 16:51:26,964   Batch size = 32
2023-03-27 16:51:26,965 ***** Eval results *****
2023-03-27 16:51:26,965   att_loss = 0.4134378297384395
2023-03-27 16:51:26,965   cls_loss = 0.0
2023-03-27 16:51:26,966   global_step = 749
2023-03-27 16:51:26,966   loss = 1.0912425207537273
2023-03-27 16:51:26,966   rep_loss = 0.6778046899063642
2023-03-27 16:51:26,973 ***** Save model *****
2023-03-27 16:51:38,020 ***** Running evaluation *****
2023-03-27 16:51:38,020   Epoch = 2 iter 799 step
2023-03-27 16:51:38,020   Num examples = 1043
2023-03-27 16:51:38,020   Batch size = 32
2023-03-27 16:51:38,022 ***** Eval results *****
2023-03-27 16:51:38,022   att_loss = 0.4134491615700272
2023-03-27 16:51:38,022   cls_loss = 0.0
2023-03-27 16:51:38,022   global_step = 799
2023-03-27 16:51:38,022   loss = 1.0893800404836547
2023-03-27 16:51:38,022   rep_loss = 0.6759308770017803
2023-03-27 16:51:38,030 ***** Save model *****
2023-03-27 16:51:49,086 ***** Running evaluation *****
2023-03-27 16:51:49,086   Epoch = 3 iter 849 step
2023-03-27 16:51:49,086   Num examples = 1043
2023-03-27 16:51:49,086   Batch size = 32
2023-03-27 16:51:49,087 ***** Eval results *****
2023-03-27 16:51:49,088   att_loss = 0.3948137064774831
2023-03-27 16:51:49,088   cls_loss = 0.0
2023-03-27 16:51:49,088   global_step = 849
2023-03-27 16:51:49,088   loss = 1.0505981830259163
2023-03-27 16:51:49,088   rep_loss = 0.6557844715813795
2023-03-27 16:51:49,096 ***** Save model *****
2023-03-27 16:52:00,184 ***** Running evaluation *****
2023-03-27 16:52:00,184   Epoch = 3 iter 899 step
2023-03-27 16:52:00,184   Num examples = 1043
2023-03-27 16:52:00,184   Batch size = 32
2023-03-27 16:52:00,186 ***** Eval results *****
2023-03-27 16:52:00,186   att_loss = 0.40008256964537564
2023-03-27 16:52:00,186   cls_loss = 0.0
2023-03-27 16:52:00,186   global_step = 899
2023-03-27 16:52:00,186   loss = 1.0546442020912559
2023-03-27 16:52:00,186   rep_loss = 0.6545616345746177
2023-03-27 16:52:00,194 ***** Save model *****
2023-03-27 16:52:11,327 ***** Running evaluation *****
2023-03-27 16:52:11,328   Epoch = 3 iter 949 step
2023-03-27 16:52:11,328   Num examples = 1043
2023-03-27 16:52:11,329   Batch size = 32
2023-03-27 16:52:11,330 ***** Eval results *****
2023-03-27 16:52:11,330   att_loss = 0.403970867395401
2023-03-27 16:52:11,330   cls_loss = 0.0
2023-03-27 16:52:11,331   global_step = 949
2023-03-27 16:52:11,331   loss = 1.059445704560022
2023-03-27 16:52:11,331   rep_loss = 0.6554748407892279
2023-03-27 16:52:11,339 ***** Save model *****
2023-03-27 16:52:22,466 ***** Running evaluation *****
2023-03-27 16:52:22,467   Epoch = 3 iter 999 step
2023-03-27 16:52:22,467   Num examples = 1043
2023-03-27 16:52:22,467   Batch size = 32
2023-03-27 16:52:22,468 ***** Eval results *****
2023-03-27 16:52:22,469   att_loss = 0.4025316471704329
2023-03-27 16:52:22,469   cls_loss = 0.0
2023-03-27 16:52:22,469   global_step = 999
2023-03-27 16:52:22,469   loss = 1.056138980870295
2023-03-27 16:52:22,470   rep_loss = 0.6536073377638152
2023-03-27 16:52:22,478 ***** Save model *****
2023-03-27 16:52:33,630 ***** Running evaluation *****
2023-03-27 16:52:33,630   Epoch = 3 iter 1049 step
2023-03-27 16:52:33,630   Num examples = 1043
2023-03-27 16:52:33,630   Batch size = 32
2023-03-27 16:52:33,631 ***** Eval results *****
2023-03-27 16:52:33,632   att_loss = 0.4021085189475167
2023-03-27 16:52:33,632   cls_loss = 0.0
2023-03-27 16:52:33,632   global_step = 1049
2023-03-27 16:52:33,632   loss = 1.0544886514544487
2023-03-27 16:52:33,633   rep_loss = 0.6523801371935876
2023-03-27 16:52:33,640 ***** Save model *****
2023-03-27 16:52:44,901 ***** Running evaluation *****
2023-03-27 16:52:44,902   Epoch = 4 iter 1099 step
2023-03-27 16:52:44,902   Num examples = 1043
2023-03-27 16:52:44,902   Batch size = 32
2023-03-27 16:52:44,903 ***** Eval results *****
2023-03-27 16:52:44,903   att_loss = 0.4054439740796243
2023-03-27 16:52:44,903   cls_loss = 0.0
2023-03-27 16:52:44,904   global_step = 1099
2023-03-27 16:52:44,904   loss = 1.0505533641384495
2023-03-27 16:52:44,904   rep_loss = 0.6451093842906337
2023-03-27 16:52:44,906 ***** Save model *****
2023-03-27 16:52:56,067 ***** Running evaluation *****
2023-03-27 16:52:56,067   Epoch = 4 iter 1149 step
2023-03-27 16:52:56,068   Num examples = 1043
2023-03-27 16:52:56,068   Batch size = 32
2023-03-27 16:52:56,069 ***** Eval results *****
2023-03-27 16:52:56,069   att_loss = 0.403627230429355
2023-03-27 16:52:56,069   cls_loss = 0.0
2023-03-27 16:52:56,069   global_step = 1149
2023-03-27 16:52:56,070   loss = 1.0472957109227592
2023-03-27 16:52:56,070   rep_loss = 0.6436684801254744
2023-03-27 16:52:56,077 ***** Save model *****
2023-03-27 16:53:07,277 ***** Running evaluation *****
2023-03-27 16:53:07,277   Epoch = 4 iter 1199 step
2023-03-27 16:53:07,277   Num examples = 1043
2023-03-27 16:53:07,278   Batch size = 32
2023-03-27 16:53:07,279 ***** Eval results *****
2023-03-27 16:53:07,279   att_loss = 0.3985099150934292
2023-03-27 16:53:07,280   cls_loss = 0.0
2023-03-27 16:53:07,280   global_step = 1199
2023-03-27 16:53:07,280   loss = 1.039177829527673
2023-03-27 16:53:07,280   rep_loss = 0.6406679135242491
2023-03-27 16:53:07,288 ***** Save model *****
2023-03-27 16:53:18,477 ***** Running evaluation *****
2023-03-27 16:53:18,477   Epoch = 4 iter 1249 step
2023-03-27 16:53:18,477   Num examples = 1043
2023-03-27 16:53:18,477   Batch size = 32
2023-03-27 16:53:18,478 ***** Eval results *****
2023-03-27 16:53:18,479   att_loss = 0.3953841019071927
2023-03-27 16:53:18,479   cls_loss = 0.0
2023-03-27 16:53:18,479   global_step = 1249
2023-03-27 16:53:18,479   loss = 1.0335068396441844
2023-03-27 16:53:18,479   rep_loss = 0.6381227354318397
2023-03-27 16:53:18,486 ***** Save model *****
2023-03-27 16:53:29,718 ***** Running evaluation *****
2023-03-27 16:53:29,719   Epoch = 4 iter 1299 step
2023-03-27 16:53:29,719   Num examples = 1043
2023-03-27 16:53:29,719   Batch size = 32
2023-03-27 16:53:29,720 ***** Eval results *****
2023-03-27 16:53:29,720   att_loss = 0.3963005565977716
2023-03-27 16:53:29,720   cls_loss = 0.0
2023-03-27 16:53:29,721   global_step = 1299
2023-03-27 16:53:29,721   loss = 1.0330461070135042
2023-03-27 16:53:29,721   rep_loss = 0.6367455486095313
2023-03-27 16:53:29,728 ***** Save model *****
2023-03-27 16:53:40,965 ***** Running evaluation *****
2023-03-27 16:53:40,965   Epoch = 5 iter 1349 step
2023-03-27 16:53:40,966   Num examples = 1043
2023-03-27 16:53:40,966   Batch size = 32
2023-03-27 16:53:40,967 ***** Eval results *****
2023-03-27 16:53:40,967   att_loss = 0.3825896169458117
2023-03-27 16:53:40,968   cls_loss = 0.0
2023-03-27 16:53:40,968   global_step = 1349
2023-03-27 16:53:40,968   loss = 1.006858697959355
2023-03-27 16:53:40,968   rep_loss = 0.6242690810135433
2023-03-27 16:53:40,976 ***** Save model *****
2023-03-27 16:53:52,212 ***** Running evaluation *****
2023-03-27 16:53:52,213   Epoch = 5 iter 1399 step
2023-03-27 16:53:52,213   Num examples = 1043
2023-03-27 16:53:52,213   Batch size = 32
2023-03-27 16:53:52,214 ***** Eval results *****
2023-03-27 16:53:52,214   att_loss = 0.39165697526186705
2023-03-27 16:53:52,215   cls_loss = 0.0
2023-03-27 16:53:52,215   global_step = 1399
2023-03-27 16:53:52,215   loss = 1.0195242576301098
2023-03-27 16:53:52,215   rep_loss = 0.627867279574275
2023-03-27 16:53:52,222 ***** Save model *****
2023-03-27 16:54:03,466 ***** Running evaluation *****
2023-03-27 16:54:03,466   Epoch = 5 iter 1449 step
2023-03-27 16:54:03,467   Num examples = 1043
2023-03-27 16:54:03,467   Batch size = 32
2023-03-27 16:54:03,468 ***** Eval results *****
2023-03-27 16:54:03,468   att_loss = 0.3876965087756776
2023-03-27 16:54:03,468   cls_loss = 0.0
2023-03-27 16:54:03,468   global_step = 1449
2023-03-27 16:54:03,469   loss = 1.0139673153559368
2023-03-27 16:54:03,469   rep_loss = 0.6262708081488024
2023-03-27 16:54:03,476 ***** Save model *****
2023-03-27 16:54:14,742 ***** Running evaluation *****
2023-03-27 16:54:14,742   Epoch = 5 iter 1499 step
2023-03-27 16:54:14,743   Num examples = 1043
2023-03-27 16:54:14,743   Batch size = 32
2023-03-27 16:54:14,745 ***** Eval results *****
2023-03-27 16:54:14,745   att_loss = 0.3887402369845204
2023-03-27 16:54:14,745   cls_loss = 0.0
2023-03-27 16:54:14,746   global_step = 1499
2023-03-27 16:54:14,746   loss = 1.0137573829511317
2023-03-27 16:54:14,746   rep_loss = 0.6250171483289905
2023-03-27 16:54:14,753 ***** Save model *****
2023-03-27 16:54:26,035 ***** Running evaluation *****
2023-03-27 16:54:26,036   Epoch = 5 iter 1549 step
2023-03-27 16:54:26,036   Num examples = 1043
2023-03-27 16:54:26,036   Batch size = 32
2023-03-27 16:54:26,037 ***** Eval results *****
2023-03-27 16:54:26,037   att_loss = 0.38855596165233686
2023-03-27 16:54:26,038   cls_loss = 0.0
2023-03-27 16:54:26,038   global_step = 1549
2023-03-27 16:54:26,038   loss = 1.0123969088090914
2023-03-27 16:54:26,038   rep_loss = 0.6238409478530705
2023-03-27 16:54:26,046 ***** Save model *****
2023-03-27 16:54:37,349 ***** Running evaluation *****
2023-03-27 16:54:37,349   Epoch = 5 iter 1599 step
2023-03-27 16:54:37,349   Num examples = 1043
2023-03-27 16:54:37,349   Batch size = 32
2023-03-27 16:54:37,350 ***** Eval results *****
2023-03-27 16:54:37,350   att_loss = 0.3891174128335534
2023-03-27 16:54:37,350   cls_loss = 0.0
2023-03-27 16:54:37,350   global_step = 1599
2023-03-27 16:54:37,351   loss = 1.012273676016114
2023-03-27 16:54:37,351   rep_loss = 0.6231562637469985
2023-03-27 16:54:37,357 ***** Save model *****
2023-03-27 16:54:48,644 ***** Running evaluation *****
2023-03-27 16:54:48,644   Epoch = 6 iter 1649 step
2023-03-27 16:54:48,644   Num examples = 1043
2023-03-27 16:54:48,644   Batch size = 32
2023-03-27 16:54:48,646 ***** Eval results *****
2023-03-27 16:54:48,646   att_loss = 0.37915646903058314
2023-03-27 16:54:48,646   cls_loss = 0.0
2023-03-27 16:54:48,647   global_step = 1649
2023-03-27 16:54:48,647   loss = 0.9920509089814856
2023-03-27 16:54:48,647   rep_loss = 0.6128944424872703
2023-03-27 16:54:48,654 ***** Save model *****
2023-03-27 16:54:59,958 ***** Running evaluation *****
2023-03-27 16:54:59,958   Epoch = 6 iter 1699 step
2023-03-27 16:54:59,958   Num examples = 1043
2023-03-27 16:54:59,959   Batch size = 32
2023-03-27 16:54:59,960 ***** Eval results *****
2023-03-27 16:54:59,960   att_loss = 0.38268627548955153
2023-03-27 16:54:59,960   cls_loss = 0.0
2023-03-27 16:54:59,960   global_step = 1699
2023-03-27 16:54:59,960   loss = 0.9963785729457423
2023-03-27 16:54:59,960   rep_loss = 0.6136922959199885
2023-03-27 16:54:59,968 ***** Save model *****
2023-03-27 16:55:11,255 ***** Running evaluation *****
2023-03-27 16:55:11,255   Epoch = 6 iter 1749 step
2023-03-27 16:55:11,256   Num examples = 1043
2023-03-27 16:55:11,256   Batch size = 32
2023-03-27 16:55:11,257 ***** Eval results *****
2023-03-27 16:55:11,257   att_loss = 0.3831973888841616
2023-03-27 16:55:11,257   cls_loss = 0.0
2023-03-27 16:55:11,257   global_step = 1749
2023-03-27 16:55:11,257   loss = 0.9965345859527588
2023-03-27 16:55:11,257   rep_loss = 0.6133371928111225
2023-03-27 16:55:11,265 ***** Save model *****
2023-03-27 16:55:22,557 ***** Running evaluation *****
2023-03-27 16:55:22,557   Epoch = 6 iter 1799 step
2023-03-27 16:55:22,558   Num examples = 1043
2023-03-27 16:55:22,558   Batch size = 32
2023-03-27 16:55:22,559 ***** Eval results *****
2023-03-27 16:55:22,559   att_loss = 0.3837708675014186
2023-03-27 16:55:22,560   cls_loss = 0.0
2023-03-27 16:55:22,560   global_step = 1799
2023-03-27 16:55:22,560   loss = 0.9967658495539942
2023-03-27 16:55:22,560   rep_loss = 0.6129949791782399
2023-03-27 16:55:22,567 ***** Save model *****
2023-03-27 16:55:33,864 ***** Running evaluation *****
2023-03-27 16:55:33,865   Epoch = 6 iter 1849 step
2023-03-27 16:55:33,865   Num examples = 1043
2023-03-27 16:55:33,866   Batch size = 32
2023-03-27 16:55:33,866 ***** Eval results *****
2023-03-27 16:55:33,867   att_loss = 0.3837572080403687
2023-03-27 16:55:33,867   cls_loss = 0.0
2023-03-27 16:55:33,867   global_step = 1849
2023-03-27 16:55:33,867   loss = 0.995730897916956
2023-03-27 16:55:33,868   rep_loss = 0.6119736879460724
2023-03-27 16:55:33,870 ***** Save model *****
2023-03-27 16:55:45,174 ***** Running evaluation *****
2023-03-27 16:55:45,174   Epoch = 7 iter 1899 step
2023-03-27 16:55:45,174   Num examples = 1043
2023-03-27 16:55:45,174   Batch size = 32
2023-03-27 16:55:45,175 ***** Eval results *****
2023-03-27 16:55:45,176   att_loss = 0.3680061727762222
2023-03-27 16:55:45,176   cls_loss = 0.0
2023-03-27 16:55:45,176   global_step = 1899
2023-03-27 16:55:45,176   loss = 0.9695807774861653
2023-03-27 16:55:45,176   rep_loss = 0.6015746057033539
2023-03-27 16:55:45,183 ***** Save model *****
2023-03-27 16:55:56,521 ***** Running evaluation *****
2023-03-27 16:55:56,521   Epoch = 7 iter 1949 step
2023-03-27 16:55:56,521   Num examples = 1043
2023-03-27 16:55:56,521   Batch size = 32
2023-03-27 16:55:56,523 ***** Eval results *****
2023-03-27 16:55:56,523   att_loss = 0.37605895958840846
2023-03-27 16:55:56,524   cls_loss = 0.0
2023-03-27 16:55:56,524   global_step = 1949
2023-03-27 16:55:56,524   loss = 0.9791359029710293
2023-03-27 16:55:56,524   rep_loss = 0.603076945245266
2023-03-27 16:55:56,531 ***** Save model *****
2023-03-27 16:56:07,848 ***** Running evaluation *****
2023-03-27 16:56:07,848   Epoch = 7 iter 1999 step
2023-03-27 16:56:07,848   Num examples = 1043
2023-03-27 16:56:07,848   Batch size = 32
2023-03-27 16:56:07,850 ***** Eval results *****
2023-03-27 16:56:07,850   att_loss = 0.3776113028709705
2023-03-27 16:56:07,850   cls_loss = 0.0
2023-03-27 16:56:07,851   global_step = 1999
2023-03-27 16:56:07,851   loss = 0.9810894507628221
2023-03-27 16:56:07,851   rep_loss = 0.6034781492673433
2023-03-27 16:56:07,858 ***** Save model *****
2023-03-27 16:56:19,180 ***** Running evaluation *****
2023-03-27 16:56:19,181   Epoch = 7 iter 2049 step
2023-03-27 16:56:19,181   Num examples = 1043
2023-03-27 16:56:19,181   Batch size = 32
2023-03-27 16:56:19,182 ***** Eval results *****
2023-03-27 16:56:19,182   att_loss = 0.37888545162147946
2023-03-27 16:56:19,182   cls_loss = 0.0
2023-03-27 16:56:19,182   global_step = 2049
2023-03-27 16:56:19,182   loss = 0.9825270805093977
2023-03-27 16:56:19,183   rep_loss = 0.6036416288879183
2023-03-27 16:56:19,190 ***** Save model *****
2023-03-27 16:56:30,482 ***** Running evaluation *****
2023-03-27 16:56:30,483   Epoch = 7 iter 2099 step
2023-03-27 16:56:30,483   Num examples = 1043
2023-03-27 16:56:30,483   Batch size = 32
2023-03-27 16:56:30,485 ***** Eval results *****
2023-03-27 16:56:30,485   att_loss = 0.37885430895763894
2023-03-27 16:56:30,485   cls_loss = 0.0
2023-03-27 16:56:30,486   global_step = 2099
2023-03-27 16:56:30,486   loss = 0.9826755585877792
2023-03-27 16:56:30,486   rep_loss = 0.6038212475569352
2023-03-27 16:56:30,493 ***** Save model *****
2023-03-27 16:56:41,813 ***** Running evaluation *****
2023-03-27 16:56:41,813   Epoch = 8 iter 2149 step
2023-03-27 16:56:41,813   Num examples = 1043
2023-03-27 16:56:41,813   Batch size = 32
2023-03-27 16:56:41,815 ***** Eval results *****
2023-03-27 16:56:41,815   att_loss = 0.37249561227284944
2023-03-27 16:56:41,815   cls_loss = 0.0
2023-03-27 16:56:41,815   global_step = 2149
2023-03-27 16:56:41,816   loss = 0.9767011495736929
2023-03-27 16:56:41,816   rep_loss = 0.6042055441783025
2023-03-27 16:56:41,823 ***** Save model *****
2023-03-27 16:56:53,091 ***** Running evaluation *****
2023-03-27 16:56:53,092   Epoch = 8 iter 2199 step
2023-03-27 16:56:53,092   Num examples = 1043
2023-03-27 16:56:53,092   Batch size = 32
2023-03-27 16:56:53,093 ***** Eval results *****
2023-03-27 16:56:53,093   att_loss = 0.3739604065342555
2023-03-27 16:56:53,093   cls_loss = 0.0
2023-03-27 16:56:53,093   global_step = 2199
2023-03-27 16:56:53,094   loss = 0.9735859604108901
2023-03-27 16:56:53,094   rep_loss = 0.5996255600263202
2023-03-27 16:56:53,101 ***** Save model *****
2023-03-27 16:57:04,383 ***** Running evaluation *****
2023-03-27 16:57:04,383   Epoch = 8 iter 2249 step
2023-03-27 16:57:04,383   Num examples = 1043
2023-03-27 16:57:04,383   Batch size = 32
2023-03-27 16:57:04,385 ***** Eval results *****
2023-03-27 16:57:04,385   att_loss = 0.37495181439197167
2023-03-27 16:57:04,386   cls_loss = 0.0
2023-03-27 16:57:04,386   global_step = 2249
2023-03-27 16:57:04,386   loss = 0.9739686002773521
2023-03-27 16:57:04,386   rep_loss = 0.5990167914238651
2023-03-27 16:57:04,393 ***** Save model *****
2023-03-27 16:57:15,673 ***** Running evaluation *****
2023-03-27 16:57:15,673   Epoch = 8 iter 2299 step
2023-03-27 16:57:15,674   Num examples = 1043
2023-03-27 16:57:15,674   Batch size = 32
2023-03-27 16:57:15,675 ***** Eval results *****
2023-03-27 16:57:15,675   att_loss = 0.37278810436008897
2023-03-27 16:57:15,675   cls_loss = 0.0
2023-03-27 16:57:15,676   global_step = 2299
2023-03-27 16:57:15,676   loss = 0.9706643439509386
2023-03-27 16:57:15,676   rep_loss = 0.5978762434304126
2023-03-27 16:57:15,684 ***** Save model *****
2023-03-27 16:57:26,964 ***** Running evaluation *****
2023-03-27 16:57:26,964   Epoch = 8 iter 2349 step
2023-03-27 16:57:26,964   Num examples = 1043
2023-03-27 16:57:26,965   Batch size = 32
2023-03-27 16:57:26,966 ***** Eval results *****
2023-03-27 16:57:26,966   att_loss = 0.37329492952342325
2023-03-27 16:57:26,966   cls_loss = 0.0
2023-03-27 16:57:26,966   global_step = 2349
2023-03-27 16:57:26,966   loss = 0.9703382127721545
2023-03-27 16:57:26,966   rep_loss = 0.5970432870264905
2023-03-27 16:57:26,973 ***** Save model *****
2023-03-27 16:57:38,228 ***** Running evaluation *****
2023-03-27 16:57:38,229   Epoch = 8 iter 2399 step
2023-03-27 16:57:38,229   Num examples = 1043
2023-03-27 16:57:38,229   Batch size = 32
2023-03-27 16:57:38,231 ***** Eval results *****
2023-03-27 16:57:38,231   att_loss = 0.3753601510941756
2023-03-27 16:57:38,231   cls_loss = 0.0
2023-03-27 16:57:38,232   global_step = 2399
2023-03-27 16:57:38,232   loss = 0.9723713835382642
2023-03-27 16:57:38,232   rep_loss = 0.5970112352770091
2023-03-27 16:57:38,239 ***** Save model *****
2023-03-27 16:57:49,512 ***** Running evaluation *****
2023-03-27 16:57:49,512   Epoch = 9 iter 2449 step
2023-03-27 16:57:49,513   Num examples = 1043
2023-03-27 16:57:49,513   Batch size = 32
2023-03-27 16:57:49,514 ***** Eval results *****
2023-03-27 16:57:49,514   att_loss = 0.37606995844322705
2023-03-27 16:57:49,514   cls_loss = 0.0
2023-03-27 16:57:49,515   global_step = 2449
2023-03-27 16:57:49,515   loss = 0.970309818568437
2023-03-27 16:57:49,515   rep_loss = 0.5942398607730865
2023-03-27 16:57:49,522 ***** Save model *****
2023-03-27 16:58:00,769 ***** Running evaluation *****
2023-03-27 16:58:00,769   Epoch = 9 iter 2499 step
2023-03-27 16:58:00,769   Num examples = 1043
2023-03-27 16:58:00,770   Batch size = 32
2023-03-27 16:58:00,770 ***** Eval results *****
2023-03-27 16:58:00,771   att_loss = 0.377351771419247
2023-03-27 16:58:00,771   cls_loss = 0.0
2023-03-27 16:58:00,771   global_step = 2499
2023-03-27 16:58:00,771   loss = 0.9706456822653612
2023-03-27 16:58:00,771   rep_loss = 0.5932939114669958
2023-03-27 16:58:00,777 ***** Save model *****
2023-03-27 16:58:11,994 ***** Running evaluation *****
2023-03-27 16:58:11,994   Epoch = 9 iter 2549 step
2023-03-27 16:58:11,994   Num examples = 1043
2023-03-27 16:58:11,994   Batch size = 32
2023-03-27 16:58:11,996 ***** Eval results *****
2023-03-27 16:58:11,996   att_loss = 0.37518155554386035
2023-03-27 16:58:11,996   cls_loss = 0.0
2023-03-27 16:58:11,997   global_step = 2549
2023-03-27 16:58:11,997   loss = 0.967600991872892
2023-03-27 16:58:11,997   rep_loss = 0.5924194369414081
2023-03-27 16:58:12,004 ***** Save model *****
2023-03-27 16:58:23,189 ***** Running evaluation *****
2023-03-27 16:58:23,190   Epoch = 9 iter 2599 step
2023-03-27 16:58:23,190   Num examples = 1043
2023-03-27 16:58:23,190   Batch size = 32
2023-03-27 16:58:23,192 ***** Eval results *****
2023-03-27 16:58:23,192   att_loss = 0.37535735827927685
2023-03-27 16:58:23,192   cls_loss = 0.0
2023-03-27 16:58:23,192   global_step = 2599
2023-03-27 16:58:23,193   loss = 0.9676433208645606
2023-03-27 16:58:23,193   rep_loss = 0.5922859615209152
2023-03-27 16:58:23,195 ***** Save model *****
2023-03-27 16:58:34,451 ***** Running evaluation *****
2023-03-27 16:58:34,451   Epoch = 9 iter 2649 step
2023-03-27 16:58:34,452   Num examples = 1043
2023-03-27 16:58:34,452   Batch size = 32
2023-03-27 16:58:34,453 ***** Eval results *****
2023-03-27 16:58:34,453   att_loss = 0.3735832095873065
2023-03-27 16:58:34,454   cls_loss = 0.0
2023-03-27 16:58:34,454   global_step = 2649
2023-03-27 16:58:34,454   loss = 0.9649801624984276
2023-03-27 16:58:34,454   rep_loss = 0.5913969506093157
2023-03-27 16:58:34,461 ***** Save model *****
2023-03-27 16:58:45,728 ***** Running evaluation *****
2023-03-27 16:58:45,729   Epoch = 10 iter 2699 step
2023-03-27 16:58:45,729   Num examples = 1043
2023-03-27 16:58:45,730   Batch size = 32
2023-03-27 16:58:45,731 ***** Eval results *****
2023-03-27 16:58:45,732   att_loss = 0.3778485018631508
2023-03-27 16:58:45,732   cls_loss = 0.0
2023-03-27 16:58:45,732   global_step = 2699
2023-03-27 16:58:45,732   loss = 0.9691038830526943
2023-03-27 16:58:45,733   rep_loss = 0.5912553709128807
2023-03-27 16:58:45,740 ***** Save model *****
2023-03-27 16:58:56,986 ***** Running evaluation *****
2023-03-27 16:58:56,986   Epoch = 10 iter 2749 step
2023-03-27 16:58:56,986   Num examples = 1043
2023-03-27 16:58:56,986   Batch size = 32
2023-03-27 16:58:56,987 ***** Eval results *****
2023-03-27 16:58:56,988   att_loss = 0.3741782605648041
2023-03-27 16:58:56,988   cls_loss = 0.0
2023-03-27 16:58:56,988   global_step = 2749
2023-03-27 16:58:56,988   loss = 0.9635008875327774
2023-03-27 16:58:56,988   rep_loss = 0.5893226243272612
2023-03-27 16:58:56,995 ***** Save model *****
2023-03-27 16:59:08,231 ***** Running evaluation *****
2023-03-27 16:59:08,232   Epoch = 10 iter 2799 step
2023-03-27 16:59:08,232   Num examples = 1043
2023-03-27 16:59:08,232   Batch size = 32
2023-03-27 16:59:08,233 ***** Eval results *****
2023-03-27 16:59:08,233   att_loss = 0.37086535816968874
2023-03-27 16:59:08,233   cls_loss = 0.0
2023-03-27 16:59:08,234   global_step = 2799
2023-03-27 16:59:08,234   loss = 0.9589860162069631
2023-03-27 16:59:08,234   rep_loss = 0.5881206568821457
2023-03-27 16:59:08,241 ***** Save model *****
2023-03-27 16:59:19,502 ***** Running evaluation *****
2023-03-27 16:59:19,503   Epoch = 10 iter 2849 step
2023-03-27 16:59:19,503   Num examples = 1043
2023-03-27 16:59:19,503   Batch size = 32
2023-03-27 16:59:19,504 ***** Eval results *****
2023-03-27 16:59:19,505   att_loss = 0.37269004746522316
2023-03-27 16:59:19,505   cls_loss = 0.0
2023-03-27 16:59:19,505   global_step = 2849
2023-03-27 16:59:19,505   loss = 0.9606597107215966
2023-03-27 16:59:19,506   rep_loss = 0.5879696627568932
2023-03-27 16:59:19,513 ***** Save model *****
2023-03-27 16:59:30,755 ***** Running evaluation *****
2023-03-27 16:59:30,755   Epoch = 10 iter 2899 step
2023-03-27 16:59:30,756   Num examples = 1043
2023-03-27 16:59:30,756   Batch size = 32
2023-03-27 16:59:30,757 ***** Eval results *****
2023-03-27 16:59:30,757   att_loss = 0.3725128065810974
2023-03-27 16:59:30,757   cls_loss = 0.0
2023-03-27 16:59:30,758   global_step = 2899
2023-03-27 16:59:30,758   loss = 0.9593727932226189
2023-03-27 16:59:30,758   rep_loss = 0.5868599872922272
2023-03-27 16:59:30,765 ***** Save model *****
2023-03-27 16:59:42,009 ***** Running evaluation *****
2023-03-27 16:59:42,009   Epoch = 11 iter 2949 step
2023-03-27 16:59:42,009   Num examples = 1043
2023-03-27 16:59:42,010   Batch size = 32
2023-03-27 16:59:42,011 ***** Eval results *****
2023-03-27 16:59:42,011   att_loss = 0.37356120596329373
2023-03-27 16:59:42,011   cls_loss = 0.0
2023-03-27 16:59:42,011   global_step = 2949
2023-03-27 16:59:42,012   loss = 0.9578048288822174
2023-03-27 16:59:42,012   rep_loss = 0.5842436204353968
2023-03-27 16:59:42,019 ***** Save model *****
2023-03-27 16:59:53,310 ***** Running evaluation *****
2023-03-27 16:59:53,310   Epoch = 11 iter 2999 step
2023-03-27 16:59:53,310   Num examples = 1043
2023-03-27 16:59:53,310   Batch size = 32
2023-03-27 16:59:53,312 ***** Eval results *****
2023-03-27 16:59:53,313   att_loss = 0.3657850844244803
2023-03-27 16:59:53,313   cls_loss = 0.0
2023-03-27 16:59:53,313   global_step = 2999
2023-03-27 16:59:53,313   loss = 0.9461709030212895
2023-03-27 16:59:53,313   rep_loss = 0.5803858195581744
2023-03-27 16:59:53,321 ***** Save model *****
2023-03-27 17:00:04,585 ***** Running evaluation *****
2023-03-27 17:00:04,585   Epoch = 11 iter 3049 step
2023-03-27 17:00:04,586   Num examples = 1043
2023-03-27 17:00:04,586   Batch size = 32
2023-03-27 17:00:04,587 ***** Eval results *****
2023-03-27 17:00:04,587   att_loss = 0.3677858928484576
2023-03-27 17:00:04,587   cls_loss = 0.0
2023-03-27 17:00:04,588   global_step = 3049
2023-03-27 17:00:04,588   loss = 0.9492979012429714
2023-03-27 17:00:04,588   rep_loss = 0.5815120099910668
2023-03-27 17:00:04,594 ***** Save model *****
2023-03-27 17:00:15,872 ***** Running evaluation *****
2023-03-27 17:00:15,872   Epoch = 11 iter 3099 step
2023-03-27 17:00:15,872   Num examples = 1043
2023-03-27 17:00:15,872   Batch size = 32
2023-03-27 17:00:15,873 ***** Eval results *****
2023-03-27 17:00:15,874   att_loss = 0.3670987071078501
2023-03-27 17:00:15,874   cls_loss = 0.0
2023-03-27 17:00:15,874   global_step = 3099
2023-03-27 17:00:15,874   loss = 0.9486219058802099
2023-03-27 17:00:15,875   rep_loss = 0.5815231987723598
2023-03-27 17:00:15,877 ***** Save model *****
2023-03-27 17:00:27,120 ***** Running evaluation *****
2023-03-27 17:00:27,121   Epoch = 11 iter 3149 step
2023-03-27 17:00:27,121   Num examples = 1043
2023-03-27 17:00:27,121   Batch size = 32
2023-03-27 17:00:27,122 ***** Eval results *****
2023-03-27 17:00:27,122   att_loss = 0.3682414715020162
2023-03-27 17:00:27,123   cls_loss = 0.0
2023-03-27 17:00:27,123   global_step = 3149
2023-03-27 17:00:27,123   loss = 0.9490287838117132
2023-03-27 17:00:27,123   rep_loss = 0.5807873125908509
2023-03-27 17:00:27,130 ***** Save model *****
2023-03-27 17:00:38,391 ***** Running evaluation *****
2023-03-27 17:00:38,391   Epoch = 11 iter 3199 step
2023-03-27 17:00:38,392   Num examples = 1043
2023-03-27 17:00:38,392   Batch size = 32
2023-03-27 17:00:38,393 ***** Eval results *****
2023-03-27 17:00:38,394   att_loss = 0.3688863188255834
2023-03-27 17:00:38,394   cls_loss = 0.0
2023-03-27 17:00:38,394   global_step = 3199
2023-03-27 17:00:38,394   loss = 0.9498974917499163
2023-03-27 17:00:38,395   rep_loss = 0.5810111726968343
2023-03-27 17:00:38,402 ***** Save model *****
2023-03-27 17:00:49,676 ***** Running evaluation *****
2023-03-27 17:00:49,677   Epoch = 12 iter 3249 step
2023-03-27 17:00:49,677   Num examples = 1043
2023-03-27 17:00:49,677   Batch size = 32
2023-03-27 17:00:49,678 ***** Eval results *****
2023-03-27 17:00:49,678   att_loss = 0.36401362816492716
2023-03-27 17:00:49,679   cls_loss = 0.0
2023-03-27 17:00:49,679   global_step = 3249
2023-03-27 17:00:49,679   loss = 0.9390530043178135
2023-03-27 17:00:49,679   rep_loss = 0.5750393761528863
2023-03-27 17:00:49,686 ***** Save model *****
2023-03-27 17:01:00,963 ***** Running evaluation *****
2023-03-27 17:01:00,963   Epoch = 12 iter 3299 step
2023-03-27 17:01:00,963   Num examples = 1043
2023-03-27 17:01:00,963   Batch size = 32
2023-03-27 17:01:00,965 ***** Eval results *****
2023-03-27 17:01:00,965   att_loss = 0.36827822735435084
2023-03-27 17:01:00,966   cls_loss = 0.0
2023-03-27 17:01:00,966   global_step = 3299
2023-03-27 17:01:00,966   loss = 0.9449168286825481
2023-03-27 17:01:00,966   rep_loss = 0.5766386032104492
2023-03-27 17:01:00,974 ***** Save model *****
2023-03-27 17:01:12,197 ***** Running evaluation *****
2023-03-27 17:01:12,198   Epoch = 12 iter 3349 step
2023-03-27 17:01:12,198   Num examples = 1043
2023-03-27 17:01:12,198   Batch size = 32
2023-03-27 17:01:12,199 ***** Eval results *****
2023-03-27 17:01:12,199   att_loss = 0.3675343817677991
2023-03-27 17:01:12,200   cls_loss = 0.0
2023-03-27 17:01:12,200   global_step = 3349
2023-03-27 17:01:12,200   loss = 0.9441294793424935
2023-03-27 17:01:12,200   rep_loss = 0.57659510045216
2023-03-27 17:01:12,208 ***** Save model *****
2023-03-27 17:01:23,472 ***** Running evaluation *****
2023-03-27 17:01:23,472   Epoch = 12 iter 3399 step
2023-03-27 17:01:23,473   Num examples = 1043
2023-03-27 17:01:23,473   Batch size = 32
2023-03-27 17:01:23,474 ***** Eval results *****
2023-03-27 17:01:23,474   att_loss = 0.3665121289399954
2023-03-27 17:01:23,474   cls_loss = 0.0
2023-03-27 17:01:23,474   global_step = 3399
2023-03-27 17:01:23,474   loss = 0.9435237578856639
2023-03-27 17:01:23,474   rep_loss = 0.5770116310853225
2023-03-27 17:01:23,482 ***** Save model *****
2023-03-27 17:01:34,729 ***** Running evaluation *****
2023-03-27 17:01:34,729   Epoch = 12 iter 3449 step
2023-03-27 17:01:34,730   Num examples = 1043
2023-03-27 17:01:34,730   Batch size = 32
2023-03-27 17:01:34,731 ***** Eval results *****
2023-03-27 17:01:34,732   att_loss = 0.36674884423917653
2023-03-27 17:01:34,732   cls_loss = 0.0
2023-03-27 17:01:34,732   global_step = 3449
2023-03-27 17:01:34,732   loss = 0.9436628682272775
2023-03-27 17:01:34,733   rep_loss = 0.5769140245963116
2023-03-27 17:01:34,740 ***** Save model *****
2023-03-27 17:01:46,020 ***** Running evaluation *****
2023-03-27 17:01:46,021   Epoch = 13 iter 3499 step
2023-03-27 17:01:46,021   Num examples = 1043
2023-03-27 17:01:46,021   Batch size = 32
2023-03-27 17:01:46,023 ***** Eval results *****
2023-03-27 17:01:46,023   att_loss = 0.3755521146314485
2023-03-27 17:01:46,023   cls_loss = 0.0
2023-03-27 17:01:46,023   global_step = 3499
2023-03-27 17:01:46,024   loss = 0.9527277094977242
2023-03-27 17:01:46,024   rep_loss = 0.5771755959306445
2023-03-27 17:01:46,031 ***** Save model *****
2023-03-27 17:01:57,289 ***** Running evaluation *****
2023-03-27 17:01:57,290   Epoch = 13 iter 3549 step
2023-03-27 17:01:57,290   Num examples = 1043
2023-03-27 17:01:57,290   Batch size = 32
2023-03-27 17:01:57,291 ***** Eval results *****
2023-03-27 17:01:57,291   att_loss = 0.3685899510597571
2023-03-27 17:01:57,291   cls_loss = 0.0
2023-03-27 17:01:57,292   global_step = 3549
2023-03-27 17:01:57,292   loss = 0.9436863699020483
2023-03-27 17:01:57,292   rep_loss = 0.5750964184602102
2023-03-27 17:01:57,300 ***** Save model *****
2023-03-27 17:02:08,534 ***** Running evaluation *****
2023-03-27 17:02:08,535   Epoch = 13 iter 3599 step
2023-03-27 17:02:08,535   Num examples = 1043
2023-03-27 17:02:08,535   Batch size = 32
2023-03-27 17:02:08,537 ***** Eval results *****
2023-03-27 17:02:08,537   att_loss = 0.36805658671073616
2023-03-27 17:02:08,537   cls_loss = 0.0
2023-03-27 17:02:08,537   global_step = 3599
2023-03-27 17:02:08,537   loss = 0.9426065948791802
2023-03-27 17:02:08,538   rep_loss = 0.5745500093325973
2023-03-27 17:02:08,545 ***** Save model *****
2023-03-27 17:02:19,816 ***** Running evaluation *****
2023-03-27 17:02:19,816   Epoch = 13 iter 3649 step
2023-03-27 17:02:19,816   Num examples = 1043
2023-03-27 17:02:19,816   Batch size = 32
2023-03-27 17:02:19,817 ***** Eval results *****
2023-03-27 17:02:19,818   att_loss = 0.36668392245689135
2023-03-27 17:02:19,818   cls_loss = 0.0
2023-03-27 17:02:19,818   global_step = 3649
2023-03-27 17:02:19,818   loss = 0.9407992028118519
2023-03-27 17:02:19,818   rep_loss = 0.5741152813595333
2023-03-27 17:02:19,826 ***** Save model *****
2023-03-27 17:02:31,095 ***** Running evaluation *****
2023-03-27 17:02:31,096   Epoch = 13 iter 3699 step
2023-03-27 17:02:31,096   Num examples = 1043
2023-03-27 17:02:31,096   Batch size = 32
2023-03-27 17:02:31,097 ***** Eval results *****
2023-03-27 17:02:31,097   att_loss = 0.36648582902393845
2023-03-27 17:02:31,098   cls_loss = 0.0
2023-03-27 17:02:31,098   global_step = 3699
2023-03-27 17:02:31,098   loss = 0.9406257520119349
2023-03-27 17:02:31,098   rep_loss = 0.5741399236415562
2023-03-27 17:02:31,105 ***** Save model *****
2023-03-27 17:02:42,362 ***** Running evaluation *****
2023-03-27 17:02:42,363   Epoch = 14 iter 3749 step
2023-03-27 17:02:42,363   Num examples = 1043
2023-03-27 17:02:42,363   Batch size = 32
2023-03-27 17:02:42,364 ***** Eval results *****
2023-03-27 17:02:42,364   att_loss = 0.3633784218267961
2023-03-27 17:02:42,364   cls_loss = 0.0
2023-03-27 17:02:42,365   global_step = 3749
2023-03-27 17:02:42,365   loss = 0.9308598854325034
2023-03-27 17:02:42,365   rep_loss = 0.5674814527684992
2023-03-27 17:02:42,372 ***** Save model *****
2023-03-27 17:02:53,606 ***** Running evaluation *****
2023-03-27 17:02:53,607   Epoch = 14 iter 3799 step
2023-03-27 17:02:53,607   Num examples = 1043
2023-03-27 17:02:53,607   Batch size = 32
2023-03-27 17:02:53,609 ***** Eval results *****
2023-03-27 17:02:53,609   att_loss = 0.361314805804706
2023-03-27 17:02:53,609   cls_loss = 0.0
2023-03-27 17:02:53,609   global_step = 3799
2023-03-27 17:02:53,610   loss = 0.9305757938838396
2023-03-27 17:02:53,610   rep_loss = 0.569260986124883
2023-03-27 17:02:53,617 ***** Save model *****
2023-03-27 17:03:04,829 ***** Running evaluation *****
2023-03-27 17:03:04,830   Epoch = 14 iter 3849 step
2023-03-27 17:03:04,830   Num examples = 1043
2023-03-27 17:03:04,830   Batch size = 32
2023-03-27 17:03:04,831 ***** Eval results *****
2023-03-27 17:03:04,831   att_loss = 0.35830378935143753
2023-03-27 17:03:04,832   cls_loss = 0.0
2023-03-27 17:03:04,832   global_step = 3849
2023-03-27 17:03:04,832   loss = 0.9264817753353635
2023-03-27 17:03:04,832   rep_loss = 0.5681779841045002
2023-03-27 17:03:04,840 ***** Save model *****
2023-03-27 17:03:16,079 ***** Running evaluation *****
2023-03-27 17:03:16,080   Epoch = 14 iter 3899 step
2023-03-27 17:03:16,080   Num examples = 1043
2023-03-27 17:03:16,080   Batch size = 32
2023-03-27 17:03:16,082 ***** Eval results *****
2023-03-27 17:03:16,082   att_loss = 0.3631305705686534
2023-03-27 17:03:16,082   cls_loss = 0.0
2023-03-27 17:03:16,082   global_step = 3899
2023-03-27 17:03:16,083   loss = 0.9324258373390814
2023-03-27 17:03:16,083   rep_loss = 0.5692952652895673
2023-03-27 17:03:16,090 ***** Save model *****
2023-03-27 17:03:27,375 ***** Running evaluation *****
2023-03-27 17:03:27,375   Epoch = 14 iter 3949 step
2023-03-27 17:03:27,376   Num examples = 1043
2023-03-27 17:03:27,376   Batch size = 32
2023-03-27 17:03:27,377 ***** Eval results *****
2023-03-27 17:03:27,377   att_loss = 0.3654983689152234
2023-03-27 17:03:27,377   cls_loss = 0.0
2023-03-27 17:03:27,377   global_step = 3949
2023-03-27 17:03:27,378   loss = 0.936089210883136
2023-03-27 17:03:27,378   rep_loss = 0.57059084097921
2023-03-27 17:03:27,385 ***** Save model *****
2023-03-27 17:03:38,635 ***** Running evaluation *****
2023-03-27 17:03:38,635   Epoch = 14 iter 3999 step
2023-03-27 17:03:38,635   Num examples = 1043
2023-03-27 17:03:38,635   Batch size = 32
2023-03-27 17:03:38,636 ***** Eval results *****
2023-03-27 17:03:38,637   att_loss = 0.3648476863272802
2023-03-27 17:03:38,637   cls_loss = 0.0
2023-03-27 17:03:38,637   global_step = 3999
2023-03-27 17:03:38,637   loss = 0.934935813205909
2023-03-27 17:03:38,637   rep_loss = 0.5700881257367774
2023-03-27 17:03:38,645 ***** Save model *****
2023-03-27 17:03:49,894 ***** Running evaluation *****
2023-03-27 17:03:49,894   Epoch = 15 iter 4049 step
2023-03-27 17:03:49,895   Num examples = 1043
2023-03-27 17:03:49,895   Batch size = 32
2023-03-27 17:03:49,896 ***** Eval results *****
2023-03-27 17:03:49,897   att_loss = 0.35664178363301535
2023-03-27 17:03:49,897   cls_loss = 0.0
2023-03-27 17:03:49,897   global_step = 4049
2023-03-27 17:03:49,897   loss = 0.9225755794481798
2023-03-27 17:03:49,897   rep_loss = 0.5659337964924899
2023-03-27 17:03:49,905 ***** Save model *****
2023-03-27 17:04:01,190 ***** Running evaluation *****
2023-03-27 17:04:01,190   Epoch = 15 iter 4099 step
2023-03-27 17:04:01,191   Num examples = 1043
2023-03-27 17:04:01,191   Batch size = 32
2023-03-27 17:04:01,192 ***** Eval results *****
2023-03-27 17:04:01,192   att_loss = 0.35876276994005163
2023-03-27 17:04:01,192   cls_loss = 0.0
2023-03-27 17:04:01,192   global_step = 4099
2023-03-27 17:04:01,192   loss = 0.9252404944693788
2023-03-27 17:04:01,192   rep_loss = 0.5664777248463733
2023-03-27 17:04:01,195 ***** Save model *****
2023-03-27 17:04:12,414 ***** Running evaluation *****
2023-03-27 17:04:12,415   Epoch = 15 iter 4149 step
2023-03-27 17:04:12,415   Num examples = 1043
2023-03-27 17:04:12,415   Batch size = 32
2023-03-27 17:04:12,416 ***** Eval results *****
2023-03-27 17:04:12,416   att_loss = 0.3592638565848271
2023-03-27 17:04:12,416   cls_loss = 0.0
2023-03-27 17:04:12,417   global_step = 4149
2023-03-27 17:04:12,417   loss = 0.9258616541822752
2023-03-27 17:04:12,417   rep_loss = 0.5665977973904874
2023-03-27 17:04:12,424 ***** Save model *****
2023-03-27 17:04:23,690 ***** Running evaluation *****
2023-03-27 17:04:23,691   Epoch = 15 iter 4199 step
2023-03-27 17:04:23,691   Num examples = 1043
2023-03-27 17:04:23,691   Batch size = 32
2023-03-27 17:04:23,693 ***** Eval results *****
2023-03-27 17:04:23,693   att_loss = 0.3603966518775704
2023-03-27 17:04:23,694   cls_loss = 0.0
2023-03-27 17:04:23,694   global_step = 4199
2023-03-27 17:04:23,694   loss = 0.927643975338985
2023-03-27 17:04:23,694   rep_loss = 0.5672473225396933
2023-03-27 17:04:23,701 ***** Save model *****
2023-03-27 17:04:34,956 ***** Running evaluation *****
2023-03-27 17:04:34,956   Epoch = 15 iter 4249 step
2023-03-27 17:04:34,956   Num examples = 1043
2023-03-27 17:04:34,956   Batch size = 32
2023-03-27 17:04:34,957 ***** Eval results *****
2023-03-27 17:04:34,957   att_loss = 0.36146229884175
2023-03-27 17:04:34,958   cls_loss = 0.0
2023-03-27 17:04:34,958   global_step = 4249
2023-03-27 17:04:34,958   loss = 0.9286112028067229
2023-03-27 17:04:34,958   rep_loss = 0.5671489033542696
2023-03-27 17:04:34,965 ***** Save model *****
2023-03-27 17:04:46,243 ***** Running evaluation *****
2023-03-27 17:04:46,243   Epoch = 16 iter 4299 step
2023-03-27 17:04:46,244   Num examples = 1043
2023-03-27 17:04:46,244   Batch size = 32
2023-03-27 17:04:46,245 ***** Eval results *****
2023-03-27 17:04:46,245   att_loss = 0.35486351008768435
2023-03-27 17:04:46,245   cls_loss = 0.0
2023-03-27 17:04:46,245   global_step = 4299
2023-03-27 17:04:46,245   loss = 0.9176156587070889
2023-03-27 17:04:46,245   rep_loss = 0.5627521497231943
2023-03-27 17:04:46,253 ***** Save model *****
2023-03-27 17:04:57,511 ***** Running evaluation *****
2023-03-27 17:04:57,514   Epoch = 16 iter 4349 step
2023-03-27 17:04:57,514   Num examples = 1043
2023-03-27 17:04:57,515   Batch size = 32
2023-03-27 17:04:57,516 ***** Eval results *****
2023-03-27 17:04:57,516   att_loss = 0.35528324718599197
2023-03-27 17:04:57,517   cls_loss = 0.0
2023-03-27 17:04:57,517   global_step = 4349
2023-03-27 17:04:57,517   loss = 0.9197121874078528
2023-03-27 17:04:57,517   rep_loss = 0.5644289394477745
2023-03-27 17:04:57,525 ***** Save model *****
2023-03-27 17:05:08,786 ***** Running evaluation *****
2023-03-27 17:05:08,786   Epoch = 16 iter 4399 step
2023-03-27 17:05:08,786   Num examples = 1043
2023-03-27 17:05:08,786   Batch size = 32
2023-03-27 17:05:08,788 ***** Eval results *****
2023-03-27 17:05:08,788   att_loss = 0.3597015470970334
2023-03-27 17:05:08,788   cls_loss = 0.0
2023-03-27 17:05:08,789   global_step = 4399
2023-03-27 17:05:08,789   loss = 0.9241072460422366
2023-03-27 17:05:08,789   rep_loss = 0.5644056970678916
2023-03-27 17:05:08,796 ***** Save model *****
2023-03-27 17:05:20,040 ***** Running evaluation *****
2023-03-27 17:05:20,041   Epoch = 16 iter 4449 step
2023-03-27 17:05:20,041   Num examples = 1043
2023-03-27 17:05:20,041   Batch size = 32
2023-03-27 17:05:20,042 ***** Eval results *****
2023-03-27 17:05:20,042   att_loss = 0.36183081255794247
2023-03-27 17:05:20,042   cls_loss = 0.0
2023-03-27 17:05:20,042   global_step = 4449
2023-03-27 17:05:20,042   loss = 0.9269797970346139
2023-03-27 17:05:20,043   rep_loss = 0.5651489826245496
2023-03-27 17:05:20,050 ***** Save model *****
2023-03-27 17:05:31,321 ***** Running evaluation *****
2023-03-27 17:05:31,321   Epoch = 16 iter 4499 step
2023-03-27 17:05:31,321   Num examples = 1043
2023-03-27 17:05:31,321   Batch size = 32
2023-03-27 17:05:31,323 ***** Eval results *****
2023-03-27 17:05:31,323   att_loss = 0.3611216550356491
2023-03-27 17:05:31,323   cls_loss = 0.0
2023-03-27 17:05:31,324   global_step = 4499
2023-03-27 17:05:31,324   loss = 0.9258502708657723
2023-03-27 17:05:31,324   rep_loss = 0.5647286139920945
2023-03-27 17:05:31,331 ***** Save model *****
2023-03-27 17:05:42,588 ***** Running evaluation *****
2023-03-27 17:05:42,588   Epoch = 17 iter 4549 step
2023-03-27 17:05:42,589   Num examples = 1043
2023-03-27 17:05:42,589   Batch size = 32
2023-03-27 17:05:42,590 ***** Eval results *****
2023-03-27 17:05:42,590   att_loss = 0.3545015811920166
2023-03-27 17:05:42,590   cls_loss = 0.0
2023-03-27 17:05:42,590   global_step = 4549
2023-03-27 17:05:42,590   loss = 0.9135738670825958
2023-03-27 17:05:42,591   rep_loss = 0.5590722799301148
2023-03-27 17:05:42,598 ***** Save model *****
2023-03-27 17:05:53,815 ***** Running evaluation *****
2023-03-27 17:05:53,815   Epoch = 17 iter 4599 step
2023-03-27 17:05:53,815   Num examples = 1043
2023-03-27 17:05:53,815   Batch size = 32
2023-03-27 17:05:53,817 ***** Eval results *****
2023-03-27 17:05:53,817   att_loss = 0.3561781495809555
2023-03-27 17:05:53,817   cls_loss = 0.0
2023-03-27 17:05:53,817   global_step = 4599
2023-03-27 17:05:53,818   loss = 0.9163094222545624
2023-03-27 17:05:53,818   rep_loss = 0.5601312726736069
2023-03-27 17:05:53,825 ***** Save model *****
2023-03-27 17:06:05,141 ***** Running evaluation *****
2023-03-27 17:06:05,142   Epoch = 17 iter 4649 step
2023-03-27 17:06:05,142   Num examples = 1043
2023-03-27 17:06:05,142   Batch size = 32
2023-03-27 17:06:05,144 ***** Eval results *****
2023-03-27 17:06:05,144   att_loss = 0.3577201648191972
2023-03-27 17:06:05,144   cls_loss = 0.0
2023-03-27 17:06:05,144   global_step = 4649
2023-03-27 17:06:05,144   loss = 0.9184086160226301
2023-03-27 17:06:05,145   rep_loss = 0.5606884517452934
2023-03-27 17:06:05,152 ***** Save model *****
2023-03-27 17:06:16,389 ***** Running evaluation *****
2023-03-27 17:06:16,389   Epoch = 17 iter 4699 step
2023-03-27 17:06:16,389   Num examples = 1043
2023-03-27 17:06:16,389   Batch size = 32
2023-03-27 17:06:16,391 ***** Eval results *****
2023-03-27 17:06:16,391   att_loss = 0.35805344898253677
2023-03-27 17:06:16,392   cls_loss = 0.0
2023-03-27 17:06:16,392   global_step = 4699
2023-03-27 17:06:16,392   loss = 0.9187954448163509
2023-03-27 17:06:16,392   rep_loss = 0.5607419952750206
2023-03-27 17:06:16,395 ***** Save model *****
2023-03-27 17:06:27,632 ***** Running evaluation *****
2023-03-27 17:06:27,633   Epoch = 17 iter 4749 step
2023-03-27 17:06:27,633   Num examples = 1043
2023-03-27 17:06:27,633   Batch size = 32
2023-03-27 17:06:27,634 ***** Eval results *****
2023-03-27 17:06:27,634   att_loss = 0.3596189423685982
2023-03-27 17:06:27,635   cls_loss = 0.0
2023-03-27 17:06:27,635   global_step = 4749
2023-03-27 17:06:27,635   loss = 0.9208216635953812
2023-03-27 17:06:27,635   rep_loss = 0.5612027210848672
2023-03-27 17:06:27,642 ***** Save model *****
2023-03-27 17:06:38,919 ***** Running evaluation *****
2023-03-27 17:06:38,919   Epoch = 17 iter 4799 step
2023-03-27 17:06:38,920   Num examples = 1043
2023-03-27 17:06:38,920   Batch size = 32
2023-03-27 17:06:38,922 ***** Eval results *****
2023-03-27 17:06:38,922   att_loss = 0.35991380902437065
2023-03-27 17:06:38,922   cls_loss = 0.0
2023-03-27 17:06:38,922   global_step = 4799
2023-03-27 17:06:38,922   loss = 0.921135202050209
2023-03-27 17:06:38,922   rep_loss = 0.5612213925673412
2023-03-27 17:06:38,930 ***** Save model *****
2023-03-27 17:06:50,174 ***** Running evaluation *****
2023-03-27 17:06:50,174   Epoch = 18 iter 4849 step
2023-03-27 17:06:50,174   Num examples = 1043
2023-03-27 17:06:50,174   Batch size = 32
2023-03-27 17:06:50,175 ***** Eval results *****
2023-03-27 17:06:50,176   att_loss = 0.3606119744999464
2023-03-27 17:06:50,176   cls_loss = 0.0
2023-03-27 17:06:50,176   global_step = 4849
2023-03-27 17:06:50,176   loss = 0.9198890952176826
2023-03-27 17:06:50,176   rep_loss = 0.5592771158661953
2023-03-27 17:06:50,184 ***** Save model *****
2023-03-27 17:07:01,432 ***** Running evaluation *****
2023-03-27 17:07:01,432   Epoch = 18 iter 4899 step
2023-03-27 17:07:01,432   Num examples = 1043
2023-03-27 17:07:01,432   Batch size = 32
2023-03-27 17:07:01,434 ***** Eval results *****
2023-03-27 17:07:01,434   att_loss = 0.3603455571718113
2023-03-27 17:07:01,434   cls_loss = 0.0
2023-03-27 17:07:01,434   global_step = 4899
2023-03-27 17:07:01,434   loss = 0.9191303362128556
2023-03-27 17:07:01,435   rep_loss = 0.5587847777592239
2023-03-27 17:07:01,442 ***** Save model *****
2023-03-27 17:07:12,707 ***** Running evaluation *****
2023-03-27 17:07:12,707   Epoch = 18 iter 4949 step
2023-03-27 17:07:12,707   Num examples = 1043
2023-03-27 17:07:12,707   Batch size = 32
2023-03-27 17:07:12,708 ***** Eval results *****
2023-03-27 17:07:12,709   att_loss = 0.3577866810601908
2023-03-27 17:07:12,709   cls_loss = 0.0
2023-03-27 17:07:12,709   global_step = 4949
2023-03-27 17:07:12,709   loss = 0.9164324377800201
2023-03-27 17:07:12,709   rep_loss = 0.5586457565114215
2023-03-27 17:07:12,716 ***** Save model *****
2023-03-27 17:07:23,994 ***** Running evaluation *****
2023-03-27 17:07:23,994   Epoch = 18 iter 4999 step
2023-03-27 17:07:23,995   Num examples = 1043
2023-03-27 17:07:23,995   Batch size = 32
2023-03-27 17:07:23,996 ***** Eval results *****
2023-03-27 17:07:23,997   att_loss = 0.3584821424953678
2023-03-27 17:07:23,997   cls_loss = 0.0
2023-03-27 17:07:23,997   global_step = 4999
2023-03-27 17:07:23,997   loss = 0.91769359179729
2023-03-27 17:07:23,997   rep_loss = 0.5592114499195869
2023-03-27 17:07:24,005 ***** Save model *****
2023-03-27 17:07:35,248 ***** Running evaluation *****
2023-03-27 17:07:35,249   Epoch = 18 iter 5049 step
2023-03-27 17:07:35,249   Num examples = 1043
2023-03-27 17:07:35,249   Batch size = 32
2023-03-27 17:07:35,250 ***** Eval results *****
2023-03-27 17:07:35,250   att_loss = 0.35883482297261554
2023-03-27 17:07:35,251   cls_loss = 0.0
2023-03-27 17:07:35,251   global_step = 5049
2023-03-27 17:07:35,251   loss = 0.918224887838089
2023-03-27 17:07:35,251   rep_loss = 0.55939006511076
2023-03-27 17:07:35,258 ***** Save model *****
2023-03-27 17:07:46,494 ***** Running evaluation *****
2023-03-27 17:07:46,494   Epoch = 19 iter 5099 step
2023-03-27 17:07:46,495   Num examples = 1043
2023-03-27 17:07:46,495   Batch size = 32
2023-03-27 17:07:46,497 ***** Eval results *****
2023-03-27 17:07:46,497   att_loss = 0.3549548742862848
2023-03-27 17:07:46,497   cls_loss = 0.0
2023-03-27 17:07:46,497   global_step = 5099
2023-03-27 17:07:46,497   loss = 0.9101639137818263
2023-03-27 17:07:46,498   rep_loss = 0.555209036056812
2023-03-27 17:07:46,500 ***** Save model *****
2023-03-27 17:07:57,758 ***** Running evaluation *****
2023-03-27 17:07:57,758   Epoch = 19 iter 5149 step
2023-03-27 17:07:57,758   Num examples = 1043
2023-03-27 17:07:57,758   Batch size = 32
2023-03-27 17:07:57,759 ***** Eval results *****
2023-03-27 17:07:57,759   att_loss = 0.3505826604209448
2023-03-27 17:07:57,760   cls_loss = 0.0
2023-03-27 17:07:57,760   global_step = 5149
2023-03-27 17:07:57,760   loss = 0.9039634732823623
2023-03-27 17:07:57,760   rep_loss = 0.5533808140378249
2023-03-27 17:07:57,767 ***** Save model *****
2023-03-27 17:08:09,009 ***** Running evaluation *****
2023-03-27 17:08:09,010   Epoch = 19 iter 5199 step
2023-03-27 17:08:09,010   Num examples = 1043
2023-03-27 17:08:09,010   Batch size = 32
2023-03-27 17:08:09,012 ***** Eval results *****
2023-03-27 17:08:09,012   att_loss = 0.3531617914873456
2023-03-27 17:08:09,012   cls_loss = 0.0
2023-03-27 17:08:09,013   global_step = 5199
2023-03-27 17:08:09,013   loss = 0.9071955879529318
2023-03-27 17:08:09,013   rep_loss = 0.5540337974116916
2023-03-27 17:08:09,020 ***** Save model *****
2023-03-27 17:08:20,287 ***** Running evaluation *****
2023-03-27 17:08:20,287   Epoch = 19 iter 5249 step
2023-03-27 17:08:20,287   Num examples = 1043
2023-03-27 17:08:20,287   Batch size = 32
2023-03-27 17:08:20,289 ***** Eval results *****
2023-03-27 17:08:20,289   att_loss = 0.35502941127527843
2023-03-27 17:08:20,289   cls_loss = 0.0
2023-03-27 17:08:20,290   global_step = 5249
2023-03-27 17:08:20,290   loss = 0.9098794375630942
2023-03-27 17:08:20,290   rep_loss = 0.554850025949153
2023-03-27 17:08:20,297 ***** Save model *****
2023-03-27 17:08:31,555 ***** Running evaluation *****
2023-03-27 17:08:31,555   Epoch = 19 iter 5299 step
2023-03-27 17:08:31,555   Num examples = 1043
2023-03-27 17:08:31,555   Batch size = 32
2023-03-27 17:08:31,557 ***** Eval results *****
2023-03-27 17:08:31,557   att_loss = 0.3566045816493245
2023-03-27 17:08:31,557   cls_loss = 0.0
2023-03-27 17:08:31,557   global_step = 5299
2023-03-27 17:08:31,557   loss = 0.912560475874791
2023-03-27 17:08:31,558   rep_loss = 0.5559558936979918
2023-03-27 17:08:31,565 ***** Save model *****
2023-03-27 17:08:42,767 ***** Running evaluation *****
2023-03-27 17:08:42,767   Epoch = 20 iter 5349 step
2023-03-27 17:08:42,767   Num examples = 1043
2023-03-27 17:08:42,767   Batch size = 32
2023-03-27 17:08:42,768 ***** Eval results *****
2023-03-27 17:08:42,769   att_loss = 0.35213108857472736
2023-03-27 17:08:42,769   cls_loss = 0.0
2023-03-27 17:08:42,769   global_step = 5349
2023-03-27 17:08:42,769   loss = 0.9054079982969496
2023-03-27 17:08:42,769   rep_loss = 0.5532769097222222
2023-03-27 17:08:42,777 ***** Save model *****
2023-03-27 17:08:54,035 ***** Running evaluation *****
2023-03-27 17:08:54,035   Epoch = 20 iter 5399 step
2023-03-27 17:08:54,035   Num examples = 1043
2023-03-27 17:08:54,035   Batch size = 32
2023-03-27 17:08:54,037 ***** Eval results *****
2023-03-27 17:08:54,037   att_loss = 0.35217027593467193
2023-03-27 17:08:54,037   cls_loss = 0.0
2023-03-27 17:08:54,037   global_step = 5399
2023-03-27 17:08:54,038   loss = 0.9058200282565618
2023-03-27 17:08:54,038   rep_loss = 0.5536497508065176
2023-03-27 17:08:54,045 ***** Save model *****
2023-03-27 17:09:05,294 ***** Running evaluation *****
2023-03-27 17:09:05,295   Epoch = 20 iter 5449 step
2023-03-27 17:09:05,295   Num examples = 1043
2023-03-27 17:09:05,295   Batch size = 32
2023-03-27 17:09:05,296 ***** Eval results *****
2023-03-27 17:09:05,297   att_loss = 0.3531615444279592
2023-03-27 17:09:05,297   cls_loss = 0.0
2023-03-27 17:09:05,297   global_step = 5449
2023-03-27 17:09:05,297   loss = 0.9075280766968333
2023-03-27 17:09:05,297   rep_loss = 0.5543665306283794
2023-03-27 17:09:05,300 ***** Save model *****
2023-03-27 17:09:16,547 ***** Running evaluation *****
2023-03-27 17:09:16,547   Epoch = 20 iter 5499 step
2023-03-27 17:09:16,547   Num examples = 1043
2023-03-27 17:09:16,547   Batch size = 32
2023-03-27 17:09:16,549 ***** Eval results *****
2023-03-27 17:09:16,549   att_loss = 0.35326949436709565
2023-03-27 17:09:16,549   cls_loss = 0.0
2023-03-27 17:09:16,549   global_step = 5499
2023-03-27 17:09:16,550   loss = 0.9076275461874668
2023-03-27 17:09:16,550   rep_loss = 0.5543580497585753
2023-03-27 17:09:16,557 ***** Save model *****
2023-03-27 17:09:27,806 ***** Running evaluation *****
2023-03-27 17:09:27,806   Epoch = 20 iter 5549 step
2023-03-27 17:09:27,806   Num examples = 1043
2023-03-27 17:09:27,806   Batch size = 32
2023-03-27 17:09:27,807 ***** Eval results *****
2023-03-27 17:09:27,807   att_loss = 0.3555484535306264
2023-03-27 17:09:27,808   cls_loss = 0.0
2023-03-27 17:09:27,808   global_step = 5549
2023-03-27 17:09:27,808   loss = 0.9104477138610548
2023-03-27 17:09:27,808   rep_loss = 0.5548992596174541
2023-03-27 17:09:27,811 ***** Save model *****
2023-03-27 17:09:39,045 ***** Running evaluation *****
2023-03-27 17:09:39,046   Epoch = 20 iter 5599 step
2023-03-27 17:09:39,046   Num examples = 1043
2023-03-27 17:09:39,046   Batch size = 32
2023-03-27 17:09:39,047 ***** Eval results *****
2023-03-27 17:09:39,047   att_loss = 0.355227511476826
2023-03-27 17:09:39,047   cls_loss = 0.0
2023-03-27 17:09:39,047   global_step = 5599
2023-03-27 17:09:39,047   loss = 0.9099252502430359
2023-03-27 17:09:39,048   rep_loss = 0.554697737730608
2023-03-27 17:09:39,055 ***** Save model *****
2023-03-27 17:09:50,332 ***** Running evaluation *****
2023-03-27 17:09:50,332   Epoch = 21 iter 5649 step
2023-03-27 17:09:50,333   Num examples = 1043
2023-03-27 17:09:50,333   Batch size = 32
2023-03-27 17:09:50,334 ***** Eval results *****
2023-03-27 17:09:50,334   att_loss = 0.3574288771266029
2023-03-27 17:09:50,334   cls_loss = 0.0
2023-03-27 17:09:50,335   global_step = 5649
2023-03-27 17:09:50,335   loss = 0.9101725660619282
2023-03-27 17:09:50,335   rep_loss = 0.5527436903544835
2023-03-27 17:09:50,338 ***** Save model *****
2023-03-27 17:10:01,581 ***** Running evaluation *****
2023-03-27 17:10:01,582   Epoch = 21 iter 5699 step
2023-03-27 17:10:01,582   Num examples = 1043
2023-03-27 17:10:01,583   Batch size = 32
2023-03-27 17:10:01,585 ***** Eval results *****
2023-03-27 17:10:01,585   att_loss = 0.3562379683489385
2023-03-27 17:10:01,585   cls_loss = 0.0
2023-03-27 17:10:01,586   global_step = 5699
2023-03-27 17:10:01,586   loss = 0.9104550454927527
2023-03-27 17:10:01,586   rep_loss = 0.5542170742283696
2023-03-27 17:10:01,594 ***** Save model *****
2023-03-27 17:10:12,837 ***** Running evaluation *****
2023-03-27 17:10:12,837   Epoch = 21 iter 5749 step
2023-03-27 17:10:12,837   Num examples = 1043
2023-03-27 17:10:12,837   Batch size = 32
2023-03-27 17:10:12,838 ***** Eval results *****
2023-03-27 17:10:12,839   att_loss = 0.3538544465538482
2023-03-27 17:10:12,839   cls_loss = 0.0
2023-03-27 17:10:12,839   global_step = 5749
2023-03-27 17:10:12,839   loss = 0.906833461892437
2023-03-27 17:10:12,840   rep_loss = 0.552979010931203
2023-03-27 17:10:12,847 ***** Save model *****
2023-03-27 17:10:24,144 ***** Running evaluation *****
2023-03-27 17:10:24,145   Epoch = 21 iter 5799 step
2023-03-27 17:10:24,145   Num examples = 1043
2023-03-27 17:10:24,145   Batch size = 32
2023-03-27 17:10:24,146 ***** Eval results *****
2023-03-27 17:10:24,146   att_loss = 0.35456382436677814
2023-03-27 17:10:24,147   cls_loss = 0.0
2023-03-27 17:10:24,147   global_step = 5799
2023-03-27 17:10:24,147   loss = 0.9079253058880568
2023-03-27 17:10:24,147   rep_loss = 0.5533614785720905
2023-03-27 17:10:24,155 ***** Save model *****
2023-03-27 17:10:35,441 ***** Running evaluation *****
2023-03-27 17:10:35,441   Epoch = 21 iter 5849 step
2023-03-27 17:10:35,441   Num examples = 1043
2023-03-27 17:10:35,441   Batch size = 32
2023-03-27 17:10:35,443 ***** Eval results *****
2023-03-27 17:10:35,443   att_loss = 0.35501208063984707
2023-03-27 17:10:35,443   cls_loss = 0.0
2023-03-27 17:10:35,444   global_step = 5849
2023-03-27 17:10:35,444   loss = 0.9084060982731749
2023-03-27 17:10:35,444   rep_loss = 0.5533940154166261
2023-03-27 17:10:35,451 ***** Save model *****
2023-03-27 17:10:46,734 ***** Running evaluation *****
2023-03-27 17:10:46,734   Epoch = 22 iter 5899 step
2023-03-27 17:10:46,734   Num examples = 1043
2023-03-27 17:10:46,734   Batch size = 32
2023-03-27 17:10:46,736 ***** Eval results *****
2023-03-27 17:10:46,736   att_loss = 0.34554816484451295
2023-03-27 17:10:46,737   cls_loss = 0.0
2023-03-27 17:10:46,737   global_step = 5899
2023-03-27 17:10:46,737   loss = 0.8960385489463806
2023-03-27 17:10:46,737   rep_loss = 0.5504903864860534
2023-03-27 17:10:46,745 ***** Save model *****
2023-03-27 17:10:58,011 ***** Running evaluation *****
2023-03-27 17:10:58,011   Epoch = 22 iter 5949 step
2023-03-27 17:10:58,011   Num examples = 1043
2023-03-27 17:10:58,011   Batch size = 32
2023-03-27 17:10:58,012 ***** Eval results *****
2023-03-27 17:10:58,013   att_loss = 0.35059054374694826
2023-03-27 17:10:58,013   cls_loss = 0.0
2023-03-27 17:10:58,013   global_step = 5949
2023-03-27 17:10:58,013   loss = 0.8990005882581075
2023-03-27 17:10:58,013   rep_loss = 0.5484100453058879
2023-03-27 17:10:58,020 ***** Save model *****
2023-03-27 17:11:09,297 ***** Running evaluation *****
2023-03-27 17:11:09,297   Epoch = 22 iter 5999 step
2023-03-27 17:11:09,298   Num examples = 1043
2023-03-27 17:11:09,298   Batch size = 32
2023-03-27 17:11:09,300 ***** Eval results *****
2023-03-27 17:11:09,300   att_loss = 0.35119980430603026
2023-03-27 17:11:09,300   cls_loss = 0.0
2023-03-27 17:11:09,300   global_step = 5999
2023-03-27 17:11:09,301   loss = 0.9007394161224366
2023-03-27 17:11:09,301   rep_loss = 0.5495396127700806
2023-03-27 17:11:09,304 ***** Save model *****
2023-03-27 17:11:20,557 ***** Running evaluation *****
2023-03-27 17:11:20,557   Epoch = 22 iter 6049 step
2023-03-27 17:11:20,557   Num examples = 1043
2023-03-27 17:11:20,558   Batch size = 32
2023-03-27 17:11:20,558 ***** Eval results *****
2023-03-27 17:11:20,558   att_loss = 0.3527162730693817
2023-03-27 17:11:20,559   cls_loss = 0.0
2023-03-27 17:11:20,559   global_step = 6049
2023-03-27 17:11:20,559   loss = 0.9033918823514666
2023-03-27 17:11:20,559   rep_loss = 0.5506756104741778
2023-03-27 17:11:20,566 ***** Save model *****
-27 17:11:27,336 Writing example 0 of 8551
2023-03-27 17:11:27,337 *** Example ***
2023-03-27 17:11:27,337 guid: train-0
2023-03-27 17:11:27,337 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 17:11:27,337 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:11:27,337 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:11:27,337 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:11:27,337 label: 1
2023-03-27 17:11:27,338 label_id: 1
2023-03-27 17:11:28,293 Writing example 0 of 1043
2023-03-27 17:11:28,294 *** Example ***
2023-03-27 17:11:28,294 guid: dev-0
2023-03-27 17:11:28,294 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 17:11:28,294 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:11:28,294 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:11:28,294 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:11:28,295 label: 1
2023-03-27 17:11:28,295 label_id: 1
2023-03-27 17:11:28,412 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 17:11:28,414 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 17:11:31,801 ***** Running evaluation *****
2023-03-27 17:11:31,801   Epoch = 22 iter 6099 step
2023-03-27 17:11:31,802   Num examples = 1043
2023-03-27 17:11:31,802   Batch size = 32
2023-03-27 17:11:31,803 ***** Eval results *****
2023-03-27 17:11:31,803   att_loss = 0.354451904296875
2023-03-27 17:11:31,803   cls_loss = 0.0
2023-03-27 17:11:31,804   global_step = 6099
2023-03-27 17:11:31,804   loss = 0.9052893543243408
2023-03-27 17:11:31,804   rep_loss = 0.5508374508221944
2023-03-27 17:11:31,811 ***** Save model *****
2023-03-27 17:11:43,080 ***** Running evaluation *****
2023-03-27 17:11:43,081   Epoch = 23 iter 6149 step
2023-03-27 17:11:43,081   Num examples = 1043
2023-03-27 17:11:43,081   Batch size = 32
2023-03-27 17:11:43,082 ***** Eval results *****
2023-03-27 17:11:43,082   att_loss = 0.3573148846626282
2023-03-27 17:11:43,083   cls_loss = 0.0
2023-03-27 17:11:43,083   global_step = 6149
2023-03-27 17:11:43,083   loss = 0.9072972163558006
2023-03-27 17:11:43,083   rep_loss = 0.549982339143753
2023-03-27 17:11:43,090 ***** Save model *****
2023-03-27 17:11:54,345 ***** Running evaluation *****
2023-03-27 17:11:54,345   Epoch = 23 iter 6199 step
2023-03-27 17:11:54,346   Num examples = 1043
2023-03-27 17:11:54,346   Batch size = 32
2023-03-27 17:11:54,347 ***** Eval results *****
2023-03-27 17:11:54,348   att_loss = 0.35313930079854766
2023-03-27 17:11:54,348   cls_loss = 0.0
2023-03-27 17:11:54,348   global_step = 6199
2023-03-27 17:11:54,348   loss = 0.9040112392655735
2023-03-27 17:11:54,348   rep_loss = 0.5508719405223583
2023-03-27 17:11:54,355 ***** Save model *****
ls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 17:11:32,438 ***** Running training *****
2023-03-27 17:11:32,439   Num examples = 8551
2023-03-27 17:11:32,439   Batch size = 32
2023-03-27 17:11:32,439   Num steps = 8010
2023-03-27 17:11:32,439 n: bert.embeddings.word_embeddings.weight
2023-03-27 17:11:32,439 n: bert.embeddings.position_embeddings.weight
2023-03-27 17:11:32,440 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 17:11:32,440 n: bert.embeddings.LayerNorm.weight
2023-03-27 17:11:32,440 n: bert.embeddings.LayerNorm.bias
2023-03-27 17:11:32,440 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 17:11:32,440 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 17:11:32,440 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 17:11:32,440 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 17:11:32,440 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 17:11:32,441 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 17:11:32,441 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 17:11:32,441 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 17:11:32,441 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 17:11:32,441 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 17:11:32,441 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 17:11:32,441 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 17:11:32,441 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 17:11:32,441 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 17:11:32,441 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 17:11:32,442 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 17:11:32,442 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 17:11:32,442 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 17:11:32,442 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 17:11:32,442 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 17:11:32,442 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 17:11:32,442 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 17:11:32,442 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 17:11:32,442 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 17:11:32,442 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 17:11:32,443 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 17:11:32,443 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 17:11:32,443 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 17:11:32,443 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 17:11:32,443 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 17:11:32,443 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 17:11:32,443 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 17:11:32,443 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 17:11:32,443 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 17:11:32,443 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 17:11:32,444 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 17:11:32,444 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 17:11:32,444 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 17:11:32,444 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 17:11:32,444 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 17:11:32,444 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 17:11:32,444 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 17:11:32,444 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 17:11:32,444 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 17:11:32,444 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 17:11:32,445 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 17:11:32,445 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 17:11:32,445 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 17:11:32,445 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 17:11:32,445 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 17:11:32,445 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 17:11:32,445 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 17:11:32,445 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 17:11:32,445 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 17:11:32,445 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 17:11:32,446 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 17:11:32,446 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 17:11:32,446 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 17:11:32,446 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 17:11:32,446 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 17:11:32,446 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 17:11:32,446 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 17:11:32,446 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 17:11:32,446 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 17:11:32,447 n: bert.pooler.dense.weight
2023-03-27 17:11:32,447 n: bert.pooler.dense.bias
2023-03-27 17:11:32,447 n: classifier.weight
2023-03-27 17:11:32,447 n: classifier.bias
2023-03-27 17:11:32,447 n: fit_dense.weight
2023-03-27 17:11:32,447 n: fit_dense.bias
2023-03-27 17:11:32,447 Total parameters: 14591258
2023-03-27 17:12:05,621 ***** Running evaluation *****
2023-03-27 17:12:05,621   Epoch = 23 iter 6249 step
2023-03-27 17:12:05,622   Num examples = 1043
2023-03-27 17:12:05,622   Batch size = 32
2023-03-27 17:12:05,623 ***** Eval results *****
2023-03-27 17:12:05,623   att_loss = 0.3556817000110944
2023-03-27 17:12:05,623   cls_loss = 0.0
2023-03-27 17:12:05,623   global_step = 6249
2023-03-27 17:12:05,624   loss = 0.9062765875348339
2023-03-27 17:12:05,624   rep_loss = 0.5505948883515818
2023-03-27 17:12:05,631 ***** Save model *****
2023-03-27 17:12:16,861 ***** Running evaluation *****
2023-03-27 17:12:16,861   Epoch = 23 iter 6299 step
2023-03-27 17:12:16,862   Num examples = 1043
2023-03-27 17:12:16,862   Batch size = 32
2023-03-27 17:12:16,864 ***** Eval results *****
2023-03-27 17:12:16,864   att_loss = 0.35452684363986875
2023-03-27 17:12:16,864   cls_loss = 0.0
2023-03-27 17:12:16,864   global_step = 6299
2023-03-27 17:12:16,864   loss = 0.9047295998168897
2023-03-27 17:12:16,864   rep_loss = 0.5502027563656433
2023-03-27 17:12:16,871 ***** Save model *****
2023-03-27 17:12:28,110 ***** Running evaluation *****
2023-03-27 17:12:28,110   Epoch = 23 iter 6349 step
2023-03-27 17:12:28,110   Num examples = 1043
2023-03-27 17:12:28,110   Batch size = 32
2023-03-27 17:12:28,112 ***** Eval results *****
2023-03-27 17:12:28,112   att_loss = 0.3544137552380562
2023-03-27 17:12:28,112   cls_loss = 0.0
2023-03-27 17:12:28,112   global_step = 6349
2023-03-27 17:12:28,112   loss = 0.9041727494734985
2023-03-27 17:12:28,113   rep_loss = 0.5497589953816854
2023-03-27 17:12:28,120 ***** Save model *****
2023-03-27 17:12:39,375 ***** Running evaluation *****
2023-03-27 17:12:39,376   Epoch = 23 iter 6399 step
2023-03-27 17:12:39,376   Num examples = 1043
2023-03-27 17:12:39,376   Batch size = 32
2023-03-27 17:12:39,377 ***** Eval results *****
2023-03-27 17:12:39,377   att_loss = 0.3542936869369921
2023-03-27 17:12:39,378   cls_loss = 0.0
2023-03-27 17:12:39,378   global_step = 6399
2023-03-27 17:12:39,378   loss = 0.9043235298275023
2023-03-27 17:12:39,378   rep_loss = 0.5500298435835875
2023-03-27 17:12:39,385 ***** Save model *****
2023-03-27 17:12:50,627 ***** Running evaluation *****
2023-03-27 17:12:50,627   Epoch = 24 iter 6449 step
2023-03-27 17:12:50,627   Num examples = 1043
2023-03-27 17:12:50,627   Batch size = 32
2023-03-27 17:12:50,629 ***** Eval results *****
2023-03-27 17:12:50,629   att_loss = 0.35174437412401527
2023-03-27 17:12:50,629   cls_loss = 0.0
2023-03-27 17:12:50,630   global_step = 6449
2023-03-27 17:12:50,630   loss = 0.8973404372610697
2023-03-27 17:12:50,630   rep_loss = 0.5455960704059135
2023-03-27 17:12:50,637 ***** Save model *****
2023-03-27 17:13:01,884 ***** Running evaluation *****
2023-03-27 17:13:01,885   Epoch = 24 iter 6499 step
2023-03-27 17:13:01,885   Num examples = 1043
2023-03-27 17:13:01,885   Batch size = 32
2023-03-27 17:13:01,886 ***** Eval results *****
2023-03-27 17:13:01,886   att_loss = 0.35144626275523677
2023-03-27 17:13:01,886   cls_loss = 0.0
2023-03-27 17:13:01,886   global_step = 6499
2023-03-27 17:13:01,887   loss = 0.8977541399526072
2023-03-27 17:13:01,887   rep_loss = 0.5463078801448529
2023-03-27 17:13:01,893 ***** Save model *****
2023-03-27 17:13:13,143 ***** Running evaluation *****
2023-03-27 17:13:13,144   Epoch = 24 iter 6549 step
2023-03-27 17:13:13,144   Num examples = 1043
2023-03-27 17:13:13,144   Batch size = 32
2023-03-27 17:13:13,145 ***** Eval results *****
2023-03-27 17:13:13,145   att_loss = 0.3500900942805811
2023-03-27 17:13:13,145   cls_loss = 0.0
2023-03-27 17:13:13,146   global_step = 6549
2023-03-27 17:13:13,146   loss = 0.896411621401496
2023-03-27 17:13:13,146   rep_loss = 0.5463215286004628
2023-03-27 17:13:13,153 ***** Save model *****
2023-03-27 17:13:24,428 ***** Running evaluation *****
2023-03-27 17:13:24,428   Epoch = 24 iter 6599 step
2023-03-27 17:13:24,428   Num examples = 1043
2023-03-27 17:13:24,428   Batch size = 32
2023-03-27 17:13:24,430 ***** Eval results *****
2023-03-27 17:13:24,430   att_loss = 0.35255671109204517
2023-03-27 17:13:24,431   cls_loss = 0.0
2023-03-27 17:13:24,431   global_step = 6599
2023-03-27 17:13:24,431   loss = 0.8998240772342183
2023-03-27 17:13:24,431   rep_loss = 0.5472673661421731
2023-03-27 17:13:24,439 ***** Save model *****
2023-03-27 17:13:35,707 ***** Running evaluation *****
2023-03-27 17:13:35,707   Epoch = 24 iter 6649 step
2023-03-27 17:13:35,707   Num examples = 1043
2023-03-27 17:13:35,707   Batch size = 32
2023-03-27 17:13:35,708 ***** Eval results *****
2023-03-27 17:13:35,709   att_loss = 0.3523872902779164
2023-03-27 17:13:35,709   cls_loss = 0.0
2023-03-27 17:13:35,709   global_step = 6649
2023-03-27 17:13:35,709   loss = 0.8997482486780254
2023-03-27 17:13:35,709   rep_loss = 0.5473609588947533
2023-03-27 17:13:35,717 ***** Save model *****
2023-03-27 17:13:46,986 ***** Running evaluation *****
2023-03-27 17:13:46,986   Epoch = 25 iter 6699 step
2023-03-27 17:13:46,986   Num examples = 1043
2023-03-27 17:13:46,987   Batch size = 32
2023-03-27 17:13:46,987 ***** Eval results *****
2023-03-27 17:13:46,988   att_loss = 0.3481614577273528
2023-03-27 17:13:46,988   cls_loss = 0.0
2023-03-27 17:13:46,988   global_step = 6699
2023-03-27 17:13:46,988   loss = 0.8953962425390879
2023-03-27 17:13:46,988   rep_loss = 0.5472347910205523
2023-03-27 17:13:46,995 ***** Save model *****
2023-03-27 17:13:58,233 ***** Running evaluation *****
2023-03-27 17:13:58,234   Epoch = 25 iter 6749 step
2023-03-27 17:13:58,234   Num examples = 1043
2023-03-27 17:13:58,234   Batch size = 32
2023-03-27 17:13:58,235 ***** Eval results *****
2023-03-27 17:13:58,236   att_loss = 0.35218436935463465
2023-03-27 17:13:58,236   cls_loss = 0.0
2023-03-27 17:13:58,236   global_step = 6749
2023-03-27 17:13:58,236   loss = 0.8996054505979693
2023-03-27 17:13:58,237   rep_loss = 0.5474210856734095
2023-03-27 17:13:58,244 ***** Save model *****
2023-03-27 17:14:09,492 ***** Running evaluation *****
2023-03-27 17:14:09,492   Epoch = 25 iter 6799 step
2023-03-27 17:14:09,492   Num examples = 1043
2023-03-27 17:14:09,492   Batch size = 32
2023-03-27 17:14:09,494 ***** Eval results *****
2023-03-27 17:14:09,494   att_loss = 0.3513663043418238
2023-03-27 17:14:09,494   cls_loss = 0.0
2023-03-27 17:14:09,494   global_step = 6799
2023-03-27 17:14:09,494   loss = 0.8973875656243293
2023-03-27 17:14:09,494   rep_loss = 0.5460212644069425
2023-03-27 17:14:09,502 ***** Save model *****
2023-03-27 17:14:20,712 ***** Running evaluation *****
2023-03-27 17:14:20,712   Epoch = 25 iter 6849 step
2023-03-27 17:14:20,712   Num examples = 1043
2023-03-27 17:14:20,712   Batch size = 32
2023-03-27 17:14:20,713 ***** Eval results *****
2023-03-27 17:14:20,714   att_loss = 0.35155741185292433
2023-03-27 17:14:20,714   cls_loss = 0.0
2023-03-27 17:14:20,714   global_step = 6849
2023-03-27 17:14:20,714   loss = 0.8972644641481596
2023-03-27 17:14:20,714   rep_loss = 0.5457070555495119
2023-03-27 17:14:20,722 ***** Save model *****
2023-03-27 17:14:31,988 ***** Running evaluation *****
2023-03-27 17:14:31,988   Epoch = 25 iter 6899 step
2023-03-27 17:14:31,989   Num examples = 1043
2023-03-27 17:14:31,989   Batch size = 32
2023-03-27 17:14:31,990 ***** Eval results *****
2023-03-27 17:14:31,990   att_loss = 0.35101380571722984
2023-03-27 17:14:31,990   cls_loss = 0.0
2023-03-27 17:14:31,991   global_step = 6899
2023-03-27 17:14:31,991   loss = 0.896530303039721
2023-03-27 17:14:31,991   rep_loss = 0.5455164994512286
2023-03-27 17:14:31,998 ***** Save model *****
2023-03-27 17:14:43,290 ***** Running evaluation *****
2023-03-27 17:14:43,291   Epoch = 26 iter 6949 step
2023-03-27 17:14:43,291   Num examples = 1043
2023-03-27 17:14:43,291   Batch size = 32
2023-03-27 17:14:43,292 ***** Eval results *****
2023-03-27 17:14:43,292   att_loss = 0.3517813895429884
2023-03-27 17:14:43,293   cls_loss = 0.0
2023-03-27 17:14:43,293   global_step = 6949
2023-03-27 17:14:43,293   loss = 0.8914167199816022
2023-03-27 17:14:43,293   rep_loss = 0.5396353346960885
2023-03-27 17:14:43,296 ***** Save model *****
2023-03-27 17:14:54,550 ***** Running evaluation *****
2023-03-27 17:14:54,550   Epoch = 26 iter 6999 step
2023-03-27 17:14:54,550   Num examples = 1043
2023-03-27 17:14:54,550   Batch size = 32
2023-03-27 17:14:54,551 ***** Eval results *****
2023-03-27 17:14:54,552   att_loss = 0.3519183512319598
2023-03-27 17:14:54,552   cls_loss = 0.0
2023-03-27 17:14:54,552   global_step = 6999
2023-03-27 17:14:54,552   loss = 0.8969227180146334
2023-03-27 17:14:54,552   rep_loss = 0.5450043657369781
2023-03-27 17:14:54,559 ***** Save model *****
2023-03-27 17:15:05,828 ***** Running evaluation *****
2023-03-27 17:15:05,828   Epoch = 26 iter 7049 step
2023-03-27 17:15:05,828   Num examples = 1043
2023-03-27 17:15:05,828   Batch size = 32
2023-03-27 17:15:05,829 ***** Eval results *****
2023-03-27 17:15:05,830   att_loss = 0.35151463830582447
2023-03-27 17:15:05,830   cls_loss = 0.0
2023-03-27 17:15:05,830   global_step = 7049
2023-03-27 17:15:05,830   loss = 0.895769428984027
2023-03-27 17:15:05,830   rep_loss = 0.5442547903996762
2023-03-27 17:15:05,837 ***** Save model *****
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:14:52,492 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:14:52,492 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:14:52,492 label: 1
2023-03-27 17:14:52,492 label_id: 1
2023-03-27 17:14:53,442 Writing example 0 of 1043
2023-03-27 17:14:53,443 *** Example ***
2023-03-27 17:14:53,443 guid: dev-0
2023-03-27 17:14:53,443 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 17:14:53,443 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:14:53,443 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:14:53,443 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:14:53,443 label: 1
2023-03-27 17:14:53,444 label_id: 1
2023-03-27 17:14:53,560 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 17:14:53,562 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 17:14:55,324 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 17:14:55,495 loading model...
2023-03-27 17:14:55,582 done!
2023-03-27 17:14:55,583 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 17:14:56,720 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 17:14:56,722 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 17:14:56,947 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 17:14:56,972 loading model...
2023-03-27 17:14:56,984 done!
2023-03-27 17:14:56,984 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 17:14:56,985 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 17:14:57,000 ***** Running training *****
2023-03-27 17:14:57,000   Num examples = 8551
2023-03-27 17:14:57,000   Batch size = 32
2023-03-27 17:14:57,000   Num steps = 8010
2023-03-27 17:14:57,001 n: bert.embeddings.word_embeddings.weight
2023-03-27 17:14:57,001 n: bert.embeddings.position_embeddings.weight
2023-03-27 17:14:57,001 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 17:14:57,001 n: bert.embeddings.LayerNorm.weight
2023-03-27 17:14:57,001 n: bert.embeddings.LayerNorm.bias
2023-03-27 17:14:57,001 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 17:14:57,001 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 17:14:57,002 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 17:14:57,002 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 17:14:57,002 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 17:14:57,002 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 17:14:57,002 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 17:14:57,002 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 17:14:57,002 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 17:14:57,002 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 17:14:57,002 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 17:14:57,003 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 17:14:57,003 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 17:14:57,003 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 17:14:57,003 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 17:14:57,003 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 17:14:57,003 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 17:14:57,003 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 17:14:57,003 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 17:14:57,004 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 17:14:57,004 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 17:14:57,004 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 17:14:57,004 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 17:14:57,004 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 17:14:57,004 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 17:14:57,004 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 17:14:57,004 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 17:14:57,004 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 17:14:57,005 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 17:14:57,005 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 17:14:57,005 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 17:14:57,005 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 17:14:57,005 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 17:14:57,005 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 17:14:57,005 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 17:14:57,006 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 17:14:57,006 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 17:14:57,006 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 17:14:57,006 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 17:14:57,006 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 17:14:57,006 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 17:14:57,006 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 17:14:57,006 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 17:14:57,006 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 17:14:57,006 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 17:14:57,007 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 17:14:57,007 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 17:14:57,007 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 17:14:57,007 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 17:14:57,007 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 17:14:57,007 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 17:14:57,007 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 17:14:57,007 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 17:14:57,007 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 17:14:57,007 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 17:14:57,008 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 17:14:57,008 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 17:14:57,008 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 17:14:57,008 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 17:14:57,008 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 17:14:57,008 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 17:14:57,008 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 17:14:57,008 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 17:14:57,008 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 17:14:57,008 n: bert.pooler.dense.weight
2023-03-27 17:14:57,009 n: bert.pooler.dense.bias
2023-03-27 17:14:57,009 n: classifier.weight
2023-03-27 17:14:57,009 n: classifier.bias
2023-03-27 17:14:57,009 n: fit_dense.weight
2023-03-27 17:14:57,009 n: fit_dense.bias
2023-03-27 17:14:57,009 Total parameters: 14591258
2023-03-27 17:15:11,087 ***** Running evaluation *****
2023-03-27 17:15:11,090   Epoch = 0 iter 49 step
2023-03-27 17:15:11,090   Num examples = 1043
2023-03-27 17:15:11,091   Batch size = 32
2023-03-27 17:15:11,099 ***** Eval results *****
2023-03-27 17:15:11,099   att_loss = 87931.29145408163
2023-03-27 17:15:11,099   cls_loss = 0.0
2023-03-27 17:15:11,099   global_step = 49
2023-03-27 17:15:11,100   loss = 87932.94339923469
2023-03-27 17:15:11,100   rep_loss = 1.6513117430161457
2023-03-27 17:15:11,107 ***** Save model *****
2023-03-27 17:15:25,596 ***** Running evaluation *****
2023-03-27 17:15:25,599   Epoch = 0 iter 99 step
2023-03-27 17:15:25,599   Num examples = 1043
2023-03-27 17:15:25,599   Batch size = 32
2023-03-27 17:15:25,600 ***** Eval results *****
2023-03-27 17:15:25,600   att_loss = 81003.76309974748
2023-03-27 17:15:25,600   cls_loss = 0.0
2023-03-27 17:15:25,600   global_step = 99
2023-03-27 17:15:25,601   loss = 81005.23358585859
2023-03-27 17:15:25,601   rep_loss = 1.4703123858480742
2023-03-27 17:15:25,608 ***** Save model *****
2023-03-27 17:15:40,199 ***** Running evaluation *****
2023-03-27 17:15:40,200   Epoch = 0 iter 149 step
2023-03-27 17:15:40,200   Num examples = 1043
2023-03-27 17:15:40,200   Batch size = 32
2023-03-27 17:15:40,201 ***** Eval results *****
2023-03-27 17:15:40,201   att_loss = 77320.39518666107
2023-03-27 17:15:40,201   cls_loss = 0.0
2023-03-27 17:15:40,201   global_step = 149
2023-03-27 17:15:40,201   loss = 77321.76995071309
2023-03-27 17:15:40,202   rep_loss = 1.3747404909773961
2023-03-27 17:15:40,209 ***** Save model *****
2023-03-27 17:15:54,834 ***** Running evaluation *****
2023-03-27 17:15:54,834   Epoch = 0 iter 199 step
2023-03-27 17:15:54,834   Num examples = 1043
2023-03-27 17:15:54,834   Batch size = 32
2023-03-27 17:15:54,836 ***** Eval results *****
2023-03-27 17:15:54,836   att_loss = 75214.95997565954
2023-03-27 17:15:54,836   cls_loss = 0.0
2023-03-27 17:15:54,836   global_step = 199
2023-03-27 17:15:54,836   loss = 75216.27066975503
2023-03-27 17:15:54,836   rep_loss = 1.3108177125154428
2023-03-27 17:15:54,843 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2023-03-27 17:16:09,524 ***** Running evaluation *****
2023-03-27 17:16:09,525   Epoch = 0 iter 249 step
2023-03-27 17:16:09,526   Num examples = 1043
2023-03-27 17:16:09,526   Batch size = 32
2023-03-27 17:16:09,527 ***** Eval results *****
2023-03-27 17:16:09,527   att_loss = 73815.35584525602
2023-03-27 17:16:09,527   cls_loss = 0.0
2023-03-27 17:16:09,528   global_step = 249
2023-03-27 17:16:09,528   loss = 73816.61985441767
2023-03-27 17:16:09,528   rep_loss = 1.2641592231620267
2023-03-27 17:16:09,535 ***** Save model *****
2023-03-27 17:16:24,244 ***** Running evaluation *****
2023-03-27 17:16:24,245   Epoch = 1 iter 299 step
2023-03-27 17:16:24,245   Num examples = 1043
2023-03-27 17:16:24,245   Batch size = 32
2023-03-27 17:16:24,246 ***** Eval results *****
2023-03-27 17:16:24,246   att_loss = 67201.44909667969
2023-03-27 17:16:24,246   cls_loss = 0.0
2023-03-27 17:16:24,246   global_step = 299
2023-03-27 17:16:24,246   loss = 67202.47998046875
2023-03-27 17:16:24,247   rep_loss = 1.03079491853714
2023-03-27 17:16:24,254 ***** Save model *****
2023-03-27 17:16:39,003 ***** Running evaluation *****
2023-03-27 17:16:39,003   Epoch = 1 iter 349 step
2023-03-27 17:16:39,003   Num examples = 1043
2023-03-27 17:16:39,003   Batch size = 32
2023-03-27 17:16:39,004 ***** Eval results *****
2023-03-27 17:16:39,004   att_loss = 66847.08555640244
2023-03-27 17:16:39,004   cls_loss = 0.0
2023-03-27 17:16:39,005   global_step = 349
2023-03-27 17:16:39,005   loss = 66848.09846608232
2023-03-27 17:16:39,005   rep_loss = 1.0131130756401434
2023-03-27 17:16:39,012 ***** Save model *****
                   2023-03-27 17:16:53,785 ***** Running evaluation *****
2023-03-27 17:16:53,785   Epoch = 1 iter 399 step
2023-03-27 17:16:53,785   Num examples = 1043
2023-03-27 17:16:53,785   Batch size = 32
2023-03-27 17:16:53,787 ***** Eval results *****
2023-03-27 17:16:53,787   att_loss = 66627.01532907198
2023-03-27 17:16:53,787   cls_loss = 0.0
2023-03-27 17:16:53,787   global_step = 399
2023-03-27 17:16:53,788   loss = 66628.01473721591
2023-03-27 17:16:53,788   rep_loss = 0.9995790575489854
2023-03-27 17:16:53,795 ***** Save model *****
2023-03-27 17:17:08,568 ***** Running evaluation *****
2023-03-27 17:17:08,568   Epoch = 1 iter 449 step
2023-03-27 17:17:08,568   Num examples = 1043
2023-03-27 17:17:08,568   Batch size = 32
2023-03-27 17:17:08,570 ***** Eval results *****
2023-03-27 17:17:08,571   att_loss = 66578.13800652472
2023-03-27 17:17:08,571   cls_loss = 0.0
2023-03-27 17:17:08,571   global_step = 449
2023-03-27 17:17:08,571   loss = 66579.12549364698
2023-03-27 17:17:08,571   rep_loss = 0.9876524223076119
2023-03-27 17:17:08,578 ***** Save model *****
          2023-03-27 17:17:23,414 ***** Running evaluation *****
2023-03-27 17:17:23,414   Epoch = 1 iter 499 step
2023-03-27 17:17:23,414   Num examples = 1043
2023-03-27 17:17:23,414   Batch size = 32
2023-03-27 17:17:23,415 ***** Eval results *****
2023-03-27 17:17:23,416   att_loss = 66222.681640625
2023-03-27 17:17:23,416   cls_loss = 0.0
2023-03-27 17:17:23,416   global_step = 499
2023-03-27 17:17:23,416   loss = 66223.65645204742
2023-03-27 17:17:23,416   rep_loss = 0.9749656119223299
2023-03-27 17:17:23,423 ***** Save model *****
2023-03-27 17:17:38,255 ***** Running evaluation *****
2023-03-27 17:17:38,256   Epoch = 2 iter 549 step
2023-03-27 17:17:38,256   Num examples = 1043
2023-03-27 17:17:38,256   Batch size = 32
2023-03-27 17:17:38,258 ***** Eval results *****
2023-03-27 17:17:38,258   att_loss = 62684.85364583333
2023-03-27 17:17:38,258   cls_loss = 0.0
2023-03-27 17:17:38,258   global_step = 549
2023-03-27 17:17:38,258   loss = 62685.74947916667
2023-03-27 17:17:38,258   rep_loss = 0.8955575903256734
2023-03-27 17:17:38,266 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2023-03-27 17:17:53,576 ***** Running evaluation *****
2023-03-27 17:17:53,576   Epoch = 2 iter 599 step
2023-03-27 17:17:53,576   Num examples = 1043
2023-03-27 17:17:53,577   Batch size = 32
2023-03-27 17:17:53,577 ***** Eval results *****
2023-03-27 17:17:53,578   att_loss = 64007.72692307692
2023-03-27 17:17:53,578   cls_loss = 0.0
2023-03-27 17:17:53,578   global_step = 599
2023-03-27 17:17:53,578   loss = 64008.62127403846
2023-03-27 17:17:53,578   rep_loss = 0.8944675986583416
2023-03-27 17:17:53,585 ***** Save model *****
2023-03-27 17:18:08,442 ***** Running evaluation *****
2023-03-27 17:18:08,443   Epoch = 2 iter 649 step
2023-03-27 17:18:08,443   Num examples = 1043
2023-03-27 17:18:08,443   Batch size = 32
2023-03-27 17:18:08,444 ***** Eval results *****
2023-03-27 17:18:08,444   att_loss = 63687.09507472826
2023-03-27 17:18:08,444   cls_loss = 0.0
2023-03-27 17:18:08,444   global_step = 649
2023-03-27 17:18:08,444   loss = 63687.98138586956
2023-03-27 17:18:08,445   rep_loss = 0.886397311480149
2023-03-27 17:18:08,452 ***** Save model *****
 model *****
2023-03-27 17:18:06,205 ***** Running evaluation *****
2023-03-27 17:18:06,205   Epoch = 29 iter 7849 step
2023-03-27 17:18:06,205   Num examples = 1043
2023-03-27 17:18:06,205   Batch size = 32
2023-03-27 17:18:06,206 ***** Eval results *****
2023-03-27 17:18:06,207   att_loss = 0.34778975906237114
2023-03-27 17:18:06,207   cls_loss = 0.0
2023-03-27 17:18:06,207   global_step = 7849
2023-03-27 17:18:06,207   loss = 0.8872956605452411
2023-03-27 17:18:06,207   rep_loss = 0.5395059040132558
2023-03-27 17:18:06,210 ***** Save model *****
2023-03-27 17:18:23,298 ***** Running evaluation *****
2023-03-27 17:18:23,299   Epoch = 2 iter 699 step
2023-03-27 17:18:23,299   Num examples = 1043
2023-03-27 17:18:23,299   Batch size = 32
2023-03-27 17:18:23,300 ***** Eval results *****
2023-03-27 17:18:23,300   att_loss = 63858.153622159094
2023-03-27 17:18:23,300   cls_loss = 0.0
2023-03-27 17:18:23,300   global_step = 699
2023-03-27 17:18:23,300   loss = 63859.03475378788
2023-03-27 17:18:23,300   rep_loss = 0.881149757269657
2023-03-27 17:18:23,307 ***** Save model *****
2023-03-27 17:18:38,153 ***** Running evaluation *****
2023-03-27 17:18:38,153   Epoch = 2 iter 749 step
2023-03-27 17:18:38,153   Num examples = 1043
2023-03-27 17:18:38,154   Batch size = 32
2023-03-27 17:18:38,157 ***** Eval results *****
2023-03-27 17:18:38,157   att_loss = 63967.00650436046
2023-03-27 17:18:38,157   cls_loss = 0.0
2023-03-27 17:18:38,157   global_step = 749
2023-03-27 17:18:38,157   loss = 63967.88168604651
2023-03-27 17:18:38,157   rep_loss = 0.8752815651339154
2023-03-27 17:18:38,164 ***** Save model *****
el *****
2023-03-27 17:18:40,061 ***** Running evaluation *****
2023-03-27 17:18:40,061   Epoch = 29 iter 7999 step
2023-03-27 17:18:40,062   Num examples = 1043
2023-03-27 17:18:40,062   Batch size = 32
2023-03-27 17:18:40,063 ***** Eval results *****
2023-03-27 17:18:40,063   att_loss = 0.34720711526460946
2023-03-27 17:18:40,063   cls_loss = 0.0
2023-03-27 17:18:40,063   global_step = 7999
2023-03-27 17:18:40,064   loss = 0.8872488944325596
2023-03-27 17:18:40,064   rep_loss = 0.540041780564934
2023-03-27 17:18:40,071 ***** Save model *****
2023-03-27 17:18:52,995 ***** Running evaluation *****
2023-03-27 17:18:52,995   Epoch = 2 iter 799 step
2023-03-27 17:18:52,995   Num examples = 1043
2023-03-27 17:18:52,995   Batch size = 32
2023-03-27 17:18:52,996 ***** Eval results *****
2023-03-27 17:18:52,997   att_loss = 63787.399380896226
2023-03-27 17:18:52,997   cls_loss = 0.0
2023-03-27 17:18:52,997   global_step = 799
2023-03-27 17:18:52,997   loss = 63788.26789504717
2023-03-27 17:18:52,998   rep_loss = 0.8685840433498598
2023-03-27 17:18:53,000 ***** Save model *****
2023-03-27 17:19:07,850 ***** Running evaluation *****
2023-03-27 17:19:07,850   Epoch = 3 iter 849 step
2023-03-27 17:19:07,851   Num examples = 1043
2023-03-27 17:19:07,851   Batch size = 32
2023-03-27 17:19:07,852 ***** Eval results *****
2023-03-27 17:19:07,852   att_loss = 61943.833333333336
2023-03-27 17:19:07,852   cls_loss = 0.0
2023-03-27 17:19:07,852   global_step = 849
2023-03-27 17:19:07,853   loss = 61944.658528645836
2023-03-27 17:19:07,853   rep_loss = 0.8253672048449516
2023-03-27 17:19:07,855 ***** Save model *****
2023-03-27 17:19:22,746 ***** Running evaluation *****
2023-03-27 17:19:22,747   Epoch = 3 iter 899 step
2023-03-27 17:19:22,747   Num examples = 1043
2023-03-27 17:19:22,747   Batch size = 32
2023-03-27 17:19:22,749 ***** Eval results *****
2023-03-27 17:19:22,749   att_loss = 62359.498804209186
2023-03-27 17:19:22,749   cls_loss = 0.0
2023-03-27 17:19:22,749   global_step = 899
2023-03-27 17:19:22,749   loss = 62360.3209502551
2023-03-27 17:19:22,749   rep_loss = 0.8220250278103108
2023-03-27 17:19:22,756 ***** Save model *****
2023-03-27 17:19:37,665 ***** Running evaluation *****
2023-03-27 17:19:37,665   Epoch = 3 iter 949 step
2023-03-27 17:19:37,666   Num examples = 1043
2023-03-27 17:19:37,666   Batch size = 32
2023-03-27 17:19:37,666 ***** Eval results *****
2023-03-27 17:19:37,667   att_loss = 62623.11248944257
2023-03-27 17:19:37,667   cls_loss = 0.0
2023-03-27 17:19:37,667   global_step = 949
2023-03-27 17:19:37,667   loss = 62623.93179898649
2023-03-27 17:19:37,667   rep_loss = 0.8191265238297952
2023-03-27 17:19:37,670 ***** Save model *****
2023-03-27 17:19:52,566 ***** Running evaluation *****
2023-03-27 17:19:52,566   Epoch = 3 iter 999 step
2023-03-27 17:19:52,566   Num examples = 1043
2023-03-27 17:19:52,566   Batch size = 32
2023-03-27 17:19:52,568 ***** Eval results *****
2023-03-27 17:19:52,568   att_loss = 62429.35661300505
2023-03-27 17:19:52,568   cls_loss = 0.0
2023-03-27 17:19:52,568   global_step = 999
2023-03-27 17:19:52,569   loss = 62430.17116477273
2023-03-27 17:19:52,569   rep_loss = 0.8144021771772944
2023-03-27 17:19:52,576 ***** Save model *****
2023-03-27 17:20:07,425 ***** Running evaluation *****
2023-03-27 17:20:07,426   Epoch = 3 iter 1049 step
2023-03-27 17:20:07,426   Num examples = 1043
2023-03-27 17:20:07,426   Batch size = 32
2023-03-27 17:20:07,427 ***** Eval results *****
2023-03-27 17:20:07,427   att_loss = 62496.59581338205
2023-03-27 17:20:07,427   cls_loss = 0.0
2023-03-27 17:20:07,427   global_step = 1049
2023-03-27 17:20:07,427   loss = 62497.40717930948
2023-03-27 17:20:07,427   rep_loss = 0.8112194701548545
2023-03-27 17:20:07,429 ***** Save model *****
2023-03-27 17:20:22,296 ***** Running evaluation *****
2023-03-27 17:20:22,296   Epoch = 4 iter 1099 step
2023-03-27 17:20:22,297   Num examples = 1043
2023-03-27 17:20:22,297   Batch size = 32
2023-03-27 17:20:22,298 ***** Eval results *****
2023-03-27 17:20:22,298   att_loss = 61596.19417842742
2023-03-27 17:20:22,298   cls_loss = 0.0
2023-03-27 17:20:22,299   global_step = 1099
2023-03-27 17:20:22,299   loss = 61596.98361895161
2023-03-27 17:20:22,299   rep_loss = 0.7891431220116154
2023-03-27 17:20:22,306 ***** Save model *****
2023-03-27 17:20:37,195 ***** Running evaluation *****
2023-03-27 17:20:37,195   Epoch = 4 iter 1149 step
2023-03-27 17:20:37,195   Num examples = 1043
2023-03-27 17:20:37,195   Batch size = 32
2023-03-27 17:20:37,196 ***** Eval results *****
2023-03-27 17:20:37,197   att_loss = 61808.39988425926
2023-03-27 17:20:37,197   cls_loss = 0.0
2023-03-27 17:20:37,197   global_step = 1149
2023-03-27 17:20:37,197   loss = 61809.18711419753
2023-03-27 17:20:37,198   rep_loss = 0.7873016684143631
2023-03-27 17:20:37,205 ***** Save model *****
2023-03-27 17:20:52,217 ***** Running evaluation *****
2023-03-27 17:20:52,217   Epoch = 4 iter 1199 step
2023-03-27 17:20:52,218   Num examples = 1043
2023-03-27 17:20:52,218   Batch size = 32
2023-03-27 17:20:52,219 ***** Eval results *****
2023-03-27 17:20:52,219   att_loss = 61505.06989503817
2023-03-27 17:20:52,219   cls_loss = 0.0
2023-03-27 17:20:52,220   global_step = 1199
2023-03-27 17:20:52,220   loss = 61505.853083253816
2023-03-27 17:20:52,220   rep_loss = 0.7831872979193243
2023-03-27 17:20:52,227 ***** Save model *****
2023-03-27 17:30:43,800 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP/TempTinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=30.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='KL_ATTN_SWEEP', kl_attn_weight=None)
2023-03-27 17:30:45,231 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 17:30:47,840 device: cuda n_gpu: 1
2023-03-27 17:30:47,949 Writing example 0 of 8551
2023-03-27 17:30:47,950 *** Example ***
2023-03-27 17:30:47,950 guid: train-0
2023-03-27 17:30:47,950 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 17:30:47,950 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:30:47,950 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:30:47,950 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:30:47,950 label: 1
2023-03-27 17:30:47,951 label_id: 1
2023-03-27 17:30:48,902 Writing example 0 of 1043
2023-03-27 17:30:48,903 *** Example ***
2023-03-27 17:30:48,903 guid: dev-0
2023-03-27 17:30:48,903 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 17:30:48,903 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:30:48,903 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:30:48,903 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:30:48,903 label: 1
2023-03-27 17:30:48,903 label_id: 1
2023-03-27 17:30:49,020 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 17:30:49,022 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 17:30:50,790 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 17:30:50,956 loading model...
2023-03-27 17:30:51,042 done!
2023-03-27 17:30:51,043 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 17:30:52,173 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 17:30:52,176 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 17:30:52,402 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 17:30:52,423 loading model...
2023-03-27 17:30:52,436 done!
2023-03-27 17:30:52,436 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 17:30:52,436 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 17:30:52,453 ***** Running training *****
2023-03-27 17:30:52,453   Num examples = 8551
2023-03-27 17:30:52,453   Batch size = 32
2023-03-27 17:30:52,453   Num steps = 8010
2023-03-27 17:30:52,454 n: bert.embeddings.word_embeddings.weight
2023-03-27 17:30:52,454 n: bert.embeddings.position_embeddings.weight
2023-03-27 17:30:52,454 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 17:30:52,454 n: bert.embeddings.LayerNorm.weight
2023-03-27 17:30:52,454 n: bert.embeddings.LayerNorm.bias
2023-03-27 17:30:52,454 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 17:30:52,455 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 17:30:52,455 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 17:30:52,455 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 17:30:52,455 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 17:30:52,455 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 17:30:52,455 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 17:30:52,455 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 17:30:52,455 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 17:30:52,456 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 17:30:52,456 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 17:30:52,456 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 17:30:52,456 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 17:30:52,456 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 17:30:52,456 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 17:30:52,456 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 17:30:52,456 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 17:30:52,456 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 17:30:52,456 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 17:30:52,457 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 17:30:52,457 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 17:30:52,457 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 17:30:52,457 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 17:30:52,457 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 17:30:52,457 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 17:30:52,457 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 17:30:52,457 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 17:30:52,457 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 17:30:52,457 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 17:30:52,457 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 17:30:52,458 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 17:30:52,458 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 17:30:52,458 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 17:30:52,458 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 17:30:52,458 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 17:30:52,458 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 17:30:52,458 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 17:30:52,458 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 17:30:52,458 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 17:30:52,458 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 17:30:52,459 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 17:30:52,459 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 17:30:52,459 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 17:30:52,459 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 17:30:52,459 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 17:30:52,459 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 17:30:52,459 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 17:30:52,459 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 17:30:52,459 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 17:30:52,459 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 17:30:52,460 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 17:30:52,460 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 17:30:52,460 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 17:30:52,460 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 17:30:52,460 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 17:30:52,460 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 17:30:52,460 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 17:30:52,460 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 17:30:52,460 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 17:30:52,460 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 17:30:52,460 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 17:30:52,461 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 17:30:52,461 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 17:30:52,461 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 17:30:52,461 n: bert.pooler.dense.weight
2023-03-27 17:30:52,461 n: bert.pooler.dense.bias
2023-03-27 17:30:52,461 n: classifier.weight
2023-03-27 17:30:52,461 n: classifier.bias
2023-03-27 17:30:52,461 n: fit_dense.weight
2023-03-27 17:30:52,461 n: fit_dense.bias
2023-03-27 17:30:52,461 Total parameters: 14591258
2023-03-27 17:32:08,402 ***** Running evaluation *****
2023-03-27 17:32:08,403   Epoch = 0 iter 49 step
2023-03-27 17:32:08,403   Num examples = 1043
2023-03-27 17:32:08,403   Batch size = 32
2023-03-27 17:32:08,413 ***** Eval results *****
2023-03-27 17:32:08,413   att_loss = 87931.29145408163
2023-03-27 17:32:08,414   cls_loss = 0.0
2023-03-27 17:32:08,414   global_step = 49
2023-03-27 17:32:08,414   loss = 87932.94339923469
2023-03-27 17:32:08,414   rep_loss = 1.6513117430161457
2023-03-27 17:32:08,422 ***** Save model *****
2023-03-27 17:33:26,072 ***** Running evaluation *****
2023-03-27 17:33:26,072   Epoch = 0 iter 99 step
2023-03-27 17:33:26,072   Num examples = 1043
2023-03-27 17:33:26,073   Batch size = 32
2023-03-27 17:33:26,077 ***** Eval results *****
2023-03-27 17:33:26,078   att_loss = 81003.76302083333
2023-03-27 17:33:26,078   cls_loss = 0.0
2023-03-27 17:33:26,078   global_step = 99
2023-03-27 17:33:26,079   loss = 81005.23350694444
2023-03-27 17:33:26,079   rep_loss = 1.4703123858480742
2023-03-27 17:33:26,083 ***** Save model *****
2023-03-27 17:34:43,725 ***** Running evaluation *****
2023-03-27 17:34:43,725   Epoch = 0 iter 149 step
2023-03-27 17:34:43,725   Num examples = 1043
2023-03-27 17:34:43,725   Batch size = 32
2023-03-27 17:34:43,728 ***** Eval results *****
2023-03-27 17:34:43,728   att_loss = 77320.39675964766
2023-03-27 17:34:43,728   cls_loss = 0.0
2023-03-27 17:34:43,728   global_step = 149
2023-03-27 17:34:43,729   loss = 77321.77152369966
2023-03-27 17:34:43,729   rep_loss = 1.374740506978643
2023-03-27 17:34:43,736 ***** Save model *****
2023-03-27 17:36:01,370 ***** Running evaluation *****
2023-03-27 17:36:01,370   Epoch = 0 iter 199 step
2023-03-27 17:36:01,370   Num examples = 1043
2023-03-27 17:36:01,370   Batch size = 32
2023-03-27 17:36:01,374 ***** Eval results *****
2023-03-27 17:36:01,374   att_loss = 75214.9434084485
2023-03-27 17:36:01,375   cls_loss = 0.0
2023-03-27 17:36:01,375   global_step = 199
2023-03-27 17:36:01,375   loss = 75216.25410254397
2023-03-27 17:36:01,375   rep_loss = 1.310818225894142
2023-03-27 17:36:01,382 ***** Save model *****
2023-03-27 17:37:19,030 ***** Running evaluation *****
2023-03-27 17:37:19,030   Epoch = 0 iter 249 step
2023-03-27 17:37:19,031   Num examples = 1043
2023-03-27 17:37:19,031   Batch size = 32
2023-03-27 17:37:19,033 ***** Eval results *****
2023-03-27 17:37:19,033   att_loss = 73815.33485504518
2023-03-27 17:37:19,033   cls_loss = 0.0
2023-03-27 17:37:19,033   global_step = 249
2023-03-27 17:37:19,033   loss = 73816.59886420683
2023-03-27 17:37:19,033   rep_loss = 1.2641595645123218
2023-03-27 17:37:19,041 ***** Save model *****
2023-03-27 17:38:36,685 ***** Running evaluation *****
2023-03-27 17:38:36,686   Epoch = 1 iter 299 step
2023-03-27 17:38:36,686   Num examples = 1043
2023-03-27 17:38:36,686   Batch size = 32
2023-03-27 17:38:36,688 ***** Eval results *****
2023-03-27 17:38:36,688   att_loss = 67201.49450683594
2023-03-27 17:38:36,689   cls_loss = 0.0
2023-03-27 17:38:36,689   global_step = 299
2023-03-27 17:38:36,689   loss = 67202.525390625
2023-03-27 17:38:36,689   rep_loss = 1.0307952258735895
2023-03-27 17:38:36,696 ***** Save model *****
2023-03-27 17:39:54,331 ***** Running evaluation *****
2023-03-27 17:39:54,331   Epoch = 1 iter 349 step
2023-03-27 17:39:54,331   Num examples = 1043
2023-03-27 17:39:54,331   Batch size = 32
2023-03-27 17:39:54,335 ***** Eval results *****
2023-03-27 17:39:54,335   att_loss = 66846.81230945123
2023-03-27 17:39:54,336   cls_loss = 0.0
2023-03-27 17:39:54,336   global_step = 349
2023-03-27 17:39:54,336   loss = 66847.8252191311
2023-03-27 17:39:54,336   rep_loss = 1.0131107147147016
2023-03-27 17:39:54,345 ***** Save model *****
2023-03-27 17:41:11,975 ***** Running evaluation *****
2023-03-27 17:41:11,976   Epoch = 1 iter 399 step
2023-03-27 17:41:11,976   Num examples = 1043
2023-03-27 17:41:11,976   Batch size = 32
2023-03-27 17:41:11,978 ***** Eval results *****
2023-03-27 17:41:11,978   att_loss = 66627.06131628787
2023-03-27 17:41:11,978   cls_loss = 0.0
2023-03-27 17:41:11,978   global_step = 399
2023-03-27 17:41:11,979   loss = 66628.06072443182
2023-03-27 17:41:11,979   rep_loss = 0.999575703884616
2023-03-27 17:41:11,987 ***** Save model *****
2023-03-27 17:42:29,621 ***** Running evaluation *****
2023-03-27 17:42:29,621   Epoch = 1 iter 449 step
2023-03-27 17:42:29,621   Num examples = 1043
2023-03-27 17:42:29,622   Batch size = 32
2023-03-27 17:42:29,624 ***** Eval results *****
2023-03-27 17:42:29,624   att_loss = 66578.25613839286
2023-03-27 17:42:29,625   cls_loss = 0.0
2023-03-27 17:42:29,625   global_step = 449
2023-03-27 17:42:29,625   loss = 66579.24362551511
2023-03-27 17:42:29,625   rep_loss = 0.9876491846619072
2023-03-27 17:42:29,632 ***** Save model *****
2023-03-27 17:43:47,278 ***** Running evaluation *****
2023-03-27 17:43:47,278   Epoch = 1 iter 499 step
2023-03-27 17:43:47,279   Num examples = 1043
2023-03-27 17:43:47,279   Batch size = 32
2023-03-27 17:43:47,280 ***** Eval results *****
2023-03-27 17:43:47,281   att_loss = 66222.74611058728
2023-03-27 17:43:47,283   cls_loss = 0.0
2023-03-27 17:43:47,283   global_step = 499
2023-03-27 17:43:47,283   loss = 66223.7209220097
2023-03-27 17:43:47,283   rep_loss = 0.9749582872308534
2023-03-27 17:43:47,285 ***** Save model *****
2023-03-27 17:45:04,929 ***** Running evaluation *****
2023-03-27 17:45:04,929   Epoch = 2 iter 549 step
2023-03-27 17:45:04,929   Num examples = 1043
2023-03-27 17:45:04,930   Batch size = 32
2023-03-27 17:45:04,932 ***** Eval results *****
2023-03-27 17:45:04,932   att_loss = 62684.7078125
2023-03-27 17:45:04,932   cls_loss = 0.0
2023-03-27 17:45:04,932   global_step = 549
2023-03-27 17:45:04,933   loss = 62685.60364583333
2023-03-27 17:45:04,933   rep_loss = 0.8954790075620015
2023-03-27 17:45:04,939 ***** Save model *****
2023-03-27 17:46:22,601 ***** Running evaluation *****
2023-03-27 17:46:22,601   Epoch = 2 iter 599 step
2023-03-27 17:46:22,601   Num examples = 1043
2023-03-27 17:46:22,601   Batch size = 32
2023-03-27 17:46:22,604 ***** Eval results *****
2023-03-27 17:46:22,604   att_loss = 64006.52025240385
2023-03-27 17:46:22,604   cls_loss = 0.0
2023-03-27 17:46:22,604   global_step = 599
2023-03-27 17:46:22,605   loss = 64007.41424278846
2023-03-27 17:46:22,605   rep_loss = 0.8941186565619249
2023-03-27 17:46:22,612 ***** Save model *****
2023-03-27 17:47:40,232 ***** Running evaluation *****
2023-03-27 17:47:40,232   Epoch = 2 iter 649 step
2023-03-27 17:47:40,232   Num examples = 1043
2023-03-27 17:47:40,232   Batch size = 32
2023-03-27 17:47:40,234 ***** Eval results *****
2023-03-27 17:47:40,234   att_loss = 63685.93984375
2023-03-27 17:47:40,234   cls_loss = 0.0
2023-03-27 17:47:40,235   global_step = 649
2023-03-27 17:47:40,235   loss = 63686.825815217395
2023-03-27 17:47:40,235   rep_loss = 0.8859487305516782
2023-03-27 17:47:40,242 ***** Save model *****
2023-03-27 17:48:57,857 ***** Running evaluation *****
2023-03-27 17:48:57,858   Epoch = 2 iter 699 step
2023-03-27 17:48:57,858   Num examples = 1043
2023-03-27 17:48:57,858   Batch size = 32
2023-03-27 17:48:57,860 ***** Eval results *****
2023-03-27 17:48:57,860   att_loss = 63857.5324810606
2023-03-27 17:48:57,860   cls_loss = 0.0
2023-03-27 17:48:57,861   global_step = 699
2023-03-27 17:48:57,861   loss = 63858.41304450758
2023-03-27 17:48:57,861   rep_loss = 0.8805751872785164
2023-03-27 17:48:57,865 ***** Save model *****
2023-03-27 17:50:15,527 ***** Running evaluation *****
2023-03-27 17:50:15,527   Epoch = 2 iter 749 step
2023-03-27 17:50:15,528   Num examples = 1043
2023-03-27 17:50:15,528   Batch size = 32
2023-03-27 17:50:15,532 ***** Eval results *****
2023-03-27 17:50:15,533   att_loss = 63965.97685319767
2023-03-27 17:50:15,533   cls_loss = 0.0
2023-03-27 17:50:15,533   global_step = 749
2023-03-27 17:50:15,534   loss = 63966.851380813954
2023-03-27 17:50:15,534   rep_loss = 0.8746178654737251
2023-03-27 17:50:15,541 ***** Save model *****
2023-03-27 17:50:42,297 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP/TempTinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=30.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='KL_ATTN_SWEEP', kl_attn_weight=None)
2023-03-27 17:50:44,176 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 17:50:47,255 device: cuda n_gpu: 1
2023-03-27 17:50:58,206 device: cuda n_gpu: 1
2023-03-27 17:50:58,307 Writing example 0 of 8551
2023-03-27 17:50:58,308 *** Example ***
2023-03-27 17:50:58,308 guid: train-0
2023-03-27 17:50:58,308 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 17:50:58,308 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:50:58,309 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:50:58,309 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:50:58,309 label: 1
2023-03-27 17:50:58,310 label_id: 1
2023-03-27 17:50:59,284 Writing example 0 of 1043
2023-03-27 17:50:59,284 *** Example ***
2023-03-27 17:50:59,284 guid: dev-0
2023-03-27 17:50:59,284 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 17:50:59,285 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:50:59,285 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:50:59,285 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 17:50:59,285 label: 1
2023-03-27 17:50:59,285 label_id: 1
2023-03-27 17:50:59,405 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 17:50:59,407 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 17:51:01,265 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 17:51:05,606 loading model...
2023-03-27 17:51:05,738 done!
2023-03-27 17:51:05,738 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 17:51:10,754 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 17:51:10,757 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 17:51:10,983 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 17:51:11,631 loading model...
2023-03-27 17:51:11,648 done!
2023-03-27 17:51:11,650 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 17:51:11,651 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 17:51:11,672 ***** Running training *****
2023-03-27 17:51:11,672   Num examples = 8551
2023-03-27 17:51:11,673   Batch size = 32
2023-03-27 17:51:11,673   Num steps = 8010
2023-03-27 17:51:11,674 n: bert.embeddings.word_embeddings.weight
2023-03-27 17:51:11,674 n: bert.embeddings.position_embeddings.weight
2023-03-27 17:51:11,675 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 17:51:11,675 n: bert.embeddings.LayerNorm.weight
2023-03-27 17:51:11,676 n: bert.embeddings.LayerNorm.bias
2023-03-27 17:51:11,676 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 17:51:11,676 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 17:51:11,677 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 17:51:11,677 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 17:51:11,677 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 17:51:11,677 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 17:51:11,678 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 17:51:11,678 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 17:51:11,678 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 17:51:11,678 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 17:51:11,678 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 17:51:11,678 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 17:51:11,678 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 17:51:11,679 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 17:51:11,679 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 17:51:11,679 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 17:51:11,679 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 17:51:11,679 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 17:51:11,679 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 17:51:11,679 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 17:51:11,680 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 17:51:11,680 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 17:51:11,680 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 17:51:11,680 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 17:51:11,680 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 17:51:11,680 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 17:51:11,680 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 17:51:11,681 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 17:51:11,681 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 17:51:11,681 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 17:51:11,681 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 17:51:11,681 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 17:51:11,681 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 17:51:11,681 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 17:51:11,682 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 17:51:11,682 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 17:51:11,682 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 17:51:11,682 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 17:51:11,682 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 17:51:11,682 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 17:51:11,683 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 17:51:11,683 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 17:51:11,683 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 17:51:11,683 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 17:51:11,683 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 17:51:11,683 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 17:51:11,683 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 17:51:11,683 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 17:51:11,683 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 17:51:11,684 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 17:51:11,684 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 17:51:11,684 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 17:51:11,684 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 17:51:11,684 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 17:51:11,684 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 17:51:11,684 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 17:51:11,684 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 17:51:11,684 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 17:51:11,685 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 17:51:11,685 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 17:51:11,685 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 17:51:11,685 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 17:51:11,685 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 17:51:11,685 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 17:51:11,685 n: bert.pooler.dense.weight
2023-03-27 17:51:11,685 n: bert.pooler.dense.bias
2023-03-27 17:51:11,685 n: classifier.weight
2023-03-27 17:51:11,686 n: classifier.bias
2023-03-27 17:51:11,686 n: fit_dense.weight
2023-03-27 17:51:11,686 n: fit_dense.bias
2023-03-27 17:51:11,686 Total parameters: 14591258
2023-03-27 17:51:29,787 ***** Running evaluation *****
2023-03-27 17:51:29,788   Epoch = 0 iter 49 step
2023-03-27 17:51:29,788   Num examples = 1043
2023-03-27 17:51:29,788   Batch size = 32
2023-03-27 17:51:29,797 ***** Eval results *****
2023-03-27 17:51:29,797   att_loss = 8772141.010204082
2023-03-27 17:51:29,797   cls_loss = 0.0
2023-03-27 17:51:29,797   global_step = 49
2023-03-27 17:51:29,798   loss = 8772142.663265307
2023-03-27 17:51:29,798   rep_loss = 1.6544393957877646
2023-03-27 17:51:29,805 ***** Save model *****
2023-03-27 17:51:44,686 ***** Running evaluation *****
2023-03-27 17:51:44,687   Epoch = 0 iter 99 step
2023-03-27 17:51:44,687   Num examples = 1043
2023-03-27 17:51:44,688   Batch size = 32
2023-03-27 17:51:44,689 ***** Eval results *****
2023-03-27 17:51:44,689   att_loss = 8095873.777777778
2023-03-27 17:51:44,689   cls_loss = 0.0
2023-03-27 17:51:44,689   global_step = 99
2023-03-27 17:51:44,690   loss = 8095875.313131313
2023-03-27 17:51:44,690   rep_loss = 1.4731342250650579
2023-03-27 17:51:44,696 ***** Save model *****
2023-03-27 17:51:59,618 ***** Running evaluation *****
2023-03-27 17:51:59,618   Epoch = 0 iter 149 step
2023-03-27 17:51:59,618   Num examples = 1043
2023-03-27 17:51:59,619   Batch size = 32
2023-03-27 17:51:59,620 ***** Eval results *****
2023-03-27 17:51:59,620   att_loss = 7731587.533557047
2023-03-27 17:51:59,620   cls_loss = 0.0
2023-03-27 17:51:59,620   global_step = 149
2023-03-27 17:51:59,620   loss = 7731588.889261745
2023-03-27 17:51:59,620   rep_loss = 1.3760941428626143
2023-03-27 17:51:59,628 ***** Save model *****
2023-03-27 17:52:14,633 ***** Running evaluation *****
2023-03-27 17:52:14,633   Epoch = 0 iter 199 step
2023-03-27 17:52:14,633   Num examples = 1043
2023-03-27 17:52:14,634   Batch size = 32
2023-03-27 17:52:14,635 ***** Eval results *****
2023-03-27 17:52:14,635   att_loss = 7513581.600502512
2023-03-27 17:52:14,636   cls_loss = 0.0
2023-03-27 17:52:14,636   global_step = 199
2023-03-27 17:52:14,636   loss = 7513582.866834171
2023-03-27 17:52:14,636   rep_loss = 1.3118175202278635
2023-03-27 17:52:14,643 ***** Save model *****
2023-03-27 17:52:29,693 ***** Running evaluation *****
2023-03-27 17:52:29,694   Epoch = 0 iter 249 step
2023-03-27 17:52:29,694   Num examples = 1043
2023-03-27 17:52:29,694   Batch size = 32
2023-03-27 17:52:29,695 ***** Eval results *****
2023-03-27 17:52:29,695   att_loss = 7373775.008032128
2023-03-27 17:52:29,695   cls_loss = 0.0
2023-03-27 17:52:29,696   global_step = 249
2023-03-27 17:52:29,696   loss = 7373776.220883534
2023-03-27 17:52:29,696   rep_loss = 1.2644298761245236
2023-03-27 17:52:29,703 ***** Save model *****
2023-03-27 17:52:44,823 ***** Running evaluation *****
2023-03-27 17:52:44,823   Epoch = 1 iter 299 step
2023-03-27 17:52:44,823   Num examples = 1043
2023-03-27 17:52:44,824   Batch size = 32
2023-03-27 17:52:44,825 ***** Eval results *****
2023-03-27 17:52:44,825   att_loss = 6672022.328125
2023-03-27 17:52:44,825   cls_loss = 0.0
2023-03-27 17:52:44,826   global_step = 299
2023-03-27 17:52:44,826   loss = 6672023.328125
2023-03-27 17:52:44,826   rep_loss = 1.029799522832036
2023-03-27 17:52:44,833 ***** Save model *****
2023-03-27 17:52:59,994 ***** Running evaluation *****
2023-03-27 17:52:59,995   Epoch = 1 iter 349 step
2023-03-27 17:52:59,995   Num examples = 1043
2023-03-27 17:52:59,995   Batch size = 32
2023-03-27 17:52:59,996 ***** Eval results *****
2023-03-27 17:52:59,996   att_loss = 6660208.4878048785
2023-03-27 17:52:59,996   cls_loss = 0.0
2023-03-27 17:52:59,996   global_step = 349
2023-03-27 17:52:59,997   loss = 6660209.4878048785
2023-03-27 17:52:59,997   rep_loss = 1.0150874576917508
2023-03-27 17:53:00,004 ***** Save model *****
2023-03-27 17:53:15,230 ***** Running evaluation *****
2023-03-27 17:53:15,230   Epoch = 1 iter 399 step
2023-03-27 17:53:15,230   Num examples = 1043
2023-03-27 17:53:15,230   Batch size = 32
2023-03-27 17:53:15,232 ***** Eval results *****
2023-03-27 17:53:15,232   att_loss = 6637574.810606061
2023-03-27 17:53:15,232   cls_loss = 0.0
2023-03-27 17:53:15,232   global_step = 399
2023-03-27 17:53:15,232   loss = 6637575.810606061
2023-03-27 17:53:15,232   rep_loss = 1.0018209223494385
2023-03-27 17:53:15,235 ***** Save model *****
2023-03-27 17:53:30,512 ***** Running evaluation *****
2023-03-27 17:53:30,512   Epoch = 1 iter 449 step
2023-03-27 17:53:30,513   Num examples = 1043
2023-03-27 17:53:30,513   Batch size = 32
2023-03-27 17:53:30,515 ***** Eval results *****
2023-03-27 17:53:30,515   att_loss = 6645768.269230769
2023-03-27 17:53:30,515   cls_loss = 0.0
2023-03-27 17:53:30,515   global_step = 449
2023-03-27 17:53:30,515   loss = 6645769.269230769
2023-03-27 17:53:30,515   rep_loss = 0.9900920106159462
2023-03-27 17:53:30,517 ***** Save model *****
2023-03-27 17:53:45,805 ***** Running evaluation *****
2023-03-27 17:53:45,805   Epoch = 1 iter 499 step
2023-03-27 17:53:45,805   Num examples = 1043
2023-03-27 17:53:45,805   Batch size = 32
2023-03-27 17:53:45,809 ***** Eval results *****
2023-03-27 17:53:45,809   att_loss = 6609073.810344827
2023-03-27 17:53:45,809   cls_loss = 0.0
2023-03-27 17:53:45,809   global_step = 499
2023-03-27 17:53:45,809   loss = 6609074.810344827
2023-03-27 17:53:45,810   rep_loss = 0.9775924500206421
2023-03-27 17:53:45,813 ***** Save model *****
2023-03-27 17:54:01,152 ***** Running evaluation *****
2023-03-27 17:54:01,152   Epoch = 2 iter 549 step
2023-03-27 17:54:01,152   Num examples = 1043
2023-03-27 17:54:01,152   Batch size = 32
2023-03-27 17:54:01,156 ***** Eval results *****
2023-03-27 17:54:01,156   att_loss = 6229635.6
2023-03-27 17:54:01,156   cls_loss = 0.0
2023-03-27 17:54:01,156   global_step = 549
2023-03-27 17:54:01,156   loss = 6229636.6
2023-03-27 17:54:01,157   rep_loss = 0.8912290930747986
2023-03-27 17:54:01,161 ***** Save model *****
2023-03-27 17:54:16,559 ***** Running evaluation *****
2023-03-27 17:54:16,559   Epoch = 2 iter 599 step
2023-03-27 17:54:16,559   Num examples = 1043
2023-03-27 17:54:16,559   Batch size = 32
2023-03-27 17:54:16,560 ***** Eval results *****
2023-03-27 17:54:16,561   att_loss = 6386022.876923077
2023-03-27 17:54:16,561   cls_loss = 0.0
2023-03-27 17:54:16,561   global_step = 599
2023-03-27 17:54:16,561   loss = 6386023.876923077
2023-03-27 17:54:16,561   rep_loss = 0.8927238812813392
2023-03-27 17:54:16,564 ***** Save model *****
2023-03-27 17:54:32,004 ***** Running evaluation *****
2023-03-27 17:54:32,005   Epoch = 2 iter 649 step
2023-03-27 17:54:32,005   Num examples = 1043
2023-03-27 17:54:32,005   Batch size = 32
2023-03-27 17:54:32,007 ***** Eval results *****
2023-03-27 17:54:32,008   att_loss = 6368222.195652174
2023-03-27 17:54:32,008   cls_loss = 0.0
2023-03-27 17:54:32,008   global_step = 649
2023-03-27 17:54:32,008   loss = 6368223.195652174
2023-03-27 17:54:32,008   rep_loss = 0.8834083085474761
2023-03-27 17:54:32,011 ***** Save model *****
2023-03-27 17:54:47,466 ***** Running evaluation *****
2023-03-27 17:54:47,467   Epoch = 2 iter 699 step
2023-03-27 17:54:47,467   Num examples = 1043
2023-03-27 17:54:47,467   Batch size = 32
2023-03-27 17:54:47,468 ***** Eval results *****
2023-03-27 17:54:47,469   att_loss = 6382634.796969697
2023-03-27 17:54:47,469   cls_loss = 0.0
2023-03-27 17:54:47,469   global_step = 699
2023-03-27 17:54:47,469   loss = 6382635.796969697
2023-03-27 17:54:47,469   rep_loss = 0.8774340831872188
2023-03-27 17:54:47,476 ***** Save model *****
2023-03-27 17:55:02,976 ***** Running evaluation *****
2023-03-27 17:55:02,977   Epoch = 2 iter 749 step
2023-03-27 17:55:02,977   Num examples = 1043
2023-03-27 17:55:02,977   Batch size = 32
2023-03-27 17:55:02,978 ***** Eval results *****
2023-03-27 17:55:02,978   att_loss = 6387473.902325582
2023-03-27 17:55:02,978   cls_loss = 0.0
2023-03-27 17:55:02,979   global_step = 749
2023-03-27 17:55:02,979   loss = 6387474.902325582
2023-03-27 17:55:02,979   rep_loss = 0.8715457830318185
2023-03-27 17:55:02,986 ***** Save model *****
2023-03-27 17:55:18,507 ***** Running evaluation *****
2023-03-27 17:55:18,507   Epoch = 2 iter 799 step
2023-03-27 17:55:18,508   Num examples = 1043
2023-03-27 17:55:18,508   Batch size = 32
2023-03-27 17:55:18,509 ***** Eval results *****
2023-03-27 17:55:18,509   att_loss = 6375219.01509434
2023-03-27 17:55:18,509   cls_loss = 0.0
2023-03-27 17:55:18,509   global_step = 799
2023-03-27 17:55:18,509   loss = 6375220.01509434
2023-03-27 17:55:18,509   rep_loss = 0.8653223440332233
2023-03-27 17:55:18,517 ***** Save model *****
2023-03-27 17:55:34,072 ***** Running evaluation *****
2023-03-27 17:55:34,073   Epoch = 3 iter 849 step
2023-03-27 17:55:34,073   Num examples = 1043
2023-03-27 17:55:34,073   Batch size = 32
2023-03-27 17:55:34,075 ***** Eval results *****
2023-03-27 17:55:34,075   att_loss = 6165580.25
2023-03-27 17:55:34,075   cls_loss = 0.0
2023-03-27 17:55:34,075   global_step = 849
2023-03-27 17:55:34,076   loss = 6165581.25
2023-03-27 17:55:34,076   rep_loss = 0.8207402353485426
2023-03-27 17:55:34,083 ***** Save model *****
2023-03-27 17:55:49,682 ***** Running evaluation *****
2023-03-27 17:55:49,682   Epoch = 3 iter 899 step
2023-03-27 17:55:49,682   Num examples = 1043
2023-03-27 17:55:49,682   Batch size = 32
2023-03-27 17:55:49,683 ***** Eval results *****
2023-03-27 17:55:49,684   att_loss = 6197056.882653061
2023-03-27 17:55:49,684   cls_loss = 0.0
2023-03-27 17:55:49,684   global_step = 899
2023-03-27 17:55:49,684   loss = 6197057.882653061
2023-03-27 17:55:49,684   rep_loss = 0.8164106169525458
2023-03-27 17:55:49,691 ***** Save model *****
2023-03-27 17:56:05,294 ***** Running evaluation *****
2023-03-27 17:56:05,295   Epoch = 3 iter 949 step
2023-03-27 17:56:05,295   Num examples = 1043
2023-03-27 17:56:05,295   Batch size = 32
2023-03-27 17:56:05,296 ***** Eval results *****
2023-03-27 17:56:05,296   att_loss = 6221389.14527027
2023-03-27 17:56:05,297   cls_loss = 0.0
2023-03-27 17:56:05,297   global_step = 949
2023-03-27 17:56:05,297   loss = 6221390.14527027
2023-03-27 17:56:05,297   rep_loss = 0.8136629570980329
2023-03-27 17:56:05,304 ***** Save model *****
2023-03-27 17:56:20,937 ***** Running evaluation *****
2023-03-27 17:56:20,937   Epoch = 3 iter 999 step
2023-03-27 17:56:20,937   Num examples = 1043
2023-03-27 17:56:20,938   Batch size = 32
2023-03-27 17:56:20,939 ***** Eval results *****
2023-03-27 17:56:20,939   att_loss = 6223054.1338383835
2023-03-27 17:56:20,940   cls_loss = 0.0
2023-03-27 17:56:20,940   global_step = 999
2023-03-27 17:56:20,940   loss = 6223055.1338383835
2023-03-27 17:56:20,940   rep_loss = 0.8092799466667753
2023-03-27 17:56:20,942 ***** Save model *****
2023-03-27 17:56:36,600 ***** Running evaluation *****
2023-03-27 17:56:36,600   Epoch = 3 iter 1049 step
2023-03-27 17:56:36,601   Num examples = 1043
2023-03-27 17:56:36,601   Batch size = 32
2023-03-27 17:56:36,602 ***** Eval results *****
2023-03-27 17:56:36,602   att_loss = 6227096.895161291
2023-03-27 17:56:36,602   cls_loss = 0.0
2023-03-27 17:56:36,602   global_step = 1049
2023-03-27 17:56:36,603   loss = 6227097.893145162
2023-03-27 17:56:36,603   rep_loss = 0.8048880674665974
2023-03-27 17:56:36,610 ***** Save model *****
2023-03-27 17:56:52,273 ***** Running evaluation *****
2023-03-27 17:56:52,273   Epoch = 4 iter 1099 step
2023-03-27 17:56:52,273   Num examples = 1043
2023-03-27 17:56:52,273   Batch size = 32
2023-03-27 17:56:52,275 ***** Eval results *****
2023-03-27 17:56:52,275   att_loss = 6232949.725806451
2023-03-27 17:56:52,275   cls_loss = 0.0
2023-03-27 17:56:52,275   global_step = 1099
2023-03-27 17:56:52,275   loss = 6232950.70967742
2023-03-27 17:56:52,275   rep_loss = 0.7795228054446559
2023-03-27 17:56:52,282 ***** Save model *****
2023-03-27 17:57:07,974 ***** Running evaluation *****
2023-03-27 17:57:07,974   Epoch = 4 iter 1149 step
2023-03-27 17:57:07,975   Num examples = 1043
2023-03-27 17:57:07,975   Batch size = 32
2023-03-27 17:57:07,976 ***** Eval results *****
2023-03-27 17:57:07,976   att_loss = 6191505.209876543
2023-03-27 17:57:07,976   cls_loss = 0.0
2023-03-27 17:57:07,976   global_step = 1149
2023-03-27 17:57:07,976   loss = 6191506.179012346
2023-03-27 17:57:07,977   rep_loss = 0.776390775486275
2023-03-27 17:57:07,979 ***** Save model *****
2023-03-27 17:57:23,669 ***** Running evaluation *****
2023-03-27 17:57:23,669   Epoch = 4 iter 1199 step
2023-03-27 17:57:23,670   Num examples = 1043
2023-03-27 17:57:23,670   Batch size = 32
2023-03-27 17:57:23,671 ***** Eval results *****
2023-03-27 17:57:23,671   att_loss = 6164221.721374046
2023-03-27 17:57:23,671   cls_loss = 0.0
2023-03-27 17:57:23,671   global_step = 1199
2023-03-27 17:57:23,672   loss = 6164222.652671755
2023-03-27 17:57:23,672   rep_loss = 0.7712424383818648
2023-03-27 17:57:23,679 ***** Save model *****
2023-03-27 17:57:39,399 ***** Running evaluation *****
2023-03-27 17:57:39,400   Epoch = 4 iter 1249 step
2023-03-27 17:57:39,400   Num examples = 1043
2023-03-27 17:57:39,400   Batch size = 32
2023-03-27 17:57:39,402 ***** Eval results *****
2023-03-27 17:57:39,402   att_loss = 6144026.690607735
2023-03-27 17:57:39,402   cls_loss = 0.0
2023-03-27 17:57:39,402   global_step = 1249
2023-03-27 17:57:39,402   loss = 6144027.585635359
2023-03-27 17:57:39,403   rep_loss = 0.7670526402431298
2023-03-27 17:57:39,410 ***** Save model *****
2023-03-27 17:57:55,121 ***** Running evaluation *****
2023-03-27 17:57:55,122   Epoch = 4 iter 1299 step
2023-03-27 17:57:55,122   Num examples = 1043
2023-03-27 17:57:55,122   Batch size = 32
2023-03-27 17:57:55,123 ***** Eval results *****
2023-03-27 17:57:55,123   att_loss = 6151419.233766234
2023-03-27 17:57:55,124   cls_loss = 0.0
2023-03-27 17:57:55,124   global_step = 1299
2023-03-27 17:57:55,124   loss = 6151420.106060606
2023-03-27 17:57:55,124   rep_loss = 0.7644575777507964
2023-03-27 17:57:55,132 ***** Save model *****
2023-03-27 17:58:10,875 ***** Running evaluation *****
2023-03-27 17:58:10,875   Epoch = 5 iter 1349 step
2023-03-27 17:58:10,876   Num examples = 1043
2023-03-27 17:58:10,876   Batch size = 32
2023-03-27 17:58:10,877 ***** Eval results *****
2023-03-27 17:58:10,877   att_loss = 6131093.892857143
2023-03-27 17:58:10,877   cls_loss = 0.0
2023-03-27 17:58:10,877   global_step = 1349
2023-03-27 17:58:10,878   loss = 6131094.571428572
2023-03-27 17:58:10,878   rep_loss = 0.7445162492138999
2023-03-27 17:58:10,881 ***** Save model *****
2023-03-27 17:58:26,603 ***** Running evaluation *****
2023-03-27 17:58:26,603   Epoch = 5 iter 1399 step
2023-03-27 17:58:26,603   Num examples = 1043
2023-03-27 17:58:26,603   Batch size = 32
2023-03-27 17:58:26,604 ***** Eval results *****
2023-03-27 17:58:26,604   att_loss = 6091730.4375
2023-03-27 17:58:26,605   cls_loss = 0.0
2023-03-27 17:58:26,605   global_step = 1399
2023-03-27 17:58:26,605   loss = 6091731.0859375
2023-03-27 17:58:26,605   rep_loss = 0.7443858087062836
2023-03-27 17:58:26,612 ***** Save model *****
2023-03-27 17:58:42,388 ***** Running evaluation *****
2023-03-27 17:58:42,389   Epoch = 5 iter 1449 step
2023-03-27 17:58:42,389   Num examples = 1043
2023-03-27 17:58:42,389   Batch size = 32
2023-03-27 17:58:42,391 ***** Eval results *****
2023-03-27 17:58:42,391   att_loss = 6075497.206140351
2023-03-27 17:58:42,391   cls_loss = 0.0
2023-03-27 17:58:42,392   global_step = 1449
2023-03-27 17:58:42,392   loss = 6075497.868421053
2023-03-27 17:58:42,392   rep_loss = 0.7421848601416537
2023-03-27 17:58:42,399 ***** Save model *****
2023-03-27 17:58:58,174 ***** Running evaluation *****
2023-03-27 17:58:58,175   Epoch = 5 iter 1499 step
2023-03-27 17:58:58,175   Num examples = 1043
2023-03-27 17:58:58,175   Batch size = 32
2023-03-27 17:58:58,176 ***** Eval results *****
2023-03-27 17:58:58,176   att_loss = 6078840.320121951
2023-03-27 17:58:58,176   cls_loss = 0.0
2023-03-27 17:58:58,176   global_step = 1499
2023-03-27 17:58:58,177   loss = 6078840.957317073
2023-03-27 17:58:58,177   rep_loss = 0.7397081888303524
2023-03-27 17:58:58,179 ***** Save model *****
2023-03-27 17:59:13,990 ***** Running evaluation *****
2023-03-27 17:59:13,990   Epoch = 5 iter 1549 step
2023-03-27 17:59:13,990   Num examples = 1043
2023-03-27 17:59:13,990   Batch size = 32
2023-03-27 17:59:13,992 ***** Eval results *****
2023-03-27 17:59:13,992   att_loss = 6074558.289719626
2023-03-27 17:59:13,992   cls_loss = 0.0
2023-03-27 17:59:13,993   global_step = 1549
2023-03-27 17:59:13,993   loss = 6074558.908878504
2023-03-27 17:59:13,993   rep_loss = 0.737111648387998
2023-03-27 17:59:14,001 ***** Save model *****
2023-03-27 17:59:29,810 ***** Running evaluation *****
2023-03-27 17:59:29,810   Epoch = 5 iter 1599 step
2023-03-27 17:59:29,811   Num examples = 1043
2023-03-27 17:59:29,811   Batch size = 32
2023-03-27 17:59:29,812 ***** Eval results *****
2023-03-27 17:59:29,812   att_loss = 6071087.333333333
2023-03-27 17:59:29,812   cls_loss = 0.0
2023-03-27 17:59:29,812   global_step = 1599
2023-03-27 17:59:29,812   loss = 6071087.9375
2023-03-27 17:59:29,812   rep_loss = 0.7351166739156751
2023-03-27 17:59:29,819 ***** Save model *****
2023-03-27 17:59:45,641 ***** Running evaluation *****
2023-03-27 17:59:45,642   Epoch = 6 iter 1649 step
2023-03-27 17:59:45,642   Num examples = 1043
2023-03-27 17:59:45,642   Batch size = 32
2023-03-27 17:59:45,644 ***** Eval results *****
2023-03-27 17:59:45,644   att_loss = 5978689.872340426
2023-03-27 17:59:45,645   cls_loss = 0.0
2023-03-27 17:59:45,645   global_step = 1649
2023-03-27 17:59:45,645   loss = 5978690.393617021
2023-03-27 17:59:45,645   rep_loss = 0.7182075736370492
2023-03-27 17:59:45,653 ***** Save model *****
2023-03-27 18:00:01,459 ***** Running evaluation *****
2023-03-27 18:00:01,459   Epoch = 6 iter 1699 step
2023-03-27 18:00:01,459   Num examples = 1043
2023-03-27 18:00:01,459   Batch size = 32
2023-03-27 18:00:01,461 ***** Eval results *****
2023-03-27 18:00:01,461   att_loss = 6002773.0051546395
2023-03-27 18:00:01,461   cls_loss = 0.0
2023-03-27 18:00:01,461   global_step = 1699
2023-03-27 18:00:01,462   loss = 6002773.520618557
2023-03-27 18:00:01,462   rep_loss = 0.7192459677912525
2023-03-27 18:00:01,469 ***** Save model *****
2023-03-27 18:00:17,284 ***** Running evaluation *****
2023-03-27 18:00:17,285   Epoch = 6 iter 1749 step
2023-03-27 18:00:17,285   Num examples = 1043
2023-03-27 18:00:17,285   Batch size = 32
2023-03-27 18:00:17,286 ***** Eval results *****
2023-03-27 18:00:17,286   att_loss = 6004321.452380952
2023-03-27 18:00:17,286   cls_loss = 0.0
2023-03-27 18:00:17,287   global_step = 1749
2023-03-27 18:00:17,287   loss = 6004321.969387755
2023-03-27 18:00:17,287   rep_loss = 0.7184097787149909
2023-03-27 18:00:17,294 ***** Save model *****
2023-03-27 18:00:33,150 ***** Running evaluation *****
2023-03-27 18:00:33,150   Epoch = 6 iter 1799 step
2023-03-27 18:00:33,150   Num examples = 1043
2023-03-27 18:00:33,151   Batch size = 32
2023-03-27 18:00:33,152 ***** Eval results *****
2023-03-27 18:00:33,152   att_loss = 6000767.175126904
2023-03-27 18:00:33,152   cls_loss = 0.0
2023-03-27 18:00:33,152   global_step = 1799
2023-03-27 18:00:33,152   loss = 6000767.687817259
2023-03-27 18:00:33,152   rep_loss = 0.7168312142343085
2023-03-27 18:00:33,159 ***** Save model *****
2023-03-27 18:01:26,929 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_6pm/TempTinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=30.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='KL_ATTN_SWEEP_6pm', kl_attn_weight=None)
2023-03-27 18:01:28,285 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 18:01:31,184 device: cuda n_gpu: 1
2023-03-27 18:01:31,240 Writing example 0 of 8551
2023-03-27 18:01:31,240 *** Example ***
2023-03-27 18:01:31,241 guid: train-0
2023-03-27 18:01:31,241 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 18:01:31,241 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:01:31,241 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:01:31,242 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:01:31,242 label: 1
2023-03-27 18:01:31,242 label_id: 1
2023-03-27 18:01:32,263 Writing example 0 of 1043
2023-03-27 18:01:32,263 *** Example ***
2023-03-27 18:01:32,263 guid: dev-0
2023-03-27 18:01:32,263 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 18:01:32,263 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:01:32,264 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:01:32,264 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:01:32,264 label: 1
2023-03-27 18:01:32,264 label_id: 1
2023-03-27 18:01:32,384 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 18:01:32,386 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 18:01:34,151 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 18:01:34,339 loading model...
2023-03-27 18:01:34,390 done!
2023-03-27 18:01:34,391 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 18:01:35,476 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 18:01:35,477 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 18:01:35,703 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 18:01:35,734 loading model...
2023-03-27 18:01:35,746 done!
2023-03-27 18:01:35,746 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 18:01:35,746 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 18:01:35,763 ***** Running training *****
2023-03-27 18:01:35,763   Num examples = 8551
2023-03-27 18:01:35,764   Batch size = 32
2023-03-27 18:01:35,764   Num steps = 8010
2023-03-27 18:01:35,765 n: bert.embeddings.word_embeddings.weight
2023-03-27 18:01:35,765 n: bert.embeddings.position_embeddings.weight
2023-03-27 18:01:35,765 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 18:01:35,766 n: bert.embeddings.LayerNorm.weight
2023-03-27 18:01:35,766 n: bert.embeddings.LayerNorm.bias
2023-03-27 18:01:35,766 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 18:01:35,767 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 18:01:35,767 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 18:01:35,767 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 18:01:35,767 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 18:01:35,767 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 18:01:35,768 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 18:01:35,768 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 18:01:35,768 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 18:01:35,769 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 18:01:35,769 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 18:01:35,769 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 18:01:35,769 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 18:01:35,770 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 18:01:35,770 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 18:01:35,770 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 18:01:35,770 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 18:01:35,770 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 18:01:35,771 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 18:01:35,771 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 18:01:35,771 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 18:01:35,771 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 18:01:35,772 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 18:01:35,772 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 18:01:35,772 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 18:01:35,772 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 18:01:35,772 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 18:01:35,773 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 18:01:35,773 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 18:01:35,773 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 18:01:35,773 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 18:01:35,774 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 18:01:35,774 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 18:01:35,774 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 18:01:35,774 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 18:01:35,775 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 18:01:35,775 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 18:01:35,775 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 18:01:35,775 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 18:01:35,775 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 18:01:35,776 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 18:01:35,776 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 18:01:35,776 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 18:01:35,776 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 18:01:35,777 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 18:01:35,777 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 18:01:35,777 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 18:01:35,777 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 18:01:35,778 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 18:01:35,778 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 18:01:35,778 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 18:01:35,778 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 18:01:35,779 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 18:01:35,779 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 18:01:35,779 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 18:01:35,779 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 18:01:35,780 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 18:01:35,780 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 18:01:35,780 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 18:01:35,780 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 18:01:35,780 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 18:01:35,781 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 18:01:35,781 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 18:01:35,781 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 18:01:35,781 n: bert.pooler.dense.weight
2023-03-27 18:01:35,782 n: bert.pooler.dense.bias
2023-03-27 18:01:35,782 n: classifier.weight
2023-03-27 18:01:35,782 n: classifier.bias
2023-03-27 18:01:35,782 n: fit_dense.weight
2023-03-27 18:01:35,783 n: fit_dense.bias
2023-03-27 18:01:35,783 Total parameters: 14591258
2023-03-27 18:01:50,622 ***** Running evaluation *****
2023-03-27 18:01:50,622   Epoch = 0 iter 49 step
2023-03-27 18:01:50,623   Num examples = 1043
2023-03-27 18:01:50,623   Batch size = 32
2023-03-27 18:01:50,631 ***** Eval results *****
2023-03-27 18:01:50,631   att_loss = 87721.4024234694
2023-03-27 18:01:50,631   cls_loss = 0.0
2023-03-27 18:01:50,632   global_step = 49
2023-03-27 18:01:50,632   loss = 87723.05660076531
2023-03-27 18:01:50,632   rep_loss = 1.654427389709317
2023-03-27 18:01:50,639 ***** Save model *****
2023-03-27 18:02:05,923 ***** Running evaluation *****
2023-03-27 18:02:05,924   Epoch = 0 iter 99 step
2023-03-27 18:02:05,925   Num examples = 1043
2023-03-27 18:02:05,925   Batch size = 32
2023-03-27 18:02:05,927 ***** Eval results *****
2023-03-27 18:02:05,927   att_loss = 80958.73508522728
2023-03-27 18:02:05,927   cls_loss = 0.0
2023-03-27 18:02:05,928   global_step = 99
2023-03-27 18:02:05,928   loss = 80960.20801767676
2023-03-27 18:02:05,928   rep_loss = 1.473113158736566
2023-03-27 18:02:05,935 ***** Save model *****
2023-03-27 18:02:21,370 ***** Running evaluation *****
2023-03-27 18:02:21,370   Epoch = 0 iter 149 step
2023-03-27 18:02:21,370   Num examples = 1043
2023-03-27 18:02:21,370   Batch size = 32
2023-03-27 18:02:21,372 ***** Eval results *****
2023-03-27 18:02:21,372   att_loss = 77315.87827705537
2023-03-27 18:02:21,372   cls_loss = 0.0
2023-03-27 18:02:21,373   global_step = 149
2023-03-27 18:02:21,373   loss = 77317.25422084732
2023-03-27 18:02:21,373   rep_loss = 1.3760655486343691
2023-03-27 18:02:21,380 ***** Save model *****
2023-03-27 18:02:36,887 ***** Running evaluation *****
2023-03-27 18:02:36,887   Epoch = 0 iter 199 step
2023-03-27 18:02:36,887   Num examples = 1043
2023-03-27 18:02:36,887   Batch size = 32
2023-03-27 18:02:36,889 ***** Eval results *****
2023-03-27 18:02:36,889   att_loss = 75135.82090138191
2023-03-27 18:02:36,890   cls_loss = 0.0
2023-03-27 18:02:36,890   global_step = 199
2023-03-27 18:02:36,890   loss = 75137.13259657663
2023-03-27 18:02:36,890   rep_loss = 1.311782240268573
2023-03-27 18:02:36,897 ***** Save model *****
2023-03-27 18:02:52,959 ***** Running evaluation *****
2023-03-27 18:02:52,959   Epoch = 0 iter 249 step
2023-03-27 18:02:52,959   Num examples = 1043
2023-03-27 18:02:52,959   Batch size = 32
2023-03-27 18:02:52,961 ***** Eval results *****
2023-03-27 18:02:52,961   att_loss = 73737.75665160643
2023-03-27 18:02:52,961   cls_loss = 0.0
2023-03-27 18:02:52,961   global_step = 249
2023-03-27 18:02:52,962   loss = 73739.02095883535
2023-03-27 18:02:52,962   rep_loss = 1.2643884607108242
2023-03-27 18:02:52,970 ***** Save model *****
2023-03-27 18:03:08,587 ***** Running evaluation *****
2023-03-27 18:03:08,587   Epoch = 1 iter 299 step
2023-03-27 18:03:08,587   Num examples = 1043
2023-03-27 18:03:08,588   Batch size = 32
2023-03-27 18:03:08,589 ***** Eval results *****
2023-03-27 18:03:08,589   att_loss = 66720.23303222656
2023-03-27 18:03:08,589   cls_loss = 0.0
2023-03-27 18:03:08,590   global_step = 299
2023-03-27 18:03:08,590   loss = 66721.26245117188
2023-03-27 18:03:08,590   rep_loss = 1.029721885919571
2023-03-27 18:03:08,597 ***** Save model *****
2023-03-27 18:03:24,247 ***** Running evaluation *****
2023-03-27 18:03:24,247   Epoch = 1 iter 349 step
2023-03-27 18:03:24,247   Num examples = 1043
2023-03-27 18:03:24,248   Batch size = 32
2023-03-27 18:03:24,249 ***** Eval results *****
2023-03-27 18:03:24,249   att_loss = 66602.09694169207
2023-03-27 18:03:24,249   cls_loss = 0.0
2023-03-27 18:03:24,249   global_step = 349
2023-03-27 18:03:24,249   loss = 66603.11161394817
2023-03-27 18:03:24,249   rep_loss = 1.0150063466735002
2023-03-27 18:03:24,251 ***** Save model *****
2023-03-27 18:03:39,936 ***** Running evaluation *****
2023-03-27 18:03:39,937   Epoch = 1 iter 399 step
2023-03-27 18:03:39,937   Num examples = 1043
2023-03-27 18:03:39,937   Batch size = 32
2023-03-27 18:03:39,939 ***** Eval results *****
2023-03-27 18:03:39,939   att_loss = 66375.76284327652
2023-03-27 18:03:39,939   cls_loss = 0.0
2023-03-27 18:03:39,939   global_step = 399
2023-03-27 18:03:39,939   loss = 66376.76438210228
2023-03-27 18:03:39,940   rep_loss = 1.0017361794457291
2023-03-27 18:03:39,942 ***** Save model *****
2023-03-27 18:03:55,660 ***** Running evaluation *****
2023-03-27 18:03:55,661   Epoch = 1 iter 449 step
2023-03-27 18:03:55,661   Num examples = 1043
2023-03-27 18:03:55,661   Batch size = 32
2023-03-27 18:03:55,662 ***** Eval results *****
2023-03-27 18:03:55,662   att_loss = 66457.69404618819
2023-03-27 18:03:55,663   cls_loss = 0.0
2023-03-27 18:03:55,663   global_step = 449
2023-03-27 18:03:55,663   loss = 66458.68398008242
2023-03-27 18:03:55,663   rep_loss = 0.9900039833980602
2023-03-27 18:03:55,671 ***** Save model *****
2023-03-27 18:04:11,446 ***** Running evaluation *****
2023-03-27 18:04:11,446   Epoch = 1 iter 499 step
2023-03-27 18:04:11,446   Num examples = 1043
2023-03-27 18:04:11,446   Batch size = 32
2023-03-27 18:04:11,448 ***** Eval results *****
2023-03-27 18:04:11,449   att_loss = 66090.7532495959
2023-03-27 18:04:11,449   cls_loss = 0.0
2023-03-27 18:04:11,449   global_step = 499
2023-03-27 18:04:11,449   loss = 66091.73065396013
2023-03-27 18:04:11,449   rep_loss = 0.977501534696283
2023-03-27 18:04:11,456 ***** Save model *****
2023-03-27 18:04:27,231 ***** Running evaluation *****
2023-03-27 18:04:27,232   Epoch = 2 iter 549 step
2023-03-27 18:04:27,232   Num examples = 1043
2023-03-27 18:04:27,232   Batch size = 32
2023-03-27 18:04:27,234 ***** Eval results *****
2023-03-27 18:04:27,234   att_loss = 62296.372395833336
2023-03-27 18:04:27,234   cls_loss = 0.0
2023-03-27 18:04:27,234   global_step = 549
2023-03-27 18:04:27,234   loss = 62297.26354166667
2023-03-27 18:04:27,235   rep_loss = 0.8911201238632203
2023-03-27 18:04:27,242 ***** Save model *****
2023-03-27 18:04:43,025 ***** Running evaluation *****
2023-03-27 18:04:43,026   Epoch = 2 iter 599 step
2023-03-27 18:04:43,026   Num examples = 1043
2023-03-27 18:04:43,026   Batch size = 32
2023-03-27 18:04:43,027 ***** Eval results *****
2023-03-27 18:04:43,027   att_loss = 63860.24543269231
2023-03-27 18:04:43,028   cls_loss = 0.0
2023-03-27 18:04:43,028   global_step = 599
2023-03-27 18:04:43,028   loss = 63861.138040865386
2023-03-27 18:04:43,028   rep_loss = 0.892612178509052
2023-03-27 18:04:43,035 ***** Save model *****
2023-03-27 18:04:58,820 ***** Running evaluation *****
2023-03-27 18:04:58,820   Epoch = 2 iter 649 step
2023-03-27 18:04:58,821   Num examples = 1043
2023-03-27 18:04:58,821   Batch size = 32
2023-03-27 18:04:58,822 ***** Eval results *****
2023-03-27 18:04:58,822   att_loss = 63682.23950407609
2023-03-27 18:04:58,822   cls_loss = 0.0
2023-03-27 18:04:58,823   global_step = 649
2023-03-27 18:04:58,823   loss = 63683.122690217395
2023-03-27 18:04:58,823   rep_loss = 0.8832949938981429
2023-03-27 18:04:58,830 ***** Save model *****
2023-03-27 18:05:14,660 ***** Running evaluation *****
2023-03-27 18:05:14,661   Epoch = 2 iter 699 step
2023-03-27 18:05:14,661   Num examples = 1043
2023-03-27 18:05:14,661   Batch size = 32
2023-03-27 18:05:14,662 ***** Eval results *****
2023-03-27 18:05:14,663   att_loss = 63826.36654829545
2023-03-27 18:05:14,663   cls_loss = 0.0
2023-03-27 18:05:14,663   global_step = 699
2023-03-27 18:05:14,663   loss = 63827.24375
2023-03-27 18:05:14,663   rep_loss = 0.8773194742925239
2023-03-27 18:05:14,671 ***** Save model *****
2023-03-27 18:05:30,489 ***** Running evaluation *****
2023-03-27 18:05:30,490   Epoch = 2 iter 749 step
2023-03-27 18:05:30,490   Num examples = 1043
2023-03-27 18:05:30,490   Batch size = 32
2023-03-27 18:05:30,491 ***** Eval results *****
2023-03-27 18:05:30,492   att_loss = 63874.75957485465
2023-03-27 18:05:30,492   cls_loss = 0.0
2023-03-27 18:05:30,492   global_step = 749
2023-03-27 18:05:30,492   loss = 63875.63088662791
2023-03-27 18:05:30,493   rep_loss = 0.8714296795601069
2023-03-27 18:05:30,500 ***** Save model *****
2023-03-27 18:05:46,290 ***** Running evaluation *****
2023-03-27 18:05:46,290   Epoch = 2 iter 799 step
2023-03-27 18:05:46,290   Num examples = 1043
2023-03-27 18:05:46,290   Batch size = 32
2023-03-27 18:05:46,292 ***** Eval results *****
2023-03-27 18:05:46,292   att_loss = 63752.210760613205
2023-03-27 18:05:46,293   cls_loss = 0.0
2023-03-27 18:05:46,293   global_step = 799
2023-03-27 18:05:46,293   loss = 63753.0758254717
2023-03-27 18:05:46,293   rep_loss = 0.865205015326446
2023-03-27 18:05:46,300 ***** Save model *****
2023-03-27 18:06:02,132 ***** Running evaluation *****
2023-03-27 18:06:02,132   Epoch = 3 iter 849 step
2023-03-27 18:06:02,132   Num examples = 1043
2023-03-27 18:06:02,132   Batch size = 32
2023-03-27 18:06:02,133 ***** Eval results *****
2023-03-27 18:06:02,133   att_loss = 61655.83243815104
2023-03-27 18:06:02,133   cls_loss = 0.0
2023-03-27 18:06:02,134   global_step = 849
2023-03-27 18:06:02,134   loss = 61656.65340169271
2023-03-27 18:06:02,134   rep_loss = 0.8206141131619612
2023-03-27 18:06:02,136 ***** Save model *****
2023-03-27 18:06:17,957 ***** Running evaluation *****
2023-03-27 18:06:17,957   Epoch = 3 iter 899 step
2023-03-27 18:06:17,957   Num examples = 1043
2023-03-27 18:06:17,957   Batch size = 32
2023-03-27 18:06:17,959 ***** Eval results *****
2023-03-27 18:06:17,959   att_loss = 61970.60188137755
2023-03-27 18:06:17,959   cls_loss = 0.0
2023-03-27 18:06:17,960   global_step = 899
2023-03-27 18:06:17,960   loss = 61971.41836734694
2023-03-27 18:06:17,960   rep_loss = 0.8162840744670556
2023-03-27 18:06:17,967 ***** Save model *****
2023-03-27 18:06:33,794 ***** Running evaluation *****
2023-03-27 18:06:33,794   Epoch = 3 iter 949 step
2023-03-27 18:06:33,795   Num examples = 1043
2023-03-27 18:06:33,795   Batch size = 32
2023-03-27 18:06:33,796 ***** Eval results *****
2023-03-27 18:06:33,796   att_loss = 62213.92079286317
2023-03-27 18:06:33,797   cls_loss = 0.0
2023-03-27 18:06:33,797   global_step = 949
2023-03-27 18:06:33,797   loss = 62214.73445418074
2023-03-27 18:06:33,797   rep_loss = 0.8135361176084828
2023-03-27 18:06:33,800 ***** Save model *****
2023-03-27 18:06:49,672 ***** Running evaluation *****
2023-03-27 18:06:49,672   Epoch = 3 iter 999 step
2023-03-27 18:06:49,672   Num examples = 1043
2023-03-27 18:06:49,672   Batch size = 32
2023-03-27 18:06:49,674 ***** Eval results *****
2023-03-27 18:06:49,674   att_loss = 62230.571811868685
2023-03-27 18:06:49,675   cls_loss = 0.0
2023-03-27 18:06:49,675   global_step = 999
2023-03-27 18:06:49,675   loss = 62231.38105666036
2023-03-27 18:06:49,675   rep_loss = 0.8091530264025987
2023-03-27 18:06:49,677 ***** Save model *****
2023-03-27 18:07:05,541 ***** Running evaluation *****
2023-03-27 18:07:05,542   Epoch = 3 iter 1049 step
2023-03-27 18:07:05,542   Num examples = 1043
2023-03-27 18:07:05,542   Batch size = 32
2023-03-27 18:07:05,543 ***** Eval results *****
2023-03-27 18:07:05,544   att_loss = 62270.99974798387
2023-03-27 18:07:05,544   cls_loss = 0.0
2023-03-27 18:07:05,544   global_step = 1049
2023-03-27 18:07:05,544   loss = 62271.80456149193
2023-03-27 18:07:05,544   rep_loss = 0.8047607240657653
2023-03-27 18:07:05,551 ***** Save model *****
2023-03-27 18:07:21,406 ***** Running evaluation *****
2023-03-27 18:07:21,406   Epoch = 4 iter 1099 step
2023-03-27 18:07:21,407   Num examples = 1043
2023-03-27 18:07:21,407   Batch size = 32
2023-03-27 18:07:21,408 ***** Eval results *****
2023-03-27 18:07:21,409   att_loss = 62329.516885080644
2023-03-27 18:07:21,409   cls_loss = 0.0
2023-03-27 18:07:21,409   global_step = 1099
2023-03-27 18:07:21,409   loss = 62330.29662298387
2023-03-27 18:07:21,410   rep_loss = 0.7793946958357288
2023-03-27 18:07:21,417 ***** Save model *****
2023-03-27 18:07:37,279 ***** Running evaluation *****
2023-03-27 18:07:37,279   Epoch = 4 iter 1149 step
2023-03-27 18:07:37,279   Num examples = 1043
2023-03-27 18:07:37,279   Batch size = 32
2023-03-27 18:07:37,280 ***** Eval results *****
2023-03-27 18:07:37,280   att_loss = 61915.08420138889
2023-03-27 18:07:37,281   cls_loss = 0.0
2023-03-27 18:07:37,281   global_step = 1149
2023-03-27 18:07:37,281   loss = 61915.86038773148
2023-03-27 18:07:37,281   rep_loss = 0.7762623951758867
2023-03-27 18:07:37,288 ***** Save model *****
2023-03-27 18:07:53,159 ***** Running evaluation *****
2023-03-27 18:07:53,160   Epoch = 4 iter 1199 step
2023-03-27 18:07:53,160   Num examples = 1043
2023-03-27 18:07:53,160   Batch size = 32
2023-03-27 18:07:53,164 ***** Eval results *****
2023-03-27 18:07:53,164   att_loss = 61642.25098401718
2023-03-27 18:07:53,164   cls_loss = 0.0
2023-03-27 18:07:53,164   global_step = 1199
2023-03-27 18:07:53,164   loss = 61643.0221552958
2023-03-27 18:07:53,164   rep_loss = 0.771115017756251
2023-03-27 18:07:53,167 ***** Save model *****
2023-03-27 18:08:09,062 ***** Running evaluation *****
2023-03-27 18:08:09,062   Epoch = 4 iter 1249 step
2023-03-27 18:08:09,063   Num examples = 1043
2023-03-27 18:08:09,063   Batch size = 32
2023-03-27 18:08:09,064 ***** Eval results *****
2023-03-27 18:08:09,064   att_loss = 61440.29726346685
2023-03-27 18:08:09,064   cls_loss = 0.0
2023-03-27 18:08:09,065   global_step = 1249
2023-03-27 18:08:09,065   loss = 61441.0643128453
2023-03-27 18:08:09,065   rep_loss = 0.7669257798247574
2023-03-27 18:08:09,072 ***** Save model *****
2023-03-27 18:08:24,938 ***** Running evaluation *****
2023-03-27 18:08:24,938   Epoch = 4 iter 1299 step
2023-03-27 18:08:24,939   Num examples = 1043
2023-03-27 18:08:24,939   Batch size = 32
2023-03-27 18:08:24,942 ***** Eval results *****
2023-03-27 18:08:24,942   att_loss = 61514.22297754329
2023-03-27 18:08:24,943   cls_loss = 0.0
2023-03-27 18:08:24,943   global_step = 1299
2023-03-27 18:08:24,943   loss = 61514.98738501082
2023-03-27 18:08:24,943   rep_loss = 0.764331015415522
2023-03-27 18:08:24,950 ***** Save model *****
2023-03-27 18:08:40,849 ***** Running evaluation *****
2023-03-27 18:08:40,849   Epoch = 5 iter 1349 step
2023-03-27 18:08:40,849   Num examples = 1043
2023-03-27 18:08:40,849   Batch size = 32
2023-03-27 18:08:40,851 ***** Eval results *****
2023-03-27 18:08:40,851   att_loss = 61310.960658482145
2023-03-27 18:08:40,851   cls_loss = 0.0
2023-03-27 18:08:40,852   global_step = 1349
2023-03-27 18:08:40,852   loss = 61311.704799107145
2023-03-27 18:08:40,852   rep_loss = 0.7443919650145939
2023-03-27 18:08:40,854 ***** Save model *****
2023-03-27 18:08:56,765 ***** Running evaluation *****
2023-03-27 18:08:56,765   Epoch = 5 iter 1399 step
2023-03-27 18:08:56,765   Num examples = 1043
2023-03-27 18:08:56,766   Batch size = 32
2023-03-27 18:08:56,767 ***** Eval results *****
2023-03-27 18:08:56,767   att_loss = 60917.3203125
2023-03-27 18:08:56,767   cls_loss = 0.0
2023-03-27 18:08:56,768   global_step = 1399
2023-03-27 18:08:56,768   loss = 60918.06481933594
2023-03-27 18:08:56,768   rep_loss = 0.7442618440836668
2023-03-27 18:08:56,775 ***** Save model *****
2023-03-27 18:09:12,630 ***** Running evaluation *****
2023-03-27 18:09:12,630   Epoch = 5 iter 1449 step
2023-03-27 18:09:12,630   Num examples = 1043
2023-03-27 18:09:12,631   Batch size = 32
2023-03-27 18:09:12,631 ***** Eval results *****
2023-03-27 18:09:12,632   att_loss = 60754.990885416664
2023-03-27 18:09:12,632   cls_loss = 0.0
2023-03-27 18:09:12,632   global_step = 1449
2023-03-27 18:09:12,632   loss = 60755.73317571272
2023-03-27 18:09:12,632   rep_loss = 0.7420619286988911
2023-03-27 18:09:12,635 ***** Save model *****
2023-03-27 18:09:28,519 ***** Running evaluation *****
2023-03-27 18:09:28,519   Epoch = 5 iter 1499 step
2023-03-27 18:09:28,519   Num examples = 1043
2023-03-27 18:09:28,520   Batch size = 32
2023-03-27 18:09:28,521 ***** Eval results *****
2023-03-27 18:09:28,521   att_loss = 60788.427567644816
2023-03-27 18:09:28,522   cls_loss = 0.0
2023-03-27 18:09:28,522   global_step = 1499
2023-03-27 18:09:28,522   loss = 60789.167349466465
2023-03-27 18:09:28,522   rep_loss = 0.7395858699228706
2023-03-27 18:09:28,529 ***** Save model *****
2023-03-27 18:09:44,423 ***** Running evaluation *****
2023-03-27 18:09:44,424   Epoch = 5 iter 1549 step
2023-03-27 18:09:44,424   Num examples = 1043
2023-03-27 18:09:44,424   Batch size = 32
2023-03-27 18:09:44,427 ***** Eval results *****
2023-03-27 18:09:44,428   att_loss = 60745.610762266355
2023-03-27 18:09:44,428   cls_loss = 0.0
2023-03-27 18:09:44,428   global_step = 1549
2023-03-27 18:09:44,428   loss = 60746.34794830607
2023-03-27 18:09:44,429   rep_loss = 0.7369899655056891
2023-03-27 18:09:44,435 ***** Save model *****
2023-03-27 18:10:00,316 ***** Running evaluation *****
2023-03-27 18:10:00,316   Epoch = 5 iter 1599 step
2023-03-27 18:10:00,316   Num examples = 1043
2023-03-27 18:10:00,317   Batch size = 32
2023-03-27 18:10:00,318 ***** Eval results *****
2023-03-27 18:10:00,318   att_loss = 60710.90263967803
2023-03-27 18:10:00,319   cls_loss = 0.0
2023-03-27 18:10:00,319   global_step = 1599
2023-03-27 18:10:00,319   loss = 60711.63776929451
2023-03-27 18:10:00,319   rep_loss = 0.7349957629586711
2023-03-27 18:10:00,322 ***** Save model *****
2023-03-27 18:10:16,212 ***** Running evaluation *****
2023-03-27 18:10:16,213   Epoch = 6 iter 1649 step
2023-03-27 18:10:16,213   Num examples = 1043
2023-03-27 18:10:16,213   Batch size = 32
2023-03-27 18:10:16,214 ***** Eval results *****
2023-03-27 18:10:16,214   att_loss = 59786.93484042553
2023-03-27 18:10:16,215   cls_loss = 0.0
2023-03-27 18:10:16,215   global_step = 1649
2023-03-27 18:10:16,215   loss = 59787.65325797872
2023-03-27 18:10:16,215   rep_loss = 0.7180915665119252
2023-03-27 18:10:16,222 ***** Save model *****
2023-03-27 18:10:32,107 ***** Running evaluation *****
2023-03-27 18:10:32,107   Epoch = 6 iter 1699 step
2023-03-27 18:10:32,107   Num examples = 1043
2023-03-27 18:10:32,108   Batch size = 32
2023-03-27 18:10:32,109 ***** Eval results *****
2023-03-27 18:10:32,110   att_loss = 60027.76349065721
2023-03-27 18:10:32,110   cls_loss = 0.0
2023-03-27 18:10:32,110   global_step = 1699
2023-03-27 18:10:32,110   loss = 60028.48268363402
2023-03-27 18:10:32,111   rep_loss = 0.7191294757361265
2023-03-27 18:10:32,118 ***** Save model *****
2023-03-27 18:10:48,018 ***** Running evaluation *****
2023-03-27 18:10:48,018   Epoch = 6 iter 1749 step
2023-03-27 18:10:48,018   Num examples = 1043
2023-03-27 18:10:48,019   Batch size = 32
2023-03-27 18:10:48,020 ***** Eval results *****
2023-03-27 18:10:48,020   att_loss = 60043.24481823979
2023-03-27 18:10:48,020   cls_loss = 0.0
2023-03-27 18:10:48,021   global_step = 1749
2023-03-27 18:10:48,021   loss = 60043.96308992347
2023-03-27 18:10:48,021   rep_loss = 0.7182949193480874
2023-03-27 18:10:48,028 ***** Save model *****
2023-03-27 18:11:03,931 ***** Running evaluation *****
2023-03-27 18:11:03,932   Epoch = 6 iter 1799 step
2023-03-27 18:11:03,932   Num examples = 1043
2023-03-27 18:11:03,932   Batch size = 32
2023-03-27 18:11:03,933 ***** Eval results *****
2023-03-27 18:11:03,933   att_loss = 60007.7052268401
2023-03-27 18:11:03,934   cls_loss = 0.0
2023-03-27 18:11:03,934   global_step = 1799
2023-03-27 18:11:03,934   loss = 60008.42189482868
2023-03-27 18:11:03,934   rep_loss = 0.7167177890157942
2023-03-27 18:11:03,936 ***** Save model *****
2023-03-27 18:11:19,807 ***** Running evaluation *****
2023-03-27 18:11:19,807   Epoch = 6 iter 1849 step
2023-03-27 18:11:19,808   Num examples = 1043
2023-03-27 18:11:19,808   Batch size = 32
2023-03-27 18:11:19,809 ***** Eval results *****
2023-03-27 18:11:19,809   att_loss = 60065.05421305668
2023-03-27 18:11:19,809   cls_loss = 0.0
2023-03-27 18:11:19,809   global_step = 1849
2023-03-27 18:11:19,810   loss = 60065.76853491903
2023-03-27 18:11:19,810   rep_loss = 0.7143831822553627
2023-03-27 18:11:19,817 ***** Save model *****
2023-03-27 18:11:35,729 ***** Running evaluation *****
2023-03-27 18:11:35,730   Epoch = 7 iter 1899 step
2023-03-27 18:11:35,730   Num examples = 1043
2023-03-27 18:11:35,730   Batch size = 32
2023-03-27 18:11:35,732 ***** Eval results *****
2023-03-27 18:11:35,732   att_loss = 58909.61770833333
2023-03-27 18:11:35,733   cls_loss = 0.0
2023-03-27 18:11:35,733   global_step = 1899
2023-03-27 18:11:35,733   loss = 58910.316145833334
2023-03-27 18:11:35,733   rep_loss = 0.6986500740051269
2023-03-27 18:11:35,740 ***** Save model *****
2023-03-27 18:11:52,142 ***** Running evaluation *****
2023-03-27 18:11:52,142   Epoch = 7 iter 1949 step
2023-03-27 18:11:52,143   Num examples = 1043
2023-03-27 18:11:52,143   Batch size = 32
2023-03-27 18:11:52,144 ***** Eval results *****
2023-03-27 18:11:52,144   att_loss = 59382.23286132813
2023-03-27 18:11:52,144   cls_loss = 0.0
2023-03-27 18:11:52,145   global_step = 1949
2023-03-27 18:11:52,145   loss = 59382.93212890625
2023-03-27 18:11:52,145   rep_loss = 0.6994230128824711
2023-03-27 18:11:52,147 ***** Save model *****
2023-03-27 18:12:08,045 ***** Running evaluation *****
2023-03-27 18:12:08,045   Epoch = 7 iter 1999 step
2023-03-27 18:12:08,045   Num examples = 1043
2023-03-27 18:12:08,045   Batch size = 32
2023-03-27 18:12:08,047 ***** Eval results *****
2023-03-27 18:12:08,047   att_loss = 59635.662740384614
2023-03-27 18:12:08,047   cls_loss = 0.0
2023-03-27 18:12:08,047   global_step = 1999
2023-03-27 18:12:08,048   loss = 59636.36256009615
2023-03-27 18:12:08,048   rep_loss = 0.6999905393673823
2023-03-27 18:12:08,055 ***** Save model *****
2023-03-27 18:12:23,966 ***** Running evaluation *****
2023-03-27 18:12:23,966   Epoch = 7 iter 2049 step
2023-03-27 18:12:23,966   Num examples = 1043
2023-03-27 18:12:23,966   Batch size = 32
2023-03-27 18:12:23,967 ***** Eval results *****
2023-03-27 18:12:23,968   att_loss = 59706.98409288195
2023-03-27 18:12:23,968   cls_loss = 0.0
2023-03-27 18:12:23,968   global_step = 2049
2023-03-27 18:12:23,968   loss = 59707.68413628472
2023-03-27 18:12:23,968   rep_loss = 0.700262976023886
2023-03-27 18:12:23,976 ***** Save model *****
2023-03-27 18:12:39,866 ***** Running evaluation *****
2023-03-27 18:12:39,866   Epoch = 7 iter 2099 step
2023-03-27 18:12:39,866   Num examples = 1043
2023-03-27 18:12:39,867   Batch size = 32
2023-03-27 18:12:39,868 ***** Eval results *****
2023-03-27 18:12:39,869   att_loss = 59688.302055027176
2023-03-27 18:12:39,869   cls_loss = 0.0
2023-03-27 18:12:39,869   global_step = 2099
2023-03-27 18:12:39,869   loss = 59689.00105298913
2023-03-27 18:12:39,869   rep_loss = 0.6992113914178766
2023-03-27 18:12:39,877 ***** Save model *****
2023-03-27 18:12:55,767 ***** Running evaluation *****
2023-03-27 18:12:55,768   Epoch = 8 iter 2149 step
2023-03-27 18:12:55,768   Num examples = 1043
2023-03-27 18:12:55,768   Batch size = 32
2023-03-27 18:12:55,770 ***** Eval results *****
2023-03-27 18:12:55,770   att_loss = 59012.353966346156
2023-03-27 18:12:55,770   cls_loss = 0.0
2023-03-27 18:12:55,770   global_step = 2149
2023-03-27 18:12:55,771   loss = 59013.043870192305
2023-03-27 18:12:55,771   rep_loss = 0.6903510322937598
2023-03-27 18:12:55,778 ***** Save model *****
2023-03-27 18:13:11,666 ***** Running evaluation *****
2023-03-27 18:13:11,666   Epoch = 8 iter 2199 step
2023-03-27 18:13:11,666   Num examples = 1043
2023-03-27 18:13:11,666   Batch size = 32
2023-03-27 18:13:11,668 ***** Eval results *****
2023-03-27 18:13:11,668   att_loss = 58928.318452380954
2023-03-27 18:13:11,668   cls_loss = 0.0
2023-03-27 18:13:11,669   global_step = 2199
2023-03-27 18:13:11,669   loss = 58929.00570436508
2023-03-27 18:13:11,669   rep_loss = 0.6873630285263062
2023-03-27 18:13:11,676 ***** Save model *****
2023-03-27 18:13:27,606 ***** Running evaluation *****
2023-03-27 18:13:27,606   Epoch = 8 iter 2249 step
2023-03-27 18:13:27,606   Num examples = 1043
2023-03-27 18:13:27,606   Batch size = 32
2023-03-27 18:13:27,607 ***** Eval results *****
2023-03-27 18:13:27,608   att_loss = 59018.50044939159
2023-03-27 18:13:27,608   cls_loss = 0.0
2023-03-27 18:13:27,608   global_step = 2249
2023-03-27 18:13:27,608   loss = 59019.1890210177
2023-03-27 18:13:27,608   rep_loss = 0.68857695993069
2023-03-27 18:13:27,610 ***** Save model *****
2023-03-27 18:13:43,526 ***** Running evaluation *****
2023-03-27 18:13:43,526   Epoch = 8 iter 2299 step
2023-03-27 18:13:43,526   Num examples = 1043
2023-03-27 18:13:43,526   Batch size = 32
2023-03-27 18:13:43,528 ***** Eval results *****
2023-03-27 18:13:43,528   att_loss = 58834.83382860429
2023-03-27 18:13:43,529   cls_loss = 0.0
2023-03-27 18:13:43,529   global_step = 2299
2023-03-27 18:13:43,529   loss = 58835.520921203984
2023-03-27 18:13:43,529   rep_loss = 0.6870436664739269
2023-03-27 18:13:43,536 ***** Save model *****
2023-03-27 18:13:59,423 ***** Running evaluation *****
2023-03-27 18:13:59,424   Epoch = 8 iter 2349 step
2023-03-27 18:13:59,424   Num examples = 1043
2023-03-27 18:13:59,424   Batch size = 32
2023-03-27 18:13:59,426 ***** Eval results *****
2023-03-27 18:13:59,426   att_loss = 58852.93867370892
2023-03-27 18:13:59,426   cls_loss = 0.0
2023-03-27 18:13:59,426   global_step = 2349
2023-03-27 18:13:59,426   loss = 58853.62421141432
2023-03-27 18:13:59,427   rep_loss = 0.685498617064785
2023-03-27 18:13:59,429 ***** Save model *****
2023-03-27 18:14:15,336 ***** Running evaluation *****
2023-03-27 18:14:15,336   Epoch = 8 iter 2399 step
2023-03-27 18:14:15,336   Num examples = 1043
2023-03-27 18:14:15,336   Batch size = 32
2023-03-27 18:14:15,338 ***** Eval results *****
2023-03-27 18:14:15,338   att_loss = 59022.67979146863
2023-03-27 18:14:15,338   cls_loss = 0.0
2023-03-27 18:14:15,338   global_step = 2399
2023-03-27 18:14:15,339   loss = 59023.36495960076
2023-03-27 18:14:15,339   rep_loss = 0.6851414865867267
2023-03-27 18:14:15,346 ***** Save model *****
2023-03-27 18:14:31,227 ***** Running evaluation *****
2023-03-27 18:14:31,227   Epoch = 9 iter 2449 step
2023-03-27 18:14:31,227   Num examples = 1043
2023-03-27 18:14:31,227   Batch size = 32
2023-03-27 18:14:31,228 ***** Eval results *****
2023-03-27 18:14:31,228   att_loss = 58303.533882472824
2023-03-27 18:14:31,229   cls_loss = 0.0
2023-03-27 18:14:31,229   global_step = 2449
2023-03-27 18:14:31,229   loss = 58304.211786684784
2023-03-27 18:14:31,229   rep_loss = 0.6776113665622213
2023-03-27 18:14:31,237 ***** Save model *****
2023-03-27 18:14:47,136 ***** Running evaluation *****
2023-03-27 18:14:47,136   Epoch = 9 iter 2499 step
2023-03-27 18:14:47,136   Num examples = 1043
2023-03-27 18:14:47,136   Batch size = 32
2023-03-27 18:14:47,138 ***** Eval results *****
2023-03-27 18:14:47,138   att_loss = 58755.935709635414
2023-03-27 18:14:47,138   cls_loss = 0.0
2023-03-27 18:14:47,138   global_step = 2499
2023-03-27 18:14:47,138   loss = 58756.613037109375
2023-03-27 18:14:47,139   rep_loss = 0.6771719592312971
2023-03-27 18:14:47,146 ***** Save model *****
2023-03-27 18:15:03,039 ***** Running evaluation *****
2023-03-27 18:15:03,039   Epoch = 9 iter 2549 step
2023-03-27 18:15:03,039   Num examples = 1043
2023-03-27 18:15:03,039   Batch size = 32
2023-03-27 18:15:03,040 ***** Eval results *****
2023-03-27 18:15:03,041   att_loss = 58709.52547089041
2023-03-27 18:15:03,041   cls_loss = 0.0
2023-03-27 18:15:03,041   global_step = 2549
2023-03-27 18:15:03,041   loss = 58710.20192101884
2023-03-27 18:15:03,041   rep_loss = 0.6763283904284647
2023-03-27 18:15:03,043 ***** Save model *****
2023-03-27 18:15:18,917 ***** Running evaluation *****
2023-03-27 18:15:18,917   Epoch = 9 iter 2599 step
2023-03-27 18:15:18,917   Num examples = 1043
2023-03-27 18:15:18,918   Batch size = 32
2023-03-27 18:15:18,919 ***** Eval results *****
2023-03-27 18:15:18,920   att_loss = 58777.605707908166
2023-03-27 18:15:18,920   cls_loss = 0.0
2023-03-27 18:15:18,921   global_step = 2599
2023-03-27 18:15:18,921   loss = 58778.28138950893
2023-03-27 18:15:18,922   rep_loss = 0.6755705573120896
2023-03-27 18:15:18,930 ***** Save model *****
2023-03-27 18:15:34,828 ***** Running evaluation *****
2023-03-27 18:15:34,828   Epoch = 9 iter 2649 step
2023-03-27 18:15:34,828   Num examples = 1043
2023-03-27 18:15:34,829   Batch size = 32
2023-03-27 18:15:34,830 ***** Eval results *****
2023-03-27 18:15:34,830   att_loss = 58655.959857723574
2023-03-27 18:15:34,830   cls_loss = 0.0
2023-03-27 18:15:34,830   global_step = 2649
2023-03-27 18:15:34,830   loss = 58656.634559197155
2023-03-27 18:15:34,830   rep_loss = 0.6746115151459608
2023-03-27 18:15:34,838 ***** Save model *****
2023-03-27 18:15:50,756 ***** Running evaluation *****
2023-03-27 18:15:50,756   Epoch = 10 iter 2699 step
2023-03-27 18:15:50,756   Num examples = 1043
2023-03-27 18:15:50,756   Batch size = 32
2023-03-27 18:15:50,758 ***** Eval results *****
2023-03-27 18:15:50,758   att_loss = 58752.31223060345
2023-03-27 18:15:50,759   cls_loss = 0.0
2023-03-27 18:15:50,759   global_step = 2699
2023-03-27 18:15:50,759   loss = 58752.98141163793
2023-03-27 18:15:50,759   rep_loss = 0.6689566784891589
2023-03-27 18:15:50,766 ***** Save model *****
2023-03-27 18:16:06,684 ***** Running evaluation *****
2023-03-27 18:16:06,685   Epoch = 10 iter 2749 step
2023-03-27 18:16:06,685   Num examples = 1043
2023-03-27 18:16:06,685   Batch size = 32
2023-03-27 18:16:06,686 ***** Eval results *****
2023-03-27 18:16:06,687   att_loss = 58351.0507318038
2023-03-27 18:16:06,687   cls_loss = 0.0
2023-03-27 18:16:06,687   global_step = 2749
2023-03-27 18:16:06,687   loss = 58351.719640031646
2023-03-27 18:16:06,687   rep_loss = 0.6688004490695422
2023-03-27 18:16:06,695 ***** Save model *****
2023-03-27 18:16:22,602 ***** Running evaluation *****
2023-03-27 18:16:22,602   Epoch = 10 iter 2799 step
2023-03-27 18:16:22,602   Num examples = 1043
2023-03-27 18:16:22,602   Batch size = 32
2023-03-27 18:16:22,603 ***** Eval results *****
2023-03-27 18:16:22,604   att_loss = 58109.57209907946
2023-03-27 18:16:22,604   cls_loss = 0.0
2023-03-27 18:16:22,604   global_step = 2799
2023-03-27 18:16:22,604   loss = 58110.239855862405
2023-03-27 18:16:22,604   rep_loss = 0.6676837880482045
2023-03-27 18:16:22,606 ***** Save model *****
2023-03-27 18:16:38,515 ***** Running evaluation *****
2023-03-27 18:16:38,516   Epoch = 10 iter 2849 step
2023-03-27 18:16:38,516   Num examples = 1043
2023-03-27 18:16:38,516   Batch size = 32
2023-03-27 18:16:38,517 ***** Eval results *****
2023-03-27 18:16:38,517   att_loss = 58385.78251571229
2023-03-27 18:16:38,517   cls_loss = 0.0
2023-03-27 18:16:38,518   global_step = 2849
2023-03-27 18:16:38,518   loss = 58386.450004364524
2023-03-27 18:16:38,518   rep_loss = 0.6674674912537942
2023-03-27 18:16:38,520 ***** Save model *****
2023-03-27 18:16:54,414 ***** Running evaluation *****
2023-03-27 18:16:54,414   Epoch = 10 iter 2899 step
2023-03-27 18:16:54,414   Num examples = 1043
2023-03-27 18:16:54,414   Batch size = 32
2023-03-27 18:16:54,416 ***** Eval results *****
2023-03-27 18:16:54,416   att_loss = 58356.05347639192
2023-03-27 18:16:54,417   cls_loss = 0.0
2023-03-27 18:16:54,417   global_step = 2899
2023-03-27 18:16:54,417   loss = 58356.719824645195
2023-03-27 18:16:54,417   rep_loss = 0.6663521465776269
2023-03-27 18:16:54,425 ***** Save model *****
2023-03-27 18:17:10,346 ***** Running evaluation *****
2023-03-27 18:17:10,346   Epoch = 11 iter 2949 step
2023-03-27 18:17:10,346   Num examples = 1043
2023-03-27 18:17:10,347   Batch size = 32
2023-03-27 18:17:10,348 ***** Eval results *****
2023-03-27 18:17:10,348   att_loss = 58624.925455729164
2023-03-27 18:17:10,349   cls_loss = 0.0
2023-03-27 18:17:10,349   global_step = 2949
2023-03-27 18:17:10,349   loss = 58625.58984375
2023-03-27 18:17:10,349   rep_loss = 0.664642850557963
2023-03-27 18:17:10,359 ***** Save model *****
2023-03-27 18:17:26,296 ***** Running evaluation *****
2023-03-27 18:17:26,297   Epoch = 11 iter 2999 step
2023-03-27 18:17:26,297   Num examples = 1043
2023-03-27 18:17:26,297   Batch size = 32
2023-03-27 18:17:26,299 ***** Eval results *****
2023-03-27 18:17:26,299   att_loss = 58006.99584173387
2023-03-27 18:17:26,299   cls_loss = 0.0
2023-03-27 18:17:26,300   global_step = 2999
2023-03-27 18:17:26,300   loss = 58007.655367943546
2023-03-27 18:17:26,300   rep_loss = 0.6594741998180267
2023-03-27 18:17:26,307 ***** Save model *****
2023-03-27 18:17:42,230 ***** Running evaluation *****
2023-03-27 18:17:42,231   Epoch = 11 iter 3049 step
2023-03-27 18:17:42,231   Num examples = 1043
2023-03-27 18:17:42,231   Batch size = 32
2023-03-27 18:17:42,232 ***** Eval results *****
2023-03-27 18:17:42,233   att_loss = 58204.309291294645
2023-03-27 18:17:42,233   cls_loss = 0.0
2023-03-27 18:17:42,233   global_step = 3049
2023-03-27 18:17:42,233   loss = 58204.96976143973
2023-03-27 18:17:42,234   rep_loss = 0.6604776909308774
2023-03-27 18:17:42,241 ***** Save model *****
2023-03-27 18:17:58,214 ***** Running evaluation *****
2023-03-27 18:17:58,214   Epoch = 11 iter 3099 step
2023-03-27 18:17:58,215   Num examples = 1043
2023-03-27 18:17:58,215   Batch size = 32
2023-03-27 18:17:58,216 ***** Eval results *****
2023-03-27 18:17:58,217   att_loss = 58150.8138744213
2023-03-27 18:17:58,217   cls_loss = 0.0
2023-03-27 18:17:58,217   global_step = 3099
2023-03-27 18:17:58,217   loss = 58151.4735001929
2023-03-27 18:17:58,218   rep_loss = 0.6595963990246808
2023-03-27 18:17:58,225 ***** Save model *****
2023-03-27 18:18:14,167 ***** Running evaluation *****
2023-03-27 18:18:14,167   Epoch = 11 iter 3149 step
2023-03-27 18:18:14,167   Num examples = 1043
2023-03-27 18:18:14,168   Batch size = 32
2023-03-27 18:18:14,170 ***** Eval results *****
2023-03-27 18:18:14,170   att_loss = 58131.51811247052
2023-03-27 18:18:14,171   cls_loss = 0.0
2023-03-27 18:18:14,171   global_step = 3149
2023-03-27 18:18:14,171   loss = 58132.17729215802
2023-03-27 18:18:14,171   rep_loss = 0.6590981146074691
2023-03-27 18:18:14,178 ***** Save model *****
2023-03-27 18:18:30,101 ***** Running evaluation *****
2023-03-27 18:18:30,101   Epoch = 11 iter 3199 step
2023-03-27 18:18:30,102   Num examples = 1043
2023-03-27 18:18:30,102   Batch size = 32
2023-03-27 18:18:30,104 ***** Eval results *****
2023-03-27 18:18:30,104   att_loss = 58110.648392771946
2023-03-27 18:18:30,104   cls_loss = 0.0
2023-03-27 18:18:30,104   global_step = 3199
2023-03-27 18:18:30,104   loss = 58111.30714754294
2023-03-27 18:18:30,104   rep_loss = 0.6586664027840127
2023-03-27 18:18:30,111 ***** Save model *****
2023-03-27 18:18:46,030 ***** Running evaluation *****
2023-03-27 18:18:46,031   Epoch = 12 iter 3249 step
2023-03-27 18:18:46,031   Num examples = 1043
2023-03-27 18:18:46,031   Batch size = 32
2023-03-27 18:18:46,032 ***** Eval results *****
2023-03-27 18:18:46,032   att_loss = 57756.63446180556
2023-03-27 18:18:46,033   cls_loss = 0.0
2023-03-27 18:18:46,033   global_step = 3249
2023-03-27 18:18:46,033   loss = 57757.28723958333
2023-03-27 18:18:46,033   rep_loss = 0.6528308471043904
2023-03-27 18:18:46,036 ***** Save model *****
2023-03-27 18:19:01,995 ***** Running evaluation *****
2023-03-27 18:19:01,995   Epoch = 12 iter 3299 step
2023-03-27 18:19:01,995   Num examples = 1043
2023-03-27 18:19:01,995   Batch size = 32
2023-03-27 18:19:01,997 ***** Eval results *****
2023-03-27 18:19:01,997   att_loss = 58101.07956414474
2023-03-27 18:19:01,997   cls_loss = 0.0
2023-03-27 18:19:01,997   global_step = 3299
2023-03-27 18:19:01,997   loss = 58101.73355263158
2023-03-27 18:19:01,997   rep_loss = 0.6541447965722335
2023-03-27 18:19:02,000 ***** Save model *****
2023-03-27 18:19:17,890 ***** Running evaluation *****
2023-03-27 18:19:17,890   Epoch = 12 iter 3349 step
2023-03-27 18:19:17,890   Num examples = 1043
2023-03-27 18:19:17,890   Batch size = 32
2023-03-27 18:19:17,892 ***** Eval results *****
2023-03-27 18:19:17,892   att_loss = 58045.9275862069
2023-03-27 18:19:17,892   cls_loss = 0.0
2023-03-27 18:19:17,893   global_step = 3349
2023-03-27 18:19:17,893   loss = 58046.580765086204
2023-03-27 18:19:17,893   rep_loss = 0.6532355082446131
2023-03-27 18:19:17,900 ***** Save model *****
2023-03-27 18:19:33,825 ***** Running evaluation *****
2023-03-27 18:19:33,826   Epoch = 12 iter 3399 step
2023-03-27 18:19:33,826   Num examples = 1043
2023-03-27 18:19:33,826   Batch size = 32
2023-03-27 18:19:33,827 ***** Eval results *****
2023-03-27 18:19:33,827   att_loss = 57905.380348557694
2023-03-27 18:19:33,827   cls_loss = 0.0
2023-03-27 18:19:33,827   global_step = 3399
2023-03-27 18:19:33,828   loss = 57906.03331330128
2023-03-27 18:19:33,828   rep_loss = 0.6529452981092991
2023-03-27 18:19:33,834 ***** Save model *****
2023-03-27 18:19:49,746 ***** Running evaluation *****
2023-03-27 18:19:49,746   Epoch = 12 iter 3449 step
2023-03-27 18:19:49,746   Num examples = 1043
2023-03-27 18:19:49,746   Batch size = 32
2023-03-27 18:19:49,747 ***** Eval results *****
2023-03-27 18:19:49,748   att_loss = 57841.91986607143
2023-03-27 18:19:49,748   cls_loss = 0.0
2023-03-27 18:19:49,748   global_step = 3449
2023-03-27 18:19:49,748   loss = 57842.57275191326
2023-03-27 18:19:49,748   rep_loss = 0.6528829112344858
2023-03-27 18:19:49,755 ***** Save model *****
2023-03-27 18:20:05,673 ***** Running evaluation *****
2023-03-27 18:20:05,673   Epoch = 13 iter 3499 step
2023-03-27 18:20:05,673   Num examples = 1043
2023-03-27 18:20:05,673   Batch size = 32
2023-03-27 18:20:05,674 ***** Eval results *****
2023-03-27 18:20:05,675   att_loss = 58379.591099330355
2023-03-27 18:20:05,675   cls_loss = 0.0
2023-03-27 18:20:05,675   global_step = 3499
2023-03-27 18:20:05,675   loss = 58380.2421875
2023-03-27 18:20:05,675   rep_loss = 0.6510994540793555
2023-03-27 18:20:05,677 ***** Save model *****
2023-03-27 18:20:21,585 ***** Running evaluation *****
2023-03-27 18:20:21,585   Epoch = 13 iter 3549 step
2023-03-27 18:20:21,585   Num examples = 1043
2023-03-27 18:20:21,585   Batch size = 32
2023-03-27 18:20:21,586 ***** Eval results *****
2023-03-27 18:20:21,586   att_loss = 57607.585286458336
2023-03-27 18:20:21,587   cls_loss = 0.0
2023-03-27 18:20:21,587   global_step = 3549
2023-03-27 18:20:21,587   loss = 57608.232972756414
2023-03-27 18:20:21,587   rep_loss = 0.6478755397674365
2023-03-27 18:20:21,594 ***** Save model *****
2023-03-27 18:20:37,493 ***** Running evaluation *****
2023-03-27 18:20:37,493   Epoch = 13 iter 3599 step
2023-03-27 18:20:37,494   Num examples = 1043
2023-03-27 18:20:37,494   Batch size = 32
2023-03-27 18:20:37,495 ***** Eval results *****
2023-03-27 18:20:37,495   att_loss = 57539.988342285156
2023-03-27 18:20:37,496   cls_loss = 0.0
2023-03-27 18:20:37,496   global_step = 3599
2023-03-27 18:20:37,496   loss = 57540.63427734375
2023-03-27 18:20:37,496   rep_loss = 0.6461333986371756
2023-03-27 18:20:37,503 ***** Save model *****
2023-03-27 18:20:53,864 ***** Running evaluation *****
2023-03-27 18:20:53,864   Epoch = 13 iter 3649 step
2023-03-27 18:20:53,864   Num examples = 1043
2023-03-27 18:20:53,865   Batch size = 32
2023-03-27 18:20:53,868 ***** Eval results *****
2023-03-27 18:20:53,868   att_loss = 57438.84572507023
2023-03-27 18:20:53,868   cls_loss = 0.0
2023-03-27 18:20:53,868   global_step = 3649
2023-03-27 18:20:53,869   loss = 57439.491090238764
2023-03-27 18:20:53,869   rep_loss = 0.6455202996730804
2023-03-27 18:20:53,872 ***** Save model *****
2023-03-27 18:21:09,751 ***** Running evaluation *****
2023-03-27 18:21:09,751   Epoch = 13 iter 3699 step
2023-03-27 18:21:09,751   Num examples = 1043
2023-03-27 18:21:09,752   Batch size = 32
2023-03-27 18:21:09,753 ***** Eval results *****
2023-03-27 18:21:09,753   att_loss = 57445.44281112939
2023-03-27 18:21:09,754   cls_loss = 0.0
2023-03-27 18:21:09,754   global_step = 3699
2023-03-27 18:21:09,754   loss = 57446.08809621711
2023-03-27 18:21:09,754   rep_loss = 0.6454012613547476
2023-03-27 18:21:09,761 ***** Save model *****
2023-03-27 18:21:25,675 ***** Running evaluation *****
2023-03-27 18:21:25,676   Epoch = 14 iter 3749 step
2023-03-27 18:21:25,676   Num examples = 1043
2023-03-27 18:21:25,676   Batch size = 32
2023-03-27 18:21:25,677 ***** Eval results *****
2023-03-27 18:21:25,678   att_loss = 56235.87926136364
2023-03-27 18:21:25,678   cls_loss = 0.0
2023-03-27 18:21:25,678   global_step = 3749
2023-03-27 18:21:25,678   loss = 56236.51953125
2023-03-27 18:21:25,678   rep_loss = 0.6404730677604675
2023-03-27 18:21:25,680 ***** Save model *****
2023-03-27 18:21:41,587 ***** Running evaluation *****
2023-03-27 18:21:41,587   Epoch = 14 iter 3799 step
2023-03-27 18:21:41,587   Num examples = 1043
2023-03-27 18:21:41,587   Batch size = 32
2023-03-27 18:21:41,589 ***** Eval results *****
2023-03-27 18:21:41,589   att_loss = 57247.22508965164
2023-03-27 18:21:41,589   cls_loss = 0.0
2023-03-27 18:21:41,589   global_step = 3799
2023-03-27 18:21:41,590   loss = 57247.86737961065
2023-03-27 18:21:41,590   rep_loss = 0.6422057063853155
2023-03-27 18:21:41,592 ***** Save model *****
2023-03-27 18:21:57,484 ***** Running evaluation *****
2023-03-27 18:21:57,485   Epoch = 14 iter 3849 step
2023-03-27 18:21:57,485   Num examples = 1043
2023-03-27 18:21:57,485   Batch size = 32
2023-03-27 18:21:57,486 ***** Eval results *****
2023-03-27 18:21:57,486   att_loss = 57060.573198198195
2023-03-27 18:21:57,486   cls_loss = 0.0
2023-03-27 18:21:57,487   global_step = 3849
2023-03-27 18:21:57,487   loss = 57061.21480855856
2023-03-27 18:21:57,487   rep_loss = 0.6415261996758951
2023-03-27 18:21:57,494 ***** Save model *****
2023-03-27 18:22:13,421 ***** Running evaluation *****
2023-03-27 18:22:13,421   Epoch = 14 iter 3899 step
2023-03-27 18:22:13,422   Num examples = 1043
2023-03-27 18:22:13,422   Batch size = 32
2023-03-27 18:22:13,423 ***** Eval results *****
2023-03-27 18:22:13,424   att_loss = 57317.99529309006
2023-03-27 18:22:13,424   cls_loss = 0.0
2023-03-27 18:22:13,424   global_step = 3899
2023-03-27 18:22:13,424   loss = 57318.638392857145
2023-03-27 18:22:13,424   rep_loss = 0.6430355469632593
2023-03-27 18:22:13,431 ***** Save model *****
2023-03-27 18:22:29,343 ***** Running evaluation *****
2023-03-27 18:22:29,343   Epoch = 14 iter 3949 step
2023-03-27 18:22:29,343   Num examples = 1043
2023-03-27 18:22:29,343   Batch size = 32
2023-03-27 18:22:29,345 ***** Eval results *****
2023-03-27 18:22:29,345   att_loss = 57413.03758145735
2023-03-27 18:22:29,345   cls_loss = 0.0
2023-03-27 18:22:29,345   global_step = 3949
2023-03-27 18:22:29,345   loss = 57413.68020586493
2023-03-27 18:22:29,346   rep_loss = 0.6425544348373232
2023-03-27 18:22:29,353 ***** Save model *****
2023-03-27 18:22:45,250 ***** Running evaluation *****
2023-03-27 18:22:45,250   Epoch = 14 iter 3999 step
2023-03-27 18:22:45,250   Num examples = 1043
2023-03-27 18:22:45,250   Batch size = 32
2023-03-27 18:22:45,251 ***** Eval results *****
2023-03-27 18:22:45,252   att_loss = 57389.29049928161
2023-03-27 18:22:45,252   cls_loss = 0.0
2023-03-27 18:22:45,252   global_step = 3999
2023-03-27 18:22:45,252   loss = 57389.932381465514
2023-03-27 18:22:45,252   rep_loss = 0.6418620820703178
2023-03-27 18:22:45,259 ***** Save model *****
2023-03-27 18:23:01,185 ***** Running evaluation *****
2023-03-27 18:23:01,186   Epoch = 15 iter 4049 step
2023-03-27 18:23:01,186   Num examples = 1043
2023-03-27 18:23:01,186   Batch size = 32
2023-03-27 18:23:01,187 ***** Eval results *****
2023-03-27 18:23:01,187   att_loss = 56648.16610440341
2023-03-27 18:23:01,187   cls_loss = 0.0
2023-03-27 18:23:01,187   global_step = 4049
2023-03-27 18:23:01,187   loss = 56648.802734375
2023-03-27 18:23:01,187   rep_loss = 0.6365350539034064
2023-03-27 18:23:01,194 ***** Save model *****
2023-03-27 18:23:17,088 ***** Running evaluation *****
2023-03-27 18:23:17,089   Epoch = 15 iter 4099 step
2023-03-27 18:23:17,089   Num examples = 1043
2023-03-27 18:23:17,089   Batch size = 32
2023-03-27 18:23:17,091 ***** Eval results *****
2023-03-27 18:23:17,091   att_loss = 56864.40026595745
2023-03-27 18:23:17,091   cls_loss = 0.0
2023-03-27 18:23:17,091   global_step = 4099
2023-03-27 18:23:17,092   loss = 56865.03723404255
2023-03-27 18:23:17,092   rep_loss = 0.636973427331194
2023-03-27 18:23:17,099 ***** Save model *****
2023-03-27 18:23:32,991 ***** Running evaluation *****
2023-03-27 18:23:32,991   Epoch = 15 iter 4149 step
2023-03-27 18:23:32,991   Num examples = 1043
2023-03-27 18:23:32,991   Batch size = 32
2023-03-27 18:23:32,993 ***** Eval results *****
2023-03-27 18:23:32,993   att_loss = 56828.988742404516
2023-03-27 18:23:32,993   cls_loss = 0.0
2023-03-27 18:23:32,993   global_step = 4149
2023-03-27 18:23:32,993   loss = 56829.625786675344
2023-03-27 18:23:32,993   rep_loss = 0.6370476612614261
2023-03-27 18:23:33,000 ***** Save model *****
2023-03-27 18:23:48,931 ***** Running evaluation *****
2023-03-27 18:23:48,931   Epoch = 15 iter 4199 step
2023-03-27 18:23:48,932   Num examples = 1043
2023-03-27 18:23:48,932   Batch size = 32
2023-03-27 18:23:48,933 ***** Eval results *****
2023-03-27 18:23:48,933   att_loss = 56934.08772954252
2023-03-27 18:23:48,934   cls_loss = 0.0
2023-03-27 18:23:48,934   global_step = 4199
2023-03-27 18:23:48,934   loss = 56934.72462951031
2023-03-27 18:23:48,934   rep_loss = 0.636944845165174
2023-03-27 18:23:48,936 ***** Save model *****
2023-03-27 18:24:04,858 ***** Running evaluation *****
2023-03-27 18:24:04,858   Epoch = 15 iter 4249 step
2023-03-27 18:24:04,859   Num examples = 1043
2023-03-27 18:24:04,859   Batch size = 32
2023-03-27 18:24:04,860 ***** Eval results *****
2023-03-27 18:24:04,860   att_loss = 57031.409195696724
2023-03-27 18:24:04,860   cls_loss = 0.0
2023-03-27 18:24:04,861   global_step = 4249
2023-03-27 18:24:04,861   loss = 57032.045914446724
2023-03-27 18:24:04,861   rep_loss = 0.636736313583421
2023-03-27 18:24:04,868 ***** Save model *****
2023-03-27 18:24:20,913 ***** Running evaluation *****
2023-03-27 18:24:20,913   Epoch = 16 iter 4299 step
2023-03-27 18:24:20,914   Num examples = 1043
2023-03-27 18:24:20,914   Batch size = 32
2023-03-27 18:24:20,915 ***** Eval results *****
2023-03-27 18:24:20,916   att_loss = 56302.311631944445
2023-03-27 18:24:20,916   cls_loss = 0.0
2023-03-27 18:24:20,916   global_step = 4299
2023-03-27 18:24:20,916   loss = 56302.942274305555
2023-03-27 18:24:20,917   rep_loss = 0.6309794297924748
2023-03-27 18:24:20,924 ***** Save model *****
2023-03-27 18:24:36,827 ***** Running evaluation *****
2023-03-27 18:24:36,828   Epoch = 16 iter 4349 step
2023-03-27 18:24:36,828   Num examples = 1043
2023-03-27 18:24:36,828   Batch size = 32
2023-03-27 18:24:36,830 ***** Eval results *****
2023-03-27 18:24:36,830   att_loss = 56222.932021103894
2023-03-27 18:24:36,830   cls_loss = 0.0
2023-03-27 18:24:36,831   global_step = 4349
2023-03-27 18:24:36,831   loss = 56223.563007305194
2023-03-27 18:24:36,831   rep_loss = 0.6312528213897308
2023-03-27 18:24:36,839 ***** Save model *****
2023-03-27 18:24:52,770 ***** Running evaluation *****
2023-03-27 18:24:52,771   Epoch = 16 iter 4399 step
2023-03-27 18:24:52,771   Num examples = 1043
2023-03-27 18:24:52,771   Batch size = 32
2023-03-27 18:24:52,774 ***** Eval results *****
2023-03-27 18:24:52,775   att_loss = 56710.219765009846
2023-03-27 18:24:52,775   cls_loss = 0.0
2023-03-27 18:24:52,775   global_step = 4399
2023-03-27 18:24:52,776   loss = 56710.85165477362
2023-03-27 18:24:52,776   rep_loss = 0.6319969376241128
2023-03-27 18:24:52,783 ***** Save model *****
2023-03-27 18:25:08,691 ***** Running evaluation *****
2023-03-27 18:25:08,691   Epoch = 16 iter 4449 step
2023-03-27 18:25:08,691   Num examples = 1043
2023-03-27 18:25:08,691   Batch size = 32
2023-03-27 18:25:08,692 ***** Eval results *****
2023-03-27 18:25:08,692   att_loss = 56789.56523658192
2023-03-27 18:25:08,693   cls_loss = 0.0
2023-03-27 18:25:08,693   global_step = 4449
2023-03-27 18:25:08,693   loss = 56790.19793873587
2023-03-27 18:25:08,693   rep_loss = 0.6327654894462413
2023-03-27 18:25:08,700 ***** Save model *****
2023-03-27 18:25:24,618 ***** Running evaluation *****
2023-03-27 18:25:24,619   Epoch = 16 iter 4499 step
2023-03-27 18:25:24,619   Num examples = 1043
2023-03-27 18:25:24,619   Batch size = 32
2023-03-27 18:25:24,621 ***** Eval results *****
2023-03-27 18:25:24,621   att_loss = 56741.19765280837
2023-03-27 18:25:24,621   cls_loss = 0.0
2023-03-27 18:25:24,621   global_step = 4499
2023-03-27 18:25:24,621   loss = 56741.83000068833
2023-03-27 18:25:24,621   rep_loss = 0.6323656515928092
2023-03-27 18:25:24,628 ***** Save model *****
2023-03-27 18:25:40,545 ***** Running evaluation *****
2023-03-27 18:25:40,545   Epoch = 17 iter 4549 step
2023-03-27 18:25:40,545   Num examples = 1043
2023-03-27 18:25:40,545   Batch size = 32
2023-03-27 18:25:40,547 ***** Eval results *****
2023-03-27 18:25:40,547   att_loss = 57011.206640625
2023-03-27 18:25:40,547   cls_loss = 0.0
2023-03-27 18:25:40,547   global_step = 4549
2023-03-27 18:25:40,548   loss = 57011.83828125
2023-03-27 18:25:40,548   rep_loss = 0.6315541446208954
2023-03-27 18:25:40,550 ***** Save model *****
2023-03-27 18:25:56,490 ***** Running evaluation *****
2023-03-27 18:25:56,490   Epoch = 17 iter 4599 step
2023-03-27 18:25:56,490   Num examples = 1043
2023-03-27 18:25:56,491   Batch size = 32
2023-03-27 18:25:56,492 ***** Eval results *****
2023-03-27 18:25:56,492   att_loss = 56348.9484375
2023-03-27 18:25:56,492   cls_loss = 0.0
2023-03-27 18:25:56,492   global_step = 4599
2023-03-27 18:25:56,492   loss = 56349.578776041664
2023-03-27 18:25:56,492   rep_loss = 0.630164717634519
2023-03-27 18:25:56,499 ***** Save model *****
2023-03-27 18:26:12,425 ***** Running evaluation *****
2023-03-27 18:26:12,425   Epoch = 17 iter 4649 step
2023-03-27 18:26:12,425   Num examples = 1043
2023-03-27 18:26:12,425   Batch size = 32
2023-03-27 18:26:12,428 ***** Eval results *****
2023-03-27 18:26:12,428   att_loss = 56440.21139914773
2023-03-27 18:26:12,429   cls_loss = 0.0
2023-03-27 18:26:12,429   global_step = 4649
2023-03-27 18:26:12,429   loss = 56440.840767045454
2023-03-27 18:26:12,429   rep_loss = 0.62922781272368
2023-03-27 18:26:12,437 ***** Save model *****
2023-03-27 18:26:28,331 ***** Running evaluation *****
2023-03-27 18:26:28,331   Epoch = 17 iter 4699 step
2023-03-27 18:26:28,331   Num examples = 1043
2023-03-27 18:26:28,332   Batch size = 32
2023-03-27 18:26:28,333 ***** Eval results *****
2023-03-27 18:26:28,333   att_loss = 56437.571020507814
2023-03-27 18:26:28,334   cls_loss = 0.0
2023-03-27 18:26:28,334   global_step = 4699
2023-03-27 18:26:28,334   loss = 56438.200390625
2023-03-27 18:26:28,334   rep_loss = 0.6292120188474655
2023-03-27 18:26:28,342 ***** Save model *****
2023-03-27 18:26:44,243 ***** Running evaluation *****
2023-03-27 18:26:44,243   Epoch = 17 iter 4749 step
2023-03-27 18:26:44,243   Num examples = 1043
2023-03-27 18:26:44,243   Batch size = 32
2023-03-27 18:26:44,245 ***** Eval results *****
2023-03-27 18:26:44,246   att_loss = 56590.61491815476
2023-03-27 18:26:44,246   cls_loss = 0.0
2023-03-27 18:26:44,246   global_step = 4749
2023-03-27 18:26:44,246   loss = 56591.24436383929
2023-03-27 18:26:44,246   rep_loss = 0.6293586603232793
2023-03-27 18:26:44,249 ***** Save model *****
2023-03-27 18:27:00,155 ***** Running evaluation *****
2023-03-27 18:27:00,155   Epoch = 17 iter 4799 step
2023-03-27 18:27:00,155   Num examples = 1043
2023-03-27 18:27:00,155   Batch size = 32
2023-03-27 18:27:00,157 ***** Eval results *****
2023-03-27 18:27:00,157   att_loss = 56677.689858774036
2023-03-27 18:27:00,157   cls_loss = 0.0
2023-03-27 18:27:00,158   global_step = 4799
2023-03-27 18:27:00,158   loss = 56678.31956129808
2023-03-27 18:27:00,158   rep_loss = 0.6296104192733765
2023-03-27 18:27:00,166 ***** Save model *****
2023-03-27 18:27:16,085 ***** Running evaluation *****
2023-03-27 18:27:16,085   Epoch = 18 iter 4849 step
2023-03-27 18:27:16,085   Num examples = 1043
2023-03-27 18:27:16,086   Batch size = 32
2023-03-27 18:27:16,087 ***** Eval results *****
2023-03-27 18:27:16,087   att_loss = 56533.94431322674
2023-03-27 18:27:16,087   cls_loss = 0.0
2023-03-27 18:27:16,088   global_step = 4849
2023-03-27 18:27:16,088   loss = 56534.57040334302
2023-03-27 18:27:16,088   rep_loss = 0.6262150836545367
2023-03-27 18:27:16,095 ***** Save model *****
2023-03-27 18:27:31,996 ***** Running evaluation *****
2023-03-27 18:27:31,997   Epoch = 18 iter 4899 step
2023-03-27 18:27:31,997   Num examples = 1043
2023-03-27 18:27:31,997   Batch size = 32
2023-03-27 18:27:31,999 ***** Eval results *****
2023-03-27 18:27:31,999   att_loss = 56603.943380376346
2023-03-27 18:27:31,999   cls_loss = 0.0
2023-03-27 18:27:31,999   global_step = 4899
2023-03-27 18:27:32,000   loss = 56604.57018649193
2023-03-27 18:27:32,000   rep_loss = 0.6269028007343251
2023-03-27 18:27:32,007 ***** Save model *****
2023-03-27 18:27:47,948 ***** Running evaluation *****
2023-03-27 18:27:47,948   Epoch = 18 iter 4949 step
2023-03-27 18:27:47,949   Num examples = 1043
2023-03-27 18:27:47,949   Batch size = 32
2023-03-27 18:27:47,951 ***** Eval results *****
2023-03-27 18:27:47,951   att_loss = 56331.815914554194
2023-03-27 18:27:47,951   cls_loss = 0.0
2023-03-27 18:27:47,951   global_step = 4949
2023-03-27 18:27:47,951   loss = 56332.44233500874
2023-03-27 18:27:47,952   rep_loss = 0.6263925745770648
2023-03-27 18:27:47,959 ***** Save model *****
2023-03-27 18:28:03,871 ***** Running evaluation *****
2023-03-27 18:28:03,871   Epoch = 18 iter 4999 step
2023-03-27 18:28:03,872   Num examples = 1043
2023-03-27 18:28:03,872   Batch size = 32
2023-03-27 18:28:03,873 ***** Eval results *****
2023-03-27 18:28:03,874   att_loss = 56320.35158273964
2023-03-27 18:28:03,874   cls_loss = 0.0
2023-03-27 18:28:03,874   global_step = 4999
2023-03-27 18:28:03,874   loss = 56320.977756638604
2023-03-27 18:28:03,874   rep_loss = 0.6261117328633916
2023-03-27 18:28:03,876 ***** Save model *****
2023-03-27 18:28:19,791 ***** Running evaluation *****
2023-03-27 18:28:19,791   Epoch = 18 iter 5049 step
2023-03-27 18:28:19,791   Num examples = 1043
2023-03-27 18:28:19,791   Batch size = 32
2023-03-27 18:28:19,793 ***** Eval results *****
2023-03-27 18:28:19,793   att_loss = 56468.270447530864
2023-03-27 18:28:19,793   cls_loss = 0.0
2023-03-27 18:28:19,793   global_step = 5049
2023-03-27 18:28:19,794   loss = 56468.89679783951
2023-03-27 18:28:19,794   rep_loss = 0.6263373835096634
2023-03-27 18:28:19,796 ***** Save model *****
2023-03-27 18:28:35,711 ***** Running evaluation *****
2023-03-27 18:28:35,712   Epoch = 19 iter 5099 step
2023-03-27 18:28:35,712   Num examples = 1043
2023-03-27 18:28:35,712   Batch size = 32
2023-03-27 18:28:35,713 ***** Eval results *****
2023-03-27 18:28:35,714   att_loss = 56542.715144230766
2023-03-27 18:28:35,714   cls_loss = 0.0
2023-03-27 18:28:35,714   global_step = 5099
2023-03-27 18:28:35,714   loss = 56543.337890625
2023-03-27 18:28:35,714   rep_loss = 0.6229742077680734
2023-03-27 18:28:35,722 ***** Save model *****
2023-03-27 18:28:51,614 ***** Running evaluation *****
2023-03-27 18:28:51,614   Epoch = 19 iter 5149 step
2023-03-27 18:28:51,614   Num examples = 1043
2023-03-27 18:28:51,615   Batch size = 32
2023-03-27 18:28:51,616 ***** Eval results *****
2023-03-27 18:28:51,616   att_loss = 56335.94500411184
2023-03-27 18:28:51,616   cls_loss = 0.0
2023-03-27 18:28:51,616   global_step = 5149
2023-03-27 18:28:51,616   loss = 56336.56799958881
2023-03-27 18:28:51,616   rep_loss = 0.6230214269537675
2023-03-27 18:28:51,623 ***** Save model *****
2023-03-27 18:29:07,537 ***** Running evaluation *****
2023-03-27 18:29:07,538   Epoch = 19 iter 5199 step
2023-03-27 18:29:07,538   Num examples = 1043
2023-03-27 18:29:07,538   Batch size = 32
2023-03-27 18:29:07,539 ***** Eval results *****
2023-03-27 18:29:07,539   att_loss = 56314.977058531746
2023-03-27 18:29:07,540   cls_loss = 0.0
2023-03-27 18:29:07,540   global_step = 5199
2023-03-27 18:29:07,540   loss = 56315.600105406746
2023-03-27 18:29:07,540   rep_loss = 0.6229783149938735
2023-03-27 18:29:07,547 ***** Save model *****
2023-03-27 18:29:23,479 ***** Running evaluation *****
2023-03-27 18:29:23,479   Epoch = 19 iter 5249 step
2023-03-27 18:29:23,480   Num examples = 1043
2023-03-27 18:29:23,480   Batch size = 32
2023-03-27 18:29:23,481 ***** Eval results *****
2023-03-27 18:29:23,481   att_loss = 56351.91867897727
2023-03-27 18:29:23,481   cls_loss = 0.0
2023-03-27 18:29:23,481   global_step = 5249
2023-03-27 18:29:23,482   loss = 56352.54221413352
2023-03-27 18:29:23,482   rep_loss = 0.6235122487626292
2023-03-27 18:29:23,489 ***** Save model *****
2023-03-27 18:29:39,402 ***** Running evaluation *****
2023-03-27 18:29:39,402   Epoch = 19 iter 5299 step
2023-03-27 18:29:39,403   Num examples = 1043
2023-03-27 18:29:39,403   Batch size = 32
2023-03-27 18:29:39,404 ***** Eval results *****
2023-03-27 18:29:39,405   att_loss = 56331.553477599555
2023-03-27 18:29:39,405   cls_loss = 0.0
2023-03-27 18:29:39,405   global_step = 5299
2023-03-27 18:29:39,405   loss = 56332.17726769912
2023-03-27 18:29:39,405   rep_loss = 0.6237677852664374
2023-03-27 18:29:39,412 ***** Save model *****
2023-03-27 18:29:55,820 ***** Running evaluation *****
2023-03-27 18:29:55,820   Epoch = 20 iter 5349 step
2023-03-27 18:29:55,821   Num examples = 1043
2023-03-27 18:29:55,821   Batch size = 32
2023-03-27 18:29:55,823 ***** Eval results *****
2023-03-27 18:29:55,823   att_loss = 56258.819010416664
2023-03-27 18:29:55,823   cls_loss = 0.0
2023-03-27 18:29:55,823   global_step = 5349
2023-03-27 18:29:55,823   loss = 56259.43923611111
2023-03-27 18:29:55,824   rep_loss = 0.6201067301962111
2023-03-27 18:29:55,831 ***** Save model *****
2023-03-27 18:30:11,727 ***** Running evaluation *****
2023-03-27 18:30:11,728   Epoch = 20 iter 5399 step
2023-03-27 18:30:11,728   Num examples = 1043
2023-03-27 18:30:11,728   Batch size = 32
2023-03-27 18:30:11,729 ***** Eval results *****
2023-03-27 18:30:11,730   att_loss = 55994.91657838983
2023-03-27 18:30:11,730   cls_loss = 0.0
2023-03-27 18:30:11,730   global_step = 5399
2023-03-27 18:30:11,731   loss = 55995.536480402545
2023-03-27 18:30:11,731   rep_loss = 0.6200067885851456
2023-03-27 18:30:11,738 ***** Save model *****
2023-03-27 18:30:27,655 ***** Running evaluation *****
2023-03-27 18:30:27,656   Epoch = 20 iter 5449 step
2023-03-27 18:30:27,656   Num examples = 1043
2023-03-27 18:30:27,656   Batch size = 32
2023-03-27 18:30:27,657 ***** Eval results *****
2023-03-27 18:30:27,657   att_loss = 55855.58034690367
2023-03-27 18:30:27,658   cls_loss = 0.0
2023-03-27 18:30:27,658   global_step = 5449
2023-03-27 18:30:27,658   loss = 55856.200007167434
2023-03-27 18:30:27,659   rep_loss = 0.6196823995047753
2023-03-27 18:30:27,662 ***** Save model *****
2023-03-27 18:30:43,551 ***** Running evaluation *****
2023-03-27 18:30:43,551   Epoch = 20 iter 5499 step
2023-03-27 18:30:43,552   Num examples = 1043
2023-03-27 18:30:43,552   Batch size = 32
2023-03-27 18:30:43,554 ***** Eval results *****
2023-03-27 18:30:43,554   att_loss = 55886.971747248426
2023-03-27 18:30:43,554   cls_loss = 0.0
2023-03-27 18:30:43,555   global_step = 5499
2023-03-27 18:30:43,555   loss = 55887.591710888366
2023-03-27 18:30:43,555   rep_loss = 0.6200616209761901
2023-03-27 18:30:43,562 ***** Save model *****
2023-03-27 18:30:59,472 ***** Running evaluation *****
2023-03-27 18:30:59,472   Epoch = 20 iter 5549 step
2023-03-27 18:30:59,472   Num examples = 1043
2023-03-27 18:30:59,473   Batch size = 32
2023-03-27 18:30:59,474 ***** Eval results *****
2023-03-27 18:30:59,474   att_loss = 56079.55051958732
2023-03-27 18:30:59,474   cls_loss = 0.0
2023-03-27 18:30:59,474   global_step = 5549
2023-03-27 18:30:59,474   loss = 56080.170734898325
2023-03-27 18:30:59,475   rep_loss = 0.6202507150230225
2023-03-27 18:30:59,482 ***** Save model *****
2023-03-27 18:31:15,403 ***** Running evaluation *****
2023-03-27 18:31:15,403   Epoch = 20 iter 5599 step
2023-03-27 18:31:15,403   Num examples = 1043
2023-03-27 18:31:15,403   Batch size = 32
2023-03-27 18:31:15,405 ***** Eval results *****
2023-03-27 18:31:15,405   att_loss = 56103.92216155888
2023-03-27 18:31:15,406   cls_loss = 0.0
2023-03-27 18:31:15,406   global_step = 5599
2023-03-27 18:31:15,406   loss = 56104.542621862936
2023-03-27 18:31:15,406   rep_loss = 0.6205209017260195
2023-03-27 18:31:15,413 ***** Save model *****
2023-03-27 18:31:31,343 ***** Running evaluation *****
2023-03-27 18:31:31,343   Epoch = 21 iter 5649 step
2023-03-27 18:31:31,343   Num examples = 1043
2023-03-27 18:31:31,344   Batch size = 32
2023-03-27 18:31:31,345 ***** Eval results *****
2023-03-27 18:31:31,345   att_loss = 55812.861049107145
2023-03-27 18:31:31,345   cls_loss = 0.0
2023-03-27 18:31:31,345   global_step = 5649
2023-03-27 18:31:31,346   loss = 55813.47702752976
2023-03-27 18:31:31,346   rep_loss = 0.6162158477874029
2023-03-27 18:31:31,353 ***** Save model *****
2023-03-27 18:31:47,249 ***** Running evaluation *****
2023-03-27 18:31:47,249   Epoch = 21 iter 5699 step
2023-03-27 18:31:47,249   Num examples = 1043
2023-03-27 18:31:47,249   Batch size = 32
2023-03-27 18:31:47,251 ***** Eval results *****
2023-03-27 18:31:47,252   att_loss = 55943.61302649457
2023-03-27 18:31:47,252   cls_loss = 0.0
2023-03-27 18:31:47,252   global_step = 5699
2023-03-27 18:31:47,252   loss = 55944.23004415761
2023-03-27 18:31:47,252   rep_loss = 0.6171251354010209
2023-03-27 18:31:47,259 ***** Save model *****
2023-03-27 18:32:03,181 ***** Running evaluation *****
2023-03-27 18:32:03,182   Epoch = 21 iter 5749 step
2023-03-27 18:32:03,182   Num examples = 1043
2023-03-27 18:32:03,182   Batch size = 32
2023-03-27 18:32:03,183 ***** Eval results *****
2023-03-27 18:32:03,184   att_loss = 55777.15809308979
2023-03-27 18:32:03,184   cls_loss = 0.0
2023-03-27 18:32:03,184   global_step = 5749
2023-03-27 18:32:03,184   loss = 55777.77409771127
2023-03-27 18:32:03,184   rep_loss = 0.6161594533584487
2023-03-27 18:32:03,191 ***** Save model *****
2023-03-27 18:32:19,121 ***** Running evaluation *****
2023-03-27 18:32:19,121   Epoch = 21 iter 5799 step
2023-03-27 18:32:19,121   Num examples = 1043
2023-03-27 18:32:19,121   Batch size = 32
2023-03-27 18:32:19,122 ***** Eval results *****
2023-03-27 18:32:19,123   att_loss = 55840.279296875
2023-03-27 18:32:19,123   cls_loss = 0.0
2023-03-27 18:32:19,123   global_step = 5799
2023-03-27 18:32:19,123   loss = 55840.89585367838
2023-03-27 18:32:19,123   rep_loss = 0.6166863646358252
2023-03-27 18:32:19,130 ***** Save model *****
2023-03-27 18:32:35,062 ***** Running evaluation *****
2023-03-27 18:32:35,062   Epoch = 21 iter 5849 step
2023-03-27 18:32:35,062   Num examples = 1043
2023-03-27 18:32:35,063   Batch size = 32
2023-03-27 18:32:35,064 ***** Eval results *****
2023-03-27 18:32:35,064   att_loss = 55927.77506779442
2023-03-27 18:32:35,064   cls_loss = 0.0
2023-03-27 18:32:35,064   global_step = 5849
2023-03-27 18:32:35,064   loss = 55928.392061596074
2023-03-27 18:32:35,064   rep_loss = 0.6170834243790178
2023-03-27 18:32:35,071 ***** Save model *****
2023-03-27 18:32:50,990 ***** Running evaluation *****
2023-03-27 18:32:50,990   Epoch = 22 iter 5899 step
2023-03-27 18:32:50,990   Num examples = 1043
2023-03-27 18:32:50,991   Batch size = 32
2023-03-27 18:32:50,992 ***** Eval results *****
2023-03-27 18:32:50,993   att_loss = 54975.23015625
2023-03-27 18:32:50,993   cls_loss = 0.0
2023-03-27 18:32:50,993   global_step = 5899
2023-03-27 18:32:50,994   loss = 54975.843125
2023-03-27 18:32:50,994   rep_loss = 0.6131848645210266
2023-03-27 18:32:50,996 ***** Save model *****
2023-03-27 18:33:06,890 ***** Running evaluation *****
2023-03-27 18:33:06,891   Epoch = 22 iter 5949 step
2023-03-27 18:33:06,891   Num examples = 1043
2023-03-27 18:33:06,891   Batch size = 32
2023-03-27 18:33:06,892 ***** Eval results *****
2023-03-27 18:33:06,892   att_loss = 55503.15515625
2023-03-27 18:33:06,892   cls_loss = 0.0
2023-03-27 18:33:06,893   global_step = 5949
2023-03-27 18:33:06,893   loss = 55503.766875
2023-03-27 18:33:06,893   rep_loss = 0.6118201168378194
2023-03-27 18:33:06,895 ***** Save model *****
2023-03-27 18:33:22,800 ***** Running evaluation *****
2023-03-27 18:33:22,800   Epoch = 22 iter 5999 step
2023-03-27 18:33:22,800   Num examples = 1043
2023-03-27 18:33:22,801   Batch size = 32
2023-03-27 18:33:22,802 ***** Eval results *****
2023-03-27 18:33:22,802   att_loss = 55532.69346875
2023-03-27 18:33:22,802   cls_loss = 0.0
2023-03-27 18:33:22,803   global_step = 5999
2023-03-27 18:33:22,803   loss = 55533.306875
2023-03-27 18:33:22,803   rep_loss = 0.6133519186973572
2023-03-27 18:33:22,810 ***** Save model *****
2023-03-27 18:33:38,741 ***** Running evaluation *****
2023-03-27 18:33:38,742   Epoch = 22 iter 6049 step
2023-03-27 18:33:38,742   Num examples = 1043
2023-03-27 18:33:38,742   Batch size = 32
2023-03-27 18:33:38,743 ***** Eval results *****
2023-03-27 18:33:38,743   att_loss = 55614.33176339286
2023-03-27 18:33:38,744   cls_loss = 0.0
2023-03-27 18:33:38,744   global_step = 6049
2023-03-27 18:33:38,744   loss = 55614.94584821429
2023-03-27 18:33:38,744   rep_loss = 0.6141337544577462
2023-03-27 18:33:38,751 ***** Save model *****
2023-03-27 18:33:54,675 ***** Running evaluation *****
2023-03-27 18:33:54,675   Epoch = 22 iter 6099 step
2023-03-27 18:33:54,676   Num examples = 1043
2023-03-27 18:33:54,676   Batch size = 32
2023-03-27 18:33:54,677 ***** Eval results *****
2023-03-27 18:33:54,677   att_loss = 55714.01303819445
2023-03-27 18:33:54,677   cls_loss = 0.0
2023-03-27 18:33:54,677   global_step = 6099
2023-03-27 18:33:54,677   loss = 55714.627222222225
2023-03-27 18:33:54,677   rep_loss = 0.6142329565684
2023-03-27 18:33:54,684 ***** Save model *****
2023-03-27 18:34:10,615 ***** Running evaluation *****
2023-03-27 18:34:10,615   Epoch = 23 iter 6149 step
2023-03-27 18:34:10,615   Num examples = 1043
2023-03-27 18:34:10,615   Batch size = 32
2023-03-27 18:34:10,616 ***** Eval results *****
2023-03-27 18:34:10,617   att_loss = 55119.33837890625
2023-03-27 18:34:10,617   cls_loss = 0.0
2023-03-27 18:34:10,617   global_step = 6149
2023-03-27 18:34:10,617   loss = 55119.94921875
2023-03-27 18:34:10,617   rep_loss = 0.6103878393769264
2023-03-27 18:34:10,620 ***** Save model *****
2023-03-27 18:34:26,529 ***** Running evaluation *****
2023-03-27 18:34:26,529   Epoch = 23 iter 6199 step
2023-03-27 18:34:26,530   Num examples = 1043
2023-03-27 18:34:26,530   Batch size = 32
2023-03-27 18:34:26,531 ***** Eval results *****
2023-03-27 18:34:26,531   att_loss = 55618.730603448275
2023-03-27 18:34:26,532   cls_loss = 0.0
2023-03-27 18:34:26,532   global_step = 6199
2023-03-27 18:34:26,532   loss = 55619.345299030174
2023-03-27 18:34:26,532   rep_loss = 0.6146174175985928
2023-03-27 18:34:26,539 ***** Save model *****
2023-03-27 18:34:42,454 ***** Running evaluation *****
2023-03-27 18:34:42,455   Epoch = 23 iter 6249 step
2023-03-27 18:34:42,455   Num examples = 1043
2023-03-27 18:34:42,455   Batch size = 32
2023-03-27 18:34:42,456 ***** Eval results *****
2023-03-27 18:34:42,457   att_loss = 55814.651656539354
2023-03-27 18:34:42,457   cls_loss = 0.0
2023-03-27 18:34:42,457   global_step = 6249
2023-03-27 18:34:42,458   loss = 55815.26508246528
2023-03-27 18:34:42,458   rep_loss = 0.6134708352662899
2023-03-27 18:34:42,460 ***** Save model *****
2023-03-27 18:34:58,391 ***** Running evaluation *****
2023-03-27 18:34:58,391   Epoch = 23 iter 6299 step
2023-03-27 18:34:58,391   Num examples = 1043
2023-03-27 18:34:58,391   Batch size = 32
2023-03-27 18:34:58,393 ***** Eval results *****
2023-03-27 18:34:58,393   att_loss = 55737.69125791139
2023-03-27 18:34:58,393   cls_loss = 0.0
2023-03-27 18:34:58,394   global_step = 6299
2023-03-27 18:34:58,394   loss = 55738.3046380538
2023-03-27 18:34:58,394   rep_loss = 0.6133761277681664
2023-03-27 18:34:58,401 ***** Save model *****
2023-03-27 18:35:14,319 ***** Running evaluation *****
2023-03-27 18:35:14,320   Epoch = 23 iter 6349 step
2023-03-27 18:35:14,320   Num examples = 1043
2023-03-27 18:35:14,320   Batch size = 32
2023-03-27 18:35:14,322 ***** Eval results *****
2023-03-27 18:35:14,322   att_loss = 55763.33745868389
2023-03-27 18:35:14,322   cls_loss = 0.0
2023-03-27 18:35:14,322   global_step = 6349
2023-03-27 18:35:14,322   loss = 55763.95047701322
2023-03-27 18:35:14,323   rep_loss = 0.6129527177948219
2023-03-27 18:35:14,329 ***** Save model *****
2023-03-27 18:35:30,263 ***** Running evaluation *****
2023-03-27 18:35:30,263   Epoch = 23 iter 6399 step
2023-03-27 18:35:30,263   Num examples = 1043
2023-03-27 18:35:30,263   Batch size = 32
2023-03-27 18:35:30,265 ***** Eval results *****
2023-03-27 18:35:30,265   att_loss = 55719.44467659884
2023-03-27 18:35:30,266   cls_loss = 0.0
2023-03-27 18:35:30,266   global_step = 6399
2023-03-27 18:35:30,266   loss = 55720.05773074128
2023-03-27 18:35:30,266   rep_loss = 0.6130489769370057
2023-03-27 18:35:30,274 ***** Save model *****
2023-03-27 18:35:46,230 ***** Running evaluation *****
2023-03-27 18:35:46,230   Epoch = 24 iter 6449 step
2023-03-27 18:35:46,230   Num examples = 1043
2023-03-27 18:35:46,230   Batch size = 32
2023-03-27 18:35:46,231 ***** Eval results *****
2023-03-27 18:35:46,232   att_loss = 55404.79792301829
2023-03-27 18:35:46,232   cls_loss = 0.0
2023-03-27 18:35:46,232   global_step = 6449
2023-03-27 18:35:46,232   loss = 55405.40958460366
2023-03-27 18:35:46,232   rep_loss = 0.6115307299102225
2023-03-27 18:35:46,240 ***** Save model *****
2023-03-27 18:36:02,172 ***** Running evaluation *****
2023-03-27 18:36:02,172   Epoch = 24 iter 6499 step
2023-03-27 18:36:02,173   Num examples = 1043
2023-03-27 18:36:02,173   Batch size = 32
2023-03-27 18:36:02,174 ***** Eval results *****
2023-03-27 18:36:02,174   att_loss = 55345.90521978022
2023-03-27 18:36:02,175   cls_loss = 0.0
2023-03-27 18:36:02,175   global_step = 6499
2023-03-27 18:36:02,175   loss = 55346.516569368134
2023-03-27 18:36:02,175   rep_loss = 0.6113479818616595
2023-03-27 18:36:02,182 ***** Save model *****
2023-03-27 18:36:18,097 ***** Running evaluation *****
2023-03-27 18:36:18,098   Epoch = 24 iter 6549 step
2023-03-27 18:36:18,098   Num examples = 1043
2023-03-27 18:36:18,098   Batch size = 32
2023-03-27 18:36:18,099 ***** Eval results *****
2023-03-27 18:36:18,099   att_loss = 55284.16783023049
2023-03-27 18:36:18,099   cls_loss = 0.0
2023-03-27 18:36:18,099   global_step = 6549
2023-03-27 18:36:18,100   loss = 55284.778756648935
2023-03-27 18:36:18,100   rep_loss = 0.6109229279748092
2023-03-27 18:36:18,107 ***** Save model *****
2023-03-27 18:36:34,027 ***** Running evaluation *****
2023-03-27 18:36:34,028   Epoch = 24 iter 6599 step
2023-03-27 18:36:34,028   Num examples = 1043
2023-03-27 18:36:34,028   Batch size = 32
2023-03-27 18:36:34,030 ***** Eval results *****
2023-03-27 18:36:34,030   att_loss = 55601.610356675395
2023-03-27 18:36:34,030   cls_loss = 0.0
2023-03-27 18:36:34,030   global_step = 6599
2023-03-27 18:36:34,031   loss = 55602.22177683246
2023-03-27 18:36:34,031   rep_loss = 0.6114252702727991
2023-03-27 18:36:34,038 ***** Save model *****
2023-03-27 18:36:49,945 ***** Running evaluation *****
2023-03-27 18:36:49,945   Epoch = 24 iter 6649 step
2023-03-27 18:36:49,946   Num examples = 1043
2023-03-27 18:36:49,946   Batch size = 32
2023-03-27 18:36:49,947 ***** Eval results *****
2023-03-27 18:36:49,947   att_loss = 55520.1809841805
2023-03-27 18:36:49,948   cls_loss = 0.0
2023-03-27 18:36:49,948   global_step = 6649
2023-03-27 18:36:49,948   loss = 55520.792158324686
2023-03-27 18:36:49,948   rep_loss = 0.61116689641446
2023-03-27 18:36:49,956 ***** Save model *****
2023-03-27 18:37:05,877 ***** Running evaluation *****
2023-03-27 18:37:05,877   Epoch = 25 iter 6699 step
2023-03-27 18:37:05,877   Num examples = 1043
2023-03-27 18:37:05,877   Batch size = 32
2023-03-27 18:37:05,879 ***** Eval results *****
2023-03-27 18:37:05,880   att_loss = 55411.501627604164
2023-03-27 18:37:05,880   cls_loss = 0.0
2023-03-27 18:37:05,880   global_step = 6699
2023-03-27 18:37:05,880   loss = 55412.111328125
2023-03-27 18:37:05,880   rep_loss = 0.6096731722354889
2023-03-27 18:37:05,887 ***** Save model *****
2023-03-27 18:37:21,817 ***** Running evaluation *****
2023-03-27 18:37:21,818   Epoch = 25 iter 6749 step
2023-03-27 18:37:21,818   Num examples = 1043
2023-03-27 18:37:21,818   Batch size = 32
2023-03-27 18:37:21,820 ***** Eval results *****
2023-03-27 18:37:21,820   att_loss = 55536.35082347973
2023-03-27 18:37:21,820   cls_loss = 0.0
2023-03-27 18:37:21,820   global_step = 6749
2023-03-27 18:37:21,821   loss = 55536.96194045608
2023-03-27 18:37:21,821   rep_loss = 0.6109799476894172
2023-03-27 18:37:21,828 ***** Save model *****
2023-03-27 18:37:37,781 ***** Running evaluation *****
2023-03-27 18:37:37,781   Epoch = 25 iter 6799 step
2023-03-27 18:37:37,781   Num examples = 1043
2023-03-27 18:37:37,781   Batch size = 32
2023-03-27 18:37:37,783 ***** Eval results *****
2023-03-27 18:37:37,783   att_loss = 55541.45202242943
2023-03-27 18:37:37,783   cls_loss = 0.0
2023-03-27 18:37:37,783   global_step = 6799
2023-03-27 18:37:37,783   loss = 55542.06205897177
2023-03-27 18:37:37,784   rep_loss = 0.6100139901522668
2023-03-27 18:37:37,786 ***** Save model *****
2023-03-27 18:37:53,721 ***** Running evaluation *****
2023-03-27 18:37:53,721   Epoch = 25 iter 6849 step
2023-03-27 18:37:53,721   Num examples = 1043
2023-03-27 18:37:53,722   Batch size = 32
2023-03-27 18:37:53,723 ***** Eval results *****
2023-03-27 18:37:53,723   att_loss = 55566.67021372126
2023-03-27 18:37:53,723   cls_loss = 0.0
2023-03-27 18:37:53,724   global_step = 6849
2023-03-27 18:37:53,724   loss = 55567.28026221264
2023-03-27 18:37:53,724   rep_loss = 0.6100693571156469
2023-03-27 18:37:53,731 ***** Save model *****
2023-03-27 18:38:09,657 ***** Running evaluation *****
2023-03-27 18:38:09,657   Epoch = 25 iter 6899 step
2023-03-27 18:38:09,657   Num examples = 1043
2023-03-27 18:38:09,658   Batch size = 32
2023-03-27 18:38:09,659 ***** Eval results *****
2023-03-27 18:38:09,660   att_loss = 55498.67344447545
2023-03-27 18:38:09,660   cls_loss = 0.0
2023-03-27 18:38:09,660   global_step = 6899
2023-03-27 18:38:09,661   loss = 55499.28226143973
2023-03-27 18:38:09,661   rep_loss = 0.6088380425104073
2023-03-27 18:38:09,668 ***** Save model *****
2023-03-27 18:38:25,589 ***** Running evaluation *****
2023-03-27 18:38:25,589   Epoch = 26 iter 6949 step
2023-03-27 18:38:25,590   Num examples = 1043
2023-03-27 18:38:25,590   Batch size = 32
2023-03-27 18:38:25,591 ***** Eval results *****
2023-03-27 18:38:25,591   att_loss = 54621.5625
2023-03-27 18:38:25,591   cls_loss = 0.0
2023-03-27 18:38:25,591   global_step = 6949
2023-03-27 18:38:25,591   loss = 54622.162388392855
2023-03-27 18:38:25,591   rep_loss = 0.6008697833333697
2023-03-27 18:38:25,599 ***** Save model *****
2023-03-27 18:38:41,535 ***** Running evaluation *****
2023-03-27 18:38:41,535   Epoch = 26 iter 6999 step
2023-03-27 18:38:41,535   Num examples = 1043
2023-03-27 18:38:41,535   Batch size = 32
2023-03-27 18:38:41,537 ***** Eval results *****
2023-03-27 18:38:41,537   att_loss = 55634.79989035088
2023-03-27 18:38:41,537   cls_loss = 0.0
2023-03-27 18:38:41,538   global_step = 6999
2023-03-27 18:38:41,538   loss = 55635.407552083336
2023-03-27 18:38:41,538   rep_loss = 0.6079958980543572
2023-03-27 18:38:41,541 ***** Save model *****
2023-03-27 18:38:57,452 ***** Running evaluation *****
2023-03-27 18:38:57,452   Epoch = 26 iter 7049 step
2023-03-27 18:38:57,452   Num examples = 1043
2023-03-27 18:38:57,452   Batch size = 32
2023-03-27 18:38:57,454 ***** Eval results *****
2023-03-27 18:38:57,454   att_loss = 55546.47769421729
2023-03-27 18:38:57,454   cls_loss = 0.0
2023-03-27 18:38:57,455   global_step = 7049
2023-03-27 18:38:57,455   loss = 55547.08480578271
2023-03-27 18:38:57,455   rep_loss = 0.6072491847466086
2023-03-27 18:38:57,462 ***** Save model *****
2023-03-27 18:39:13,391 ***** Running evaluation *****
2023-03-27 18:39:13,391   Epoch = 26 iter 7099 step
2023-03-27 18:39:13,391   Num examples = 1043
2023-03-27 18:39:13,392   Batch size = 32
2023-03-27 18:39:13,393 ***** Eval results *****
2023-03-27 18:39:13,393   att_loss = 55419.55137838376
2023-03-27 18:39:13,393   cls_loss = 0.0
2023-03-27 18:39:13,394   global_step = 7099
2023-03-27 18:39:13,394   loss = 55420.15819068471
2023-03-27 18:39:13,394   rep_loss = 0.606904050347152
2023-03-27 18:39:13,401 ***** Save model *****
2023-03-27 18:39:29,332 ***** Running evaluation *****
2023-03-27 18:39:29,333   Epoch = 26 iter 7149 step
2023-03-27 18:39:29,333   Num examples = 1043
2023-03-27 18:39:29,333   Batch size = 32
2023-03-27 18:39:29,334 ***** Eval results *****
2023-03-27 18:39:29,335   att_loss = 55465.45574803744
2023-03-27 18:39:29,335   cls_loss = 0.0
2023-03-27 18:39:29,335   global_step = 7149
2023-03-27 18:39:29,335   loss = 55466.06278306159
2023-03-27 18:39:29,336   rep_loss = 0.6071539233272202
2023-03-27 18:39:29,343 ***** Save model *****
2023-03-27 18:39:45,264 ***** Running evaluation *****
2023-03-27 18:39:45,264   Epoch = 26 iter 7199 step
2023-03-27 18:39:45,264   Num examples = 1043
2023-03-27 18:39:45,265   Batch size = 32
2023-03-27 18:39:45,266 ***** Eval results *****
2023-03-27 18:39:45,266   att_loss = 55537.6942941391
2023-03-27 18:39:45,266   cls_loss = 0.0
2023-03-27 18:39:45,267   global_step = 7199
2023-03-27 18:39:45,267   loss = 55538.3016172179
2023-03-27 18:39:45,267   rep_loss = 0.6073975335763122
2023-03-27 18:39:45,274 ***** Save model *****
2023-03-27 18:40:01,201 ***** Running evaluation *****
2023-03-27 18:40:01,201   Epoch = 27 iter 7249 step
2023-03-27 18:40:01,201   Num examples = 1043
2023-03-27 18:40:01,201   Batch size = 32
2023-03-27 18:40:01,203 ***** Eval results *****
2023-03-27 18:40:01,203   att_loss = 55621.03916015625
2023-03-27 18:40:01,203   cls_loss = 0.0
2023-03-27 18:40:01,203   global_step = 7249
2023-03-27 18:40:01,203   loss = 55621.64462890625
2023-03-27 18:40:01,204   rep_loss = 0.6054634764790535
2023-03-27 18:40:01,211 ***** Save model *****
2023-03-27 18:40:17,171 ***** Running evaluation *****
2023-03-27 18:40:17,171   Epoch = 27 iter 7299 step
2023-03-27 18:40:17,172   Num examples = 1043
2023-03-27 18:40:17,172   Batch size = 32
2023-03-27 18:40:17,173 ***** Eval results *****
2023-03-27 18:40:17,173   att_loss = 55480.37491319444
2023-03-27 18:40:17,173   cls_loss = 0.0
2023-03-27 18:40:17,173   global_step = 7299
2023-03-27 18:40:17,173   loss = 55480.98125
2023-03-27 18:40:17,173   rep_loss = 0.6063450323210822
2023-03-27 18:40:17,181 ***** Save model *****
2023-03-27 18:40:33,103 ***** Running evaluation *****
2023-03-27 18:40:33,103   Epoch = 27 iter 7349 step
2023-03-27 18:40:33,103   Num examples = 1043
2023-03-27 18:40:33,104   Batch size = 32
2023-03-27 18:40:33,105 ***** Eval results *****
2023-03-27 18:40:33,105   att_loss = 55324.901729910714
2023-03-27 18:40:33,105   cls_loss = 0.0
2023-03-27 18:40:33,105   global_step = 7349
2023-03-27 18:40:33,106   loss = 55325.50717075893
2023-03-27 18:40:33,106   rep_loss = 0.6054126577717918
2023-03-27 18:40:33,113 ***** Save model *****
2023-03-27 18:40:49,042 ***** Running evaluation *****
2023-03-27 18:40:49,042   Epoch = 27 iter 7399 step
2023-03-27 18:40:49,042   Num examples = 1043
2023-03-27 18:40:49,043   Batch size = 32
2023-03-27 18:40:49,044 ***** Eval results *****
2023-03-27 18:40:49,044   att_loss = 55320.87271792763
2023-03-27 18:40:49,045   cls_loss = 0.0
2023-03-27 18:40:49,045   global_step = 7399
2023-03-27 18:40:49,045   loss = 55321.47849506579
2023-03-27 18:40:49,045   rep_loss = 0.6058060028051075
2023-03-27 18:40:49,052 ***** Save model *****
2023-03-27 18:41:04,966 ***** Running evaluation *****
2023-03-27 18:41:04,966   Epoch = 27 iter 7449 step
2023-03-27 18:41:04,966   Num examples = 1043
2023-03-27 18:41:04,966   Batch size = 32
2023-03-27 18:41:04,968 ***** Eval results *****
2023-03-27 18:41:04,968   att_loss = 55369.779459635414
2023-03-27 18:41:04,968   cls_loss = 0.0
2023-03-27 18:41:04,968   global_step = 7449
2023-03-27 18:41:04,968   loss = 55370.38567708333
2023-03-27 18:41:04,969   rep_loss = 0.6061758210261663
2023-03-27 18:41:04,976 ***** Save model *****
2023-03-27 18:41:20,903 ***** Running evaluation *****
2023-03-27 18:41:20,904   Epoch = 28 iter 7499 step
2023-03-27 18:41:20,904   Num examples = 1043
2023-03-27 18:41:20,904   Batch size = 32
2023-03-27 18:41:20,905 ***** Eval results *****
2023-03-27 18:41:20,906   att_loss = 55479.94921875
2023-03-27 18:41:20,906   cls_loss = 0.0
2023-03-27 18:41:20,906   global_step = 7499
2023-03-27 18:41:20,906   loss = 55480.551800271736
2023-03-27 18:41:20,906   rep_loss = 0.6027756255605946
2023-03-27 18:41:20,914 ***** Save model *****
2023-03-27 18:41:36,784 ***** Running evaluation *****
2023-03-27 18:41:36,785   Epoch = 28 iter 7549 step
2023-03-27 18:41:36,785   Num examples = 1043
2023-03-27 18:41:36,785   Batch size = 32
2023-03-27 18:41:36,786 ***** Eval results *****
2023-03-27 18:41:36,786   att_loss = 55308.31811857877
2023-03-27 18:41:36,787   cls_loss = 0.0
2023-03-27 18:41:36,787   global_step = 7549
2023-03-27 18:41:36,787   loss = 55308.92267765411
2023-03-27 18:41:36,787   rep_loss = 0.6044969150464828
2023-03-27 18:41:36,795 ***** Save model *****
2023-03-27 18:41:52,714 ***** Running evaluation *****
2023-03-27 18:41:52,714   Epoch = 28 iter 7599 step
2023-03-27 18:41:52,714   Num examples = 1043
2023-03-27 18:41:52,714   Batch size = 32
2023-03-27 18:41:52,716 ***** Eval results *****
2023-03-27 18:41:52,716   att_loss = 55421.560530995936
2023-03-27 18:41:52,716   cls_loss = 0.0
2023-03-27 18:41:52,716   global_step = 7599
2023-03-27 18:41:52,717   loss = 55422.165396341465
2023-03-27 18:41:52,717   rep_loss = 0.6048283707804796
2023-03-27 18:41:52,724 ***** Save model *****
2023-03-27 18:42:08,642 ***** Running evaluation *****
2023-03-27 18:42:08,642   Epoch = 28 iter 7649 step
2023-03-27 18:42:08,642   Num examples = 1043
2023-03-27 18:42:08,642   Batch size = 32
2023-03-27 18:42:08,643 ***** Eval results *****
2023-03-27 18:42:08,643   att_loss = 55470.32724440029
2023-03-27 18:42:08,644   cls_loss = 0.0
2023-03-27 18:42:08,644   global_step = 7649
2023-03-27 18:42:08,644   loss = 55470.93219382226
2023-03-27 18:42:08,644   rep_loss = 0.6050191790382297
2023-03-27 18:42:08,651 ***** Save model *****
2023-03-27 18:42:24,581 ***** Running evaluation *****
2023-03-27 18:42:24,581   Epoch = 28 iter 7699 step
2023-03-27 18:42:24,581   Num examples = 1043
2023-03-27 18:42:24,582   Batch size = 32
2023-03-27 18:42:24,583 ***** Eval results *****
2023-03-27 18:42:24,583   att_loss = 55366.11004063901
2023-03-27 18:42:24,583   cls_loss = 0.0
2023-03-27 18:42:24,583   global_step = 7699
2023-03-27 18:42:24,583   loss = 55366.714143077355
2023-03-27 18:42:24,583   rep_loss = 0.6041634593309309
2023-03-27 18:42:24,591 ***** Save model *****
2023-03-27 18:42:40,522 ***** Running evaluation *****
2023-03-27 18:42:40,523   Epoch = 29 iter 7749 step
2023-03-27 18:42:40,523   Num examples = 1043
2023-03-27 18:42:40,523   Batch size = 32
2023-03-27 18:42:40,525 ***** Eval results *****
2023-03-27 18:42:40,525   att_loss = 54144.942057291664
2023-03-27 18:42:40,525   cls_loss = 0.0
2023-03-27 18:42:40,525   global_step = 7749
2023-03-27 18:42:40,526   loss = 54145.539713541664
2023-03-27 18:42:40,526   rep_loss = 0.5975385109583536
2023-03-27 18:42:40,534 ***** Save model *****
2023-03-27 18:42:56,459 ***** Running evaluation *****
2023-03-27 18:42:56,459   Epoch = 29 iter 7799 step
2023-03-27 18:42:56,460   Num examples = 1043
2023-03-27 18:42:56,460   Batch size = 32
2023-03-27 18:42:56,461 ***** Eval results *****
2023-03-27 18:42:56,461   att_loss = 54506.04429408482
2023-03-27 18:42:56,461   cls_loss = 0.0
2023-03-27 18:42:56,461   global_step = 7799
2023-03-27 18:42:56,462   loss = 54506.64592633928
2023-03-27 18:42:56,462   rep_loss = 0.6014143803289959
2023-03-27 18:42:56,469 ***** Save model *****
2023-03-27 18:43:12,395 ***** Running evaluation *****
2023-03-27 18:43:12,395   Epoch = 29 iter 7849 step
2023-03-27 18:43:12,395   Num examples = 1043
2023-03-27 18:43:12,396   Batch size = 32
2023-03-27 18:43:12,396 ***** Eval results *****
2023-03-27 18:43:12,397   att_loss = 54939.63804540094
2023-03-27 18:43:12,397   cls_loss = 0.0
2023-03-27 18:43:12,397   global_step = 7849
2023-03-27 18:43:12,397   loss = 54940.240639740565
2023-03-27 18:43:12,397   rep_loss = 0.6023942541401341
2023-03-27 18:43:12,404 ***** Save model *****
2023-03-27 18:43:28,349 ***** Running evaluation *****
2023-03-27 18:43:28,350   Epoch = 29 iter 7899 step
2023-03-27 18:43:28,350   Num examples = 1043
2023-03-27 18:43:28,350   Batch size = 32
2023-03-27 18:43:28,351 ***** Eval results *****
2023-03-27 18:43:28,351   att_loss = 55011.123121995195
2023-03-27 18:43:28,351   cls_loss = 0.0
2023-03-27 18:43:28,352   global_step = 7899
2023-03-27 18:43:28,352   loss = 55011.726036658656
2023-03-27 18:43:28,352   rep_loss = 0.6028107301546977
2023-03-27 18:43:28,359 ***** Save model *****
2023-03-27 18:43:44,315 ***** Running evaluation *****
2023-03-27 18:43:44,315   Epoch = 29 iter 7949 step
2023-03-27 18:43:44,316   Num examples = 1043
2023-03-27 18:43:44,316   Batch size = 32
2023-03-27 18:43:44,317 ***** Eval results *****
2023-03-27 18:43:44,318   att_loss = 54995.151831765776
2023-03-27 18:43:44,318   cls_loss = 0.0
2023-03-27 18:43:44,318   global_step = 7949
2023-03-27 18:43:44,318   loss = 54995.75502503034
2023-03-27 18:43:44,318   rep_loss = 0.6030561044378188
2023-03-27 18:43:44,325 ***** Save model *****
2023-03-27 18:44:00,191 ***** Running evaluation *****
2023-03-27 18:44:00,191   Epoch = 29 iter 7999 step
2023-03-27 18:44:00,192   Num examples = 1043
2023-03-27 18:44:00,192   Batch size = 32
2023-03-27 18:44:00,194 ***** Eval results *****
2023-03-27 18:44:00,195   att_loss = 54917.314376831055
2023-03-27 18:44:00,195   cls_loss = 0.0
2023-03-27 18:44:00,195   global_step = 7999
2023-03-27 18:44:00,195   loss = 54917.91677856445
2023-03-27 18:44:00,196   rep_loss = 0.6022903525736183
2023-03-27 18:44:00,205 ***** Save model *****
2023-03-27 18:44:16,854 device: cuda n_gpu: 1
2023-03-27 18:44:16,900 Writing example 0 of 8551
2023-03-27 18:44:16,901 *** Example ***
2023-03-27 18:44:16,901 guid: train-0
2023-03-27 18:44:16,901 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 18:44:16,902 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:44:16,902 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:44:16,902 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:44:16,902 label: 1
2023-03-27 18:44:16,903 label_id: 1
2023-03-27 18:44:17,851 Writing example 0 of 1043
2023-03-27 18:44:17,851 *** Example ***
2023-03-27 18:44:17,851 guid: dev-0
2023-03-27 18:44:17,851 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 18:44:17,852 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:44:17,852 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:44:17,852 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 18:44:17,852 label: 1
2023-03-27 18:44:17,852 label_id: 1
2023-03-27 18:44:17,968 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 18:44:17,969 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 18:44:19,745 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 18:44:19,920 loading model...
2023-03-27 18:44:19,970 done!
2023-03-27 18:44:19,970 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 18:44:20,029 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 18:44:20,032 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 18:44:20,257 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 18:44:20,278 loading model...
2023-03-27 18:44:20,286 done!
2023-03-27 18:44:20,286 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 18:44:20,286 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 18:44:20,297 ***** Running training *****
2023-03-27 18:44:20,297   Num examples = 8551
2023-03-27 18:44:20,297   Batch size = 32
2023-03-27 18:44:20,297   Num steps = 8010
2023-03-27 18:44:20,297 n: bert.embeddings.word_embeddings.weight
2023-03-27 18:44:20,297 n: bert.embeddings.position_embeddings.weight
2023-03-27 18:44:20,298 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 18:44:20,298 n: bert.embeddings.LayerNorm.weight
2023-03-27 18:44:20,298 n: bert.embeddings.LayerNorm.bias
2023-03-27 18:44:20,298 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 18:44:20,298 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 18:44:20,298 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 18:44:20,298 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 18:44:20,298 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 18:44:20,298 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 18:44:20,298 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 18:44:20,299 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 18:44:20,299 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 18:44:20,299 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 18:44:20,299 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 18:44:20,299 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 18:44:20,299 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 18:44:20,299 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 18:44:20,299 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 18:44:20,299 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 18:44:20,300 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 18:44:20,300 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 18:44:20,300 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 18:44:20,300 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 18:44:20,300 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 18:44:20,300 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 18:44:20,300 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 18:44:20,300 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 18:44:20,300 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 18:44:20,300 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 18:44:20,301 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 18:44:20,301 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 18:44:20,301 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 18:44:20,301 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 18:44:20,301 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 18:44:20,301 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 18:44:20,301 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 18:44:20,301 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 18:44:20,301 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 18:44:20,302 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 18:44:20,302 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 18:44:20,302 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 18:44:20,302 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 18:44:20,302 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 18:44:20,302 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 18:44:20,302 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 18:44:20,302 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 18:44:20,302 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 18:44:20,302 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 18:44:20,303 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 18:44:20,303 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 18:44:20,303 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 18:44:20,303 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 18:44:20,303 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 18:44:20,303 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 18:44:20,303 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 18:44:20,303 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 18:44:20,303 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 18:44:20,303 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 18:44:20,304 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 18:44:20,304 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 18:44:20,304 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 18:44:20,304 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 18:44:20,304 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 18:44:20,304 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 18:44:20,304 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 18:44:20,304 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 18:44:20,304 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 18:44:20,305 n: bert.pooler.dense.weight
2023-03-27 18:44:20,305 n: bert.pooler.dense.bias
2023-03-27 18:44:20,305 n: classifier.weight
2023-03-27 18:44:20,305 n: classifier.bias
2023-03-27 18:44:20,305 n: fit_dense.weight
2023-03-27 18:44:20,305 n: fit_dense.bias
2023-03-27 18:44:20,305 Total parameters: 14591258
2023-03-27 18:44:35,074 ***** Running evaluation *****
2023-03-27 18:44:35,074   Epoch = 0 iter 49 step
2023-03-27 18:44:35,074   Num examples = 1043
2023-03-27 18:44:35,074   Batch size = 32
2023-03-27 18:44:35,082 ***** Eval results *****
2023-03-27 18:44:35,082   att_loss = 8772141.010204082
2023-03-27 18:44:35,083   cls_loss = 0.0
2023-03-27 18:44:35,083   global_step = 49
2023-03-27 18:44:35,083   loss = 8772142.663265307
2023-03-27 18:44:35,083   rep_loss = 1.6544393957877646
2023-03-27 18:44:35,091 ***** Save model *****
2023-03-27 18:44:50,880 ***** Running evaluation *****
2023-03-27 18:44:50,880   Epoch = 0 iter 99 step
2023-03-27 18:44:50,880   Num examples = 1043
2023-03-27 18:44:50,880   Batch size = 32
2023-03-27 18:44:50,882 ***** Eval results *****
2023-03-27 18:44:50,882   att_loss = 8095873.777777778
2023-03-27 18:44:50,882   cls_loss = 0.0
2023-03-27 18:44:50,883   global_step = 99
2023-03-27 18:44:50,883   loss = 8095875.313131313
2023-03-27 18:44:50,883   rep_loss = 1.4731342250650579
2023-03-27 18:44:50,890 ***** Save model *****
2023-03-27 18:45:06,674 ***** Running evaluation *****
2023-03-27 18:45:06,675   Epoch = 0 iter 149 step
2023-03-27 18:45:06,675   Num examples = 1043
2023-03-27 18:45:06,675   Batch size = 32
2023-03-27 18:45:06,676 ***** Eval results *****
2023-03-27 18:45:06,677   att_loss = 7731587.533557047
2023-03-27 18:45:06,677   cls_loss = 0.0
2023-03-27 18:45:06,677   global_step = 149
2023-03-27 18:45:06,677   loss = 7731588.889261745
2023-03-27 18:45:06,677   rep_loss = 1.3760941428626143
2023-03-27 18:45:06,684 ***** Save model *****
2023-03-27 18:45:22,522 ***** Running evaluation *****
2023-03-27 18:45:22,522   Epoch = 0 iter 199 step
2023-03-27 18:45:22,522   Num examples = 1043
2023-03-27 18:45:22,522   Batch size = 32
2023-03-27 18:45:22,523 ***** Eval results *****
2023-03-27 18:45:22,524   att_loss = 7513581.600502512
2023-03-27 18:45:22,524   cls_loss = 0.0
2023-03-27 18:45:22,524   global_step = 199
2023-03-27 18:45:22,524   loss = 7513582.866834171
2023-03-27 18:45:22,524   rep_loss = 1.3118175202278635
2023-03-27 18:45:22,531 ***** Save model *****
2023-03-27 18:45:38,360 ***** Running evaluation *****
2023-03-27 18:45:38,361   Epoch = 0 iter 249 step
2023-03-27 18:45:38,361   Num examples = 1043
2023-03-27 18:45:38,361   Batch size = 32
2023-03-27 18:45:38,362 ***** Eval results *****
2023-03-27 18:45:38,362   att_loss = 7373775.008032128
2023-03-27 18:45:38,363   cls_loss = 0.0
2023-03-27 18:45:38,363   global_step = 249
2023-03-27 18:45:38,363   loss = 7373776.220883534
2023-03-27 18:45:38,363   rep_loss = 1.2644298761245236
2023-03-27 18:45:38,370 ***** Save model *****
2023-03-27 18:45:54,222 ***** Running evaluation *****
2023-03-27 18:45:54,223   Epoch = 1 iter 299 step
2023-03-27 18:45:54,223   Num examples = 1043
2023-03-27 18:45:54,223   Batch size = 32
2023-03-27 18:45:54,224 ***** Eval results *****
2023-03-27 18:45:54,224   att_loss = 6672022.328125
2023-03-27 18:45:54,224   cls_loss = 0.0
2023-03-27 18:45:54,224   global_step = 299
2023-03-27 18:45:54,224   loss = 6672023.328125
2023-03-27 18:45:54,225   rep_loss = 1.029799522832036
2023-03-27 18:45:54,227 ***** Save model *****
2023-03-27 18:46:10,057 ***** Running evaluation *****
2023-03-27 18:46:10,057   Epoch = 1 iter 349 step
2023-03-27 18:46:10,057   Num examples = 1043
2023-03-27 18:46:10,058   Batch size = 32
2023-03-27 18:46:10,060 ***** Eval results *****
2023-03-27 18:46:10,060   att_loss = 6660208.4878048785
2023-03-27 18:46:10,060   cls_loss = 0.0
2023-03-27 18:46:10,060   global_step = 349
2023-03-27 18:46:10,060   loss = 6660209.4878048785
2023-03-27 18:46:10,061   rep_loss = 1.0150874576917508
2023-03-27 18:46:10,064 ***** Save model *****
2023-03-27 18:46:25,930 ***** Running evaluation *****
2023-03-27 18:46:25,931   Epoch = 1 iter 399 step
2023-03-27 18:46:25,931   Num examples = 1043
2023-03-27 18:46:25,931   Batch size = 32
2023-03-27 18:46:25,935 ***** Eval results *****
2023-03-27 18:46:25,935   att_loss = 6637574.810606061
2023-03-27 18:46:25,935   cls_loss = 0.0
2023-03-27 18:46:25,935   global_step = 399
2023-03-27 18:46:25,935   loss = 6637575.810606061
2023-03-27 18:46:25,935   rep_loss = 1.0018209223494385
2023-03-27 18:46:25,943 ***** Save model *****
2023-03-27 18:46:41,820 ***** Running evaluation *****
2023-03-27 18:46:41,820   Epoch = 1 iter 449 step
2023-03-27 18:46:41,821   Num examples = 1043
2023-03-27 18:46:41,821   Batch size = 32
2023-03-27 18:46:41,822 ***** Eval results *****
2023-03-27 18:46:41,823   att_loss = 6645768.269230769
2023-03-27 18:46:41,823   cls_loss = 0.0
2023-03-27 18:46:41,823   global_step = 449
2023-03-27 18:46:41,823   loss = 6645769.269230769
2023-03-27 18:46:41,823   rep_loss = 0.9900920106159462
2023-03-27 18:46:41,831 ***** Save model *****
2023-03-27 18:46:57,751 ***** Running evaluation *****
2023-03-27 18:46:57,752   Epoch = 1 iter 499 step
2023-03-27 18:46:57,752   Num examples = 1043
2023-03-27 18:46:57,752   Batch size = 32
2023-03-27 18:46:57,753 ***** Eval results *****
2023-03-27 18:46:57,753   att_loss = 6609073.810344827
2023-03-27 18:46:57,753   cls_loss = 0.0
2023-03-27 18:46:57,753   global_step = 499
2023-03-27 18:46:57,753   loss = 6609074.810344827
2023-03-27 18:46:57,753   rep_loss = 0.9775924500206421
2023-03-27 18:46:57,760 ***** Save model *****
2023-03-27 18:47:13,643 ***** Running evaluation *****
2023-03-27 18:47:13,644   Epoch = 2 iter 549 step
2023-03-27 18:47:13,644   Num examples = 1043
2023-03-27 18:47:13,644   Batch size = 32
2023-03-27 18:47:13,646 ***** Eval results *****
2023-03-27 18:47:13,646   att_loss = 6229635.6
2023-03-27 18:47:13,647   cls_loss = 0.0
2023-03-27 18:47:13,647   global_step = 549
2023-03-27 18:47:13,647   loss = 6229636.6
2023-03-27 18:47:13,647   rep_loss = 0.8912290930747986
2023-03-27 18:47:13,654 ***** Save model *****
2023-03-27 18:47:29,529 ***** Running evaluation *****
2023-03-27 18:47:29,530   Epoch = 2 iter 599 step
2023-03-27 18:47:29,530   Num examples = 1043
2023-03-27 18:47:29,530   Batch size = 32
2023-03-27 18:47:29,531 ***** Eval results *****
2023-03-27 18:47:29,532   att_loss = 6386022.876923077
2023-03-27 18:47:29,532   cls_loss = 0.0
2023-03-27 18:47:29,532   global_step = 599
2023-03-27 18:47:29,532   loss = 6386023.876923077
2023-03-27 18:47:29,532   rep_loss = 0.8927238812813392
2023-03-27 18:47:29,539 ***** Save model *****
2023-03-27 18:47:45,418 ***** Running evaluation *****
2023-03-27 18:47:45,418   Epoch = 2 iter 649 step
2023-03-27 18:47:45,418   Num examples = 1043
2023-03-27 18:47:45,419   Batch size = 32
2023-03-27 18:47:45,420 ***** Eval results *****
2023-03-27 18:47:45,420   att_loss = 6368222.195652174
2023-03-27 18:47:45,420   cls_loss = 0.0
2023-03-27 18:47:45,420   global_step = 649
2023-03-27 18:47:45,420   loss = 6368223.195652174
2023-03-27 18:47:45,420   rep_loss = 0.8834083085474761
2023-03-27 18:47:45,427 ***** Save model *****
2023-03-27 18:48:01,346 ***** Running evaluation *****
2023-03-27 18:48:01,346   Epoch = 2 iter 699 step
2023-03-27 18:48:01,347   Num examples = 1043
2023-03-27 18:48:01,347   Batch size = 32
2023-03-27 18:48:01,350 ***** Eval results *****
2023-03-27 18:48:01,350   att_loss = 6382634.796969697
2023-03-27 18:48:01,350   cls_loss = 0.0
2023-03-27 18:48:01,350   global_step = 699
2023-03-27 18:48:01,351   loss = 6382635.796969697
2023-03-27 18:48:01,351   rep_loss = 0.8774340831872188
2023-03-27 18:48:01,358 ***** Save model *****
2023-03-27 18:48:17,266 ***** Running evaluation *****
2023-03-27 18:48:17,266   Epoch = 2 iter 749 step
2023-03-27 18:48:17,266   Num examples = 1043
2023-03-27 18:48:17,267   Batch size = 32
2023-03-27 18:48:17,268 ***** Eval results *****
2023-03-27 18:48:17,268   att_loss = 6387473.902325582
2023-03-27 18:48:17,269   cls_loss = 0.0
2023-03-27 18:48:17,269   global_step = 749
2023-03-27 18:48:17,269   loss = 6387474.902325582
2023-03-27 18:48:17,269   rep_loss = 0.8715457830318185
2023-03-27 18:48:17,276 ***** Save model *****
2023-03-27 18:48:33,198 ***** Running evaluation *****
2023-03-27 18:48:33,198   Epoch = 2 iter 799 step
2023-03-27 18:48:33,199   Num examples = 1043
2023-03-27 18:48:33,199   Batch size = 32
2023-03-27 18:48:33,200 ***** Eval results *****
2023-03-27 18:48:33,200   att_loss = 6375219.01509434
2023-03-27 18:48:33,200   cls_loss = 0.0
2023-03-27 18:48:33,200   global_step = 799
2023-03-27 18:48:33,200   loss = 6375220.01509434
2023-03-27 18:48:33,200   rep_loss = 0.8653223440332233
2023-03-27 18:48:33,207 ***** Save model *****
2023-03-27 18:48:49,133 ***** Running evaluation *****
2023-03-27 18:48:49,133   Epoch = 3 iter 849 step
2023-03-27 18:48:49,133   Num examples = 1043
2023-03-27 18:48:49,133   Batch size = 32
2023-03-27 18:48:49,135 ***** Eval results *****
2023-03-27 18:48:49,135   att_loss = 6165580.25
2023-03-27 18:48:49,135   cls_loss = 0.0
2023-03-27 18:48:49,136   global_step = 849
2023-03-27 18:48:49,136   loss = 6165581.25
2023-03-27 18:48:49,136   rep_loss = 0.8207402353485426
2023-03-27 18:48:49,143 ***** Save model *****
2023-03-27 18:49:05,104 ***** Running evaluation *****
2023-03-27 18:49:05,104   Epoch = 3 iter 899 step
2023-03-27 18:49:05,105   Num examples = 1043
2023-03-27 18:49:05,105   Batch size = 32
2023-03-27 18:49:05,105 ***** Eval results *****
2023-03-27 18:49:05,106   att_loss = 6197056.882653061
2023-03-27 18:49:05,106   cls_loss = 0.0
2023-03-27 18:49:05,106   global_step = 899
2023-03-27 18:49:05,106   loss = 6197057.882653061
2023-03-27 18:49:05,106   rep_loss = 0.8164106169525458
2023-03-27 18:49:05,113 ***** Save model *****
2023-03-27 18:49:21,047 ***** Running evaluation *****
2023-03-27 18:49:21,047   Epoch = 3 iter 949 step
2023-03-27 18:49:21,047   Num examples = 1043
2023-03-27 18:49:21,048   Batch size = 32
2023-03-27 18:49:21,049 ***** Eval results *****
2023-03-27 18:49:21,050   att_loss = 6221389.14527027
2023-03-27 18:49:21,050   cls_loss = 0.0
2023-03-27 18:49:21,050   global_step = 949
2023-03-27 18:49:21,050   loss = 6221390.14527027
2023-03-27 18:49:21,051   rep_loss = 0.8136629570980329
2023-03-27 18:49:21,058 ***** Save model *****
2023-03-27 18:49:36,961 ***** Running evaluation *****
2023-03-27 18:49:36,962   Epoch = 3 iter 999 step
2023-03-27 18:49:36,962   Num examples = 1043
2023-03-27 18:49:36,962   Batch size = 32
2023-03-27 18:49:36,963 ***** Eval results *****
2023-03-27 18:49:36,964   att_loss = 6223054.1338383835
2023-03-27 18:49:36,964   cls_loss = 0.0
2023-03-27 18:49:36,964   global_step = 999
2023-03-27 18:49:36,964   loss = 6223055.1338383835
2023-03-27 18:49:36,964   rep_loss = 0.8092799466667753
2023-03-27 18:49:36,966 ***** Save model *****
2023-03-27 18:49:52,902 ***** Running evaluation *****
2023-03-27 18:49:52,903   Epoch = 3 iter 1049 step
2023-03-27 18:49:52,903   Num examples = 1043
2023-03-27 18:49:52,903   Batch size = 32
2023-03-27 18:49:52,904 ***** Eval results *****
2023-03-27 18:49:52,904   att_loss = 6227096.895161291
2023-03-27 18:49:52,904   cls_loss = 0.0
2023-03-27 18:49:52,905   global_step = 1049
2023-03-27 18:49:52,905   loss = 6227097.893145162
2023-03-27 18:49:52,905   rep_loss = 0.8048880674665974
2023-03-27 18:49:52,912 ***** Save model *****
2023-03-27 18:50:08,849 ***** Running evaluation *****
2023-03-27 18:50:08,850   Epoch = 4 iter 1099 step
2023-03-27 18:50:08,850   Num examples = 1043
2023-03-27 18:50:08,850   Batch size = 32
2023-03-27 18:50:08,851 ***** Eval results *****
2023-03-27 18:50:08,851   att_loss = 6232949.725806451
2023-03-27 18:50:08,852   cls_loss = 0.0
2023-03-27 18:50:08,852   global_step = 1099
2023-03-27 18:50:08,852   loss = 6232950.70967742
2023-03-27 18:50:08,852   rep_loss = 0.7795228054446559
2023-03-27 18:50:08,854 ***** Save model *****
2023-03-27 18:50:24,772 ***** Running evaluation *****
2023-03-27 18:50:24,772   Epoch = 4 iter 1149 step
2023-03-27 18:50:24,772   Num examples = 1043
2023-03-27 18:50:24,772   Batch size = 32
2023-03-27 18:50:24,774 ***** Eval results *****
2023-03-27 18:50:24,774   att_loss = 6191505.209876543
2023-03-27 18:50:24,774   cls_loss = 0.0
2023-03-27 18:50:24,775   global_step = 1149
2023-03-27 18:50:24,775   loss = 6191506.179012346
2023-03-27 18:50:24,775   rep_loss = 0.776390775486275
2023-03-27 18:50:24,782 ***** Save model *****
2023-03-27 18:50:40,731 ***** Running evaluation *****
2023-03-27 18:50:40,731   Epoch = 4 iter 1199 step
2023-03-27 18:50:40,731   Num examples = 1043
2023-03-27 18:50:40,731   Batch size = 32
2023-03-27 18:50:40,732 ***** Eval results *****
2023-03-27 18:50:40,733   att_loss = 6164221.721374046
2023-03-27 18:50:40,733   cls_loss = 0.0
2023-03-27 18:50:40,733   global_step = 1199
2023-03-27 18:50:40,733   loss = 6164222.652671755
2023-03-27 18:50:40,733   rep_loss = 0.7712424383818648
2023-03-27 18:50:40,735 ***** Save model *****
2023-03-27 18:50:56,642 ***** Running evaluation *****
2023-03-27 18:50:56,642   Epoch = 4 iter 1249 step
2023-03-27 18:50:56,642   Num examples = 1043
2023-03-27 18:50:56,643   Batch size = 32
2023-03-27 18:50:56,644 ***** Eval results *****
2023-03-27 18:50:56,644   att_loss = 6144026.690607735
2023-03-27 18:50:56,644   cls_loss = 0.0
2023-03-27 18:50:56,644   global_step = 1249
2023-03-27 18:50:56,644   loss = 6144027.585635359
2023-03-27 18:50:56,645   rep_loss = 0.7670526402431298
2023-03-27 18:50:56,647 ***** Save model *****
2023-03-27 18:51:12,570 ***** Running evaluation *****
2023-03-27 18:51:12,571   Epoch = 4 iter 1299 step
2023-03-27 18:51:12,571   Num examples = 1043
2023-03-27 18:51:12,571   Batch size = 32
2023-03-27 18:51:12,573 ***** Eval results *****
2023-03-27 18:51:12,573   att_loss = 6151419.233766234
2023-03-27 18:51:12,573   cls_loss = 0.0
2023-03-27 18:51:12,573   global_step = 1299
2023-03-27 18:51:12,574   loss = 6151420.106060606
2023-03-27 18:51:12,574   rep_loss = 0.7644575777507964
2023-03-27 18:51:12,581 ***** Save model *****
2023-03-27 18:51:28,531 ***** Running evaluation *****
2023-03-27 18:51:28,531   Epoch = 5 iter 1349 step
2023-03-27 18:51:28,531   Num examples = 1043
2023-03-27 18:51:28,531   Batch size = 32
2023-03-27 18:51:28,533 ***** Eval results *****
2023-03-27 18:51:28,533   att_loss = 6131093.892857143
2023-03-27 18:51:28,533   cls_loss = 0.0
2023-03-27 18:51:28,534   global_step = 1349
2023-03-27 18:51:28,534   loss = 6131094.571428572
2023-03-27 18:51:28,534   rep_loss = 0.7445162492138999
2023-03-27 18:51:28,541 ***** Save model *****
2023-03-27 18:51:44,478 ***** Running evaluation *****
2023-03-27 18:51:44,478   Epoch = 5 iter 1399 step
2023-03-27 18:51:44,478   Num examples = 1043
2023-03-27 18:51:44,478   Batch size = 32
2023-03-27 18:51:44,479 ***** Eval results *****
2023-03-27 18:51:44,480   att_loss = 6091730.4375
2023-03-27 18:51:44,480   cls_loss = 0.0
2023-03-27 18:51:44,480   global_step = 1399
2023-03-27 18:51:44,480   loss = 6091731.0859375
2023-03-27 18:51:44,480   rep_loss = 0.7443858087062836
2023-03-27 18:51:44,487 ***** Save model *****
2023-03-27 18:52:00,408 ***** Running evaluation *****
2023-03-27 18:52:00,408   Epoch = 5 iter 1449 step
2023-03-27 18:52:00,409   Num examples = 1043
2023-03-27 18:52:00,409   Batch size = 32
2023-03-27 18:52:00,411 ***** Eval results *****
2023-03-27 18:52:00,411   att_loss = 6075497.206140351
2023-03-27 18:52:00,412   cls_loss = 0.0
2023-03-27 18:52:00,412   global_step = 1449
2023-03-27 18:52:00,412   loss = 6075497.868421053
2023-03-27 18:52:00,413   rep_loss = 0.7421848601416537
2023-03-27 18:52:00,422 ***** Save model *****
2023-03-27 18:52:16,395 ***** Running evaluation *****
2023-03-27 18:52:16,395   Epoch = 5 iter 1499 step
2023-03-27 18:52:16,395   Num examples = 1043
2023-03-27 18:52:16,396   Batch size = 32
2023-03-27 18:52:16,397 ***** Eval results *****
2023-03-27 18:52:16,397   att_loss = 6078840.320121951
2023-03-27 18:52:16,397   cls_loss = 0.0
2023-03-27 18:52:16,397   global_step = 1499
2023-03-27 18:52:16,397   loss = 6078840.957317073
2023-03-27 18:52:16,398   rep_loss = 0.7397081888303524
2023-03-27 18:52:16,405 ***** Save model *****
2023-03-27 18:52:32,325 ***** Running evaluation *****
2023-03-27 18:52:32,325   Epoch = 5 iter 1549 step
2023-03-27 18:52:32,326   Num examples = 1043
2023-03-27 18:52:32,326   Batch size = 32
2023-03-27 18:52:32,327 ***** Eval results *****
2023-03-27 18:52:32,327   att_loss = 6074558.289719626
2023-03-27 18:52:32,327   cls_loss = 0.0
2023-03-27 18:52:32,328   global_step = 1549
2023-03-27 18:52:32,328   loss = 6074558.908878504
2023-03-27 18:52:32,328   rep_loss = 0.737111648387998
2023-03-27 18:52:32,335 ***** Save model *****
2023-03-27 18:52:48,271 ***** Running evaluation *****
2023-03-27 18:52:48,272   Epoch = 5 iter 1599 step
2023-03-27 18:52:48,272   Num examples = 1043
2023-03-27 18:52:48,272   Batch size = 32
2023-03-27 18:52:48,274 ***** Eval results *****
2023-03-27 18:52:48,274   att_loss = 6071087.333333333
2023-03-27 18:52:48,274   cls_loss = 0.0
2023-03-27 18:52:48,274   global_step = 1599
2023-03-27 18:52:48,275   loss = 6071087.9375
2023-03-27 18:52:48,275   rep_loss = 0.7351166739156751
2023-03-27 18:52:48,282 ***** Save model *****
2023-03-27 18:53:04,199 ***** Running evaluation *****
2023-03-27 18:53:04,199   Epoch = 6 iter 1649 step
2023-03-27 18:53:04,199   Num examples = 1043
2023-03-27 18:53:04,199   Batch size = 32
2023-03-27 18:53:04,200 ***** Eval results *****
2023-03-27 18:53:04,201   att_loss = 5978689.872340426
2023-03-27 18:53:04,201   cls_loss = 0.0
2023-03-27 18:53:04,201   global_step = 1649
2023-03-27 18:53:04,201   loss = 5978690.393617021
2023-03-27 18:53:04,201   rep_loss = 0.7182075736370492
2023-03-27 18:53:04,204 ***** Save model *****
2023-03-27 18:53:20,138 ***** Running evaluation *****
2023-03-27 18:53:20,139   Epoch = 6 iter 1699 step
2023-03-27 18:53:20,139   Num examples = 1043
2023-03-27 18:53:20,139   Batch size = 32
2023-03-27 18:53:20,140 ***** Eval results *****
2023-03-27 18:53:20,140   att_loss = 6002773.0051546395
2023-03-27 18:53:20,140   cls_loss = 0.0
2023-03-27 18:53:20,140   global_step = 1699
2023-03-27 18:53:20,140   loss = 6002773.520618557
2023-03-27 18:53:20,141   rep_loss = 0.7192459677912525
2023-03-27 18:53:20,148 ***** Save model *****
2023-03-27 18:53:36,084 ***** Running evaluation *****
2023-03-27 18:53:36,084   Epoch = 6 iter 1749 step
2023-03-27 18:53:36,085   Num examples = 1043
2023-03-27 18:53:36,085   Batch size = 32
2023-03-27 18:53:36,086 ***** Eval results *****
2023-03-27 18:53:36,086   att_loss = 6004321.452380952
2023-03-27 18:53:36,087   cls_loss = 0.0
2023-03-27 18:53:36,087   global_step = 1749
2023-03-27 18:53:36,087   loss = 6004321.969387755
2023-03-27 18:53:36,087   rep_loss = 0.7184097787149909
2023-03-27 18:53:36,095 ***** Save model *****
2023-03-27 18:53:52,024 ***** Running evaluation *****
2023-03-27 18:53:52,024   Epoch = 6 iter 1799 step
2023-03-27 18:53:52,024   Num examples = 1043
2023-03-27 18:53:52,025   Batch size = 32
2023-03-27 18:53:52,026 ***** Eval results *****
2023-03-27 18:53:52,026   att_loss = 6000767.175126904
2023-03-27 18:53:52,026   cls_loss = 0.0
2023-03-27 18:53:52,027   global_step = 1799
2023-03-27 18:53:52,027   loss = 6000767.687817259
2023-03-27 18:53:52,027   rep_loss = 0.7168312142343085
2023-03-27 18:53:52,035 ***** Save model *****
2023-03-27 18:54:07,982 ***** Running evaluation *****
2023-03-27 18:54:07,982   Epoch = 6 iter 1849 step
2023-03-27 18:54:07,983   Num examples = 1043
2023-03-27 18:54:07,983   Batch size = 32
2023-03-27 18:54:07,984 ***** Eval results *****
2023-03-27 18:54:07,984   att_loss = 6006501.995951417
2023-03-27 18:54:07,984   cls_loss = 0.0
2023-03-27 18:54:07,985   global_step = 1849
2023-03-27 18:54:07,985   loss = 6006502.506072874
2023-03-27 18:54:07,985   rep_loss = 0.7144948055869654
2023-03-27 18:54:07,992 ***** Save model *****
2023-03-27 18:54:23,904 ***** Running evaluation *****
2023-03-27 18:54:23,905   Epoch = 7 iter 1899 step
2023-03-27 18:54:23,905   Num examples = 1043
2023-03-27 18:54:23,905   Batch size = 32
2023-03-27 18:54:23,906 ***** Eval results *****
2023-03-27 18:54:23,906   att_loss = 5890957.916666667
2023-03-27 18:54:23,907   cls_loss = 0.0
2023-03-27 18:54:23,907   global_step = 1899
2023-03-27 18:54:23,907   loss = 5890958.416666667
2023-03-27 18:54:23,907   rep_loss = 0.6987526973088583
2023-03-27 18:54:23,915 ***** Save model *****
2023-03-27 18:54:39,821 ***** Running evaluation *****
2023-03-27 18:54:39,821   Epoch = 7 iter 1949 step
2023-03-27 18:54:39,821   Num examples = 1043
2023-03-27 18:54:39,821   Batch size = 32
2023-03-27 18:54:39,824 ***** Eval results *****
2023-03-27 18:54:39,824   att_loss = 5938219.8625
2023-03-27 18:54:39,825   cls_loss = 0.0
2023-03-27 18:54:39,825   global_step = 1949
2023-03-27 18:54:39,825   loss = 5938220.3625
2023-03-27 18:54:39,825   rep_loss = 0.6995260484516621
2023-03-27 18:54:39,833 ***** Save model *****
2023-03-27 18:54:55,755 ***** Running evaluation *****
2023-03-27 18:54:55,756   Epoch = 7 iter 1999 step
2023-03-27 18:54:55,756   Num examples = 1043
2023-03-27 18:54:55,756   Batch size = 32
2023-03-27 18:54:55,758 ***** Eval results *****
2023-03-27 18:54:55,758   att_loss = 5963562.684615385
2023-03-27 18:54:55,758   cls_loss = 0.0
2023-03-27 18:54:55,758   global_step = 1999
2023-03-27 18:54:55,759   loss = 5963563.184615385
2023-03-27 18:54:55,759   rep_loss = 0.7000927008115329
2023-03-27 18:54:55,761 ***** Save model *****
2023-03-27 18:55:11,653 ***** Running evaluation *****
2023-03-27 18:55:11,653   Epoch = 7 iter 2049 step
2023-03-27 18:55:11,653   Num examples = 1043
2023-03-27 18:55:11,654   Batch size = 32
2023-03-27 18:55:11,655 ***** Eval results *****
2023-03-27 18:55:11,655   att_loss = 5970694.780555556
2023-03-27 18:55:11,655   cls_loss = 0.0
2023-03-27 18:55:11,656   global_step = 2049
2023-03-27 18:55:11,656   loss = 5970695.280555556
2023-03-27 18:55:11,656   rep_loss = 0.7003639909956191
2023-03-27 18:55:11,663 ***** Save model *****
2023-03-27 18:55:27,557 ***** Running evaluation *****
2023-03-27 18:55:27,558   Epoch = 7 iter 2099 step
2023-03-27 18:55:27,558   Num examples = 1043
2023-03-27 18:55:27,558   Batch size = 32
2023-03-27 18:55:27,559 ***** Eval results *****
2023-03-27 18:55:27,560   att_loss = 5968826.397826087
2023-03-27 18:55:27,560   cls_loss = 0.0
2023-03-27 18:55:27,560   global_step = 2099
2023-03-27 18:55:27,561   loss = 5968826.897826087
2023-03-27 18:55:27,561   rep_loss = 0.699311658869619
2023-03-27 18:55:27,563 ***** Save model *****
2023-03-27 18:55:43,474 ***** Running evaluation *****
2023-03-27 18:55:43,475   Epoch = 8 iter 2149 step
2023-03-27 18:55:43,475   Num examples = 1043
2023-03-27 18:55:43,475   Batch size = 32
2023-03-27 18:55:43,476 ***** Eval results *****
2023-03-27 18:55:43,476   att_loss = 5901230.0
2023-03-27 18:55:43,476   cls_loss = 0.0
2023-03-27 18:55:43,477   global_step = 2149
2023-03-27 18:55:43,477   loss = 5901230.5
2023-03-27 18:55:43,477   rep_loss = 0.6904450150636526
2023-03-27 18:55:43,484 ***** Save model *****
2023-03-27 18:55:59,417 ***** Running evaluation *****
2023-03-27 18:55:59,417   Epoch = 8 iter 2199 step
2023-03-27 18:55:59,417   Num examples = 1043
2023-03-27 18:55:59,417   Batch size = 32
2023-03-27 18:55:59,419 ***** Eval results *****
2023-03-27 18:55:59,419   att_loss = 5892827.849206349
2023-03-27 18:55:59,419   cls_loss = 0.0
2023-03-27 18:55:59,420   global_step = 2199
2023-03-27 18:55:59,420   loss = 5892828.349206349
2023-03-27 18:55:59,420   rep_loss = 0.6874580061624921
2023-03-27 18:55:59,427 ***** Save model *****
2023-03-27 18:56:15,357 ***** Running evaluation *****
2023-03-27 18:56:15,357   Epoch = 8 iter 2249 step
2023-03-27 18:56:15,358   Num examples = 1043
2023-03-27 18:56:15,358   Batch size = 32
2023-03-27 18:56:15,359 ***** Eval results *****
2023-03-27 18:56:15,359   att_loss = 5901845.898230089
2023-03-27 18:56:15,359   cls_loss = 0.0
2023-03-27 18:56:15,359   global_step = 2249
2023-03-27 18:56:15,359   loss = 5901846.398230089
2023-03-27 18:56:15,359   rep_loss = 0.6886715023918489
2023-03-27 18:56:15,367 ***** Save model *****
2023-03-27 18:56:31,321 ***** Running evaluation *****
2023-03-27 18:56:31,321   Epoch = 8 iter 2299 step
2023-03-27 18:56:31,321   Num examples = 1043
2023-03-27 18:56:31,321   Batch size = 32
2023-03-27 18:56:31,323 ***** Eval results *****
2023-03-27 18:56:31,323   att_loss = 5883479.54601227
2023-03-27 18:56:31,323   cls_loss = 0.0
2023-03-27 18:56:31,323   global_step = 2299
2023-03-27 18:56:31,324   loss = 5883480.04601227
2023-03-27 18:56:31,324   rep_loss = 0.6871369663923065
2023-03-27 18:56:31,331 ***** Save model *****
2023-03-27 18:56:47,239 ***** Running evaluation *****
2023-03-27 18:56:47,239   Epoch = 8 iter 2349 step
2023-03-27 18:56:47,239   Num examples = 1043
2023-03-27 18:56:47,240   Batch size = 32
2023-03-27 18:56:47,244 ***** Eval results *****
2023-03-27 18:56:47,244   att_loss = 5885289.692488263
2023-03-27 18:56:47,244   cls_loss = 0.0
2023-03-27 18:56:47,244   global_step = 2349
2023-03-27 18:56:47,245   loss = 5885290.192488263
2023-03-27 18:56:47,245   rep_loss = 0.6855912368062517
2023-03-27 18:56:47,253 ***** Save model *****
2023-03-27 18:57:03,188 ***** Running evaluation *****
2023-03-27 18:57:03,189   Epoch = 8 iter 2399 step
2023-03-27 18:57:03,189   Num examples = 1043
2023-03-27 18:57:03,189   Batch size = 32
2023-03-27 18:57:03,190 ***** Eval results *****
2023-03-27 18:57:03,190   att_loss = 5902264.169201521
2023-03-27 18:57:03,191   cls_loss = 0.0
2023-03-27 18:57:03,191   global_step = 2399
2023-03-27 18:57:03,191   loss = 5902264.669201521
2023-03-27 18:57:03,191   rep_loss = 0.6852334540606452
2023-03-27 18:57:03,198 ***** Save model *****
2023-03-27 18:57:19,162 ***** Running evaluation *****
2023-03-27 18:57:19,163   Epoch = 9 iter 2449 step
2023-03-27 18:57:19,163   Num examples = 1043
2023-03-27 18:57:19,163   Batch size = 32
2023-03-27 18:57:19,164 ***** Eval results *****
2023-03-27 18:57:19,164   att_loss = 5830348.826086956
2023-03-27 18:57:19,164   cls_loss = 0.0
2023-03-27 18:57:19,164   global_step = 2449
2023-03-27 18:57:19,164   loss = 5830349.326086956
2023-03-27 18:57:19,164   rep_loss = 0.6776974304862644
2023-03-27 18:57:19,171 ***** Save model *****
2023-03-27 18:57:35,086 ***** Running evaluation *****
2023-03-27 18:57:35,087   Epoch = 9 iter 2499 step
2023-03-27 18:57:35,087   Num examples = 1043
2023-03-27 18:57:35,087   Batch size = 32
2023-03-27 18:57:35,088 ***** Eval results *****
2023-03-27 18:57:35,088   att_loss = 5875589.192708333
2023-03-27 18:57:35,088   cls_loss = 0.0
2023-03-27 18:57:35,088   global_step = 2499
2023-03-27 18:57:35,088   loss = 5875589.692708333
2023-03-27 18:57:35,088   rep_loss = 0.6772567002723614
2023-03-27 18:57:35,095 ***** Save model *****
2023-03-27 18:57:50,993 ***** Running evaluation *****
2023-03-27 18:57:50,994   Epoch = 9 iter 2549 step
2023-03-27 18:57:50,994   Num examples = 1043
2023-03-27 18:57:50,994   Batch size = 32
2023-03-27 18:57:50,995 ***** Eval results *****
2023-03-27 18:57:50,996   att_loss = 5870948.2397260275
2023-03-27 18:57:50,996   cls_loss = 0.0
2023-03-27 18:57:50,996   global_step = 2549
2023-03-27 18:57:50,996   loss = 5870948.7397260275
2023-03-27 18:57:50,996   rep_loss = 0.6764124938069958
2023-03-27 18:57:50,999 ***** Save model *****
2023-03-27 18:58:06,929 ***** Running evaluation *****
2023-03-27 18:58:06,929   Epoch = 9 iter 2599 step
2023-03-27 18:58:06,930   Num examples = 1043
2023-03-27 18:58:06,930   Batch size = 32
2023-03-27 18:58:06,931 ***** Eval results *****
2023-03-27 18:58:06,931   att_loss = 5877756.484693877
2023-03-27 18:58:06,932   cls_loss = 0.0
2023-03-27 18:58:06,932   global_step = 2599
2023-03-27 18:58:06,932   loss = 5877756.984693877
2023-03-27 18:58:06,932   rep_loss = 0.6756538894711709
2023-03-27 18:58:06,939 ***** Save model *****
2023-03-27 18:58:22,861 ***** Running evaluation *****
2023-03-27 18:58:22,862   Epoch = 9 iter 2649 step
2023-03-27 18:58:22,862   Num examples = 1043
2023-03-27 18:58:22,862   Batch size = 32
2023-03-27 18:58:22,863 ***** Eval results *****
2023-03-27 18:58:22,864   att_loss = 5865592.215447155
2023-03-27 18:58:22,864   cls_loss = 0.0
2023-03-27 18:58:22,864   global_step = 2649
2023-03-27 18:58:22,864   loss = 5865592.715447155
2023-03-27 18:58:22,864   rep_loss = 0.6746945080718374
2023-03-27 18:58:22,871 ***** Save model *****
2023-03-27 18:58:38,773 ***** Running evaluation *****
2023-03-27 18:58:38,773   Epoch = 10 iter 2699 step
2023-03-27 18:58:38,774   Num examples = 1043
2023-03-27 18:58:38,774   Batch size = 32
2023-03-27 18:58:38,775 ***** Eval results *****
2023-03-27 18:58:38,775   att_loss = 5875227.827586207
2023-03-27 18:58:38,775   cls_loss = 0.0
2023-03-27 18:58:38,775   global_step = 2699
2023-03-27 18:58:38,776   loss = 5875228.327586207
2023-03-27 18:58:38,776   rep_loss = 0.6690344193886066
2023-03-27 18:58:38,783 ***** Save model *****
2023-03-27 18:58:54,709 ***** Running evaluation *****
2023-03-27 18:58:54,710   Epoch = 10 iter 2749 step
2023-03-27 18:58:54,710   Num examples = 1043
2023-03-27 18:58:54,710   Batch size = 32
2023-03-27 18:58:54,712 ***** Eval results *****
2023-03-27 18:58:54,712   att_loss = 5835101.329113924
2023-03-27 18:58:54,712   cls_loss = 0.0
2023-03-27 18:58:54,712   global_step = 2749
2023-03-27 18:58:54,712   loss = 5835101.829113924
2023-03-27 18:58:54,713   rep_loss = 0.6688782746278787
2023-03-27 18:58:54,720 ***** Save model *****
2023-03-27 18:59:10,636 ***** Running evaluation *****
2023-03-27 18:59:10,637   Epoch = 10 iter 2799 step
2023-03-27 18:59:10,637   Num examples = 1043
2023-03-27 18:59:10,637   Batch size = 32
2023-03-27 18:59:10,639 ***** Eval results *****
2023-03-27 18:59:10,639   att_loss = 5810953.5852713175
2023-03-27 18:59:10,640   cls_loss = 0.0
2023-03-27 18:59:10,640   global_step = 2799
2023-03-27 18:59:10,640   loss = 5810954.0852713175
2023-03-27 18:59:10,640   rep_loss = 0.6677605616029842
2023-03-27 18:59:10,647 ***** Save model *****
2023-03-27 18:59:26,570 ***** Running evaluation *****
2023-03-27 18:59:26,570   Epoch = 10 iter 2849 step
2023-03-27 18:59:26,570   Num examples = 1043
2023-03-27 18:59:26,570   Batch size = 32
2023-03-27 18:59:26,571 ***** Eval results *****
2023-03-27 18:59:26,572   att_loss = 5838574.48603352
2023-03-27 18:59:26,572   cls_loss = 0.0
2023-03-27 18:59:26,572   global_step = 2849
2023-03-27 18:59:26,573   loss = 5838574.98603352
2023-03-27 18:59:26,573   rep_loss = 0.6675438321502515
2023-03-27 18:59:26,580 ***** Save model *****
2023-03-27 18:59:42,506 ***** Running evaluation *****
2023-03-27 18:59:42,506   Epoch = 10 iter 2899 step
2023-03-27 18:59:42,507   Num examples = 1043
2023-03-27 18:59:42,507   Batch size = 32
2023-03-27 18:59:42,508 ***** Eval results *****
2023-03-27 18:59:42,508   att_loss = 5835601.519650655
2023-03-27 18:59:42,508   cls_loss = 0.0
2023-03-27 18:59:42,509   global_step = 2899
2023-03-27 18:59:42,509   loss = 5835602.019650655
2023-03-27 18:59:42,509   rep_loss = 0.6664280719632144
2023-03-27 18:59:42,511 ***** Save model *****
2023-03-27 18:59:58,477 ***** Running evaluation *****
2023-03-27 18:59:58,478   Epoch = 11 iter 2949 step
2023-03-27 18:59:58,478   Num examples = 1043
2023-03-27 18:59:58,478   Batch size = 32
2023-03-27 18:59:58,480 ***** Eval results *****
2023-03-27 18:59:58,480   att_loss = 5862490.666666667
2023-03-27 18:59:58,480   cls_loss = 0.0
2023-03-27 18:59:58,480   global_step = 2949
2023-03-27 18:59:58,480   loss = 5862491.166666667
2023-03-27 18:59:58,480   rep_loss = 0.6647147784630457
2023-03-27 18:59:58,488 ***** Save model *****
2023-03-27 19:00:14,388 ***** Running evaluation *****
2023-03-27 19:00:14,389   Epoch = 11 iter 2999 step
2023-03-27 19:00:14,389   Num examples = 1043
2023-03-27 19:00:14,389   Batch size = 32
2023-03-27 19:00:14,390 ***** Eval results *****
2023-03-27 19:00:14,390   att_loss = 5800696.008064516
2023-03-27 19:00:14,390   cls_loss = 0.0
2023-03-27 19:00:14,391   global_step = 2999
2023-03-27 19:00:14,391   loss = 5800696.508064516
2023-03-27 19:00:14,391   rep_loss = 0.6595468924891564
2023-03-27 19:00:14,393 ***** Save model *****
2023-03-27 19:00:30,323 ***** Running evaluation *****
2023-03-27 19:00:30,323   Epoch = 11 iter 3049 step
2023-03-27 19:00:30,323   Num examples = 1043
2023-03-27 19:00:30,323   Batch size = 32
2023-03-27 19:00:30,324 ***** Eval results *****
2023-03-27 19:00:30,325   att_loss = 5820427.59375
2023-03-27 19:00:30,325   cls_loss = 0.0
2023-03-27 19:00:30,325   global_step = 3049
2023-03-27 19:00:30,325   loss = 5820428.09375
2023-03-27 19:00:30,325   rep_loss = 0.6605496491704669
2023-03-27 19:00:30,332 ***** Save model *****
2023-03-27 19:00:46,270 ***** Running evaluation *****
2023-03-27 19:00:46,270   Epoch = 11 iter 3099 step
2023-03-27 19:00:46,270   Num examples = 1043
2023-03-27 19:00:46,271   Batch size = 32
2023-03-27 19:00:46,272 ***** Eval results *****
2023-03-27 19:00:46,272   att_loss = 5815077.651234568
2023-03-27 19:00:46,272   cls_loss = 0.0
2023-03-27 19:00:46,273   global_step = 3099
2023-03-27 19:00:46,273   loss = 5815078.151234568
2023-03-27 19:00:46,273   rep_loss = 0.6596678506445002
2023-03-27 19:00:46,280 ***** Save model *****
2023-03-27 19:02:17,007 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_7pm/TempTinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=30.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='KL_ATTN_SWEEP_7pm', kl_attn_weight=None)
2023-03-27 19:02:18,159 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 19:02:20,879 device: cuda n_gpu: 1
2023-03-27 19:02:20,925 Writing example 0 of 8551
2023-03-27 19:02:20,926 *** Example ***
2023-03-27 19:02:20,926 guid: train-0
2023-03-27 19:02:20,926 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 19:02:20,926 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:02:20,927 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:02:20,927 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:02:20,927 label: 1
2023-03-27 19:02:20,927 label_id: 1
2023-03-27 19:02:21,942 Writing example 0 of 1043
2023-03-27 19:02:21,943 *** Example ***
2023-03-27 19:02:21,943 guid: dev-0
2023-03-27 19:02:21,943 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 19:02:21,943 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:02:21,943 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:02:21,943 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:02:21,944 label: 1
2023-03-27 19:02:21,944 label_id: 1
2023-03-27 19:02:22,064 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 19:02:22,065 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 19:02:23,850 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 19:02:24,105 loading model...
2023-03-27 19:02:24,155 done!
2023-03-27 19:02:24,156 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 19:02:25,232 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 19:02:25,233 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 19:02:25,459 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 19:02:25,480 loading model...
2023-03-27 19:02:25,488 done!
2023-03-27 19:02:25,488 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 19:02:25,489 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 19:02:25,500 ***** Running training *****
2023-03-27 19:02:25,500   Num examples = 8551
2023-03-27 19:02:25,500   Batch size = 32
2023-03-27 19:02:25,500   Num steps = 8010
2023-03-27 19:02:25,501 n: bert.embeddings.word_embeddings.weight
2023-03-27 19:02:25,501 n: bert.embeddings.position_embeddings.weight
2023-03-27 19:02:25,501 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 19:02:25,501 n: bert.embeddings.LayerNorm.weight
2023-03-27 19:02:25,501 n: bert.embeddings.LayerNorm.bias
2023-03-27 19:02:25,501 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 19:02:25,501 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 19:02:25,501 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 19:02:25,502 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 19:02:25,502 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 19:02:25,502 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 19:02:25,502 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 19:02:25,502 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 19:02:25,502 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 19:02:25,502 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 19:02:25,502 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 19:02:25,502 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 19:02:25,502 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 19:02:25,503 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 19:02:25,503 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 19:02:25,503 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 19:02:25,503 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 19:02:25,503 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 19:02:25,503 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 19:02:25,503 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 19:02:25,503 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 19:02:25,503 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 19:02:25,503 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 19:02:25,504 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 19:02:25,504 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 19:02:25,504 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 19:02:25,504 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 19:02:25,504 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 19:02:25,504 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 19:02:25,504 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 19:02:25,504 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 19:02:25,504 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 19:02:25,505 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 19:02:25,505 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 19:02:25,505 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 19:02:25,505 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 19:02:25,505 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 19:02:25,505 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 19:02:25,505 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 19:02:25,505 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 19:02:25,505 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 19:02:25,505 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 19:02:25,506 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 19:02:25,506 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 19:02:25,506 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 19:02:25,506 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 19:02:25,506 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 19:02:25,506 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 19:02:25,506 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 19:02:25,506 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 19:02:25,506 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 19:02:25,506 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 19:02:25,507 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 19:02:25,507 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 19:02:25,507 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 19:02:25,507 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 19:02:25,507 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 19:02:25,507 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 19:02:25,507 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 19:02:25,507 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 19:02:25,507 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 19:02:25,507 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 19:02:25,508 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 19:02:25,508 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 19:02:25,508 n: bert.pooler.dense.weight
2023-03-27 19:02:25,508 n: bert.pooler.dense.bias
2023-03-27 19:02:25,508 n: classifier.weight
2023-03-27 19:02:25,508 n: classifier.bias
2023-03-27 19:02:25,508 n: fit_dense.weight
2023-03-27 19:02:25,508 n: fit_dense.bias
2023-03-27 19:02:25,508 Total parameters: 14591258
2023-03-27 19:02:40,160 ***** Running evaluation *****
2023-03-27 19:02:40,160   Epoch = 0 iter 49 step
2023-03-27 19:02:40,161   Num examples = 1043
2023-03-27 19:02:40,161   Batch size = 32
2023-03-27 19:02:40,176 ***** Eval results *****
2023-03-27 19:02:40,177   att_loss = 877.2062203543527
2023-03-27 19:02:40,177   cls_loss = 0.0
2023-03-27 19:02:40,177   global_step = 49
2023-03-27 19:02:40,178   loss = 878.859449736926
2023-03-27 19:02:40,178   rep_loss = 1.653227229507602
2023-03-27 19:02:40,185 ***** Save model *****
2023-03-27 19:02:55,405 ***** Running evaluation *****
2023-03-27 19:02:55,405   Epoch = 0 iter 99 step
2023-03-27 19:02:55,405   Num examples = 1043
2023-03-27 19:02:55,406   Batch size = 32
2023-03-27 19:02:55,407 ***** Eval results *****
2023-03-27 19:02:55,407   att_loss = 809.5841785777699
2023-03-27 19:02:55,407   cls_loss = 0.0
2023-03-27 19:02:55,408   global_step = 99
2023-03-27 19:02:55,408   loss = 811.0551881115846
2023-03-27 19:02:55,408   rep_loss = 1.4710089714840204
2023-03-27 19:02:55,415 ***** Save model *****
2023-03-27 19:03:10,752 ***** Running evaluation *****
2023-03-27 19:03:10,752   Epoch = 0 iter 149 step
2023-03-27 19:03:10,753   Num examples = 1043
2023-03-27 19:03:10,753   Batch size = 32
2023-03-27 19:03:10,754 ***** Eval results *****
2023-03-27 19:03:10,754   att_loss = 773.1608259981911
2023-03-27 19:03:10,754   cls_loss = 0.0
2023-03-27 19:03:10,754   global_step = 149
2023-03-27 19:03:10,755   loss = 774.5340367259595
2023-03-27 19:03:10,755   rep_loss = 1.3732104757488175
2023-03-27 19:03:10,762 ***** Save model *****
2023-03-27 19:03:26,164 ***** Running evaluation *****
2023-03-27 19:03:26,164   Epoch = 0 iter 199 step
2023-03-27 19:03:26,165   Num examples = 1043
2023-03-27 19:03:26,165   Batch size = 32
2023-03-27 19:03:26,166 ***** Eval results *****
2023-03-27 19:03:26,167   att_loss = 751.3628236396828
2023-03-27 19:03:26,167   cls_loss = 0.0
2023-03-27 19:03:26,167   global_step = 199
2023-03-27 19:03:26,167   loss = 752.6710830765154
2023-03-27 19:03:26,168   rep_loss = 1.3082590270881078
2023-03-27 19:03:26,175 ***** Save model *****
2023-03-27 19:03:41,668 ***** Running evaluation *****
2023-03-27 19:03:41,669   Epoch = 0 iter 249 step
2023-03-27 19:03:41,669   Num examples = 1043
2023-03-27 19:03:41,669   Batch size = 32
2023-03-27 19:03:41,670 ***** Eval results *****
2023-03-27 19:03:41,670   att_loss = 737.3840898261013
2023-03-27 19:03:41,670   cls_loss = 0.0
2023-03-27 19:03:41,671   global_step = 249
2023-03-27 19:03:41,671   loss = 738.6443420165035
2023-03-27 19:03:41,671   rep_loss = 1.2602519194285076
2023-03-27 19:03:41,678 ***** Save model *****
2023-03-27 19:03:57,229 ***** Running evaluation *****
2023-03-27 19:03:57,230   Epoch = 1 iter 299 step
2023-03-27 19:03:57,230   Num examples = 1043
2023-03-27 19:03:57,230   Batch size = 32
2023-03-27 19:03:57,231 ***** Eval results *****
2023-03-27 19:03:57,232   att_loss = 667.211950302124
2023-03-27 19:03:57,232   cls_loss = 0.0
2023-03-27 19:03:57,232   global_step = 299
2023-03-27 19:03:57,232   loss = 668.2339172363281
2023-03-27 19:03:57,232   rep_loss = 1.0219648480415344
2023-03-27 19:03:57,239 ***** Save model *****
2023-03-27 19:04:12,891 ***** Running evaluation *****
2023-03-27 19:04:12,891   Epoch = 1 iter 349 step
2023-03-27 19:04:12,891   Num examples = 1043
2023-03-27 19:04:12,891   Batch size = 32
2023-03-27 19:04:12,893 ***** Eval results *****
2023-03-27 19:04:12,894   att_loss = 666.0341744771818
2023-03-27 19:04:12,894   cls_loss = 0.0
2023-03-27 19:04:12,894   global_step = 349
2023-03-27 19:04:12,894   loss = 667.041079637481
2023-03-27 19:04:12,894   rep_loss = 1.0069033779749057
2023-03-27 19:04:12,901 ***** Save model *****
2023-03-27 19:04:28,554 ***** Running evaluation *****
2023-03-27 19:04:28,554   Epoch = 1 iter 399 step
2023-03-27 19:04:28,554   Num examples = 1043
2023-03-27 19:04:28,555   Batch size = 32
2023-03-27 19:04:28,556 ***** Eval results *****
2023-03-27 19:04:28,556   att_loss = 663.7742956912879
2023-03-27 19:04:28,557   cls_loss = 0.0
2023-03-27 19:04:28,557   global_step = 399
2023-03-27 19:04:28,557   loss = 664.7675707267992
2023-03-27 19:04:28,557   rep_loss = 0.9932746137633468
2023-03-27 19:04:28,564 ***** Save model *****
2023-03-27 19:04:44,270 ***** Running evaluation *****
2023-03-27 19:04:44,270   Epoch = 1 iter 449 step
2023-03-27 19:04:44,270   Num examples = 1043
2023-03-27 19:04:44,271   Batch size = 32
2023-03-27 19:04:44,272 ***** Eval results *****
2023-03-27 19:04:44,272   att_loss = 664.5814433674236
2023-03-27 19:04:44,272   cls_loss = 0.0
2023-03-27 19:04:44,272   global_step = 449
2023-03-27 19:04:44,273   loss = 665.5626623132727
2023-03-27 19:04:44,273   rep_loss = 0.9812177269013374
2023-03-27 19:04:44,277 ***** Save model *****
2023-03-27 19:05:00,020 ***** Running evaluation *****
2023-03-27 19:05:00,020   Epoch = 1 iter 499 step
2023-03-27 19:05:00,021   Num examples = 1043
2023-03-27 19:05:00,021   Batch size = 32
2023-03-27 19:05:00,024 ***** Eval results *****
2023-03-27 19:05:00,024   att_loss = 660.9182794505152
2023-03-27 19:05:00,025   cls_loss = 0.0
2023-03-27 19:05:00,025   global_step = 499
2023-03-27 19:05:00,025   loss = 661.8867134883486
2023-03-27 19:05:00,025   rep_loss = 0.968431441691415
2023-03-27 19:05:00,027 ***** Save model *****
2023-03-27 19:05:15,770 ***** Running evaluation *****
2023-03-27 19:05:15,770   Epoch = 2 iter 549 step
2023-03-27 19:05:15,770   Num examples = 1043
2023-03-27 19:05:15,771   Batch size = 32
2023-03-27 19:05:15,773 ***** Eval results *****
2023-03-27 19:05:15,773   att_loss = 622.9820068359375
2023-03-27 19:05:15,774   cls_loss = 0.0
2023-03-27 19:05:15,774   global_step = 549
2023-03-27 19:05:15,774   loss = 623.8623168945312
2023-03-27 19:05:15,774   rep_loss = 0.8803127447764079
2023-03-27 19:05:15,781 ***** Save model *****
2023-03-27 19:05:31,516 ***** Running evaluation *****
2023-03-27 19:05:31,517   Epoch = 2 iter 599 step
2023-03-27 19:05:31,517   Num examples = 1043
2023-03-27 19:05:31,517   Batch size = 32
2023-03-27 19:05:31,519 ***** Eval results *****
2023-03-27 19:05:31,519   att_loss = 638.6199594350961
2023-03-27 19:05:31,520   cls_loss = 0.0
2023-03-27 19:05:31,520   global_step = 599
2023-03-27 19:05:31,520   loss = 639.5015033428485
2023-03-27 19:05:31,520   rep_loss = 0.8815421214470497
2023-03-27 19:05:31,527 ***** Save model *****
2023-03-27 19:05:47,379 ***** Running evaluation *****
2023-03-27 19:05:47,380   Epoch = 2 iter 649 step
2023-03-27 19:05:47,380   Num examples = 1043
2023-03-27 19:05:47,380   Batch size = 32
2023-03-27 19:05:47,381 ***** Eval results *****
2023-03-27 19:05:47,381   att_loss = 636.8409206224525
2023-03-27 19:05:47,382   cls_loss = 0.0
2023-03-27 19:05:47,382   global_step = 649
2023-03-27 19:05:47,382   loss = 637.7129962423573
2023-03-27 19:05:47,382   rep_loss = 0.8720740183540012
2023-03-27 19:05:47,387 ***** Save model *****
2023-03-27 19:06:03,223 ***** Running evaluation *****
2023-03-27 19:06:03,223   Epoch = 2 iter 699 step
2023-03-27 19:06:03,223   Num examples = 1043
2023-03-27 19:06:03,223   Batch size = 32
2023-03-27 19:06:03,225 ***** Eval results *****
2023-03-27 19:06:03,225   att_loss = 638.2831949869792
2023-03-27 19:06:03,225   cls_loss = 0.0
2023-03-27 19:06:03,226   global_step = 699
2023-03-27 19:06:03,226   loss = 639.1491821289062
2023-03-27 19:06:03,226   rep_loss = 0.8659861777767991
2023-03-27 19:06:03,231 ***** Save model *****
2023-03-27 19:06:19,059 ***** Running evaluation *****
2023-03-27 19:06:19,059   Epoch = 2 iter 749 step
2023-03-27 19:06:19,059   Num examples = 1043
2023-03-27 19:06:19,059   Batch size = 32
2023-03-27 19:06:19,061 ***** Eval results *****
2023-03-27 19:06:19,061   att_loss = 638.7692138671875
2023-03-27 19:06:19,061   cls_loss = 0.0
2023-03-27 19:06:19,061   global_step = 749
2023-03-27 19:06:19,061   loss = 639.6291767918786
2023-03-27 19:06:19,061   rep_loss = 0.8599626139152882
2023-03-27 19:06:19,067 ***** Save model *****
2023-03-27 19:06:34,936 ***** Running evaluation *****
2023-03-27 19:06:34,936   Epoch = 2 iter 799 step
2023-03-27 19:06:34,937   Num examples = 1043
2023-03-27 19:06:34,937   Batch size = 32
2023-03-27 19:06:34,938 ***** Eval results *****
2023-03-27 19:06:34,938   att_loss = 637.5439416273584
2023-03-27 19:06:34,939   cls_loss = 0.0
2023-03-27 19:06:34,939   global_step = 799
2023-03-27 19:06:34,939   loss = 638.3975742556014
2023-03-27 19:06:34,939   rep_loss = 0.8536327721937648
2023-03-27 19:06:34,946 ***** Save model *****
2023-03-27 19:06:50,781 ***** Running evaluation *****
2023-03-27 19:06:50,782   Epoch = 3 iter 849 step
2023-03-27 19:06:50,782   Num examples = 1043
2023-03-27 19:06:50,782   Batch size = 32
2023-03-27 19:06:50,783 ***** Eval results *****
2023-03-27 19:06:50,783   att_loss = 616.5910059611002
2023-03-27 19:06:50,783   cls_loss = 0.0
2023-03-27 19:06:50,783   global_step = 849
2023-03-27 19:06:50,783   loss = 617.399299621582
2023-03-27 19:06:50,784   rep_loss = 0.8082918313642343
2023-03-27 19:06:50,789 ***** Save model *****
2023-03-27 19:07:06,628 ***** Running evaluation *****
2023-03-27 19:07:06,628   Epoch = 3 iter 899 step
2023-03-27 19:07:06,628   Num examples = 1043
2023-03-27 19:07:06,628   Batch size = 32
2023-03-27 19:07:06,629 ***** Eval results *****
2023-03-27 19:07:06,629   att_loss = 619.7410197355309
2023-03-27 19:07:06,630   cls_loss = 0.0
2023-03-27 19:07:06,630   global_step = 899
2023-03-27 19:07:06,630   loss = 620.5449654715402
2023-03-27 19:07:06,630   rep_loss = 0.8039439837543332
2023-03-27 19:07:06,637 ***** Save model *****
2023-03-27 19:07:22,485 ***** Running evaluation *****
2023-03-27 19:07:22,485   Epoch = 3 iter 949 step
2023-03-27 19:07:22,485   Num examples = 1043
2023-03-27 19:07:22,485   Batch size = 32
2023-03-27 19:07:22,486 ***** Eval results *****
2023-03-27 19:07:22,487   att_loss = 622.1709668958509
2023-03-27 19:07:22,487   cls_loss = 0.0
2023-03-27 19:07:22,487   global_step = 949
2023-03-27 19:07:22,487   loss = 622.9721551843592
2023-03-27 19:07:22,487   rep_loss = 0.8011872595226442
2023-03-27 19:07:22,494 ***** Save model *****
2023-03-27 19:07:38,360 ***** Running evaluation *****
2023-03-27 19:07:38,361   Epoch = 3 iter 999 step
2023-03-27 19:07:38,361   Num examples = 1043
2023-03-27 19:07:38,361   Batch size = 32
2023-03-27 19:07:38,362 ***** Eval results *****
2023-03-27 19:07:38,363   att_loss = 622.3388644131747
2023-03-27 19:07:38,363   cls_loss = 0.0
2023-03-27 19:07:38,363   global_step = 999
2023-03-27 19:07:38,363   loss = 623.1356848514441
2023-03-27 19:07:38,364   rep_loss = 0.7968200240472351
2023-03-27 19:07:38,368 ***** Save model *****
2023-03-27 19:07:54,248 ***** Running evaluation *****
2023-03-27 19:07:54,249   Epoch = 3 iter 1049 step
2023-03-27 19:07:54,249   Num examples = 1043
2023-03-27 19:07:54,249   Batch size = 32
2023-03-27 19:07:54,250 ***** Eval results *****
2023-03-27 19:07:54,250   att_loss = 622.7436811385617
2023-03-27 19:07:54,250   cls_loss = 0.0
2023-03-27 19:07:54,251   global_step = 1049
2023-03-27 19:07:54,251   loss = 623.5360939271989
2023-03-27 19:07:54,251   rep_loss = 0.7924124946998011
2023-03-27 19:07:54,258 ***** Save model *****
2023-03-27 19:08:10,132 ***** Running evaluation *****
2023-03-27 19:08:10,132   Epoch = 4 iter 1099 step
2023-03-27 19:08:10,132   Num examples = 1043
2023-03-27 19:08:10,132   Batch size = 32
2023-03-27 19:08:10,133 ***** Eval results *****
2023-03-27 19:08:10,134   att_loss = 623.3189953219506
2023-03-27 19:08:10,134   cls_loss = 0.0
2023-03-27 19:08:10,134   global_step = 1099
2023-03-27 19:08:10,134   loss = 624.0861245432208
2023-03-27 19:08:10,134   rep_loss = 0.7671292251156222
2023-03-27 19:08:10,139 ***** Save model *****
 cuda n_gpu: 1
2023-03-27 19:08:00,806 Writing example 0 of 8551
2023-03-27 19:08:00,807 *** Example ***
2023-03-27 19:08:00,807 guid: train-0
2023-03-27 19:08:00,807 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 19:08:00,807 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:08:00,807 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:08:00,808 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:08:00,808 label: 1
2023-03-27 19:08:00,808 label_id: 1
2023-03-27 19:08:01,794 Writing example 0 of 1043
2023-03-27 19:08:01,795 *** Example ***
2023-03-27 19:08:01,795 guid: dev-0
2023-03-27 19:08:01,796 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 19:08:01,796 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:08:01,796 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:08:01,796 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:08:01,796 label: 1
2023-03-27 19:08:01,796 label_id: 1
2023-03-27 19:08:01,914 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 19:08:01,917 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 19:08:03,759 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 19:08:07,507 loading model...
2023-03-27 19:08:07,565 done!
2023-03-27 19:08:07,566 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 19:08:11,660 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 19:08:11,663 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 19:08:11,888 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 19:08:12,429 loading model...
2023-03-27 19:08:12,444 done!
2023-03-27 19:08:12,445 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 19:08:12,445 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 19:08:12,456 ***** Running training *****
2023-03-27 19:08:12,457   Num examples = 8551
2023-03-27 19:08:12,457   Batch size = 32
2023-03-27 19:08:12,457   Num steps = 8010
2023-03-27 19:08:12,457 n: bert.embeddings.word_embeddings.weight
2023-03-27 19:08:12,457 n: bert.embeddings.position_embeddings.weight
2023-03-27 19:08:12,458 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 19:08:12,458 n: bert.embeddings.LayerNorm.weight
2023-03-27 19:08:12,458 n: bert.embeddings.LayerNorm.bias
2023-03-27 19:08:12,458 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 19:08:12,458 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 19:08:12,458 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 19:08:12,458 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 19:08:12,458 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 19:08:12,458 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 19:08:12,459 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 19:08:12,459 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 19:08:12,459 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 19:08:12,459 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 19:08:12,459 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 19:08:12,459 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 19:08:12,459 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 19:08:12,459 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 19:08:12,459 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 19:08:12,459 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 19:08:12,460 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 19:08:12,460 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 19:08:12,460 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 19:08:12,460 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 19:08:12,460 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 19:08:12,460 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 19:08:12,460 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 19:08:12,460 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 19:08:12,460 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 19:08:12,460 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 19:08:12,461 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 19:08:12,461 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 19:08:12,461 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 19:08:12,461 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 19:08:12,461 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 19:08:12,461 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 19:08:12,461 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 19:08:12,461 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 19:08:12,461 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 19:08:12,461 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 19:08:12,462 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 19:08:12,462 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 19:08:12,462 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 19:08:12,462 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 19:08:12,462 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 19:08:12,462 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 19:08:12,462 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 19:08:12,462 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 19:08:12,462 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 19:08:12,462 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 19:08:12,463 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 19:08:12,463 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 19:08:12,463 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 19:08:12,463 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 19:08:12,463 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 19:08:12,463 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 19:08:12,463 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 19:08:12,463 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 19:08:12,463 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 19:08:12,463 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 19:08:12,464 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 19:08:12,464 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 19:08:12,464 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 19:08:12,464 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 19:08:12,464 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 19:08:12,464 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 19:08:12,464 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 19:08:12,464 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 19:08:12,464 n: bert.pooler.dense.weight
2023-03-27 19:08:12,464 n: bert.pooler.dense.bias
2023-03-27 19:08:12,465 n: classifier.weight
2023-03-27 19:08:12,465 n: classifier.bias
2023-03-27 19:08:12,465 n: fit_dense.weight
2023-03-27 19:08:12,465 n: fit_dense.bias
2023-03-27 19:08:12,465 Total parameters: 14591258
2023-03-27 19:08:25,945 ***** Running evaluation *****
2023-03-27 19:08:25,946   Epoch = 0 iter 49 step
2023-03-27 19:08:25,947   Num examples = 1043
2023-03-27 19:08:25,947   Batch size = 32
2023-03-27 19:08:25,950 ***** Eval results *****
2023-03-27 19:08:25,950   att_loss = 880.228271484375
2023-03-27 19:08:25,950   cls_loss = 0.0
2023-03-27 19:08:25,950   global_step = 49
2023-03-27 19:08:25,951   loss = 881.8789486009248
2023-03-27 19:08:25,951   rep_loss = 1.6506727350001433
2023-03-27 19:08:25,953 ***** Save model *****
2023-03-27 19:08:36,424 ***** Running evaluation *****
2023-03-27 19:08:36,425   Epoch = 0 iter 99 step
2023-03-27 19:08:36,425   Num examples = 1043
2023-03-27 19:08:36,425   Batch size = 32
2023-03-27 19:08:36,427 ***** Eval results *****
2023-03-27 19:08:36,427   att_loss = 808.699309994476
2023-03-27 19:08:36,427   cls_loss = 0.0
2023-03-27 19:08:36,427   global_step = 99
2023-03-27 19:08:36,427   loss = 810.1687338472617
2023-03-27 19:08:36,427   rep_loss = 1.4694212747342659
2023-03-27 19:08:36,434 ***** Save model *****
2023-03-27 19:08:46,889 ***** Running evaluation *****
2023-03-27 19:08:46,890   Epoch = 0 iter 149 step
2023-03-27 19:08:46,890   Num examples = 1043
2023-03-27 19:08:46,890   Batch size = 32
2023-03-27 19:08:46,892 ***** Eval results *****
2023-03-27 19:08:46,892   att_loss = 773.8031120556313
2023-03-27 19:08:46,892   cls_loss = 0.0
2023-03-27 19:08:46,892   global_step = 149
2023-03-27 19:08:46,893   loss = 775.1765284186241
2023-03-27 19:08:46,893   rep_loss = 1.3734152101030286
2023-03-27 19:08:46,900 ***** Save model *****
2023-03-27 19:08:57,386 ***** Running evaluation *****
2023-03-27 19:08:57,386   Epoch = 0 iter 199 step
2023-03-27 19:08:57,386   Num examples = 1043
2023-03-27 19:08:57,386   Batch size = 32
2023-03-27 19:08:57,388 ***** Eval results *****
2023-03-27 19:08:57,388   att_loss = 753.3896300349403
2023-03-27 19:08:57,388   cls_loss = 0.0
2023-03-27 19:08:57,388   global_step = 199
2023-03-27 19:08:57,389   loss = 754.6986767946177
2023-03-27 19:08:57,389   rep_loss = 1.3090454651482741
2023-03-27 19:08:57,393 ***** Save model *****
2023-03-27 12023-03-27 19:09:29,548 ***** Running evaluation *****
2023-03-27 19:09:29,549   Epoch = 5 iter 1349 step
2023-03-27 19:09:29,549   Num examples = 1043
2023-03-27 19:09:29,549   Batch size = 32
2023-03-27 19:09:29,550 ***** Eval results *****
2023-03-27 19:09:29,551   att_loss = 613.1376865931919
2023-03-27 19:09:29,551   cls_loss = 0.0
2023-03-27 19:09:29,551   global_step = 1349
2023-03-27 19:09:29,551   loss = 613.8704223632812
2023-03-27 19:09:29,551   rep_loss = 0.7327326621328082
2023-03-27 19:09:29,558 ***** Save model *****
2023-03-27 19:09:45,476 ***** Running evaluation *****
2023-03-27 19:09:45,476   Epoch = 5 iter 1399 step
2023-03-27 19:09:45,476   Num examples = 1043
2023-03-27 19:09:45,476   Batch size = 32
2023-03-27 19:09:45,479 ***** Eval results *****
2023-03-27 19:09:45,479   att_loss = 609.1915016174316
2023-03-27 19:09:45,480   cls_loss = 0.0
2023-03-27 19:09:45,480   global_step = 1399
2023-03-27 19:09:45,480   loss = 609.924150466919
2023-03-27 19:09:45,480   rep_loss = 0.7326493328437209
2023-03-27 19:09:45,489 ***** Save model *****
2023-03-27 19:10:01,406 ***** Running evaluation *****
2023-03-27 19:10:01,406   Epoch = 5 iter 1449 step
2023-03-27 19:10:01,406   Num examples = 1043
2023-03-27 19:10:01,406   Batch size = 32
2023-03-27 19:10:01,407 ***** Eval results *****
2023-03-27 19:10:01,407   att_loss = 607.5710711562842
2023-03-27 19:10:01,407   cls_loss = 0.0
2023-03-27 19:10:01,407   global_step = 1449
2023-03-27 19:10:01,408   loss = 608.3016352067914
2023-03-27 19:10:01,408   rep_loss = 0.730565524414966
2023-03-27 19:10:01,409 ***** Save model *****
2023-03-27 19:10:17,331 ***** Running evaluation *****
2023-03-27 19:10:17,331   Epoch = 5 iter 1499 step
2023-03-27 19:10:17,331   Num examples = 1043
2023-03-27 19:10:17,331   Batch size = 32
2023-03-27 19:10:17,332 ***** Eval results *****
2023-03-27 19:10:17,333   att_loss = 607.9117811249523
2023-03-27 19:10:17,333   cls_loss = 0.0
2023-03-27 19:10:17,333   global_step = 1499
2023-03-27 19:10:17,333   loss = 608.6399480307974
2023-03-27 19:10:17,333   rep_loss = 0.7281679383865217
2023-03-27 19:10:17,340 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2023-03-27 19:10:33,221 ***** Running evaluation *****
2023-03-27 19:10:33,221   Epoch = 5 iter 1549 step
2023-03-27 19:10:33,221   Num examples = 1043
2023-03-27 19:10:33,222   Batch size = 32
2023-03-27 19:10:33,223 ***** Eval results *****
2023-03-27 19:10:33,223   att_loss = 607.4873272191699
2023-03-27 19:10:33,224   cls_loss = 0.0
2023-03-27 19:10:33,224   global_step = 1549
2023-03-27 19:10:33,224   loss = 608.2129821777344
2023-03-27 19:10:33,224   rep_loss = 0.7256556103162677
2023-03-27 19:10:33,231 ***** Save model *****
2023-03-27 19:10:49,097 ***** Running evaluation *****
2023-03-27 19:10:49,097   Epoch = 5 iter 1599 step
2023-03-27 19:10:49,097   Num examples = 1043
2023-03-27 19:10:49,098   Batch size = 32
2023-03-27 19:10:49,100 ***** Eval results *****
2023-03-27 19:10:49,100   att_loss = 607.1419684670188
2023-03-27 19:10:49,101   cls_loss = 0.0
2023-03-27 19:10:49,101   global_step = 1599
2023-03-27 19:10:49,101   loss = 607.86572265625
2023-03-27 19:10:49,101   rep_loss = 0.723754932031487
2023-03-27 19:10:49,103 ***** Save model *****
2023-03-27 19:11:04,983 ***** Running evaluation *****
2023-03-27 19:11:04,983   Epoch = 6 iter 1649 step
2023-03-27 19:11:04,984   Num examples = 1043
2023-03-27 19:11:04,984   Batch size = 32
2023-03-27 19:11:04,986 ***** Eval results *****
2023-03-27 19:11:04,986   att_loss = 597.9088355531084
2023-03-27 19:11:04,986   cls_loss = 0.0
2023-03-27 19:11:04,986   global_step = 1649
2023-03-27 19:11:04,986   loss = 598.616270674036
2023-03-27 19:11:04,987   rep_loss = 0.7074311362936142
2023-03-27 19:11:04,996 ***** Save model *****
2023-03-27 19:11:20,894 ***** Running evaluation *****
2023-03-27 19:11:20,895   Epoch = 6 iter 1699 step
2023-03-27 19:11:20,895   Num examples = 1043
2023-03-27 19:11:20,895   Batch size = 32
2023-03-27 19:11:20,896 ***** Eval results *****
2023-03-27 19:11:20,896   att_loss = 600.3139283485019
2023-03-27 19:11:20,896   cls_loss = 0.0
2023-03-27 19:11:20,896   global_step = 1699
2023-03-27 19:11:20,896   loss = 601.0223615194104
2023-03-27 19:11:20,897   rep_loss = 0.7084321754494893
2023-03-27 19:11:20,904 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    2023-03-27 19:11:36,818 ***** Running evaluation *****
2023-03-27 19:11:36,818   Epoch = 6 iter 1749 step
2023-03-27 19:11:36,818   Num examples = 1043
2023-03-27 19:11:36,818   Batch size = 32
2023-03-27 19:11:36,819 ***** Eval results *****
2023-03-27 19:11:36,820   att_loss = 600.4652797154018
2023-03-27 19:11:36,820   cls_loss = 0.0
2023-03-27 19:11:36,820   global_step = 1749
2023-03-27 19:11:36,820   loss = 601.1730508609694
2023-03-27 19:11:36,820   rep_loss = 0.7077701922987594
2023-03-27 19:11:36,822 ***** Save model *****
2023-03-27 19:11:52,711 ***** Running evaluation *****
2023-03-27 19:11:52,712   Epoch = 6 iter 1799 step
2023-03-27 19:11:52,712   Num examples = 1043
2023-03-27 19:11:52,712   Batch size = 32
2023-03-27 19:11:52,713 ***** Eval results *****
2023-03-27 19:11:52,713   att_loss = 600.1138646469503
2023-03-27 19:11:52,714   cls_loss = 0.0
2023-03-27 19:11:52,714   global_step = 1799
2023-03-27 19:11:52,714   loss = 600.8202105681909
2023-03-27 19:11:52,714   rep_loss = 0.7063448280852458
2023-03-27 19:11:52,721 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2023-03-27 19:12:08,645 ***** Running evaluation *****
2023-03-27 19:12:08,645   Epoch = 6 iter 1849 step
2023-03-27 19:12:08,645   Num examples = 1043
2023-03-27 19:12:08,645   Batch size = 32
2023-03-27 19:12:08,646 ***** Eval results *****
2023-03-27 19:12:08,646   att_loss = 600.6880683435602
2023-03-27 19:12:08,647   cls_loss = 0.0
2023-03-27 19:12:08,647   global_step = 1849
2023-03-27 19:12:08,647   loss = 601.3922615823476
2023-03-27 19:12:08,647   rep_loss = 0.7041924612724829
2023-03-27 19:12:08,654 ***** Save model *****
2023-03-27 19:12:24,562 ***** Running evaluation *****
2023-03-27 19:12:24,562   Epoch = 7 iter 1899 step
2023-03-27 19:12:24,562   Num examples = 1043
2023-03-27 19:12:24,562   Batch size = 32
2023-03-27 19:12:24,563 ***** Eval results *****
2023-03-27 19:12:24,564   att_loss = 589.1375406901042
2023-03-27 19:12:24,564   cls_loss = 0.0
2023-03-27 19:12:24,564   global_step = 1899
2023-03-27 19:12:24,564   loss = 589.8269246419271
2023-03-27 19:12:24,564   rep_loss = 0.6893837511539459
2023-03-27 19:12:24,573 ***** Save model *****
-03-27 19:11:59,752 ***** Running evaluation *****
2023-03-27 19:11:59,752   Epoch = 3 iter 1049 step
2023-03-27 19:11:59,753   Num examples = 1043
2023-03-27 19:11:59,753   Batch size = 32
2023-03-27 19:11:59,754 ***** Eval results *****
2023-03-27 19:11:59,754   att_loss = 622.562353564847
2023-03-27 19:11:59,754   cls_loss = 0.0
2023-03-27 19:11:59,754   global_step = 1049
2023-03-27 19:11:59,754   loss = 623.3568656675277
2023-03-27 19:11:59,754   rep_loss = 0.7945117844689277
2023-03-27 19:11:59,761 ***** Save model *****
2023-03-27 19:12:10,602 ***** Running evaluation *****
2023-03-27 19:12:10,602   Epoch = 4 iter 1099 step
2023-03-27 19:12:10,602   Num examples = 1043
2023-03-27 19:12:10,603   Batch size = 32
2023-03-27 19:12:10,604 ***** Eval results *****
2023-03-27 19:12:10,604   att_loss = 624.3748346144154
2023-03-27 19:12:10,604   cls_loss = 0.0
2023-03-27 19:12:10,604   global_step = 1099
2023-03-27 19:12:10,604   loss = 625.1474983461442
2023-03-27 19:12:10,605   rep_loss = 0.7726611072017301
2023-03-27 19:12:10,610 ***** Save model *****
2023-03-27 19:12:21,559 ***** Running evaluation *****
2023-03-27 19:12:21,559   Epoch = 4 iter 1149 step
2023-03-27 19:12:21,559   Num examples = 1043
2023-03-27 19:12:21,560   Batch size = 32
2023-03-27 19:12:21,561 ***** Eval results *****
2023-03-27 19:12:21,561   att_loss = 621.5156212323977
2023-03-27 19:12:21,561   cls_loss = 0.0
2023-03-27 19:12:21,561   global_step = 1149
2023-03-27 19:12:21,561   loss = 622.2837109977817
2023-03-27 19:12:21,562   rep_loss = 0.7680859918947573
2023-03-27 19:12:21,569 ***** Save model *****
2023-03-27 19:12:32,448 ***** Running evaluation *****
2023-03-27 19:12:32,449   Epoch = 4 iter 1199 step
2023-03-27 19:12:32,449   Num examples = 1043
2023-03-27 19:12:32,449   Batch size = 32
2023-03-27 19:12:32,450 ***** Eval results *****
2023-03-27 19:12:32,450   att_loss = 616.8277573913108
2023-03-27 19:12:32,450   cls_loss = 0.0
2023-03-27 19:12:32,450   global_step = 1199
2023-03-27 19:12:32,451   loss = 617.5903269061605
2023-03-27 19:12:32,451   rep_loss = 0.7625683941913926
2023-03-27 19:12:32,458 ***** Save model *****
2023-03-27 19:12:43,325 ***** Running evaluation *****
2023-03-27 19:12:43,326   Epoch = 4 iter 1249 step
2023-03-27 19:12:43,326   Num examples = 1043
2023-03-27 19:12:43,326   Batch size = 32
2023-03-27 19:12:43,327 ***** Eval results *****
2023-03-27 19:12:43,327   att_loss = 614.2045814134798
2023-03-27 19:12:43,327   cls_loss = 0.0
2023-03-27 19:12:43,328   global_step = 1249
2023-03-27 19:12:43,328   loss = 614.962925694924
2023-03-27 19:12:43,328   rep_loss = 0.7583431789229588
2023-03-27 19:12:43,335 ***** Save model *****
2023-03-27 19:12:54,233 ***** Running evaluation *****
2023-03-27 19:12:54,234   Epoch = 4 iter 1299 step
2023-03-27 19:12:54,234   Num examples = 1043
2023-03-27 19:12:54,234   Batch size = 32
2023-03-27 19:12:54,236 ***** Eval results *****
2023-03-27 19:12:54,236   att_loss = 615.9047983673228
2023-03-27 19:12:54,237   cls_loss = 0.0
2023-03-27 19:12:54,237   global_step = 1299
2023-03-27 19:12:54,237   loss = 616.6603665702787
2023-03-27 19:12:54,237   rep_loss = 0.7555672725041708
2023-03-27 19:12:54,239 ***** Save model *****
2023-03-27 19:13:28,244 ***** Running evaluation *****
2023-03-27 19:13:28,244   Epoch = 7 iter 2099 step
2023-03-27 19:13:28,244   Num examples = 1043
2023-03-27 19:13:28,244   Batch size = 32
2023-03-27 19:13:28,246 ***** Eval results *****
2023-03-27 19:13:28,246   att_loss = 596.9243477199389
2023-03-27 19:13:28,246   cls_loss = 0.0
2023-03-27 19:13:28,246   global_step = 2099
2023-03-27 19:13:28,247   loss = 597.6145513119905
2023-03-27 19:13:28,247   rep_loss = 0.6902021851228631
2023-03-27 19:13:28,254 ***** Save model *****
2023-03-27 19:13:44,216 ***** Running evaluation *****
2023-03-27 19:13:44,216   Epoch = 8 iter 2149 step
2023-03-27 19:13:44,216   Num examples = 1043
2023-03-27 19:13:44,216   Batch size = 32
2023-03-27 19:13:44,218 ***** Eval results *****
2023-03-27 19:13:44,218   att_loss = 590.1740065354568
2023-03-27 19:13:44,218   cls_loss = 0.0
2023-03-27 19:13:44,218   global_step = 2149
2023-03-27 19:13:44,219   loss = 590.8559852013221
2023-03-27 19:13:44,219   rep_loss = 0.6819750208121079
2023-03-27 19:13:44,221 ***** Save model *****
23-03-27 19:13:26,982 ***** Running evaluation *****
2023-03-27 19:13:26,983   Epoch = 5 iter 1449 step
2023-03-27 19:13:26,983   Num examples = 1043
2023-03-27 19:13:26,983   Batch size = 32
2023-03-27 19:13:26,984 ***** Eval results *****
2023-03-27 19:13:26,984   att_loss = 605.111751623321
2023-03-27 19:13:26,985   cls_loss = 0.0
2023-03-27 19:13:26,985   global_step = 1449
2023-03-27 19:13:26,985   loss = 605.8435567220052
2023-03-27 19:13:26,985   rep_loss = 0.7318032415289628
2023-03-27 19:13:26,990 ***** Save model *****
2023-03-27 19:13:37,913 ***** Running evaluation *****
2023-03-27 19:13:37,913   Epoch = 5 iter 1499 step
2023-03-27 19:13:37,914   Num examples = 1043
2023-03-27 19:13:37,914   Batch size = 32
2023-03-27 19:13:37,915 ***** Eval results *****
2023-03-27 19:13:37,915   att_loss = 606.3704018941739
2023-03-27 19:13:37,915   cls_loss = 0.0
2023-03-27 19:13:37,915   global_step = 1499
2023-03-27 19:13:37,915   loss = 607.0996826916206
2023-03-27 19:13:37,915   rep_loss = 0.7292796478765767
2023-03-27 19:13:37,922 ***** Save model *****
2023-03-27 19:13:48,862 ***** Running evaluation *****
2023-03-27 19:13:48,863   Epoch = 5 iter 1549 step
2023-03-27 19:13:48,863   Num examples = 1043
2023-03-27 19:13:48,863   Batch size = 32
2023-03-27 19:13:48,864 ***** Eval results *****
2023-03-27 19:13:48,865   att_loss = 606.1487144398913
2023-03-27 19:13:48,865   cls_loss = 0.0
2023-03-27 19:13:48,865   global_step = 1549
2023-03-27 19:13:48,865   loss = 606.8757153092143
2023-03-27 19:13:48,865   rep_loss = 0.7270010662413089
2023-03-27 19:13:48,872 ***** Save model *****
2023-03-27 19:13:59,801 ***** Running evaluation *****
2023-03-27 19:13:59,801   Epoch = 5 iter 1599 step
2023-03-27 19:13:59,801   Num examples = 1043
2023-03-27 19:13:59,801   Batch size = 32
2023-03-27 19:13:59,803 ***** Eval results *****
2023-03-27 19:13:59,803   att_loss = 606.1945951057203
2023-03-27 19:13:59,803   cls_loss = 0.0
2023-03-27 19:13:59,803   global_step = 1599
2023-03-27 19:13:59,804   loss = 606.9193864302201
2023-03-27 19:13:59,804   rep_loss = 0.7247912380279917
2023-03-27 19:13:59,810 ***** Save model *****
2023-03-27 19:14:10,713 ***** Running evaluation *****
2023-03-27 19:14:10,713   Epoch = 6 iter 1649 step
2023-03-27 19:14:10,713   Num examples = 1043
2023-03-27 19:14:10,713   Batch size = 32
2023-03-27 19:14:10,714 ***** Eval results *****
2023-03-27 19:14:10,715   att_loss = 598.7573138297872
2023-03-27 19:14:10,715   cls_loss = 0.0
2023-03-27 19:14:10,715   global_step = 1649
2023-03-27 19:14:10,715   loss = 599.4660774393285
2023-03-27 19:14:10,715   rep_loss = 0.708763431995473
2023-03-27 19:14:10,723 ***** Save model *****
2023-03-27 19:14:21,704 ***** Running evaluation *****
2023-03-27 19:14:21,704   Epoch = 6 iter 1699 step
2023-03-27 19:14:21,704   Num examples = 1043
2023-03-27 19:14:21,705   Batch size = 32
2023-03-27 19:14:21,706 ***** Eval results *****
2023-03-27 19:14:21,706   att_loss = 602.0162353515625
2023-03-27 19:14:21,706   cls_loss = 0.0
2023-03-27 19:14:21,707   global_step = 1699
2023-03-27 19:14:21,707   loss = 602.7259238331588
2023-03-27 19:14:21,707   rep_loss = 0.709688468692229
2023-03-27 19:14:21,709 ***** Save model *****
2023-03-27 19:14:32,661 ***** Running evaluation *****
2023-03-27 19:14:32,662   Epoch = 6 iter 1749 step
2023-03-27 19:14:32,662   Num examples = 1043
2023-03-27 19:14:32,662   Batch size = 32
2023-03-27 19:14:32,663 ***** Eval results *****
2023-03-27 19:14:32,663   att_loss = 601.2112904243729
2023-03-27 19:14:32,663   cls_loss = 0.0
2023-03-27 19:14:32,663   global_step = 1749
2023-03-27 19:14:32,663   loss = 601.9189523709874
2023-03-27 19:14:32,663   rep_loss = 0.707662641596632
2023-03-27 19:14:32,669 ***** Save model *****
2023-03-27 19:15:04,178 ***** Running evaluation *****
2023-03-27 19:15:04,178   Epoch = 8 iter 2399 step
2023-03-27 19:15:04,178   Num examples = 1043
2023-03-27 19:15:04,178   Batch size = 32
2023-03-27 19:15:04,179 ***** Eval results *****
2023-03-27 19:15:04,180   att_loss = 590.266643103538
2023-03-27 19:15:04,180   cls_loss = 0.0
2023-03-27 19:15:04,180   global_step = 2399
2023-03-27 19:15:04,180   loss = 590.9436018911151
2023-03-27 19:15:04,180   rep_loss = 0.6769599418223131
2023-03-27 19:15:04,182 ***** Save model *****
2023-03-27 19:15:20,080 ***** Running evaluation *****
2023-03-27 19:15:20,080   Epoch = 9 iter 2449 step
2023-03-27 19:15:20,081   Num examples = 1043
2023-03-27 19:15:20,081   Batch size = 32
2023-03-27 19:15:20,082 ***** Eval results *****
2023-03-27 19:15:20,082   att_loss = 583.0814872409986
2023-03-27 19:15:20,082   cls_loss = 0.0
2023-03-27 19:15:20,082   global_step = 2449
2023-03-27 19:15:20,082   loss = 583.7514854099439
2023-03-27 19:15:20,082   rep_loss = 0.6699973811273989
2023-03-27 19:15:20,090 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2023-03-27 19:15:36,004 ***** Running evaluation *****
2023-03-27 19:15:36,005   Epoch = 9 iter 2499 step
2023-03-27 19:15:36,005   Num examples = 1043
2023-03-27 19:15:36,005   Batch size = 32
2023-03-27 19:15:36,006 ***** Eval results *****
2023-03-27 19:15:36,006   att_loss = 587.6042823791504
2023-03-27 19:15:36,006   cls_loss = 0.0
2023-03-27 19:15:36,006   global_step = 2499
2023-03-27 19:15:36,006   loss = 588.273954073588
2023-03-27 19:15:36,007   rep_loss = 0.6696724351495504
2023-03-27 19:15:36,009 ***** Save model *****
2023-03-27 19:15:51,900 ***** Running evaluation *****
2023-03-27 19:15:51,900   Epoch = 9 iter 2549 step
2023-03-27 19:15:51,900   Num examples = 1043
2023-03-27 19:15:51,901   Batch size = 32
2023-03-27 19:15:51,902 ***** Eval results *****
2023-03-27 19:15:51,902   att_loss = 587.1403394725224
2023-03-27 19:15:51,903   cls_loss = 0.0
2023-03-27 19:15:51,903   global_step = 2549
2023-03-27 19:15:51,903   loss = 587.8092264671848
2023-03-27 19:15:51,903   rep_loss = 0.6688885194798039
2023-03-27 19:15:51,910 ***** Save model *****
 2023-03-27 19:16:07,818 ***** Running evaluation *****
2023-03-27 19:16:07,818   Epoch = 9 iter 2599 step
2023-03-27 19:16:07,818   Num examples = 1043
2023-03-27 19:16:07,818   Batch size = 32
2023-03-27 19:16:07,820 ***** Eval results *****
2023-03-27 19:16:07,820   att_loss = 587.8185515111807
2023-03-27 19:16:07,820   cls_loss = 0.0
2023-03-27 19:16:07,820   global_step = 2599
2023-03-27 19:16:07,820   loss = 588.4867524127571
2023-03-27 19:16:07,820   rep_loss = 0.6682020121691178
2023-03-27 19:16:07,828 ***** Save model *****
2023-03-27 19:16:23,781 ***** Running evaluation *****
2023-03-27 19:16:23,781   Epoch = 9 iter 2649 step
2023-03-27 19:16:23,781   Num examples = 1043
2023-03-27 19:16:23,781   Batch size = 32
2023-03-27 19:16:23,782 ***** Eval results *****
2023-03-27 19:16:23,782   att_loss = 586.5985571388306
2023-03-27 19:16:23,782   cls_loss = 0.0
2023-03-27 19:16:23,782   global_step = 2649
2023-03-27 19:16:23,782   loss = 587.2658275821344
2023-03-27 19:16:23,783   rep_loss = 0.6672711711588913
2023-03-27 19:16:23,789 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2023-03-27 19:16:39,689 ***** Running evaluation *****
2023-03-27 19:16:39,689   Epoch = 10 iter 2699 step
2023-03-27 19:16:39,689   Num examples = 1043
2023-03-27 19:16:39,689   Batch size = 32
2023-03-27 19:16:39,690 ***** Eval results *****
2023-03-27 19:16:39,691   att_loss = 587.5606247474408
2023-03-27 19:16:39,691   cls_loss = 0.0
2023-03-27 19:16:39,691   global_step = 2699
2023-03-27 19:16:39,691   loss = 588.2227235991379
2023-03-27 19:16:39,691   rep_loss = 0.662096264033482
2023-03-27 19:16:39,697 ***** Save model *****
2023-03-27 19:16:55,600 ***** Running evaluation *****
2023-03-27 19:16:55,600   Epoch = 10 iter 2749 step
2023-03-27 19:16:55,600   Num examples = 1043
2023-03-27 19:16:55,600   Batch size = 32
2023-03-27 19:16:55,602 ***** Eval results *****
2023-03-27 19:16:55,602   att_loss = 583.5511134666733
2023-03-27 19:16:55,603   cls_loss = 0.0
2023-03-27 19:16:55,603   global_step = 2749
2023-03-27 19:16:55,603   loss = 584.2130459169798
2023-03-27 19:16:55,603   rep_loss = 0.6619309616994254
2023-03-27 19:16:55,611 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       2023-03-27 19:17:11,573 ***** Running evaluation *****
2023-03-27 19:17:11,573   Epoch = 10 iter 2799 step
2023-03-27 19:17:11,573   Num examples = 1043
2023-03-27 19:17:11,573   Batch size = 32
2023-03-27 19:17:11,574 ***** Eval results *****
2023-03-27 19:17:11,575   att_loss = 581.1346094885538
2023-03-27 19:17:11,575   cls_loss = 0.0
2023-03-27 19:17:11,575   global_step = 2799
2023-03-27 19:17:11,575   loss = 581.7955123546511
2023-03-27 19:17:11,575   rep_loss = 0.6609029095302257
2023-03-27 19:17:11,582 ***** Save model *****
2023-03-27 19:17:27,460 ***** Running evaluation *****
2023-03-27 19:17:27,461   Epoch = 10 iter 2849 step
2023-03-27 19:17:27,461   Num examples = 1043
2023-03-27 19:17:27,461   Batch size = 32
2023-03-27 19:17:27,462 ***** Eval results *****
2023-03-27 19:17:27,462   att_loss = 583.8978366958362
2023-03-27 19:17:27,463   cls_loss = 0.0
2023-03-27 19:17:27,463   global_step = 2849
2023-03-27 19:17:27,463   loss = 584.5585634029111
2023-03-27 19:17:27,463   rep_loss = 0.660727197564514
2023-03-27 19:17:27,471 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       2023-03-27 19:17:43,393 ***** Running evaluation *****
2023-03-27 19:17:43,393   Epoch = 10 iter 2899 step
2023-03-27 19:17:43,393   Num examples = 1043
2023-03-27 19:17:43,393   Batch size = 32
2023-03-27 19:17:43,394 ***** Eval results *****
2023-03-27 19:17:43,394   att_loss = 583.6009854645708
2023-03-27 19:17:43,395   cls_loss = 0.0
2023-03-27 19:17:43,395   global_step = 2899
2023-03-27 19:17:43,395   loss = 584.2606326440537
2023-03-27 19:17:43,395   rep_loss = 0.6596480121258564
2023-03-27 19:17:43,402 ***** Save model *****
2023-03-27 19:17:59,291 ***** Running evaluation *****
2023-03-27 19:17:59,292   Epoch = 11 iter 2949 step
2023-03-27 19:17:59,292   Num examples = 1043
2023-03-27 19:17:59,292   Batch size = 32
2023-03-27 19:17:59,293 ***** Eval results *****
2023-03-27 19:17:59,294   att_loss = 586.265614827474
2023-03-27 19:17:59,294   cls_loss = 0.0
2023-03-27 19:17:59,294   global_step = 2949
2023-03-27 19:17:59,294   loss = 586.9238942464193
2023-03-27 19:17:59,295   rep_loss = 0.6582711090644201
2023-03-27 19:17:59,297 ***** Save model *****
023-03-27 19:17:39,743 ***** Running evaluation *****
2023-03-27 19:17:39,744   Epoch = 9 iter 2599 step
2023-03-27 19:17:39,744   Num examples = 1043
2023-03-27 19:17:39,744   Batch size = 32
2023-03-27 19:17:39,745 ***** Eval results *****
2023-03-27 19:17:39,746   att_loss = 588.3424140774474
2023-03-27 19:17:39,746   cls_loss = 0.0
2023-03-27 19:17:39,746   global_step = 2599
2023-03-27 19:17:39,746   loss = 589.0110990563218
2023-03-27 19:17:39,746   rep_loss = 0.6686853483623388
2023-03-27 19:17:39,748 ***** Save model *****
2023-03-27 19:17:50,756 ***** Running evaluation *****
2023-03-27 19:17:50,756   Epoch = 9 iter 2649 step
2023-03-27 19:17:50,756   Num examples = 1043
2023-03-27 19:17:50,756   Batch size = 32
2023-03-27 19:17:50,757 ***** Eval results *****
2023-03-27 19:17:50,758   att_loss = 587.5686484236059
2023-03-27 19:17:50,758   cls_loss = 0.0
2023-03-27 19:17:50,758   global_step = 2649
2023-03-27 19:17:50,758   loss = 588.236249474006
2023-03-27 19:17:50,758   rep_loss = 0.6676011269654685
2023-03-27 19:17:50,766 ***** Save model *****
2023-03-27 19:18:01,779 ***** Running evaluation *****
2023-03-27 19:18:01,779   Epoch = 10 iter 2699 step
2023-03-27 19:18:01,779   Num examples = 1043
2023-03-27 19:18:01,780   Batch size = 32
2023-03-27 19:18:01,780 ***** Eval results *****
2023-03-27 19:18:01,781   att_loss = 589.7147006330819
2023-03-27 19:18:01,781   cls_loss = 0.0
2023-03-27 19:18:01,781   global_step = 2699
2023-03-27 19:18:01,781   loss = 590.3789946457435
2023-03-27 19:18:01,781   rep_loss = 0.6642924156682245
2023-03-27 19:18:01,788 ***** Save model *****
2023-03-27 19:18:12,795 ***** Running evaluation *****
2023-03-27 19:18:12,796   Epoch = 10 iter 2749 step
2023-03-27 19:18:12,796   Num examples = 1043
2023-03-27 19:18:12,796   Batch size = 32
2023-03-27 19:18:12,797 ***** Eval results *****
2023-03-27 19:18:12,797   att_loss = 586.1789280372329
2023-03-27 19:18:12,797   cls_loss = 0.0
2023-03-27 19:18:12,798   global_step = 2749
2023-03-27 19:18:12,798   loss = 586.8422936548161
2023-03-27 19:18:12,798   rep_loss = 0.6633674306205556
2023-03-27 19:18:12,805 ***** Save model *****
2023-03-27 19:18:23,822 ***** Running evaluation *****
2023-03-27 19:18:23,822   Epoch = 10 iter 2799 step
2023-03-27 19:18:23,823   Num examples = 1043
2023-03-27 19:18:23,823   Batch size = 32
2023-03-27 19:18:23,824 ***** Eval results *****
2023-03-27 19:18:23,824   att_loss = 581.7803306875303
2023-03-27 19:18:23,825   cls_loss = 0.0
2023-03-27 19:18:23,825   global_step = 2799
2023-03-27 19:18:23,825   loss = 582.4418278184048
2023-03-27 19:18:23,825   rep_loss = 0.6614967575368955
2023-03-27 19:18:23,832 ***** Save model *****
2023-03-27 19:18:34,848 ***** Running evaluation *****
2023-03-27 19:18:34,848   Epoch = 10 iter 2849 step
2023-03-27 19:18:34,848   Num examples = 1043
2023-03-27 19:18:34,849   Batch size = 32
2023-03-27 19:18:34,850 ***** Eval results *****
2023-03-27 19:18:34,850   att_loss = 584.1976908252226
2023-03-27 19:18:34,850   cls_loss = 0.0
2023-03-27 19:18:34,850   global_step = 2849
2023-03-27 19:18:34,851   loss = 584.8589416162928
2023-03-27 19:18:34,851   rep_loss = 0.6612504830573525
2023-03-27 19:18:34,858 ***** Save model *****
2023-03-27 19:19:18,916 ***** Running evaluation *****
2023-03-27 19:19:18,916   Epoch = 11 iter 3199 step
2023-03-27 19:19:18,917   Num examples = 1043
2023-03-27 19:19:18,917   Batch size = 32
2023-03-27 19:19:18,918 ***** Eval results *****
2023-03-27 19:19:18,919   att_loss = 581.1451774771887
2023-03-27 19:19:18,919   cls_loss = 0.0
2023-03-27 19:19:18,919   global_step = 3199
2023-03-27 19:19:18,919   loss = 581.7975790009244
2023-03-27 19:19:18,919   rep_loss = 0.6524026220991411
2023-03-27 19:19:18,921 ***** Save model *****
2023-03-27 19:19:34,831 ***** Running evaluation *****
2023-03-27 19:19:34,832   Epoch = 12 iter 3249 step
2023-03-27 19:19:34,832   Num examples = 1043
2023-03-27 19:19:34,832   Batch size = 32
2023-03-27 19:19:34,833 ***** Eval results *****
2023-03-27 19:19:34,833   att_loss = 577.5834391276042
2023-03-27 19:19:34,834   cls_loss = 0.0
2023-03-27 19:19:34,834   global_step = 3249
2023-03-27 19:19:34,834   loss = 578.230091688368
2023-03-27 19:19:34,834   rep_loss = 0.646655821800232
2023-03-27 19:19:34,837 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2023-03-27 19:19:50,758 ***** Running evaluation *****
2023-03-27 19:19:50,759   Epoch = 12 iter 3299 step
2023-03-27 19:19:50,759   Num examples = 1043
2023-03-27 19:19:50,759   Batch size = 32
2023-03-27 19:19:50,760 ***** Eval results *****
2023-03-27 19:19:50,761   att_loss = 581.0420917711759
2023-03-27 19:19:50,761   cls_loss = 0.0
2023-03-27 19:19:50,761   global_step = 3299
2023-03-27 19:19:50,761   loss = 581.6901694849918
2023-03-27 19:19:50,762   rep_loss = 0.6480795734807064
2023-03-27 19:19:50,766 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2023-03-27 19:20:06,651 ***** Running evaluation *****
2023-03-27 19:20:06,651   Epoch = 12 iter 3349 step
2023-03-27 19:20:06,651   Num examples = 1043
2023-03-27 19:20:06,652   Batch size = 32
2023-03-27 19:20:06,654 ***** Eval results *****
2023-03-27 19:20:06,654   att_loss = 580.4944500101024
2023-03-27 19:20:06,654   cls_loss = 0.0
2023-03-27 19:20:06,654   global_step = 3349
2023-03-27 19:20:06,655   loss = 581.1417143723061
2023-03-27 19:20:06,655   rep_loss = 0.647265467150458
2023-03-27 19:20:06,656 ***** Save model *****
2023-03-27 19:20:22,523 ***** Running evaluation *****
2023-03-27 19:20:22,524   Epoch = 12 iter 3399 step
2023-03-27 19:20:22,524   Num examples = 1043
2023-03-27 19:20:22,524   Batch size = 32
2023-03-27 19:20:22,527 ***** Eval results *****
2023-03-27 19:20:22,528   att_loss = 579.0917759039463
2023-03-27 19:20:22,529   cls_loss = 0.0
2023-03-27 19:20:22,529   global_step = 3399
2023-03-27 19:20:22,529   loss = 579.7388362004207
2023-03-27 19:20:22,530   rep_loss = 0.6470593525813176
2023-03-27 19:20:22,532 ***** Save model *****
2023-03-27 19:20:38,450 ***** Running evaluation *****
2023-03-27 19:20:38,450   Epoch = 12 iter 3449 step
2023-03-27 19:20:38,450   Num examples = 1043
2023-03-27 19:20:38,450   Batch size = 32
2023-03-27 19:20:38,451 ***** Eval results *****
2023-03-27 19:20:38,451   att_loss = 578.4574841557717
2023-03-27 19:20:38,452   cls_loss = 0.0
2023-03-27 19:20:38,452   global_step = 3449
2023-03-27 19:20:38,452   loss = 579.104515106824
2023-03-27 19:20:38,452   rep_loss = 0.6470311743872507
2023-03-27 19:20:38,459 ***** Save model *****
2023-03-27 19:20:54,389 ***** Running evaluation *****
2023-03-27 19:20:54,389   Epoch = 13 iter 3499 step
2023-03-27 19:20:54,389   Num examples = 1043
2023-03-27 19:20:54,390   Batch size = 32
2023-03-27 19:20:54,391 ***** Eval results *****
2023-03-27 19:20:54,391   att_loss = 583.833986554827
2023-03-27 19:20:54,391   cls_loss = 0.0
2023-03-27 19:20:54,392   global_step = 3499
2023-03-27 19:20:54,392   loss = 584.4794202532087
2023-03-27 19:20:54,392   rep_loss = 0.6454366360391889
2023-03-27 19:20:54,399 ***** Save model *****
2023-03-27 19:21:10,313 ***** Running evaluation *****
2023-03-27 19:21:10,313   Epoch = 13 iter 3549 step
2023-03-27 19:21:10,313   Num examples = 1043
2023-03-27 19:21:10,314   Batch size = 32
2023-03-27 19:21:10,315 ***** Eval results *****
2023-03-27 19:21:10,315   att_loss = 576.108900803786
2023-03-27 19:21:10,315   cls_loss = 0.0
2023-03-27 19:21:10,316   global_step = 3549
2023-03-27 19:21:10,316   loss = 576.7511956630609
2023-03-27 19:21:10,316   rep_loss = 0.6422954071790744
2023-03-27 19:21:10,323 ***** Save model *****
2023-03-27 19:21:26,234 ***** Running evaluation *****
2023-03-27 19:21:26,234   Epoch = 13 iter 3599 step
2023-03-27 19:21:26,235   Num examples = 1043
2023-03-27 19:21:26,235   Batch size = 32
2023-03-27 19:21:26,236 ***** Eval results *****
2023-03-27 19:21:26,236   att_loss = 575.434440612793
2023-03-27 19:21:26,236   cls_loss = 0.0
2023-03-27 19:21:26,237   global_step = 3599
2023-03-27 19:21:26,237   loss = 576.075031042099
2023-03-27 19:21:26,237   rep_loss = 0.6405890546739101
2023-03-27 19:21:26,244 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2023-03-27 19:21:42,156 ***** Running evaluation *****
2023-03-27 19:21:42,156   Epoch = 13 iter 3649 step
2023-03-27 19:21:42,156   Num examples = 1043
2023-03-27 19:21:42,156   Batch size = 32
2023-03-27 19:21:42,159 ***** Eval results *****
2023-03-27 19:21:42,159   att_loss = 574.4204283296392
2023-03-27 19:21:42,159   cls_loss = 0.0
2023-03-27 19:21:42,160   global_step = 3649
2023-03-27 19:21:42,160   loss = 575.0604335484879
2023-03-27 19:21:42,160   rep_loss = 0.6400038271807553
2023-03-27 19:21:42,162 ***** Save model *****
2023-03-27 19:21:58,056 ***** Running evaluation *****
2023-03-27 19:21:58,057   Epoch = 13 iter 3699 step
2023-03-27 19:21:58,057   Num examples = 1043
2023-03-27 19:21:58,057   Batch size = 32
2023-03-27 19:21:58,058 ***** Eval results *****
2023-03-27 19:21:58,058   att_loss = 574.4879843728584
2023-03-27 19:21:58,059   cls_loss = 0.0
2023-03-27 19:21:58,059   global_step = 3699
2023-03-27 19:21:58,059   loss = 575.1278765494363
2023-03-27 19:21:58,059   rep_loss = 0.6398908976922956
2023-03-27 19:21:58,066 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2023-03-27 19:22:13,984 ***** Running evaluation *****
2023-03-27 19:22:13,984   Epoch = 14 iter 3749 step
2023-03-27 19:22:13,984   Num examples = 1043
2023-03-27 19:22:13,984   Batch size = 32
2023-03-27 19:22:13,986 ***** Eval results *****
2023-03-27 19:22:13,986   att_loss = 562.4026711203835
2023-03-27 19:22:13,986   cls_loss = 0.0
2023-03-27 19:22:13,986   global_step = 3749
2023-03-27 19:22:13,987   loss = 563.0377918590199
2023-03-27 19:22:13,987   rep_loss = 0.6351161057298834
2023-03-27 19:22:13,994 ***** Save model *****
2023-03-27 19:22:29,930 ***** Running evaluation *****
2023-03-27 19:22:29,931   Epoch = 14 iter 3799 step
2023-03-27 19:22:29,931   Num examples = 1043
2023-03-27 19:22:29,931   Batch size = 32
2023-03-27 19:22:29,933 ***** Eval results *****
2023-03-27 19:22:29,934   att_loss = 572.5107461898053
2023-03-27 19:22:29,934   cls_loss = 0.0
2023-03-27 19:22:29,934   global_step = 3799
2023-03-27 19:22:29,934   loss = 573.1476710585297
2023-03-27 19:22:29,935   rep_loss = 0.6369231548465666
2023-03-27 19:22:29,943 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2023-03-27 19:22:45,836 ***** Running evaluation *****
2023-03-27 19:22:45,836   Epoch = 14 iter 3849 step
2023-03-27 19:22:45,837   Num examples = 1043
2023-03-27 19:22:45,837   Batch size = 32
2023-03-27 19:22:45,838 ***** Eval results *****
2023-03-27 19:22:45,838   att_loss = 570.6454921413113
2023-03-27 19:22:45,838   cls_loss = 0.0
2023-03-27 19:22:45,838   global_step = 3849
2023-03-27 19:22:45,838   loss = 571.2817671492293
2023-03-27 19:22:45,838   rep_loss = 0.6362735301524669
2023-03-27 19:22:45,841 ***** Save model *****
2023-03-27 19:23:01,729 ***** Running evaluation *****
2023-03-27 19:23:01,730   Epoch = 14 iter 3899 step
2023-03-27 19:23:01,730   Num examples = 1043
2023-03-27 19:23:01,730   Batch size = 32
2023-03-27 19:23:01,733 ***** Eval results *****
2023-03-27 19:23:01,733   att_loss = 573.2188180485127
2023-03-27 19:23:01,733   cls_loss = 0.0
2023-03-27 19:23:01,734   global_step = 3899
2023-03-27 19:23:01,734   loss = 573.8565899392833
2023-03-27 19:23:01,734   rep_loss = 0.6377706416645406
2023-03-27 19:23:01,736 ***** Save model *****
2023-03-27 19:23:17,621 ***** Running evaluation *****
2023-03-27 19:23:17,622   Epoch = 14 iter 3949 step
2023-03-27 19:23:17,622   Num examples = 1043
2023-03-27 19:23:17,622   Batch size = 32
2023-03-27 19:23:17,624 ***** Eval results *****
2023-03-27 19:23:17,624   att_loss = 574.1693836953402
2023-03-27 19:23:17,624   cls_loss = 0.0
2023-03-27 19:23:17,624   global_step = 3949
2023-03-27 19:23:17,624   loss = 574.8066644894568
2023-03-27 19:23:17,625   rep_loss = 0.6372803927032868
2023-03-27 19:23:17,632 ***** Save model *****
2023-03-27 19:23:33,540 ***** Running evaluation *****
2023-03-27 19:23:33,540   Epoch = 14 iter 3999 step
2023-03-27 19:23:33,540   Num examples = 1043
2023-03-27 19:23:33,540   Batch size = 32
2023-03-27 19:23:33,542 ***** Eval results *****
2023-03-27 19:23:33,543   att_loss = 573.9317349839484
2023-03-27 19:23:33,543   cls_loss = 0.0
2023-03-27 19:23:33,543   global_step = 3999
2023-03-27 19:23:33,544   loss = 574.568327688166
2023-03-27 19:23:33,544   rep_loss = 0.6365914276276512
2023-03-27 19:23:33,551 ***** Save model *****
2023-03-27 19:23:10,008 ***** Running evaluation *****
2023-03-27 19:23:10,008   Epoch = 15 iter 4099 step
2023-03-27 19:23:10,008   Num examples = 1043
2023-03-27 19:23:10,008   Batch size = 32
2023-03-27 19:23:10,010 ***** Eval results *****
2023-03-27 19:23:10,010   att_loss = 569.5095036283452
2023-03-27 19:23:10,010   cls_loss = 0.0
2023-03-27 19:23:10,010   global_step = 4099
2023-03-27 19:23:10,010   loss = 570.1417051274726
2023-03-27 19:23:10,010   rep_loss = 0.6322026385905894
2023-03-27 19:23:10,017 ***** Save model *****
2023-03-27 19:23:21,001 ***** Running evaluation *****
2023-03-27 19:23:21,001   Epoch = 15 iter 4149 step
2023-03-27 19:23:21,001   Num examples = 1043
2023-03-27 19:23:21,001   Batch size = 32
2023-03-27 19:23:21,002 ***** Eval results *****
2023-03-27 19:23:21,002   att_loss = 568.945545832316
2023-03-27 19:23:21,003   cls_loss = 0.0
2023-03-27 19:23:21,003   global_step = 4149
2023-03-27 19:23:21,003   loss = 569.5782023535835
2023-03-27 19:23:21,003   rep_loss = 0.6326567340228293
2023-03-27 19:23:21,010 ***** Save model *****
2023-03-27 19:23:32,002 ***** Running evaluation *****
2023-03-27 19:23:32,002   Epoch = 15 iter 4199 step
2023-03-27 19:23:32,002   Num examples = 1043
2023-03-27 19:23:32,003   Batch size = 32
2023-03-27 19:23:32,003 ***** Eval results *****
2023-03-27 19:23:32,004   att_loss = 569.3022069242812
2023-03-27 19:23:32,004   cls_loss = 0.0
2023-03-27 19:23:32,004   global_step = 4199
2023-03-27 19:23:32,004   loss = 569.9351229323554
2023-03-27 19:23:32,004   rep_loss = 0.6329160062308165
2023-03-27 19:23:32,006 ***** Save model *****
2023-03-27 19:23:42,961 ***** Running evaluation *****
2023-03-27 19:23:42,961   Epoch = 15 iter 4249 step
2023-03-27 19:23:42,961   Num examples = 1043
2023-03-27 19:23:42,962   Batch size = 32
2023-03-27 19:23:42,963 ***** Eval results *****
2023-03-27 19:23:42,963   att_loss = 569.8548435148646
2023-03-27 19:23:42,963   cls_loss = 0.0
2023-03-27 19:23:42,963   global_step = 4249
2023-03-27 19:23:42,964   loss = 570.4877798361856
2023-03-27 19:23:42,964   rep_loss = 0.6329365343344017
2023-03-27 19:23:42,971 ***** Save model *****
2023-03-27 19:23:53,950 ***** Running evaluation *****
2023-03-27 19:23:53,950   Epoch = 16 iter 4299 step
2023-03-27 19:23:53,951   Num examples = 1043
2023-03-27 19:23:53,951   Batch size = 32
2023-03-27 19:23:53,953 ***** Eval results *****
2023-03-27 19:23:53,953   att_loss = 565.0167846679688
2023-03-27 19:23:53,953   cls_loss = 0.0
2023-03-27 19:23:53,953   global_step = 4299
2023-03-27 19:23:53,953   loss = 565.644874855324
2023-03-27 19:23:53,953   rep_loss = 0.6280934391198335
2023-03-27 19:23:53,961 ***** Save model *****
2023-03-27 19:24:04,954 ***** Running evaluation *****
2023-03-27 19:24:04,954   Epoch = 16 iter 4349 step
2023-03-27 19:24:04,954   Num examples = 1043
2023-03-27 19:24:04,954   Batch size = 32
2023-03-27 19:24:04,955 ***** Eval results *****
2023-03-27 19:24:04,956   att_loss = 565.8097074434355
2023-03-27 19:24:04,956   cls_loss = 0.0
2023-03-27 19:24:04,956   global_step = 4349
2023-03-27 19:24:04,956   loss = 566.4394031871449
2023-03-27 19:24:04,956   rep_loss = 0.6296940337527882
2023-03-27 19:24:04,964 ***** Save model *****
2023-03-27 19:24:15,946 ***** Running evaluation *****
2023-03-27 19:24:15,946   Epoch = 16 iter 4399 step
2023-03-27 19:24:15,946   Num examples = 1043
2023-03-27 19:24:15,946   Batch size = 32
2023-03-27 19:24:15,948 ***** Eval results *****
2023-03-27 19:24:15,948   att_loss = 568.1782522126446
2023-03-27 19:24:15,948   cls_loss = 0.0
2023-03-27 19:24:15,948   global_step = 4399
2023-03-27 19:24:15,949   loss = 568.8077118640809
2023-03-27 19:24:15,949   rep_loss = 0.6294592745660796
2023-03-27 19:24:15,956 ***** Save model *****
2023-03-27 19:24:53,858 ***** Running evaluation *****
2023-03-27 19:24:53,858   Epoch = 15 iter 4249 step
2023-03-27 19:24:53,858   Num examples = 1043
2023-03-27 19:24:53,859   Batch size = 32
2023-03-27 19:24:53,859 ***** Eval results *****
2023-03-27 19:24:53,860   att_loss = 570.352969935683
2023-03-27 19:24:53,860   cls_loss = 0.0
2023-03-27 19:24:53,860   global_step = 4249
2023-03-27 19:24:53,860   loss = 570.9846085095015
2023-03-27 19:24:53,860   rep_loss = 0.6316374271619515
2023-03-27 19:24:53,862 ***** Save model *****
2023-03-27 19:25:09,770 ***** Running evaluation *****
2023-03-27 19:25:09,770   Epoch = 16 iter 4299 step
2023-03-27 19:25:09,770   Num examples = 1043
2023-03-27 19:25:09,770   Batch size = 32
2023-03-27 19:25:09,771 ***** Eval results *****
2023-03-27 19:25:09,772   att_loss = 563.0628209997107
2023-03-27 19:25:09,772   cls_loss = 0.0
2023-03-27 19:25:09,772   global_step = 4299
2023-03-27 19:25:09,772   loss = 563.6887229636864
2023-03-27 19:25:09,772   rep_loss = 0.6258982530346623
2023-03-27 19:25:09,778 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2023-03-27 19:25:25,707 ***** Running evaluation *****
2023-03-27 19:25:25,707   Epoch = 16 iter 4349 step
2023-03-27 19:25:25,707   Num examples = 1043
2023-03-27 19:25:25,708   Batch size = 32
2023-03-27 19:25:25,709 ***** Eval results *****
2023-03-27 19:25:25,710   att_loss = 562.2738504781352
2023-03-27 19:25:25,710   cls_loss = 0.0
2023-03-27 19:25:25,710   global_step = 4349
2023-03-27 19:25:25,711   loss = 562.9001211191153
2023-03-27 19:25:25,711   rep_loss = 0.6262696834353657
2023-03-27 19:25:25,718 ***** Save model *****
2023-03-27 19:25:41,653 ***** Running evaluation *****
2023-03-27 19:25:41,653   Epoch = 16 iter 4399 step
2023-03-27 19:25:41,653   Num examples = 1043
2023-03-27 19:25:41,654   Batch size = 32
2023-03-27 19:25:41,655 ***** Eval results *****
2023-03-27 19:25:41,655   att_loss = 567.1434773122231
2023-03-27 19:25:41,656   cls_loss = 0.0
2023-03-27 19:25:41,656   global_step = 4399
2023-03-27 19:25:41,656   loss = 567.7704938753384
2023-03-27 19:25:41,656   rep_loss = 0.6270165326088433
2023-03-27 19:25:41,664 ***** Save model *****
2023-03-27 19:25:57,602 ***** Running evaluation *****
2023-03-27 19:25:57,603   Epoch = 16 iter 4449 step
2023-03-27 19:25:57,603   Num examples = 1043
2023-03-27 19:25:57,603   Batch size = 32
2023-03-27 19:25:57,604 ***** Eval results *****
2023-03-27 19:25:57,604   att_loss = 567.9355572199418
2023-03-27 19:25:57,605   cls_loss = 0.0
2023-03-27 19:25:57,605   global_step = 4449
2023-03-27 19:25:57,605   loss = 568.5633289746645
2023-03-27 19:25:57,605   rep_loss = 0.6277712607114329
2023-03-27 19:25:57,617 ***** Save model *****
2023-03-27 19:26:13,514 ***** Running evaluation *****
2023-03-27 19:26:13,514   Epoch = 16 iter 4499 step
2023-03-27 19:26:13,514   Num examples = 1043
2023-03-27 19:26:13,515   Batch size = 32
2023-03-27 19:26:13,516 ***** Eval results *****
2023-03-27 19:26:13,516   att_loss = 567.4513842880989
2023-03-27 19:26:13,516   cls_loss = 0.0
2023-03-27 19:26:13,517   global_step = 4499
2023-03-27 19:26:13,517   loss = 568.078793025752
2023-03-27 19:26:13,517   rep_loss = 0.6274081878199976
2023-03-27 19:26:13,524 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2023-03-27 19:26:29,428 ***** Running evaluation *****
2023-03-27 19:26:29,429   Epoch = 17 iter 4549 step
2023-03-27 19:26:29,429   Num examples = 1043
2023-03-27 19:26:29,429   Batch size = 32
2023-03-27 19:26:29,430 ***** Eval results *****
2023-03-27 19:26:29,430   att_loss = 570.1401672363281
2023-03-27 19:26:29,431   cls_loss = 0.0
2023-03-27 19:26:29,431   global_step = 4549
2023-03-27 19:26:29,431   loss = 570.7667724609375
2023-03-27 19:26:29,431   rep_loss = 0.6266097128391266
2023-03-27 19:26:29,439 ***** Save model *****
2023-03-27 19:26:45,348 ***** Running evaluation *****
2023-03-27 19:26:45,348   Epoch = 17 iter 4599 step
2023-03-27 19:26:45,348   Num examples = 1043
2023-03-27 19:26:45,348   Batch size = 32
2023-03-27 19:26:45,350 ***** Eval results *****
2023-03-27 19:26:45,350   att_loss = 563.5259801228841
2023-03-27 19:26:45,350   cls_loss = 0.0
2023-03-27 19:26:45,350   global_step = 4599
2023-03-27 19:26:45,350   loss = 564.1512736002604
2023-03-27 19:26:45,351   rep_loss = 0.6252946446339289
2023-03-27 19:26:45,358 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2023-03-27 19:27:01,277 ***** Running evaluation *****
2023-03-27 19:27:01,278   Epoch = 17 iter 4649 step
2023-03-27 19:27:01,278   Num examples = 1043
2023-03-27 19:27:01,278   Batch size = 32
2023-03-27 19:27:01,279 ***** Eval results *****
2023-03-27 19:27:01,279   att_loss = 564.4376770019531
2023-03-27 19:27:01,279   cls_loss = 0.0
2023-03-27 19:27:01,280   global_step = 4649
2023-03-27 19:27:01,280   loss = 565.0620139382103
2023-03-27 19:27:01,280   rep_loss = 0.6243382090871984
2023-03-27 19:27:01,285 ***** Save model *****
2023-03-27 19:27:17,153 ***** Running evaluation *****
2023-03-27 19:27:17,154   Epoch = 17 iter 4699 step
2023-03-27 19:27:17,154   Num examples = 1043
2023-03-27 19:27:17,154   Batch size = 32
2023-03-27 19:27:17,155 ***** Eval results *****
2023-03-27 19:27:17,156   att_loss = 564.4117517471313
2023-03-27 19:27:17,156   cls_loss = 0.0
2023-03-27 19:27:17,156   global_step = 4699
2023-03-27 19:27:17,156   loss = 565.0360668182373
2023-03-27 19:27:17,156   rep_loss = 0.6243147004395724
2023-03-27 19:27:17,164 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2023-03-27 19:27:33,082 ***** Running evaluation *****
2023-03-27 19:27:33,082   Epoch = 17 iter 4749 step
2023-03-27 19:27:33,082   Num examples = 1043
2023-03-27 19:27:33,083   Batch size = 32
2023-03-27 19:27:33,086 ***** Eval results *****
2023-03-27 19:27:33,086   att_loss = 565.9437890915643
2023-03-27 19:27:33,086   cls_loss = 0.0
2023-03-27 19:27:33,087   global_step = 4749
2023-03-27 19:27:33,087   loss = 566.5682503836496
2023-03-27 19:27:33,087   rep_loss = 0.6244602609248389
2023-03-27 19:27:33,094 ***** Save model *****
2023-03-27 19:27:11,871 ***** Running evaluation *****
2023-03-27 19:27:11,871   Epoch = 19 iter 5199 step
2023-03-27 19:27:11,871   Num examples = 1043
2023-03-27 19:27:11,871   Batch size = 32
2023-03-27 19:27:11,874 ***** Eval results *****
2023-03-27 19:27:11,874   att_loss = 561.1501251705109
2023-03-27 19:27:11,874   cls_loss = 0.0
2023-03-27 19:27:11,874   global_step = 5199
2023-03-27 19:27:11,875   loss = 561.7663147941469
2023-03-27 19:27:11,875   rep_loss = 0.6161900782395923
2023-03-27 19:27:11,879 ***** Save model *****
2023-03-27 19:27:22,854 ***** Running evaluation *****
2023-03-27 19:27:22,854   Epoch = 19 iter 5249 step
2023-03-27 19:27:22,855   Num examples = 1043
2023-03-27 19:27:22,855   Batch size = 32
2023-03-27 19:27:22,856 ***** Eval results *****
2023-03-27 19:27:22,856   att_loss = 562.7653406316584
2023-03-27 19:27:22,856   cls_loss = 0.0
2023-03-27 19:27:22,856   global_step = 5249
2023-03-27 19:27:22,857   loss = 563.3826956315474
2023-03-27 19:27:22,857   rep_loss = 0.6173559217290445
2023-03-27 19:27:22,864 ***** Save model *****
2023-03-27 19:27:33,932 ***** Running evaluation *****
2023-03-27 19:27:33,932   Epoch = 19 iter 5299 step
2023-03-27 19:27:33,932   Num examples = 1043
2023-03-27 19:27:33,932   Batch size = 32
2023-03-27 19:27:33,934 ***** Eval results *****
2023-03-27 19:27:33,934   att_loss = 562.8896253467661
2023-03-27 19:27:33,934   cls_loss = 0.0
2023-03-27 19:27:33,934   global_step = 5299
2023-03-27 19:27:33,934   loss = 563.5077472788042
2023-03-27 19:27:33,935   rep_loss = 0.6181222870286587
2023-03-27 19:27:33,936 ***** Save model *****
2023-03-27 19:27:44,925 ***** Running evaluation *****
2023-03-27 19:27:44,925   Epoch = 20 iter 5349 step
2023-03-27 19:27:44,926   Num examples = 1043
2023-03-27 19:27:44,926   Batch size = 32
2023-03-27 19:27:44,927 ***** Eval results *****
2023-03-27 19:27:44,927   att_loss = 559.6300896538628
2023-03-27 19:27:44,927   cls_loss = 0.0
2023-03-27 19:27:44,928   global_step = 5349
2023-03-27 19:27:44,928   loss = 560.243899875217
2023-03-27 19:27:44,928   rep_loss = 0.6138212084770203
2023-03-27 19:27:44,935 ***** Save model *****
2023-03-27 19:27:55,929 ***** Running evaluation *****
2023-03-27 19:27:55,930   Epoch = 20 iter 5399 step
2023-03-27 19:27:55,930   Num examples = 1043
2023-03-27 19:27:55,930   Batch size = 32
2023-03-27 19:27:55,931 ***** Eval results *****
2023-03-27 19:27:55,931   att_loss = 558.2477546303959
2023-03-27 19:27:55,931   cls_loss = 0.0
2023-03-27 19:27:55,931   global_step = 5399
2023-03-27 19:27:55,931   loss = 558.8631255586268
2023-03-27 19:27:55,931   rep_loss = 0.6153713464736938
2023-03-27 19:27:55,939 ***** Save model *****
2023-03-27 19:28:06,965 ***** Running evaluation *****
2023-03-27 19:28:06,966   Epoch = 20 iter 5449 step
2023-03-27 19:28:06,966   Num examples = 1043
2023-03-27 19:28:06,966   Batch size = 32
2023-03-27 19:28:06,967 ***** Eval results *****
2023-03-27 19:28:06,967   att_loss = 559.6667852839199
2023-03-27 19:28:06,967   cls_loss = 0.0
2023-03-27 19:28:06,967   global_step = 5449
2023-03-27 19:28:06,967   loss = 560.2825894137042
2023-03-27 19:28:06,967   rep_loss = 0.6158023132096737
2023-03-27 19:28:06,973 ***** Save model *****
2023-03-27 19:28:17,965 ***** Running evaluation *****
2023-03-27 19:28:17,966   Epoch = 20 iter 5499 step
2023-03-27 19:28:17,966   Num examples = 1043
2023-03-27 19:28:17,966   Batch size = 32
2023-03-27 19:28:17,968 ***** Eval results *****
2023-03-27 19:28:17,968   att_loss = 559.1339715921654
2023-03-27 19:28:17,968   cls_loss = 0.0
2023-03-27 19:28:17,969   global_step = 5499
2023-03-27 19:28:17,969   loss = 559.7494710286459
2023-03-27 19:28:17,969   rep_loss = 0.6154967996309388
2023-03-27 19:28:17,973 ***** Save model *****
2023-03-27 19:28:52,717 ***** Running evaluation *****
2023-03-27 19:28:52,717   Epoch = 18 iter 4999 step
2023-03-27 19:28:52,717   Num examples = 1043
2023-03-27 19:28:52,717   Batch size = 32
2023-03-27 19:28:52,719 ***** Eval results *****
2023-03-27 19:28:52,719   att_loss = 563.2411382013034
2023-03-27 19:28:52,719   cls_loss = 0.0
2023-03-27 19:28:52,719   global_step = 4999
2023-03-27 19:28:52,719   loss = 563.8623386837658
2023-03-27 19:28:52,720   rep_loss = 0.6212029741218053
2023-03-27 19:28:52,728 ***** Save model *****
2023-03-27 19:29:08,689 ***** Running evaluation *****
2023-03-27 19:29:08,690   Epoch = 18 iter 5049 step
2023-03-27 19:29:08,690   Num examples = 1043
2023-03-27 19:29:08,690   Batch size = 32
2023-03-27 19:29:08,691 ***** Eval results *****
2023-03-27 19:29:08,691   att_loss = 564.7198198734487
2023-03-27 19:29:08,691   cls_loss = 0.0
2023-03-27 19:29:08,692   global_step = 5049
2023-03-27 19:29:08,692   loss = 565.3412655174977
2023-03-27 19:29:08,692   rep_loss = 0.6214464915142138
2023-03-27 19:29:08,699 ***** Save model *****
2023-03-27 19:28:50,987 ***** Running evaluation *****
2023-03-27 19:28:50,987   Epoch = 21 iter 5649 step
2023-03-27 19:28:50,988   Num examples = 1043
2023-03-27 19:28:50,988   Batch size = 32
2023-03-27 19:28:50,989 ***** Eval results *****
2023-03-27 19:28:50,989   att_loss = 561.8092593238467
2023-03-27 19:28:50,989   cls_loss = 0.0
2023-03-27 19:28:50,990   global_step = 5649
2023-03-27 19:28:50,990   loss = 562.4232817150298
2023-03-27 19:28:50,990   rep_loss = 0.6140219782079969
2023-03-27 19:28:50,997 ***** Save model *****
2023-03-27 19:29:01,972 ***** Running evaluation *****
2023-03-27 19:29:01,973   Epoch = 21 iter 5699 step
2023-03-27 19:29:01,973   Num examples = 1043
2023-03-27 19:29:01,973   Batch size = 32
2023-03-27 19:29:01,974 ***** Eval results *****
2023-03-27 19:29:01,974   att_loss = 560.9070573889691
2023-03-27 19:29:01,975   cls_loss = 0.0
2023-03-27 19:29:01,975   global_step = 5699
2023-03-27 19:29:01,975   loss = 561.5220502770466
2023-03-27 19:29:01,975   rep_loss = 0.6149954484856647
2023-03-27 19:29:01,982 ***** Save model *****
2023-03-27 19:29:12,989 ***** Running evaluation *****
2023-03-27 19:29:12,989   Epoch = 21 iter 5749 step
2023-03-27 19:29:12,989   Num examples = 1043
2023-03-27 19:29:12,989   Batch size = 32
2023-03-27 19:29:12,991 ***** Eval results *****
2023-03-27 19:29:12,991   att_loss = 559.3089812372772
2023-03-27 19:29:12,991   cls_loss = 0.0
2023-03-27 19:29:12,991   global_step = 5749
2023-03-27 19:29:12,992   loss = 559.9227806413677
2023-03-27 19:29:12,992   rep_loss = 0.6138000664576678
2023-03-27 19:29:12,999 ***** Save model *****
2023-03-27 19:29:24,018 ***** Running evaluation *****
2023-03-27 19:29:24,019   Epoch = 21 iter 5799 step
2023-03-27 19:29:24,019   Num examples = 1043
2023-03-27 19:29:24,019   Batch size = 32
2023-03-27 19:29:24,021 ***** Eval results *****
2023-03-27 19:29:24,021   att_loss = 559.8490120569865
2023-03-27 19:29:24,021   cls_loss = 0.0
2023-03-27 19:29:24,021   global_step = 5799
2023-03-27 19:29:24,021   loss = 560.4630235036215
2023-03-27 19:29:24,021   rep_loss = 0.6140112259114782
2023-03-27 19:29:24,028 ***** Save model *****
2023-03-27 19:29:35,035 ***** Running evaluation *****
2023-03-27 19:29:35,035   Epoch = 21 iter 5849 step
2023-03-27 19:29:35,035   Num examples = 1043
2023-03-27 19:29:35,035   Batch size = 32
2023-03-27 19:29:35,036 ***** Eval results *****
2023-03-27 19:29:35,036   att_loss = 560.1193973761945
2023-03-27 19:29:35,036   cls_loss = 0.0
2023-03-27 19:29:35,037   global_step = 5849
2023-03-27 19:29:35,037   loss = 560.7333669110764
2023-03-27 19:29:35,037   rep_loss = 0.6139690321831663
2023-03-27 19:29:35,044 ***** Save model *****
2023-03-27 19:29:46,042 ***** Running evaluation *****
2023-03-27 19:29:46,042   Epoch = 22 iter 5899 step
2023-03-27 19:29:46,042   Num examples = 1043
2023-03-27 19:29:46,042   Batch size = 32
2023-03-27 19:29:46,044 ***** Eval results *****
2023-03-27 19:29:46,044   att_loss = 552.0250329589844
2023-03-27 19:29:46,044   cls_loss = 0.0
2023-03-27 19:29:46,044   global_step = 5899
2023-03-27 19:29:46,044   loss = 552.6345703125
2023-03-27 19:29:46,045   rep_loss = 0.6095405769348144
2023-03-27 19:29:46,052 ***** Save model *****
2023-03-27 19:29:57,056 ***** Running evaluation *****
2023-03-27 19:29:57,057   Epoch = 22 iter 5949 step
2023-03-27 19:29:57,057   Num examples = 1043
2023-03-27 19:29:57,057   Batch size = 32
2023-03-27 19:29:57,058 ***** Eval results *****
2023-03-27 19:29:57,058   att_loss = 555.4473758951823
2023-03-27 19:29:57,059   cls_loss = 0.0
2023-03-27 19:29:57,059   global_step = 5949
2023-03-27 19:29:57,059   loss = 556.0556697591146
2023-03-27 19:29:57,059   rep_loss = 0.608296070098877
2023-03-27 19:29:57,064 ***** Save model *****
2023-03-27 19:30:08,043 ***** Running evaluation *****
2023-03-27 19:30:08,043   Epoch = 22 iter 5999 step
2023-03-27 19:30:08,043   Num examples = 1043
2023-03-27 19:30:08,043   Batch size = 32
2023-03-27 19:30:08,044 ***** Eval results *****
2023-03-27 19:30:08,044   att_loss = 556.6141772460937
2023-03-27 19:30:08,045   cls_loss = 0.0
2023-03-27 19:30:08,045   global_step = 5999
2023-03-27 19:30:12,430   loss = 564.1756995807995
2023-03-27 19:30:12,431   rep_loss = 0.6186668960885569
2023-03-27 19:30:12,438 ***** Save model *****
2023-03-27 19:30:28,356 ***** Running evaluation *****
2023-03-27 19:30:28,357   Epoch = 19 iter 5299 step
2023-03-27 19:30:28,357   Num examples = 1043
2023-03-27 19:30:28,357   Batch size = 32
2023-03-27 19:30:28,358 ***** Eval results *****
2023-03-27 19:30:28,359   att_loss = 563.3531537351355
2023-03-27 19:30:28,359   cls_loss = 0.0
2023-03-27 19:30:28,359   global_step = 5299
2023-03-27 19:30:28,359   loss = 563.9720703395067
2023-03-27 19:30:28,359   rep_loss = 0.6189146877917568
2023-03-27 19:30:28,364 ***** Save model *****
2023-03-27 19:30:44,287 ***** Running evaluation *****
2023-03-27 19:30:44,287   Epoch = 20 iter 5349 step
2023-03-27 19:30:44,287   Num examples = 1043
2023-03-27 19:30:44,288   Batch size = 32
2023-03-27 19:30:44,289 ***** Eval results *****
2023-03-27 19:30:44,290   att_loss = 562.6202799479166
2023-03-27 19:30:44,290   cls_loss = 0.0
2023-03-27 19:30:44,290   global_step = 5349
2023-03-27 19:30:44,290   loss = 563.2355143229166
2023-03-27 19:30:44,290   rep_loss = 0.6152237123913236
2023-03-27 19:30:44,295 ***** Save model *****
2023-03-27 19:31:00,212 ***** Running evaluation *****
2023-03-27 19:31:00,212   Epoch = 20 iter 5399 step
2023-03-27 19:31:00,212   Num examples = 1043
2023-03-27 19:31:00,212   Batch size = 32
2023-03-27 19:31:00,214 ***** Eval results *****
2023-03-27 19:31:00,214   att_loss = 559.9910904189287
2023-03-27 19:31:00,214   cls_loss = 0.0
2023-03-27 19:31:00,214   global_step = 5399
2023-03-27 19:31:00,215   loss = 560.6063366906118
2023-03-27 19:31:00,215   rep_loss = 0.6152450896925845
2023-03-27 19:31:00,222 ***** Save model *****
2023-03-27 19:31:16,149 ***** Running evaluation *****
2023-03-27 19:31:16,149   Epoch = 20 iter 5449 step
2023-03-27 19:31:16,149   Num examples = 1043
2023-03-27 19:31:16,149   Batch size = 32
2023-03-27 19:31:16,150 ***** Eval results *****
2023-03-27 19:31:16,150   att_loss = 558.5960572968929
2023-03-27 19:31:16,150   cls_loss = 0.0
2023-03-27 19:31:16,151   global_step = 5449
2023-03-27 19:31:16,151   loss = 559.2109769768671
2023-03-27 19:31:16,151   rep_loss = 0.6149199533899989
2023-03-27 19:31:16,158 ***** Save model *****
2023-03-27 19:31:32,068 ***** Running evaluation *****
2023-03-27 19:31:32,069   Epoch = 20 iter 5499 step
2023-03-27 19:31:32,069   Num examples = 1043
2023-03-27 19:31:32,069   Batch size = 32
2023-03-27 19:31:32,070 ***** Eval results *****
2023-03-27 19:31:32,071   att_loss = 558.9076039056358
2023-03-27 19:31:32,071   cls_loss = 0.0
2023-03-27 19:31:32,071   global_step = 5499
2023-03-27 19:31:32,071   loss = 559.5228785868711
2023-03-27 19:31:32,072   rep_loss = 0.6152753203919848
2023-03-27 19:31:32,079 ***** Save model *****
2023-03-27 19:31:48,011 ***** Running evaluation *****
2023-03-27 19:31:48,012   Epoch = 20 iter 5549 step
2023-03-27 19:31:48,012   Num examples = 1043
2023-03-27 19:31:48,012   Batch size = 32
2023-03-27 19:31:48,013 ***** Eval results *****
2023-03-27 19:31:48,013   att_loss = 560.8363413833545
2023-03-27 19:31:48,014   cls_loss = 0.0
2023-03-27 19:31:48,014   global_step = 5549
2023-03-27 19:31:48,014   loss = 561.4518061733702
2023-03-27 19:31:48,014   rep_loss = 0.6154651385174984
2023-03-27 19:31:48,019 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      2023-03-27 19:32:03,942 ***** Running evaluation *****
2023-03-27 19:32:03,943   Epoch = 20 iter 5599 step
2023-03-27 19:32:03,943   Num examples = 1043
2023-03-27 19:32:03,943   Batch size = 32
2023-03-27 19:32:03,945 ***** Eval results *****
2023-03-27 19:32:03,945   att_loss = 561.0798280929506
2023-03-27 19:32:03,945   cls_loss = 0.0
2023-03-27 19:32:03,945   global_step = 5599
2023-03-27 19:32:03,945   loss = 561.6955375524101
2023-03-27 19:32:03,945   rep_loss = 0.6157100598784487
2023-03-27 19:32:03,947 ***** Save model *****
2023-03-27 19:32:19,840 ***** Running evaluation *****
2023-03-27 19:32:19,840   Epoch = 21 iter 5649 step
2023-03-27 19:32:19,840   Num examples = 1043
2023-03-27 19:32:19,841   Batch size = 32
2023-03-27 19:32:19,842 ***** Eval results *****
2023-03-27 19:32:19,842   att_loss = 558.1671273367746
2023-03-27 19:32:19,842   cls_loss = 0.0
2023-03-27 19:32:19,842   global_step = 5649
2023-03-27 19:32:19,843   loss = 558.7784670875186
2023-03-27 19:32:19,843   rep_loss = 0.611335684855779
2023-03-27 19:32:19,850 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2023-03-27 19:32:35,777 ***** Running evaluation *****
2023-03-27 19:32:35,777   Epoch = 21 iter 5699 step
2023-03-27 19:32:35,777   Num examples = 1043
2023-03-27 19:32:35,778   Batch size = 32
2023-03-27 19:32:35,779 ***** Eval results *****
2023-03-27 19:32:35,779   att_loss = 559.4685131570567
2023-03-27 19:32:35,779   cls_loss = 0.0
2023-03-27 19:32:35,779   global_step = 5699
2023-03-27 19:32:35,779   loss = 560.0808078931725
2023-03-27 19:32:35,779   rep_loss = 0.6122929544552512
2023-03-27 19:32:35,786 ***** Save model *****
2023-03-27 19:32:52,205 ***** Running evaluation *****
2023-03-27 19:32:52,205   Epoch = 21 iter 5749 step
2023-03-27 19:32:52,205   Num examples = 1043
2023-03-27 19:32:52,205   Batch size = 32
2023-03-27 19:32:52,207 ***** Eval results *****
2023-03-27 19:32:52,207   att_loss = 557.8044543199136
2023-03-27 19:32:52,207   cls_loss = 0.0
2023-03-27 19:32:52,208   global_step = 5749
2023-03-27 19:32:52,208   loss = 558.4157899668519
2023-03-27 19:32:52,208   rep_loss = 0.6113346525481049
2023-03-27 19:32:52,213 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2023-03-27 19:33:08,148 ***** Running evaluation *****
2023-03-27 19:33:08,149   Epoch = 21 iter 5799 step
2023-03-27 19:33:08,149   Num examples = 1043
2023-03-27 19:33:08,149   Batch size = 32
2023-03-27 19:33:08,152 ***** Eval results *****
2023-03-27 19:33:08,153   att_loss = 558.4374101956686
2023-03-27 19:33:08,153   cls_loss = 0.0
2023-03-27 19:33:08,153   global_step = 5799
2023-03-27 19:33:08,154   loss = 559.0492800076803
2023-03-27 19:33:08,154   rep_loss = 0.6118683541814486
2023-03-27 19:33:08,158 ***** Save model *****
2023-03-27 19:33:24,074 ***** Running evaluation *****
2023-03-27 19:33:24,074   Epoch = 21 iter 5849 step
2023-03-27 19:33:24,075   Num examples = 1043
2023-03-27 19:33:24,075   Batch size = 32
2023-03-27 19:33:24,076 ***** Eval results *****
2023-03-27 19:33:24,076   att_loss = 559.3140627017691
2023-03-27 19:33:24,076   cls_loss = 0.0
2023-03-27 19:33:24,076   global_step = 5849
2023-03-27 19:33:24,076   loss = 559.9263448163498
2023-03-27 19:33:24,077   rep_loss = 0.6122803165892924
2023-03-27 19:33:24,079 ***** Save model *****
2023-03-27 19:33:40,008 ***** Running evaluation *****
2023-03-27 19:33:40,008   Epoch = 22 iter 5899 step
2023-03-27 19:33:40,008   Num examples = 1043
2023-03-27 19:33:40,008   Batch size = 32
2023-03-27 19:33:40,010 ***** Eval results *****
2023-03-27 19:33:40,010   att_loss = 549.7930358886719
2023-03-27 19:33:40,010   cls_loss = 0.0
2023-03-27 19:33:40,011   global_step = 5899
2023-03-27 19:33:40,011   loss = 550.4015698242188
2023-03-27 19:33:40,011   rep_loss = 0.6085358738899231
2023-03-27 19:33:40,018 ***** Save model *****
2023-03-27 19:33:55,923 ***** Running evaluation *****
2023-03-27 19:33:55,923   Epoch = 22 iter 5949 step
2023-03-27 19:33:55,923   Num examples = 1043
2023-03-27 19:33:55,923   Batch size = 32
2023-03-27 19:33:55,925 ***** Eval results *****
2023-03-27 19:33:55,925   att_loss = 555.0707320149739
2023-03-27 19:33:55,925   cls_loss = 0.0
2023-03-27 19:33:55,926   global_step = 5949
2023-03-27 19:33:55,926   loss = 555.6778564453125
2023-03-27 19:33:55,926   rep_loss = 0.6071245161692301
2023-03-27 19:33:55,933 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      2023-03-27 19:34:11,885 ***** Running evaluation *****
2023-03-27 19:34:11,885   Epoch = 22 iter 5999 step
2023-03-27 19:34:11,886   Num examples = 1043
2023-03-27 19:34:11,886   Batch size = 32
2023-03-27 19:34:11,887 ***** Eval results *****
2023-03-27 19:34:11,887   att_loss = 555.3658017578125
2023-03-27 19:34:11,887   cls_loss = 0.0
2023-03-27 19:34:11,888   global_step = 5999
2023-03-27 19:34:11,888   loss = 555.9744289550781
2023-03-27 19:34:11,888   rep_loss = 0.6086266083717347
2023-03-27 19:34:11,895 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     2023-03-27 19:34:27,826 ***** Running evaluation *****
2023-03-27 19:34:27,826   Epoch = 22 iter 6049 step
2023-03-27 19:34:27,826   Num examples = 1043
2023-03-27 19:34:27,826   Batch size = 32
2023-03-27 19:34:27,827 ***** Eval results *****
2023-03-27 19:34:27,827   att_loss = 556.1812831333706
2023-03-27 19:34:27,827   cls_loss = 0.0
2023-03-27 19:34:27,827   global_step = 6049
2023-03-27 19:34:27,828   loss = 556.7906809779575
2023-03-27 19:34:27,828   rep_loss = 0.6093981385231018
2023-03-27 19:34:27,835 ***** Save model *****
2023-03-27 19:34:43,742 ***** Running evaluation *****
2023-03-27 19:34:43,742   Epoch = 22 iter 6099 step
2023-03-27 19:34:43,742   Num examples = 1043
2023-03-27 19:34:43,742   Batch size = 32
2023-03-27 19:34:43,743 ***** Eval results *****
2023-03-27 19:34:43,744   att_loss = 557.1763994683159
2023-03-27 19:34:43,744   cls_loss = 0.0
2023-03-27 19:34:43,744   global_step = 6099
2023-03-27 19:34:43,745   loss = 557.7858756510417
2023-03-27 19:34:43,745   rep_loss = 0.609476645787557
2023-03-27 19:34:43,746 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            2023-03-27 19:34:59,636 ***** Running evaluation *****
2023-03-27 19:34:59,637   Epoch = 23 iter 6149 step
2023-03-27 19:34:59,637   Num examples = 1043
2023-03-27 19:34:59,637   Batch size = 32
2023-03-27 19:34:59,638 ***** Eval results *****
2023-03-27 19:34:59,639   att_loss = 551.2215347290039
2023-03-27 19:34:59,639   cls_loss = 0.0
2023-03-27 19:34:59,639   global_step = 6149
2023-03-27 19:34:59,639   loss = 551.8270416259766
2023-03-27 19:34:59,639   rep_loss = 0.6055098176002502
2023-03-27 19:34:59,647 ***** Save model *****
2023-03-27 19:35:15,553 ***** Running evaluation *****
2023-03-27 19:35:15,553   Epoch = 23 iter 6199 step
2023-03-27 19:35:15,553   Num examples = 1043
2023-03-27 19:35:15,553   Batch size = 32
2023-03-27 19:35:15,555 ***** Eval results *****
2023-03-27 19:35:15,556   att_loss = 556.2167847732018
2023-03-27 19:35:15,556   cls_loss = 0.0
2023-03-27 19:35:15,556   global_step = 6199
2023-03-27 19:35:15,556   loss = 556.8265675511853
2023-03-27 19:35:15,556   rep_loss = 0.6097838262031818
2023-03-27 19:35:15,561 ***** Save model *****
2023-03-27 19:35:31,478 ***** Running evaluation *****
2023-03-27 19:35:31,479   Epoch = 23 iter 6249 step
2023-03-27 19:35:31,479   Num examples = 1043
2023-03-27 19:35:31,479   Batch size = 32
2023-03-27 19:35:31,480 ***** Eval results *****
2023-03-27 19:35:31,480   att_loss = 558.1817765412508
2023-03-27 19:35:31,480   cls_loss = 0.0
2023-03-27 19:35:31,481   global_step = 6249
2023-03-27 19:35:31,481   loss = 558.7904318350332
2023-03-27 19:35:31,481   rep_loss = 0.6086551077939846
2023-03-27 19:35:31,488 ***** Save model *****
2023-03-27 19:35:47,425 ***** Running evaluation *****
2023-03-27 19:35:47,425   Epoch = 23 iter 6299 step
2023-03-27 19:35:47,425   Num examples = 1043
2023-03-27 19:35:47,426   Batch size = 32
2023-03-27 19:35:47,427 ***** Eval results *****
2023-03-27 19:35:47,427   att_loss = 557.4125008884864
2023-03-27 19:35:47,427   cls_loss = 0.0
2023-03-27 19:35:47,427   global_step = 6299
2023-03-27 19:35:47,427   loss = 558.0210857149921
2023-03-27 19:35:47,427   rep_loss = 0.6085834959639779
2023-03-27 19:35:47,434 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2023-03-27 19:36:03,344 ***** Running evaluation *****
2023-03-27 19:36:03,344   Epoch = 23 iter 6349 step
2023-03-27 19:36:03,345   Num examples = 1043
2023-03-27 19:36:03,345   Batch size = 32
2023-03-27 19:36:03,346 ***** Eval results *****
2023-03-27 19:36:03,346   att_loss = 557.6689274127667
2023-03-27 19:36:03,346   cls_loss = 0.0
2023-03-27 19:36:03,346   global_step = 6349
2023-03-27 19:36:03,347   loss = 558.2770952078013
2023-03-27 19:36:03,347   rep_loss = 0.6081667628425819
2023-03-27 19:36:03,352 ***** Save model *****
2023-03-27 19:36:19,240 ***** Running evaluation *****
2023-03-27 19:36:19,240   Epoch = 23 iter 6399 step
2023-03-27 19:36:19,240   Num examples = 1043
2023-03-27 19:36:19,241   Batch size = 32
2023-03-27 19:36:19,242 ***** Eval results *****
2023-03-27 19:36:19,243   att_loss = 557.229778201081
2023-03-27 19:36:19,243   cls_loss = 0.0
2023-03-27 19:36:19,243   global_step = 6399
2023-03-27 19:36:19,243   loss = 557.8380475894426
2023-03-27 19:36:19,244   rep_loss = 0.6082693419253179
2023-03-27 19:36:19,248 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            2023-03-27 19:36:35,168 ***** Running evaluation *****
2023-03-27 19:36:35,169   Epoch = 24 iter 6449 step
2023-03-27 19:36:35,169   Num examples = 1043
2023-03-27 19:36:35,169   Batch size = 32
2023-03-27 19:36:35,170 ***** Eval results *****
2023-03-27 19:36:35,170   att_loss = 554.0843788705221
2023-03-27 19:36:35,171   cls_loss = 0.0
2023-03-27 19:36:35,171   global_step = 6449
2023-03-27 19:36:35,171   loss = 554.6911985816025
2023-03-27 19:36:35,171   rep_loss = 0.6068183663414746
2023-03-27 19:36:35,179 ***** Save model *****
2023-03-27 19:36:51,094 ***** Running evaluation *****
2023-03-27 19:36:51,094   Epoch = 24 iter 6499 step
2023-03-27 19:36:51,094   Num examples = 1043
2023-03-27 19:36:51,095   Batch size = 32
2023-03-27 19:36:51,096 ***** Eval results *****
2023-03-27 19:36:51,096   att_loss = 553.4951537415221
2023-03-27 19:36:51,096   cls_loss = 0.0
2023-03-27 19:36:51,096   global_step = 6499
2023-03-27 19:36:51,096   loss = 554.1017831655649
2023-03-27 19:36:51,096   rep_loss = 0.6066304229118011
2023-03-27 19:36:51,103 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2023-03-27 19:37:07,057 ***** Running evaluation *****
2023-03-27 19:37:07,058   Epoch = 24 iter 6549 step
2023-03-27 19:37:07,058   Num examples = 1043
2023-03-27 19:37:07,058   Batch size = 32
2023-03-27 19:37:07,060 ***** Eval results *****
2023-03-27 19:37:07,060   att_loss = 552.8800059649961
2023-03-27 19:37:07,060   cls_loss = 0.0
2023-03-27 19:37:07,060   global_step = 6549
2023-03-27 19:37:07,060   loss = 553.4861699097545
2023-03-27 19:37:07,060   rep_loss = 0.6061650227147637
2023-03-27 19:37:07,062 ***** Save model *****
2023-03-27 19:37:22,951 ***** Running evaluation *****
2023-03-27 19:37:22,951   Epoch = 24 iter 6599 step
2023-03-27 19:37:22,951   Num examples = 1043
2023-03-27 19:37:22,952   Batch size = 32
2023-03-27 19:37:22,953 ***** Eval results *****
2023-03-27 19:37:22,954   att_loss = 556.0534167863935
2023-03-27 19:37:22,954   cls_loss = 0.0
2023-03-27 19:37:22,954   global_step = 6599
2023-03-27 19:37:22,954   loss = 556.6600661352667
2023-03-27 19:37:22,955   rep_loss = 0.606650072242577
2023-03-27 19:37:22,962 ***** Save model *****
023-03-27 19:37:06,115 ***** Running evaluation *****
2023-03-27 19:37:06,115   Epoch = 29 iter 7899 step
2023-03-27 19:37:06,115   Num examples = 1043
2023-03-27 19:37:06,116   Batch size = 32
2023-03-27 19:37:06,117 ***** Eval results *****
2023-03-27 19:37:06,117   att_loss = 550.7699395204202
2023-03-27 19:37:06,118   cls_loss = 0.0
2023-03-27 19:37:06,118   global_step = 7899
2023-03-27 19:37:06,118   loss = 551.3670012645232
2023-03-27 19:37:06,118   rep_loss = 0.5970615286093491
2023-03-27 19:37:06,120 ***** Save model *****
2023-03-27 19:37:17,119 ***** Running evaluation *****
2023-03-27 19:37:17,119   Epoch = 29 iter 7949 step
2023-03-27 19:37:17,119   Num examples = 1043
2023-03-27 19:37:17,119   Batch size = 32
2023-03-27 19:37:17,121 ***** Eval results *****
2023-03-27 19:37:17,121   att_loss = 550.6687066531875
2023-03-27 19:37:17,121   cls_loss = 0.0
2023-03-27 19:37:17,121   global_step = 7949
2023-03-27 19:37:17,121   loss = 551.2664578632243
2023-03-27 19:37:17,122   rep_loss = 0.5977506805392145
2023-03-27 19:37:17,129 ***** Save model *****
2023-03-27 19:37:28,112 ***** Running evaluation *****
2023-03-27 19:37:28,112   Epoch = 29 iter 7999 step
2023-03-27 19:37:28,113   Num examples = 1043
2023-03-27 19:37:28,113   Batch size = 32
2023-03-27 19:37:28,114 ***** Eval results *****
2023-03-27 19:37:28,114   att_loss = 550.406858086586
2023-03-27 19:37:28,115   cls_loss = 0.0
2023-03-27 19:37:28,115   global_step = 7999
2023-03-27 19:37:28,115   loss = 551.0041593313217
2023-03-27 19:37:28,115   rep_loss = 0.5973005904816091
2023-03-27 19:37:28,122 ***** Save model *****
22023-03-27 19:38:10,746 ***** Running evaluation *****
2023-03-27 19:38:10,747   Epoch = 25 iter 6749 step
2023-03-27 19:38:10,747   Num examples = 1043
2023-03-27 19:38:10,747   Batch size = 32
2023-03-27 19:38:10,748 ***** Eval results *****
2023-03-27 19:38:10,748   att_loss = 555.4043488373627
2023-03-27 19:38:10,749   cls_loss = 0.0
2023-03-27 19:38:10,749   global_step = 6749
2023-03-27 19:38:10,749   loss = 556.0105561952333
2023-03-27 19:38:10,749   rep_loss = 0.6062034666538239
2023-03-27 19:38:10,756 ***** Save model *****
2023-03-27 19:38:26,671 ***** Running evaluation *****
2023-03-27 19:38:26,672   Epoch = 25 iter 6799 step
2023-03-27 19:38:26,672   Num examples = 1043
2023-03-27 19:38:26,672   Batch size = 32
2023-03-27 19:38:26,674 ***** Eval results *****
2023-03-27 19:38:26,675   att_loss = 555.4534766904769
2023-03-27 19:38:26,675   cls_loss = 0.0
2023-03-27 19:38:26,675   global_step = 6799
2023-03-27 19:38:26,675   loss = 556.058727387459
2023-03-27 19:38:26,675   rep_loss = 0.6052459761981042
2023-03-27 19:38:26,682 ***** Save model *****
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:37:42,173 label: 1
2023-03-27 19:37:42,173 label_id: 1
2023-03-27 19:37:43,166 Writing example 0 of 1043
2023-03-27 19:37:43,166 *** Example ***
2023-03-27 19:37:43,166 guid: dev-0
2023-03-27 19:37:43,167 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 19:37:43,167 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:37:43,167 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:37:43,167 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:37:43,167 label: 1
2023-03-27 19:37:43,167 label_id: 1
2023-03-27 19:37:43,287 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 19:37:43,289 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 19:37:45,002 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 19:37:45,219 loading model...
2023-03-27 19:37:45,270 done!
2023-03-27 19:37:45,271 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 19:37:45,328 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 19:37:45,329 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 19:37:45,555 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 19:37:45,580 loading model...
2023-03-27 19:37:45,588 done!
2023-03-27 19:37:45,588 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 19:37:45,589 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 19:37:45,599 ***** Running training *****
2023-03-27 19:37:45,600   Num examples = 8551
2023-03-27 19:37:45,600   Batch size = 32
2023-03-27 19:37:45,600   Num steps = 8010
2023-03-27 19:37:45,600 n: bert.embeddings.word_embeddings.weight
2023-03-27 19:37:45,600 n: bert.embeddings.position_embeddings.weight
2023-03-27 19:37:45,601 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 19:37:45,601 n: bert.embeddings.LayerNorm.weight
2023-03-27 19:37:45,601 n: bert.embeddings.LayerNorm.bias
2023-03-27 19:37:45,601 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 19:37:45,601 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 19:37:45,602 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 19:37:45,602 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 19:37:45,602 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 19:37:45,602 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 19:37:45,602 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 19:37:45,602 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 19:37:45,602 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 19:37:45,602 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 19:37:45,602 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 19:37:45,603 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 19:37:45,603 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 19:37:45,603 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 19:37:45,603 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 19:37:45,603 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 19:37:45,603 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 19:37:45,603 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 19:37:45,603 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 19:37:45,603 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 19:37:45,603 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 19:37:45,604 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 19:37:45,604 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 19:37:45,604 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 19:37:45,604 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 19:37:45,604 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 19:37:45,604 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 19:37:45,604 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 19:37:45,604 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 19:37:45,604 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 19:37:45,605 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 19:37:45,605 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 19:37:45,605 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 19:37:45,605 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 19:37:45,605 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 19:37:45,605 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 19:37:45,605 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 19:37:45,605 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 19:37:45,605 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 19:37:45,605 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 19:37:45,606 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 19:37:45,606 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 19:37:45,606 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 19:37:45,606 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 19:37:45,606 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 19:37:45,606 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 19:37:45,606 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 19:37:45,606 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 19:37:45,606 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 19:37:45,606 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 19:37:45,607 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 19:37:45,607 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 19:37:45,607 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 19:37:45,607 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 19:37:45,607 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 19:37:45,607 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 19:37:45,607 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 19:37:45,607 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 19:37:45,607 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 19:37:45,608 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 19:37:45,608 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 19:37:45,608 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 19:37:45,608 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 19:37:45,608 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 19:37:45,608 n: bert.pooler.dense.weight
2023-03-27 19:37:45,608 n: bert.pooler.dense.bias
2023-03-27 19:37:45,608 n: classifier.weight
2023-03-27 19:37:45,608 n: classifier.bias
2023-03-27 19:37:45,608 n: fit_dense.weight
2023-03-27 19:37:45,609 n: fit_dense.bias
2023-03-27 19:37:45,609 Total parameters: 14591258
2023-03-27 19:37:55,412 ***** Running evaluation *****
2023-03-27 19:37:55,412   Epoch = 0 iter 49 step
2023-03-27 19:37:55,412   Num examples = 1043
2023-03-27 19:37:55,412   Batch size = 32
2023-03-27 19:37:55,462 ***** Eval results *****
2023-03-27 19:37:55,462   att_loss = 8.809167939789441
2023-03-27 19:37:55,463   cls_loss = 0.0
2023-03-27 19:37:55,463   global_step = 49
2023-03-27 19:37:55,463   loss = 10.371721092535525
2023-03-27 19:37:55,464   rep_loss = 1.5625531941044086
2023-03-27 19:37:55,468 ***** Save model *****
2023-03-27 19:38:06,259 ***** Running evaluation *****
2023-03-27 19:38:06,260   Epoch = 0 iter 99 step
2023-03-27 19:38:06,260   Num examples = 1043
2023-03-27 19:38:06,260   Batch size = 32
2023-03-27 19:38:06,261 ***** Eval results *****
2023-03-27 19:38:06,262   att_loss = 8.11006970839067
2023-03-27 19:38:06,262   cls_loss = 0.0
2023-03-27 19:38:06,262   global_step = 99
2023-03-27 19:38:06,262   loss = 9.42553767772636
2023-03-27 19:38:06,262   rep_loss = 1.3154680168989934
2023-03-27 19:38:06,269 ***** Save model *****
2023-03-27 19:38:17,122 ***** Running evaluation *****
2023-03-27 19:38:17,122   Epoch = 0 iter 149 step
2023-03-27 19:38:17,123   Num examples = 1043
2023-03-27 19:38:17,123   Batch size = 32
2023-03-27 19:38:17,124 ***** Eval results *****
2023-03-27 19:38:17,124   att_loss = 7.770787786317352
2023-03-27 19:38:17,124   cls_loss = 0.0
2023-03-27 19:38:17,124   global_step = 149
2023-03-27 19:38:17,124   loss = 8.950733079206223
2023-03-27 19:38:17,124   rep_loss = 1.179945334892145
2023-03-27 19:38:17,131 ***** Save model *****
2023-03-27 19:38:27,997 ***** Running evaluation *****
2023-03-27 19:38:27,997   Epoch = 0 iter 199 step
2023-03-27 19:38:27,998   Num examples = 1043
2023-03-27 19:38:27,998   Batch size = 32
2023-03-27 19:38:27,999 ***** Eval results *****
2023-03-27 19:38:27,999   att_loss = 7.5706812580626215
2023-03-27 19:38:27,999   cls_loss = 0.0
2023-03-27 19:38:28,000   global_step = 199
2023-03-27 19:38:28,000   loss = 8.665091428325404
2023-03-27 19:38:28,000   rep_loss = 1.0944102029105527
2023-03-27 19:38:28,001 ***** Save model *****
2023-03-27 19:38:38,971 ***** Running evaluation *****
2023-03-27 19:38:38,971   Epoch = 0 iter 249 step
2023-03-27 19:38:38,971   Num examples = 1043
2023-03-27 19:38:38,971   Batch size = 32
2023-03-27 19:38:38,972 ***** Eval results *****
2023-03-27 19:38:38,973   att_loss = 7.428348841916127
2023-03-27 19:38:38,973   cls_loss = 0.0
2023-03-27 19:38:38,973   global_step = 249
2023-03-27 19:38:38,973   loss = 8.463673796519696
2023-03-27 19:38:38,974   rep_loss = 1.0353249778230507
2023-03-27 19:38:38,981 ***** Save model *****
2023-03-27 19:38:50,011 ***** Running evaluation *****
2023-03-27 19:38:50,011   Epoch = 1 iter 299 step
2023-03-27 19:38:50,012   Num examples = 1043
2023-03-27 19:38:50,012   Batch size = 32
2023-03-27 19:38:50,013 ***** Eval results *****
2023-03-27 19:38:50,013   att_loss = 6.6860498785972595
2023-03-27 19:38:50,013   cls_loss = 0.0
2023-03-27 19:38:50,013   global_step = 299
2023-03-27 19:38:50,013   loss = 7.455416560173035
2023-03-27 19:38:50,014   rep_loss = 0.7693666610866785
2023-03-27 19:38:50,021 ***** Save model *****
2023-03-27 19:39:01,074 ***** Running evaluation *****
2023-03-27 19:39:01,074   Epoch = 1 iter 349 step
2023-03-27 19:39:01,074   Num examples = 1043
2023-03-27 19:39:01,075   Batch size = 32
2023-03-27 19:39:01,075 ***** Eval results *****
2023-03-27 19:39:01,076   att_loss = 6.705311275110012
2023-03-27 19:39:01,076   cls_loss = 0.0
2023-03-27 19:39:01,076   global_step = 349
2023-03-27 19:39:01,076   loss = 7.467519044876099
2023-03-27 19:39:01,076   rep_loss = 0.7622077595896837
2023-03-27 19:39:01,081 ***** Save model *****
20232023-03-27 19:39:46,340 ***** Running evaluation *****
2023-03-27 19:39:46,341   Epoch = 26 iter 7049 step
2023-03-27 19:39:46,341   Num examples = 1043
2023-03-27 19:39:46,341   Batch size = 32
2023-03-27 19:39:46,343 ***** Eval results *****
2023-03-27 19:39:46,343   att_loss = 555.4985146210572
2023-03-27 19:39:46,344   cls_loss = 0.0
2023-03-27 19:39:46,344   global_step = 7049
2023-03-27 19:39:46,344   loss = 556.101072507484
2023-03-27 19:39:46,344   rep_loss = 0.6025619211597978
2023-03-27 19:39:46,346 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2023-03-27 19:40:02,300 ***** Running evaluation *****
2023-03-27 19:40:02,300   Epoch = 26 iter 7099 step
2023-03-27 19:40:02,300   Num examples = 1043
2023-03-27 19:40:02,300   Batch size = 32
2023-03-27 19:40:02,301 ***** Eval results *****
2023-03-27 19:40:02,301   att_loss = 554.2300988458524
2023-03-27 19:40:02,301   cls_loss = 0.0
2023-03-27 19:40:02,302   global_step = 7099
2023-03-27 19:40:02,302   loss = 554.8323313719148
2023-03-27 19:40:02,302   rep_loss = 0.6022350305964232
2023-03-27 19:40:02,303 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2023-03-27 19:40:18,236 ***** Running evaluation *****
2023-03-27 19:40:18,236   Epoch = 26 iter 7149 step
2023-03-27 19:40:18,236   Num examples = 1043
2023-03-27 19:40:18,236   Batch size = 32
2023-03-27 19:40:18,237 ***** Eval results *****
2023-03-27 19:40:18,238   att_loss = 554.6897086175743
2023-03-27 19:40:18,238   cls_loss = 0.0
2023-03-27 19:40:18,238   global_step = 7149
2023-03-27 19:40:18,238   loss = 555.2921937214579
2023-03-27 19:40:18,238   rep_loss = 0.6024861422137938
2023-03-27 19:40:18,240 ***** Save model *****
2023-03-27 19:40:34,152 ***** Running evaluation *****
2023-03-27 19:40:34,153   Epoch = 26 iter 7199 step
2023-03-27 19:40:34,153   Num examples = 1043
2023-03-27 19:40:34,153   Batch size = 32
2023-03-27 19:40:34,155 ***** Eval results *****
2023-03-27 19:40:34,155   att_loss = 555.4120808760944
2023-03-27 19:40:34,155   cls_loss = 0.0
2023-03-27 19:40:34,155   global_step = 7199
2023-03-27 19:40:34,156   loss = 556.0148061314445
2023-03-27 19:40:34,156   rep_loss = 0.6027262254911638
2023-03-27 19:40:34,163 ***** Save model *****
2023-03-27 19:40:50,092 ***** Running evaluation *****
2023-03-27 19:40:50,093   Epoch = 27 iter 7249 step
2023-03-27 19:40:50,093   Num examples = 1043
2023-03-27 19:40:50,093   Batch size = 32
2023-03-27 19:40:50,094 ***** Eval results *****
2023-03-27 19:40:50,094   att_loss = 556.2471214294434
2023-03-27 19:40:50,094   cls_loss = 0.0
2023-03-27 19:40:50,094   global_step = 7249
2023-03-27 19:40:50,094   loss = 556.8478202819824
2023-03-27 19:40:50,095   rep_loss = 0.6006984770298004
2023-03-27 19:40:50,102 ***** Save model *****
2023-03-27 19:41:06,037 ***** Running evaluation *****
2023-03-27 19:41:06,038   Epoch = 27 iter 7299 step
2023-03-27 19:41:06,038   Num examples = 1043
2023-03-27 19:41:06,038   Batch size = 32
2023-03-27 19:41:06,039 ***** Eval results *****
2023-03-27 19:41:06,039   att_loss = 554.8370110405816
2023-03-27 19:41:06,039   cls_loss = 0.0
2023-03-27 19:41:06,040   global_step = 7299
2023-03-27 19:41:06,040   loss = 555.4385772705078
2023-03-27 19:41:06,040   rep_loss = 0.6015672849284278
2023-03-27 19:41:06,046 ***** Save model *****
2023-03-27 19:41:21,953 ***** Running evaluation *****
2023-03-27 19:41:21,953   Epoch = 27 iter 7349 step
2023-03-27 19:41:21,953   Num examples = 1043
2023-03-27 19:41:21,954   Batch size = 32
2023-03-27 19:41:21,955 ***** Eval results *****
2023-03-27 19:41:21,955   att_loss = 553.2835499354771
2023-03-27 19:41:21,956   cls_loss = 0.0
2023-03-27 19:41:21,956   global_step = 7349
2023-03-27 19:41:21,956   loss = 553.8842112949916
2023-03-27 19:41:21,956   rep_loss = 0.6006626512323107
2023-03-27 19:41:21,963 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2023-03-27 19:41:37,886 ***** Running evaluation *****
2023-03-27 19:41:37,886   Epoch = 27 iter 7399 step
2023-03-27 19:41:37,887   Num examples = 1043
2023-03-27 19:41:37,887   Batch size = 32
2023-03-27 19:41:37,895 ***** Eval results *****
2023-03-27 19:41:37,895   att_loss = 553.2446982935855
2023-03-27 19:41:37,896   cls_loss = 0.0
2023-03-27 19:41:37,896   global_step = 7399
2023-03-27 19:41:37,896   loss = 553.8457733154297
2023-03-27 19:41:37,896   rep_loss = 0.6010756150672311
2023-03-27 19:41:37,905 ***** Save model *****
2023-03-27 19:41:53,825 ***** Running evaluation *****
2023-03-27 19:41:53,826   Epoch = 27 iter 7449 step
2023-03-27 19:41:53,826   Num examples = 1043
2023-03-27 19:41:53,826   Batch size = 32
2023-03-27 19:41:53,827 ***** Eval results *****
2023-03-27 19:41:53,827   att_loss = 553.7321013132731
2023-03-27 19:41:53,827   cls_loss = 0.0
2023-03-27 19:41:53,828   global_step = 7449
2023-03-27 19:41:53,828   loss = 554.3335475921631
2023-03-27 19:41:53,828   rep_loss = 0.6014465935528278
2023-03-27 19:41:53,835 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    2023-03-27 19:42:09,755 ***** Running evaluation *****
2023-03-27 19:42:09,755   Epoch = 28 iter 7499 step
2023-03-27 19:42:09,755   Num examples = 1043
2023-03-27 19:42:09,756   Batch size = 32
2023-03-27 19:42:09,757 ***** Eval results *****
2023-03-27 19:42:09,757   att_loss = 554.8365106997283
2023-03-27 19:42:09,757   cls_loss = 0.0
2023-03-27 19:42:09,758   global_step = 7499
2023-03-27 19:42:09,758   loss = 555.4346499235734
2023-03-27 19:42:09,758   rep_loss = 0.598135232925415
2023-03-27 19:42:09,765 ***** Save model *****
2023-03-27 19:42:25,674 ***** Running evaluation *****
2023-03-27 19:42:25,675   Epoch = 28 iter 7549 step
2023-03-27 19:42:25,675   Num examples = 1043
2023-03-27 19:42:25,675   Batch size = 32
2023-03-27 19:42:25,676 ***** Eval results *****
2023-03-27 19:42:25,676   att_loss = 553.1222943867723
2023-03-27 19:42:25,676   cls_loss = 0.0
2023-03-27 19:42:25,676   global_step = 7549
2023-03-27 19:42:25,676   loss = 553.7221207292113
2023-03-27 19:42:25,677   rep_loss = 0.5998249045790058
2023-03-27 19:42:25,683 ***** Save model *****
23-03-27 19:42:08,645 ***** Running evaluation *****
2023-03-27 19:42:08,645   Epoch = 4 iter 1199 step
2023-03-27 19:42:08,646   Num examples = 1043
2023-03-27 19:42:08,646   Batch size = 32
2023-03-27 19:42:08,647 ***** Eval results *****
2023-03-27 19:42:08,647   att_loss = 6.209990708882572
2023-03-27 19:42:08,647   cls_loss = 0.0
2023-03-27 19:42:08,648   global_step = 1199
2023-03-27 19:42:08,648   loss = 6.8683390580970825
2023-03-27 19:42:08,648   rep_loss = 0.6583483464845264
2023-03-27 19:42:08,655 ***** Save model *****
2023-03-27 19:42:19,657 ***** Running evaluation *****
2023-03-27 19:42:19,658   Epoch = 4 iter 1249 step
2023-03-27 19:42:19,658   Num examples = 1043
2023-03-27 19:42:19,658   Batch size = 32
2023-03-27 19:42:19,659 ***** Eval results *****
2023-03-27 19:42:19,660   att_loss = 6.182991162189462
2023-03-27 19:42:19,660   cls_loss = 0.0
2023-03-27 19:42:19,660   global_step = 1249
2023-03-27 19:42:19,660   loss = 6.839209419587699
2023-03-27 19:42:19,660   rep_loss = 0.6562182527879326
2023-03-27 19:42:19,668 ***** Save model *****
2023-03-27 19:42:30,677 ***** Running evaluation *****
2023-03-27 19:42:30,677   Epoch = 4 iter 1299 step
2023-03-27 19:42:30,677   Num examples = 1043
2023-03-27 19:42:30,678   Batch size = 32
2023-03-27 19:42:30,679 ***** Eval results *****
2023-03-27 19:42:30,679   att_loss = 6.198709151445529
2023-03-27 19:42:30,679   cls_loss = 0.0
2023-03-27 19:42:30,679   global_step = 1299
2023-03-27 19:42:30,679   loss = 6.853745945604333
2023-03-27 19:42:30,679   rep_loss = 0.6550367967390911
2023-03-27 19:42:30,685 ***** Save model *****
2023-03-27 19:42:41,697 ***** Running evaluation *****
2023-03-27 19:42:41,698   Epoch = 5 iter 1349 step
2023-03-27 19:42:41,698   Num examples = 1043
2023-03-27 19:42:41,698   Batch size = 32
2023-03-27 19:42:41,699 ***** Eval results *****
2023-03-27 19:42:41,699   att_loss = 6.112003667013986
2023-03-27 19:42:41,700   cls_loss = 0.0
2023-03-27 19:42:41,700   global_step = 1349
2023-03-27 19:42:41,700   loss = 6.7563982009887695
2023-03-27 19:42:41,700   rep_loss = 0.6443944871425629
2023-03-27 19:42:41,702 ***** Save model *****
2023-03-27 19:42:52,688 ***** Running evaluation *****
2023-03-27 19:42:52,689   Epoch = 5 iter 1399 step
2023-03-27 19:42:52,689   Num examples = 1043
2023-03-27 19:42:52,689   Batch size = 32
2023-03-27 19:42:52,690 ***** Eval results *****
2023-03-27 19:42:52,691   att_loss = 6.137298621237278
2023-03-27 19:42:52,691   cls_loss = 0.0
2023-03-27 19:42:52,691   global_step = 1399
2023-03-27 19:42:52,691   loss = 6.784199006855488
2023-03-27 19:42:52,691   rep_loss = 0.646900394000113
2023-03-27 19:42:52,699 ***** Save model *****
2023-03-27 19:43:03,688 ***** Running evaluation *****
2023-03-27 19:43:03,688   Epoch = 5 iter 1449 step
2023-03-27 19:43:03,688   Num examples = 1043
2023-03-27 19:43:03,688   Batch size = 32
2023-03-27 19:43:03,689 ***** Eval results *****
2023-03-27 19:43:03,690   att_loss = 6.090391828302751
2023-03-27 19:43:03,690   cls_loss = 0.0
2023-03-27 19:43:03,690   global_step = 1449
2023-03-27 19:43:03,690   loss = 6.734807675344902
2023-03-27 19:43:03,690   rep_loss = 0.6444158548848671
2023-03-27 19:43:03,695 ***** Save model *****
2023-03-27 19:43:14,677 ***** Running evaluation *****
2023-03-27 19:43:14,678   Epoch = 5 iter 1499 step
2023-03-27 19:43:14,678   Num examples = 1043
2023-03-27 19:43:14,678   Batch size = 32
2023-03-27 19:43:14,679 ***** Eval results *****
2023-03-27 19:43:14,679   att_loss = 6.102632449894417
2023-03-27 19:43:14,680   cls_loss = 0.0
2023-03-27 19:43:14,680   global_step = 1499
2023-03-27 19:43:14,680   loss = 6.7456624740507545
2023-03-27 19:43:14,680   rep_loss = 0.6430300270638815
2023-03-27 19:43:14,682 ***** Save model *****
2023-03-27 19:43:45,340 ***** Running evaluation *****
2023-03-27 19:43:45,341   Epoch = 29 iter 7799 step
2023-03-27 19:43:45,341   Num examples = 1043
2023-03-27 19:43:45,341   Batch size = 32
2023-03-27 19:43:45,342 ***** Eval results *****
2023-03-27 19:43:45,342   att_loss = 545.0957020350864
2023-03-27 19:43:45,343   cls_loss = 0.0
2023-03-27 19:43:45,343   global_step = 7799
2023-03-27 19:43:45,343   loss = 545.6925615583148
2023-03-27 19:43:45,343   rep_loss  0.6418355073884269
2023-03-27 19:43:25,661 ***** Save model *****
2023-03-27 19:43:36,677 ***** Running evaluation *****
2023-03-27 19:43:36,677   Epoch = 5 iter 1599 step
2023-03-27 19:43:36,677   Num examples = 1043
2023-03-27 19:43:36,677   Batch size = 32
2023-03-27 19:43:36,679 ***** Eval results *****
2023-03-27 19:43:36,679   att_loss = 6.100184547178673
2023-03-27 19:43:36,679   cls_loss = 0.0
2023-03-27 19:43:36,680   global_step = 1599
2023-03-27 19:43:36,680   loss = 6.741134918097294
2023-03-27 19:43:36,680   rep_loss = 0.6409503724990468
2023-03-27 19:43:36,687 ***** Save model *****
2023-03-27 19:43:47,692 ***** Running evaluation *****
2023-03-27 19:43:47,693   Epoch = 6 iter 1649 step
2023-03-27 19:43:47,693   Num examples = 1043
2023-03-27 19:43:47,693   Batch size = 32
2023-03-27 19:43:47,694 ***** Eval results *****
2023-03-27 19:43:47,694   att_loss = 6.022228636640183
2023-03-27 19:43:47,695   cls_loss = 0.0
2023-03-27 19:43:47,695   global_step = 1649
2023-03-27 19:43:47,695   loss = 6.653832597935454
2023-03-27 19:43:47,695   rep_loss = 0.6316039638316377
2023-03-27 19:43:47,703 ***** Save model *****
2023-03-27 19:43:58,732 ***** Running evaluation *****
2023-03-27 19:43:58,732   Epoch = 6 iter 1699 step
2023-03-27 19:43:58,733   Num examples = 1043
2023-03-27 19:43:58,733   Batch size = 32
2023-03-27 19:43:58,734 ***** Eval results *****
2023-03-27 19:43:58,734   att_loss = 6.054369597090888
2023-03-27 19:43:58,734   cls_loss = 0.0
2023-03-27 19:43:58,735   global_step = 1699
2023-03-27 19:43:58,735   loss = 6.686873834157727
2023-03-27 19:43:58,735   rep_loss = 0.63250422354826
2023-03-27 19:43:58,742 ***** Save model *****
2023-03-27 19:44:09,742 ***** Running evaluation *****
2023-03-27 19:44:09,743   Epoch = 6 iter 1749 step
2023-03-27 19:44:09,743   Num examples = 1043
2023-03-27 19:44:09,743   Batch size = 32
2023-03-27 19:44:09,744 ***** Eval results *****
2023-03-27 19:44:09,744   att_loss = 6.047359236243631
2023-03-27 19:44:09,744   cls_loss = 0.0
2023-03-27 19:44:09,745   global_step = 1749
2023-03-27 19:44:09,745   loss = 6.679043221635883
2023-03-27 19:44:09,745   rep_loss = 0.6316839756608821
2023-03-27 19:44:09,752 ***** Save model *****
2023-03-27 19:44:20,749 ***** Running evaluation *****
2023-03-27 19:44:20,749   Epoch = 6 iter 1799 step
2023-03-27 19:44:20,749   Num examples = 1043
2023-03-27 19:44:20,749   Batch size = 32
2023-03-27 19:44:20,751 ***** Eval results *****
2023-03-27 19:44:20,751   att_loss = 6.039677160040376
2023-03-27 19:44:20,751   cls_loss = 0.0
2023-03-27 19:44:20,751   global_step = 1799
2023-03-27 19:44:20,751   loss = 6.670705475782985
2023-03-27 19:44:20,752   rep_loss = 0.6310283081785677
2023-03-27 19:44:20,756 ***** Save model *****
2023-03-27 19:44:31,714 ***** Running evaluation *****
2023-03-27 19:44:31,715   Epoch = 6 iter 1849 step
2023-03-27 19:44:31,715   Num examples = 1043
2023-03-27 19:44:31,715   Batch size = 32
2023-03-27 19:44:31,716 ***** Eval results *****
2023-03-27 19:44:31,716   att_loss = 6.038799368900809
2023-03-27 19:44:31,716   cls_loss = 0.0
2023-03-27 19:44:31,716   global_step = 1849
2023-03-27 19:44:31,716   loss = 6.668669080927304
2023-03-27 19:44:31,717   rep_loss = 0.6298697052696939
2023-03-27 19:44:31,718 ***** Save model *****
2023-03-27 19:44:42,723 ***** Running evaluation *****
2023-03-27 19:44:42,723   Epoch = 7 iter 1899 step
2023-03-27 19:44:42,723   Num examples = 1043
2023-03-27 19:44:42,723   Batch size = 32
2023-03-27 19:44:42,725 ***** Eval results *****
2023-03-27 19:44:42,725   att_loss = 5.900824721654256
2023-03-27 19:44:42,726   cls_loss = 0.0
2023-03-27 19:44:42,726   global_step = 1899
2023-03-27 19:44:42,726   loss = 6.520616801579793
2023-03-27 19:44:42,726   rep_loss = 0.6197920620441437
2023-03-27 19:44:42,733 ***** Save model *****
2023-03-27 19:44:53,723 ***** Running evaluation *****
2023-03-27 19:44:53,723   Epoch = 7 iter 1949 step
2023-03-27 19:44:53,723   Num examples = 1043
2023-03-27 19:44:53,723   Batch size = 32
2023-03-27 19:44:53,724 ***** Eval results *****
2023-03-27 19:44:53,725   att_loss = 5.94019301533699
2023-03-27 19:44:53,725   cls_loss = 0.0
2023-03-27 19:44:53,725   global_step = 1949
2023-03-27 19:44:53,725   loss = 6.561323249340058
2023-03-27 19:44:53,725   rep_loss = 0.6211302079260349
2023-03-27 19:44:53,727 ***** Save model *****
2023-03-27 19:45:04,708 ***** Running evaluation *****
2023-03-27 19:45:04,709   Epoch = 7 iter 1999 step
2023-03-27 19:45:04,709   Num examples = 1043
2023-03-27 19:45:04,709   Batch size = 32
2023-03-27 19:45:04,710 ***** Eval results *****
2023-03-27 19:45:04,710   att_loss = 5.954231306222769
2023-03-27 19:45:04,710   cls_loss = 0.0
2023-03-27 19:45:04,710   global_step = 1999
2023-03-27 19:45:04,710   loss = 6.575478146626399
2023-03-27 19:45:04,711   rep_loss = 0.6212468138107887
2023-03-27 19:45:04,712 ***** Save model *****
2023-03-27 19:45:15,698 ***** Running evaluation *****
2023-03-27 19:45:15,699   Epoch = 7 iter 2049 step
2023-03-27 19:45:15,699   Num examples = 1043
2023-03-27 19:45:15,699   Batch size = 32
2023-03-27 19:45:15,700 ***** Eval results *****
2023-03-27 19:45:15,701   att_loss = 5.957748903168572
2023-03-27 19:45:15,701   cls_loss = 0.0
2023-03-27 19:45:15,701   global_step = 2049
2023-03-27 19:45:15,701   loss = 6.579027522934807
2023-03-27 19:45:15,701   rep_loss = 0.6212786035405264
2023-03-27 19:45:15,709 ***** Save model *****
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:45:02,361 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:45:02,361 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 19:45:02,361 label: 1
2023-03-27 19:45:02,361 label_id: 1
2023-03-27 19:45:02,552 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 19:45:02,553 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 19:45:04,298 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 19:45:04,525 loading model...
2023-03-27 19:45:04,576 done!
2023-03-27 19:45:04,576 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 19:45:04,632 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 19:45:04,633 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 19:45:04,858 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 19:45:04,879 loading model...
2023-03-27 19:45:04,887 done!
2023-03-27 19:45:04,887 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 19:45:04,887 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 19:45:04,897 ***** Running training *****
2023-03-27 19:45:04,898   Num examples = 8551
2023-03-27 19:45:04,898   Batch size = 32
2023-03-27 19:45:04,898   Num steps = 8010
2023-03-27 19:45:04,898 n: bert.embeddings.word_embeddings.weight
2023-03-27 19:45:04,898 n: bert.embeddings.position_embeddings.weight
2023-03-27 19:45:04,898 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 19:45:04,898 n: bert.embeddings.LayerNorm.weight
2023-03-27 19:45:04,899 n: bert.embeddings.LayerNorm.bias
2023-03-27 19:45:04,899 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 19:45:04,899 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 19:45:04,899 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 19:45:04,899 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 19:45:04,899 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 19:45:04,899 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 19:45:04,899 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 19:45:04,899 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 19:45:04,900 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 19:45:04,900 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 19:45:04,900 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 19:45:04,900 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 19:45:04,900 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 19:45:04,900 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 19:45:04,900 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 19:45:04,900 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 19:45:04,900 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 19:45:04,901 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 19:45:04,901 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 19:45:04,901 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 19:45:04,901 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 19:45:04,901 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 19:45:04,901 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 19:45:04,901 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 19:45:04,901 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 19:45:04,901 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 19:45:04,901 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 19:45:04,902 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 19:45:04,902 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 19:45:04,902 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 19:45:04,902 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 19:45:04,902 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 19:45:04,902 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 19:45:04,902 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 19:45:04,902 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 19:45:04,902 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 19:45:04,903 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 19:45:04,903 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 19:45:04,903 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 19:45:04,903 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 19:45:04,903 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 19:45:04,903 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 19:45:04,903 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 19:45:04,903 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 19:45:04,903 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 19:45:04,903 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 19:45:04,904 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 19:45:04,904 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 19:45:04,904 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 19:45:04,904 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 19:45:04,904 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 19:45:04,904 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 19:45:04,904 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 19:45:04,904 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 19:45:04,904 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 19:45:04,905 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 19:45:04,905 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 19:45:04,905 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 19:45:04,905 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 19:45:04,905 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 19:45:04,905 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 19:45:04,905 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 19:45:04,905 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 19:45:04,905 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 19:45:04,905 n: bert.pooler.dense.weight
2023-03-27 19:45:04,906 n: bert.pooler.dense.bias
2023-03-27 19:45:04,906 n: classifier.weight
2023-03-27 19:45:04,906 n: classifier.bias
2023-03-27 19:45:04,906 n: fit_dense.weight
2023-03-27 19:45:04,906 n: fit_dense.bias
2023-03-27 19:45:04,906 Total parameters: 14591258
2023-03-27 19:45:19,714 ***** Running evaluation *****
2023-03-27 19:45:19,714   Epoch = 0 iter 49 step
2023-03-27 19:45:19,714   Num examples = 1043
2023-03-27 19:45:19,714   Batch size = 32
2023-03-27 19:45:19,728 ***** Eval results *****
2023-03-27 19:45:19,728   att_loss = 87721.4024234694
2023-03-27 19:45:19,728   cls_loss = 0.0
2023-03-27 19:45:19,729   global_step = 49
2023-03-27 19:45:19,729   loss = 87723.05660076531
2023-03-27 19:45:19,729   rep_loss = 1.654427389709317
2023-03-27 19:45:19,735 ***** Save model *****
2023-03-27 19:45:35,575 ***** Running evaluation *****
2023-03-27 19:45:35,575   Epoch = 0 iter 99 step
2023-03-27 19:45:35,576   Num examples = 1043
2023-03-27 19:45:35,576   Batch size = 32
2023-03-27 19:45:35,577 ***** Eval results *****
2023-03-27 19:45:35,577   att_loss = 80958.73508522728
2023-03-27 19:45:35,577   cls_loss = 0.0
2023-03-27 19:45:35,577   global_step = 99
2023-03-27 19:45:35,578   loss = 80960.20801767676
2023-03-27 19:45:35,578   rep_loss = 1.473113158736566
2023-03-27 19:45:35,585 ***** Save model *****
2023-03-27 19:45:51,416 ***** Running evaluation *****
2023-03-27 19:45:51,417   Epoch = 0 iter 149 step
2023-03-27 19:45:51,417   Num examples = 1043
2023-03-27 19:45:51,417   Batch size = 32
2023-03-27 19:45:51,420 ***** Eval results *****
2023-03-27 19:45:51,421   att_loss = 77315.87827705537
2023-03-27 19:45:51,421   cls_loss = 0.0
2023-03-27 19:45:51,421   global_step = 149
2023-03-27 19:45:51,421   loss = 77317.25422084732
2023-03-27 19:45:51,422   rep_loss = 1.3760655486343691
2023-03-27 19:45:51,424 ***** Save model *****
2023-03-22023-03-27 19:45:59,757 ***** Running n *****
2023-03-27 19:46:07,279   Epoch = 0 iter 199 step
2023-03-27 19:46:07,279   Num examples = 1043
2023-03-27 19:46:07,280   Batch size = 32
2023-03-27 19:46:07,283 ***** Eval results *****
2023-03-27 19:46:07,284   att_loss = 75135.82090138191
2023-03-27 19:46:07,284   cls_loss = 0.0
2023-03-27 19:46:07,285   global_step = 199
2023-03-27 19:46:07,285   loss = 75137.13259657663
2023-03-27 19:46:07,286   rep_loss = 1.311782240268573
2023-03-27 19:46:07,293 ***** Save model *****
2023-03-27 19:46:23,196 ***** Running evaluation *****
2023-03-27 19:46:23,196   Epoch = 0 iter 249 step
2023-03-27 19:46:23,197   Num examples = 1043
2023-03-27 19:46:23,197   Batch size = 32
2023-03-27 19:46:23,198 ***** Eval results *****
2023-03-27 19:46:23,198   att_loss = 73737.75665160643
2023-03-27 19:46:23,198   cls_loss = 0.0
2023-03-27 19:46:23,198   global_step = 249
2023-03-27 19:46:23,198   loss = 73739.02095883535
2023-03-27 19:46:23,199   rep_loss = 1.2643884607108242
2023-03-27 19:46:23,203 ***** Save model *****
2023-03-27 19:46:39,076 ***** Running evaluation *****
2023-03-27 19:46:39,076   Epoch = 1 iter 299 step
2023-03-27 19:46:39,077   Num examples = 1043
2023-03-27 19:46:39,077   Batch size = 32
2023-03-27 19:46:39,078 ***** Eval results *****
2023-03-27 19:46:39,078   att_loss = 66720.23303222656
2023-03-27 19:46:39,078   cls_loss = 0.0
2023-03-27 19:46:39,079   global_step = 299
2023-03-27 19:46:39,079   loss = 66721.26245117188
2023-03-27 19:46:39,079   rep_loss = 1.029721885919571
2023-03-27 19:46:39,084 ***** Save model *****
2023-03-27 19:46:54,948 ***** Running evaluation *****
2023-03-27 19:46:54,948   Epoch = 1 iter 349 step
2023-03-27 19:46:54,948   Num examples = 1043
2023-03-27 19:46:54,948   Batch size = 32
2023-03-27 19:46:54,950 ***** Eval results *****
2023-03-27 19:46:54,950   att_loss = 66602.09694169207
2023-03-27 19:46:54,950   cls_loss = 0.0
2023-03-27 19:46:54,951   global_step = 349
2023-03-27 19:46:54,951   loss = 66603.11161394817
2023-03-27 19:46:54,951   rep_loss = 1.0150063466735002
2023-03-27 19:46:54,952 ***** Save model *****
2023-03-27 19:47:10,824 ***** Running evaluation *****
2023-03-27 19:47:10,824   Epoch = 1 iter 399 step
2023-03-27 19:47:10,824   Num examples = 1043
2023-03-27 19:47:10,825   Batch size = 32
2023-03-27 19:47:10,826 ***** Eval results *****
2023-03-27 19:47:10,827   att_loss = 66375.76284327652
2023-03-27 19:47:10,827   cls_loss = 0.0
2023-03-27 19:47:10,827   global_step = 399
2023-03-27 19:47:10,827   loss = 66376.76438210228
2023-03-27 19:47:10,827   rep_loss = 1.0017361794457291
2023-03-27 19:47:10,834 ***** Save model *****
                     2023-03-27 19:47:26,701 ***** Running evaluation *****
2023-03-27 19:47:26,701   Epoch = 1 iter 449 step
2023-03-27 19:47:26,701   Num examples = 1043
2023-03-27 19:47:26,701   Batch size = 32
2023-03-27 19:47:26,702 ***** Eval results *****
2023-03-27 19:47:26,702   att_loss = 66457.69404618819
2023-03-27 19:47:26,703   cls_loss = 0.0
2023-03-27 19:47:26,703   global_step = 449
2023-03-27 19:47:26,703   loss = 66458.68398008242
2023-03-27 19:47:26,703   rep_loss = 0.9900039833980602
2023-03-27 19:47:26,710 ***** Save model *****
2023-03-27 19:47:42,589 ***** Running evaluation *****
2023-03-27 19:47:42,590   Epoch = 1 iter 499 step
2023-03-27 19:47:42,590   Num examples = 1043
2023-03-27 19:47:42,590   Batch size = 32
2023-03-27 19:47:42,592 ***** Eval results *****
2023-03-27 19:47:42,592   att_loss = 66090.7532495959
2023-03-27 19:47:42,592   cls_loss = 0.0
2023-03-27 19:47:42,593   global_step = 499
2023-03-27 19:47:42,593   loss = 66091.73065396013
2023-03-27 19:47:42,593   rep_loss = 0.977501534696283
2023-03-27 19:47:42,597 ***** Save model *****
 *****
2023-03-27 19:47:16,840 ***** Running evaluation *****
2023-03-27 19:47:16,840   Epoch = 9 iter 2599 step
2023-03-27 19:47:16,841   Num examples = 1043
2023-03-27 19:47:16,841   Batch size = 32
2023-03-27 19:47:16,842 ***** Eval results *****
2023-03-27 19:47:16,843   att_loss = 5.917038499092569
2023-03-27 19:47:16,843   cls_loss = 0.0
2023-03-27 19:47:16,843   global_step = 2599
2023-03-27 19:47:16,843   loss = 6.526230379026764
2023-03-27 19:47:16,843   rep_loss = 0.6091918610796636
2023-03-27 19:47:16,851 ***** Save model *****
2023-03-27 19:47:27,853 ***** Running evaluation *****
2023-03-27 19:47:27,853   Epoch = 9 iter 2649 step
2023-03-27 19:47:27,854   Num examples = 1043
2023-03-27 19:47:27,854   Batch size = 32
2023-03-27 19:47:27,855 ***** Eval results *****
2023-03-27 19:47:27,855   att_loss = 5.908881408412282
2023-03-27 19:47:27,855   cls_loss = 0.0
2023-03-27 19:47:27,855   global_step = 2649
2023-03-27 19:47:27,855   loss = 6.517295480743656
2023-03-27 19:47:27,855   rep_loss = 0.6084140541592264
2023-03-27 19:47:27,863 ***** Save model *****
2023-03-27 19:47:38,888 ***** Running evaluation *****
2023-03-27 19:47:38,889   Epoch = 10 iter 2699 step
2023-03-27 19:47:38,889   Num examples = 1043
2023-03-27 19:47:38,889   Batch size = 32
2023-03-27 19:47:38,890 ***** Eval results *****
2023-03-27 19:47:38,891   att_loss = 5.932870716884218
2023-03-27 19:47:38,891   cls_loss = 0.0
2023-03-27 19:47:38,891   global_step = 2699
2023-03-27 19:47:38,891   loss = 6.54051686977518
2023-03-27 19:47:38,892   rep_loss = 0.6076461734442875
2023-03-27 19:47:38,896 ***** Save model *****
2023-03-27 19:47:49,903 ***** Running evaluation *****
2023-03-27 19:47:49,903   Epoch = 10 iter 2749 step
2023-03-27 19:47:49,903   Num examples = 1043
2023-03-27 19:47:49,903   Batch size = 32
2023-03-27 19:47:49,904 ***** Eval results *****
2023-03-27 19:47:49,905   att_loss = 5.897302398198767
2023-03-27 19:47:49,905   cls_loss = 0.0
2023-03-27 19:47:49,905   global_step = 2749
2023-03-27 19:47:49,905   loss = 6.503901590274859
2023-03-27 19:47:49,905   rep_loss = 0.6065992124472992
2023-03-27 19:47:49,913 ***** Save model *****
2023-03-27 19:48:00,885 ***** Running evaluation *****
2023-03-27 19:48:00,885   Epoch = 10 iter 2799 step
2023-03-27 19:48:00,885   Num examples = 1043
2023-03-27 19:48:00,885   Batch size = 32
2023-03-27 19:48:00,886 ***** Eval results *****
2023-03-27 19:48:00,887   att_loss = 5.84992084577102
2023-03-27 19:48:00,887   cls_loss = 0.0
2023-03-27 19:48:00,887   global_step = 2799
2023-03-27 19:48:00,887   loss = 6.455110309659973
2023-03-27 19:48:00,887   rep_loss = 0.6051894703576731
2023-03-27 19:48:00,894 ***** Save model *****
2023-03-27 19:48:11,874 ***** Running evaluation *****
2023-03-27 19:48:11,875   Epoch = 10 iter 2849 step
2023-03-27 19:48:11,875   Num examples = 1043
2023-03-27 19:48:11,875   Batch size = 32
2023-03-27 19:48:11,876 ***** Eval results *****
2023-03-27 19:48:11,876   att_loss = 5.873082107671812
2023-03-27 19:48:11,876   cls_loss = 0.0
2023-03-27 19:48:11,876   global_step = 2849
2023-03-27 19:48:11,876   loss = 6.4777805365663665
2023-03-27 19:48:11,876   rep_loss = 0.6046984245657255
2023-03-27 19:48:11,878 ***** Save model *****
2023-03-27 19:48:22,860 ***** Running evaluation *****
2023-03-27 19:48:22,860   Epoch = 10 iter 2899 step
2023-03-27 19:48:22,861   Num examples = 1043
2023-03-27 19:48:22,861   Batch size = 32
2023-03-27 19:48:22,862 ***** Eval results *****
2023-03-27 19:48:22,862   att_loss = 5.8665073677962525
2023-03-27 19:48:22,862   cls_loss = 0.0
2023-03-27 19:48:22,862   global_step = 2899
2023-03-27 19:48:22,862   loss = 6.47025100008369
2023-03-27 19:48:22,863   rep_loss = 0.6037436236981221
2023-03-27 19:48:22,870 ***** Save model *****
2023-03-27 19:49:02,137 ***** Running evaluation *****
2023-03-27 19:49:02,138   Epoch = 2 iter 749 step
2023-03-27 19:49:02,138   Num examples = 1043
2023-03-27 19:49:02,138   Batch size = 32
2023-03-27 19:49:02,139 ***** Eval results *****
2023-03-27 19:49:02,139   att_loss = 63874.75957485465
2023-03-27 19:49:02,140   cls_loss = 0.0
2023-03-27 19:49:02,140   global_step = 749
2023-03-27 19:49:02,140   loss = 63875.63088662791
2023-03-27 19:49:02,140   rep_loss = 0.8714296795601069
2023-03-27 19:49:02,146 ***** Save model *****
2023-03-27 19:49:18,062 ***** Running evaluation *****
2023-03-27 19:49:18,062   Epoch = 2 iter 799 step
2023-03-27 19:49:18,062   Num examples = 1043
2023-0023-03-27 19:48:44,884   Batch size = 32
2023-03-27 19:48:44,885 ***** Eval results *****
2023-03-27 19:48:44,885   att_loss = 5.808488668933991
2023-03-27 19:48:44,885   cls_loss = 0.0
2023-03-27 19:48:44,885   global_step = 2999
2023-03-27 19:48:44,885   loss = 6.406169837520968
2023-03-27 19:48:44,885   rep_loss = 0.5976811705097076
2023-03-27 19:48:44,893 ***** Save model *****
2023-03-27 19:48:55,885 ***** Running evaluation *****
2023-03-27 19:48:55,886   Epoch = 11 iter 3049 step
2023-03-27 19:48:55,886   Num examples = 1043
2023-03-27 19:48:55,886   Batch size = 32
2023-03-27 19:48:55,887 ***** Eval results *****
2023-03-27 19:48:55,887   att_loss = 5.826798498630524
2023-03-27 19:48:55,888   cls_loss = 0.0
2023-03-27 19:48:55,888   global_step = 3049
2023-03-27 19:48:55,888   loss = 6.425322847706931
2023-03-27 19:48:55,888   rep_loss = 0.5985243320465088
2023-03-27 19:48:55,896 ***** Save model *****
2023-03-27 19:49:06,902 ***** Running evaluation *****
2023-03-27 19:49:06,902   Epoch = 11 iter 3099 step
2023-03-27 19:49:06,902   Num examples = 1043
2023-03-27 19:49:06,902   Batch size = 32
2023-03-27 19:49:06,904 ***** Eval results *****
2023-03-27 19:49:06,904   att_loss = 5.82454584851677
2023-03-27 19:49:06,905   cls_loss = 0.0
2023-03-27 19:49:06,905   global_step = 3099
2023-03-27 19:49:06,905   loss = 6.42298725799278
2023-03-27 19:49:06,905   rep_loss = 0.5984414091080795
2023-03-27 19:49:06,912 ***** Save model *****
2023-03-27 19:49:17,891 ***** Running evaluation *****
2023-03-27 19:49:17,892   Epoch = 11 iter 3149 step
2023-03-27 19:49:17,892   Num examples = 1043
2023-03-27 19:49:17,892   Batch size = 32
2023-03-27 19:49:17,893 ***** Eval results *****
2023-03-27 19:49:17,893   att_loss = 5.824498918821227
2023-03-27 19:49:17,893   cls_loss = 0.0
2023-03-27 19:49:17,893   global_step = 3149
2023-03-27 19:49:17,893   loss = 6.4224282075774
2023-03-27 19:49:17,893   rep_loss = 0.5979292910054045
2023-03-27 19:49:17,901 ***** Save model *****
2023-03-27 19:49:28,874 ***** Running evaluation *****
2023-03-27 19:49:28,874   Epoch = 11 iter 3199 step
2023-03-27 19:49:28,874   Num examples = 1043
2023-03-27 19:49:28,875   Batch size = 32
2023-03-27 19:49:28,876 ***** Eval results *****
2023-03-27 19:49:28,876   att_loss = 5.8334097771244195
2023-03-27 19:49:28,877   cls_loss = 0.0
2023-03-27 19:49:28,877   global_step = 3199
2023-03-27 19:49:28,877   loss = 6.4317394613309675
2023-03-27 19:49:28,877   rep_loss = 0.5983296853440409
2023-03-27 19:49:28,884 ***** Save model *****
2023-03-27 19:49:39,920 ***** Running evaluation *****
2023-03-27 19:49:39,920   Epoch = 12 iter 3249 step
2023-03-27 19:49:39,920   Num examples = 1043
2023-03-27 19:49:39,921   Batch size = 32
2023-03-27 19:49:39,922 ***** Eval results *****
2023-03-27 19:49:39,922   att_loss = 5.7702862103780115
2023-03-27 19:49:39,922   cls_loss = 0.0
2023-03-27 19:49:39,922   global_step = 3249
2023-03-27 19:49:39,923   loss = 6.363251940409342
2023-03-27 19:49:39,923   rep_loss = 0.5929657035403781
2023-03-27 19:49:39,929 ***** Save model *****
2023-03-27 19:49:50,879 ***** Running evaluation *****
2023-03-27 19:49:50,879   Epoch = 12 iter 3299 step
2023-03-27 19:49:50,880   Num examples = 1043
2023-03-27 19:49:50,880   Batch size = 32
2023-03-27 19:49:50,881 ***** Eval results *****
2023-03-27 19:49:50,881   att_loss = 5.8216830353987845
2023-03-27 19:49:50,882   cls_loss = 0.0
2023-03-27 19:49:50,882   global_step = 3299
2023-03-27 19:49:50,882   loss = 6.416359319184956
2023-03-27 19:49:50,882   rep_loss = 0.5946762706104077
2023-03-27 19:49:50,884 ***** Save model *****
2023-03-27 19:50:01,871 ***** Running evaluation *****
2023-03-27 19:50:01,872   Epoch = 12 iter 3349 step
2023-03-27 19:50:01,872   Num examples = 1043
2023-03-27 19:50:01,872   Batch size = 32
2023-03-27 19:50:01,873 ***** Eval results *****
2023-03-27 19:50:01,873   att_loss = 5.815495221368198
2023-03-27 19:50:01,873   cls_loss = 0.0
2023-03-27 19:50:01,873   global_step = 3349
2023-03-27 19:50:01,873   loss = 6.409623708396122
2023-03-27 19:50:01,874   rep_loss = 0.5961176084828
2023-03-27 19:50:05,889 ***** Save model *****
2023-03-27 19:50:21,811 ***** Running evaluation *****
2023-03-27 19:50:21,812   Epoch = 3 iter 999 step
2023-03-27 19:50:21,812   Num examples = 1043
2023-03-27 19:50:21,812   Batch size = 32
2023-03-27 19:50:21,813 ***** Eval results *****
2023-03-27 19:50:21,814   att_loss = 62230.571811868685
2023-03-27 19:50:21,814   cls_loss = 0.0
2023-03-27 19:50:21,814   global_step = 999
2023-03-27 19:50:21,814   loss = 62231.38105666036
2023-03-27 19:50:21,814   rep_loss = 0.8091530264025987
2023-03-27 19:50:21,821 ***** Save model *****
2023-03-27 19:50:37,740 ***** Running evaluation *****
2023-03-27 19:50:37,741   Epoch = 3 iter 1049 step
2023-03-27 19:50:37,741   Num examples = 1043
2023-03-27 19:50:37,741   Batch size = 32
2023-03-27 19:50:37,742 ***** Eval results *****
2023-03-27 19:50:37,742   att_loss = 62270.99974798387
2023-03-27 19:50:37,742   cls_loss = 0.0
2023-03-27 19:50:37,742   global_step = 1049
2023-03-27 19:50:37,742   loss = 62271.80456149193
2023-03-27 19:50:37,743   rep_loss = 0.8047607240657653
2023-03-27 19:50:37,750 ***** Save model *****
    2023-03-27 19:50:53,681 ***** Running evaluation *****
2023-03-27 19:50:53,682   Epoch = 4 iter 1099 step
2023-03-27 19:50:53,682   Num examples = 1043
2023-03-27 19:50:53,682   Batch size = 32
2023-03-27 19:50:53,685 ***** Eval results *****
2023-03-27 19:50:53,685   att_loss = 62329.516885080644
2023-03-27 19:50:53,686   cls_loss = 0.0
2023-03-27 19:50:53,686   global_step = 1099
2023-03-27 19:50:53,686   loss = 62330.29662298387
2023-03-27 19:50:53,686   rep_loss = 0.7793946958357288
2023-03-27 19:50:53,690 ***** Save model *****
2023-03-27 19:51:09,619 ***** Running evaluation *****
2023-03-27 19:51:09,619   Epoch = 4 iter 1149 step
2023-03-27 19:51:09,619   Num examples = 1043
2023-03-27 19:51:09,620   Batch size = 32
2023-03-27 19:51:09,621 ***** Eval results *****
2023-03-27 19:51:09,621   att_loss = 61915.08420138889
2023-03-27 19:51:09,622   cls_loss = 0.0
2023-03-27 19:51:09,622   global_step = 1149
2023-03-27 19:51:09,622   loss = 61915.86038773148
2023-03-27 19:51:09,622   rep_loss = 0.7762623951758867
2023-03-27 19:51:09,629 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2023-03-27 19:51:25,552 ***** Running evaluation *****
2023-03-27 19:51:25,553   Epoch = 4 iter 1199 step
2023-03-27 19:51:25,553   Num examples = 1043
2023-03-27 19:51:25,553   Batch size = 32
2023-03-27 19:51:25,555 ***** Eval results *****
2023-03-27 19:51:25,555   att_loss = 61642.25098401718
2023-03-27 19:51:25,555   cls_loss = 0.0
2023-03-27 19:51:25,556   global_step = 1199
2023-03-27 19:51:25,556   loss = 61643.0221552958
2023-03-27 19:51:25,556   rep_loss = 0.771115017756251
2023-03-27 19:51:25,563 ***** Save model *****
2023-03-27 19:51:41,494 ***** Running evaluation *****
2023-03-27 19:51:41,495   Epoch = 4 iter 1249 step
2023-03-27 19:51:41,495   Num examples = 1043
2023-03-27 19:51:41,495   Batch size = 32
2023-03-27 19:51:41,496 ***** Eval results *****
2023-03-27 19:51:41,496   att_loss = 61440.29726346685
2023-03-27 19:51:41,496   cls_loss = 0.0
2023-03-27 19:51:41,497   global_step = 1249
2023-03-27 19:51:41,497   loss = 61441.0643128453
2023-03-27 19:51:41,497   rep_loss = 0.7669257798247574
2023-03-27 19:51:41,504 ***** Save model *****
     2023-03-27 19:51:57,404 ***** Running evaluation *****
2023-03-27 19:51:57,405   Epoch = 4 iter 1299 step
2023-03-27 19:51:57,405   Num examples = 1043
2023-03-27 19:51:57,405   Batch size = 32
2023-03-27 19:51:57,406 ***** Eval results *****
2023-03-27 19:51:57,406   att_loss = 61514.22297754329
2023-03-27 19:51:57,406   cls_loss = 0.0
2023-03-27 19:51:57,406   global_step = 1299
2023-03-27 19:51:57,406   loss = 61514.98738501082
2023-03-27 19:51:57,407   rep_loss = 0.764331015415522
2023-03-27 19:51:57,414 ***** Save model *****
2023-03-27 19:52:13,357 ***** Running evaluation *****
2023-03-27 19:52:13,358   Epoch = 5 iter 1349 step
2023-03-27 19:52:13,358   Num examples = 1043
2023-03-27 19:52:13,358   Batch size = 32
2023-03-27 19:52:13,359 ***** Eval results *****
2023-03-27 19:52:13,359   att_loss = 61310.960658482145
2023-03-27 19:52:13,360   cls_loss = 0.0
2023-03-27 19:52:13,360   global_step = 1349
2023-03-27 19:52:13,360   loss = 61311.704799107145
2023-03-27 19:52:13,360   rep_loss = 0.7443919650145939
2023-03-27 19:52:13,365 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            2023-03-27 19:52:29,269 ***** Running evaluation *****
2023-03-27 19:52:29,269   Epoch = 5 iter 1399 step
2023-03-27 19:52:29,269   Num examples = 1043
2023-03-27 19:52:29,270   Batch size = 32
2023-03-27 19:52:29,271 ***** Eval results *****
2023-03-27 19:52:29,271   att_loss = 60917.3203125
2023-03-27 19:52:29,271   cls_loss = 0.0
2023-03-27 19:52:29,271   global_step = 1399
2023-03-27 19:52:29,272   loss = 60918.06481933594
2023-03-27 19:52:29,272   rep_loss = 0.7442618440836668
2023-03-27 19:52:29,278 ***** Save model *****
2023-03-27 19:52:45,194 ***** Running evaluation *****
2023-03-27 19:52:45,194   Epoch = 5 iter 1449 step
2023-03-27 19:52:45,194   Num examples = 1043
2023-03-27 19:52:45,194   Batch size = 32
2023-03-27 19:52:45,196 ***** Eval results *****
2023-03-27 19:52:45,196   att_loss = 60754.990885416664
2023-03-27 19:52:45,196   cls_loss = 0.0
2023-03-27 19:52:45,196   global_step = 1449
2023-03-27 19:52:45,197   loss = 60755.73317571272
2023-03-27 19:52:45,197   rep_loss = 0.7420619286988911
2023-03-27 19:52:45,204 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                2023-03-27 19:53:01,117 ***** Running evaluation *****
2023-03-27 19:53:01,118   Epoch = 5 iter 1499 step
2023-03-27 19:53:01,118   Num examples = 1043
2023-03-27 19:53:01,118   Batch size = 32
2023-03-27 19:53:01,119 ***** Eval results *****
2023-03-27 19:53:01,119   att_loss = 60788.427567644816
2023-03-27 19:53:01,119   cls_loss = 0.0
2023-03-27 19:53:01,119   global_step = 1499
2023-03-27 19:53:01,120   loss = 60789.167349466465
2023-03-27 19:53:01,120   rep_loss = 0.7395858699228706
2023-03-27 19:53:01,127 ***** Save model *****
2023-03-27 19:53:17,049 ***** Running evaluation *****
2023-03-27 19:53:17,049   Epoch = 5 iter 1549 step
2023-03-27 19:53:17,049   Num examples = 1043
2023-03-27 19:53:17,050   Batch size = 32
2023-03-27 19:53:17,051 ***** Eval results *****
2023-03-27 19:53:17,051   att_loss = 60745.610762266355
2023-03-27 19:53:17,052   cls_loss = 0.0
2023-03-27 19:53:17,052   global_step = 1549
2023-03-27 19:53:17,052   loss = 60746.34794830607
2023-03-27 19:53:17,052   rep_loss = 0.7369899655056891
2023-03-27 19:53:17,059 ***** Save model *****
023-03-27 19:52:57,944 ***** Running evaluation *****
2023-03-27 19:52:57,944   Epoch = 15 iter 4149 step
2023-03-27 19:52:57,944   Num examples = 1043
2023-03-27 19:52:57,944   Batch size = 32
2023-03-27 19:52:57,945 ***** Eval results *****
2023-03-27 19:52:57,945   att_loss = 5.7179593377643165
2023-03-27 19:52:57,946   cls_loss = 0.0
2023-03-27 19:52:57,946   global_step = 4149
2023-03-27 19:52:57,946   loss = 6.302254815896352
2023-03-27 19:52:57,946   rep_loss = 0.5842954806155629
2023-03-27 19:52:57,953 ***** Save model *****
2023-03-27 19:53:08,919 ***** Running evaluation *****
2023-03-27 19:53:08,920   Epoch = 15 iter 4199 step
2023-03-27 19:53:08,920   Num examples = 1043
2023-03-27 19:53:08,920   Batch size = 32
2023-03-27 19:53:08,921 ***** Eval results *****
2023-03-27 19:53:08,922   att_loss = 5.72111948003474
2023-03-27 19:53:08,922   cls_loss = 0.0
2023-03-27 19:53:08,922   global_step = 4199
2023-03-27 19:53:08,922   loss = 6.305807150516314
2023-03-27 19:53:08,922   rep_loss = 0.584687671403295
2023-03-27 19:53:08,929 ***** Save model *****
2023-03-27 19:53:19,916 ***** Running evaluation *****
2023-03-27 19:53:19,916   Epoch = 15 iter 4249 step
2023-03-27 19:53:19,916   Num examples = 1043
2023-03-27 19:53:19,917   Batch size = 32
2023-03-27 19:53:19,918 ***** Eval results *****
2023-03-27 19:53:19,918   att_loss = 5.7266701557597175
2023-03-27 19:53:19,918   cls_loss = 0.0
2023-03-27 19:53:19,919   global_step = 4249
2023-03-27 19:53:19,919   loss = 6.311379065279101
2023-03-27 19:53:19,919   rep_loss = 0.5847089090308205
2023-03-27 19:53:19,926 ***** Save model *****
2023-03-27 19:53:30,919 ***** Running evaluation *****
2023-03-27 19:53:30,920   Epoch = 16 iter 4299 step
2023-03-27 19:53:30,920   Num examples = 1043
2023-03-27 19:53:30,920   Batch size = 32
2023-03-27 19:53:30,921 ***** Eval results *****
2023-03-27 19:53:30,921   att_loss = 5.676726694460268
2023-03-27 19:53:30,921   cls_loss = 0.0
2023-03-27 19:53:30,922   global_step = 4299
2023-03-27 19:53:30,922   loss = 6.257642763632315
2023-03-27 19:53:30,922   rep_loss = 0.5809160735872057
2023-03-27 19:53:30,929 ***** Save model *****
2023-03-27 19:53:41,911 ***** Running evaluation *****
2023-03-27 19:53:41,911   Epoch = 16 iter 4349 step
2023-03-27 19:53:41,911   Num examples = 1043
2023-03-27 19:53:41,911   Batch size = 32
2023-03-27 19:53:41,912 ***** Eval results *****
2023-03-27 19:53:41,913   att_loss = 5.68604360927235
2023-03-27 19:53:41,913   cls_loss = 0.0
2023-03-27 19:53:41,913   global_step = 4349
2023-03-27 19:53:41,913   loss = 6.268939848070021
2023-03-27 19:53:41,913   rep_loss = 0.5828962411199298
2023-03-27 19:53:41,921 ***** Save model *****
2023-03-27 19:53:52,929 ***** Running evaluation *****
2023-03-27 19:53:52,929   Epoch = 16 iter 4399 step
2023-03-27 19:53:52,930   Num examples = 1043
2023-03-27 19:53:52,930   Batch size = 32
2023-03-27 19:53:52,931 ***** Eval results *****
2023-03-27 19:53:52,931   att_loss = 5.709334887857512
2023-03-27 19:53:52,931   cls_loss = 0.0
2023-03-27 19:53:52,931   global_step = 4399
2023-03-27 19:53:52,931   loss = 6.291875411206343
2023-03-27 19:53:52,931   rep_loss = 0.5825405205328633
2023-03-27 19:53:52,939 ***** Save model *****
2023-03-27 19:54:36,618 ***** Running evaluation *****
2023-03-27 19:54:36,619   Epoch = 6 iter 1799 step
2023-03-27 19:54:36,619   Num examples = 1043
2023-03-27 19:54:36,619   Batch size = 32
2023-03-27 19:54:36,622 ***** Eval results *****
2023-03-27 19:54:36,622   att_loss = 60007.7052268401
2023-03-27 19:54:36,623   cls_loss = 0.0
2023-03-27 19:54:36,623   global_step = 1799
2023-03-27 19:54:36,623   loss = 60008.42189482868
2023-03-27 19:54:36,623   rep_loss = 0.7167177890157942
2023-03-27 19:54:36,632 ***** Save model *****
2023-03-27 19:54:52,549 ***** Running evaluation *****
2023-03-27 19:54:52,549   Epoch = 6 iter 1849 step
2023-03-27 19:54:52,549   Num examples = 1043
2023-03-27 19:54:52,549   Batch size = 32
2023-03-27 19:54:52,550 ***** Eval results *****
2023-03-27 19:54:52,550   att_loss = 60065.05421305668
2023-03-27 19:54:52,550   cls_loss = 0.0
2023-03-27 19:54:52,551   global_step = 1849
2023-03-27 19:54:52,551   loss = 60065.76853491903
2023-03-27 19:54:52,551   rep_loss = 0.7143831822553627
2023-03-27 19:54:52,558 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2023-03-27 19:55:08,513 ***** Running evaluation *****
2023-03-27 19:55:08,514   Epoch = 7 iter 1899 step
2023-03-27 19:55:08,514   Num examples = 1043
2023-03-27 19:55:08,514   Batch size = 32
2023-03-27 19:55:08,515 ***** Eval results *****
2023-03-27 19:55:08,515   att_loss = 58909.61770833333
2023-03-27 19:55:08,515   cls_loss = 0.0
2023-03-27 19:55:08,515   global_step = 1899
2023-03-27 19:55:08,516   loss = 58910.316145833334
2023-03-27 19:55:08,516   rep_loss = 0.6986500740051269
2023-03-27 19:55:08,522 ***** Save model *****
2023-03-27 19:55:24,440 ***** Running evaluation *****
2023-03-27 19:55:24,441   Epoch = 7 iter 1949 step
2023-03-27 19:55:24,441   Num examples = 1043
2023-03-27 19:55:24,441   Batch size = 32
2023-03-27 19:55:24,442 ***** Eval results *****
2023-03-27 19:55:24,443   att_loss = 59382.23286132813
2023-03-27 19:55:24,443   cls_loss = 0.0
2023-03-27 19:55:24,443   global_step = 1949
2023-03-27 19:55:24,443   loss = 59382.93212890625
2023-03-27 19:55:24,443   rep_loss = 0.6994230128824711
2023-03-27 19:55:24,451 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2023-03-27 19:55:40,380 ***** Running evaluation *****
2023-03-27 19:55:40,380   Epoch = 7 iter 1999 step
2023-03-27 19:55:40,380   Num examples = 1043
2023-03-27 19:55:40,381   Batch size = 32
2023-03-27 19:55:40,382 ***** Eval results *****
2023-03-27 19:55:40,382   att_loss = 59635.662740384614
2023-03-27 19:55:40,382   cls_loss = 0.0
2023-03-27 19:55:40,382   global_step = 1999
2023-03-27 19:55:40,383   loss = 59636.36256009615
2023-03-27 19:55:40,383   rep_loss = 0.6999905393673823
2023-03-27 19:55:40,390 ***** Save model *****
2023-03-27 19:55:56,315 ***** Running evaluation *****
2023-03-27 19:55:56,316   Epoch = 7 iter 2049 step
2023-03-27 19:55:56,316   Num examples = 1043
2023-03-27 19:55:56,316   Batch size = 32
2023-03-27 19:55:56,317 ***** Eval results *****
2023-03-27 19:55:56,317   att_loss = 59706.98409288195
2023-03-27 19:55:56,317   cls_loss = 0.0
2023-03-27 19:55:56,318   global_step = 2049
2023-03-27 19:55:56,318   loss = 59707.68413628472
2023-03-27 19:55:56,318   rep_loss = 0.700262976023886
2023-03-27 19:55:56,325 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2023-03-27 19:56:12,242 ***** Running evaluation *****
2023-03-27 19:56:12,242   Epoch = 7 iter 2099 step
2023-03-27 19:56:12,243   Num examples = 1043
2023-03-27 19:56:12,243   Batch size = 32
2023-03-27 19:56:12,244 ***** Eval results *****
2023-03-27 19:56:12,244   att_loss = 59688.302055027176
2023-03-27 19:56:12,245   cls_loss = 0.0
2023-03-27 19:56:12,245   global_step = 2099
2023-03-27 19:56:12,245   loss = 59689.00105298913
2023-03-27 19:56:12,245   rep_loss = 0.6992113914178766
2023-03-27 19:56:12,252 ***** Save model *****
2023-03-27 19:56:28,209 ***** Running evaluation *****
2023-03-27 19:56:28,210   Epoch = 8 iter 2149 step
2023-03-27 19:56:28,210   Num examples = 1043
2023-03-27 19:56:28,210   Batch size = 32
2023-03-27 19:56:28,212 ***** Eval results *****
2023-03-27 19:56:28,212   att_loss = 59012.353966346156
2023-03-27 19:56:28,212   cls_loss = 0.0
2023-03-27 19:56:28,213   global_step = 2149
2023-03-27 19:56:28,213   loss = 59013.043870192305
2023-03-27 19:56:28,213   rep_loss = 0.6903510322937598
2023-03-27 19:56:28,220 ***** Save model *****
2023-03-27 19:56:44,160 ***** Running evaluation *****
2023-03-27 19:56:44,161   Epoch = 8 iter 2199 step
2023-03-27 19:56:44,161   Num examples = 1043
2023-03-27 19:56:44,161   Batch size = 32
2023-03-27 19:56:44,163 ***** Eval results *****
2023-03-27 19:56:44,163   att_loss = 58928.318452380954
2023-03-27 19:56:44,164   cls_loss = 0.0
2023-03-27 19:56:44,164   global_step = 2199
2023-03-27 19:56:44,164   loss = 58929.00570436508
2023-03-27 19:56:44,164   rep_loss = 0.6873630285263062
2023-03-27 19:56:44,173 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2023-03-27 19:57:00,135 ***** Running evaluation *****
2023-03-27 19:57:00,136   Epoch = 8 iter 2249 step
2023-03-27 19:57:00,136   Num examples = 1043
2023-03-27 19:57:00,136   Batch size = 32
2023-03-27 19:57:00,137 ***** Eval results *****
2023-03-27 19:57:00,137   att_loss = 59018.50044939159
2023-03-27 19:57:00,137   cls_loss = 0.0
2023-03-27 19:57:00,137   global_step = 2249
2023-03-27 19:57:00,137   loss = 59019.1890210177
2023-03-27 19:57:00,137   rep_loss = 0.68857695993069
2023-03-27 19:57:00,171 ***** Save model *****
2023-03-27 19:57:16,099 ***** Running evaluation *****
2023-03-27 19:57:16,099   Epoch = 8 iter 2299 step
2023-03-27 19:57:16,099   Num examples = 1043
2023-03-27 19:57:16,100   Batch size = 32
2023-03-27 19:57:16,100 ***** Eval results *****
2023-03-27 19:57:16,101   att_loss = 58834.83382860429
2023-03-27 19:57:16,101   cls_loss = 0.0
2023-03-27 19:57:16,101   global_step = 2299
2023-03-27 19:57:16,101   loss = 58835.520921203984
2023-03-27 19:57:16,101   rep_loss = 0.6870436664739269
2023-03-27 19:57:16,108 ***** Save model *****
****
2023-03-27 19:56:59,538 ***** Running evaluation *****
2023-03-27 19:56:59,539   Epoch = 19 iter 5249 step
2023-03-27 19:56:59,539   Num examples = 1043
2023-03-27 19:56:59,539   Batch size = 32
2023-03-27 19:56:59,540 ***** Eval results *****
2023-03-27 19:56:59,540   att_loss = 5.65401131998409
2023-03-27 19:56:59,540   cls_loss = 0.0
2023-03-27 19:56:59,540   global_step = 5249
2023-03-27 19:56:59,540   loss = 6.22770064256408
2023-03-27 19:56:59,541   rep_loss = 0.5736893002282489
2023-03-27 19:56:59,548 ***** Save model *****
2023-03-27 19:57:10,508 ***** Running evaluation *****
2023-03-27 19:57:10,508   Epoch = 19 iter 5299 step
2023-03-27 19:57:10,508   Num examples = 1043
2023-03-27 19:57:10,509   Batch size = 32
2023-03-27 19:57:10,510 ***** Eval results *****
2023-03-27 19:57:10,510   att_loss = 5.655241997896042
2023-03-27 19:57:10,511   cls_loss = 0.0
2023-03-27 19:57:10,511   global_step = 5299
2023-03-27 19:57:10,511   loss = 6.229768107422685
2023-03-27 19:57:10,511   rep_loss = 0.5745260889551281
2023-03-27 19:57:10,519 ***** Save model *****
2023-03-27 19:57:21,475 ***** Running evaluation *****
2023-03-27 19:57:21,475   Epoch = 20 iter 5349 step
2023-03-27 19:57:21,475   Num examples = 1043
2023-03-27 19:57:21,475   Batch size = 32
2023-03-27 19:57:21,476 ***** Eval results *****
2023-03-27 19:57:21,477   att_loss = 5.621240191989475
2023-03-27 19:57:21,477   cls_loss = 0.0
2023-03-27 19:57:21,477   global_step = 5349
2023-03-27 19:57:21,477   loss = 6.19248718685574
2023-03-27 19:57:21,477   rep_loss = 0.5712471074528165
2023-03-27 19:57:21,485 ***** Save model *****
2023-03-27 19:57:32,447 ***** Running evaluation *****
2023-03-27 19:57:32,447   Epoch = 20 iter 5399 step
2023-03-27 19:57:32,447   Num examples = 1043
2023-03-27 19:57:32,447   Batch size = 32
2023-03-27 19:57:32,449 ***** Eval results *****
2023-03-27 19:57:32,449   att_loss = 5.607511859829143
2023-03-27 19:57:32,449   cls_loss = 0.0
2023-03-27 19:57:32,449   global_step = 5399
2023-03-27 19:57:32,449   loss = 6.180335828813456
2023-03-27 19:57:32,449   rep_loss = 0.5728239619125755
2023-03-27 19:57:32,456 ***** Save model *****
2023-03-27 19:57:43,434 ***** Running evaluation *****
2023-03-27 19:57:43,434   Epoch = 20 iter 5449 step
2023-03-27 19:57:43,434   Num examples = 1043
2023-03-27 19:57:43,434   Batch size = 32
2023-03-27 19:57:43,435 ***** Eval results *****
2023-03-27 19:57:43,435   att_loss = 5.620653060598111
2023-03-27 19:57:43,435   cls_loss = 0.0
2023-03-27 19:57:43,436   global_step = 5449
2023-03-27 19:57:43,436   loss = 6.1939080702055485
2023-03-27 19:57:43,436   rep_loss = 0.5732550216377328
2023-03-27 19:57:43,443 ***** Save model *****
2023-03-27 19:57:54,436 ***** Running evaluation *****
2023-03-27 19:57:54,437   Epoch = 20 iter 5499 step
2023-03-27 19:57:54,437   Num examples = 1043
2023-03-27 19:57:54,437   Batch size = 32
2023-03-27 19:57:54,438 ***** Eval results *****
2023-03-27 19:57:54,439   att_loss = 5.6155187528838155
2023-03-27 19:57:54,439   cls_loss = 0.0
2023-03-27 19:57:54,439   global_step = 5499
2023-03-27 19:57:54,439   loss = 6.188683605793887
2023-03-27 19:57:54,439   rep_loss = 0.5731648604075114
2023-03-27 19:57:54,446 ***** Save model *****
2023-03-27 19:58:05,415 ***** Running evaluation *****
2023-03-27 19:58:05,415   Epoch = 20 iter 5549 step
2023-03-27 19:58:05,415   Num examples = 1043
2023-03-27 19:58:05,415   Batch size = 32
2023-03-27 19:58:05,416 ***** Eval results *****
2023-03-27 19:58:05,417   att_loss = 5.627147715627862
2023-03-27 19:58:05,417   cls_loss = 0.0
2023-03-27 19:58:05,417   global_step = 5549
2023-03-27 19:58:05,417   loss = 6.200652115653006
2023-03-27 19:58:05,417   rep_loss = 0.5735044100067832
2023-03-27 19:58:05,424 ***** Save model *****
2023-03-27 19:58:35,827 ***** Running evaluation *****
2023-03-27 19:58:35,827   Epoch = 9 iter 2549 step
2023-03-27 19:58:35,827   Num examples = 1043
2023-03-27 19:58:35,828   Batch size = 32
2023-03-27 19:58:35,830 ***** Eval results *****
2023-03-27 19:58:35,830   att_loss = 58709.52547089041
2023-03-27 19:58:35,830   cls_loss = 0.0
2023-03-27 19:58:35,831   global_step = 2549
2023-03-27 19:58:35,831   loss = 58710.20192101884
2023-03-27 19:58:35,831   rep_loss = 0.6763283904284647
2023-03-27 19:58:35,838 ***** Save model *****
2023-03-27 19:58:51,733 ***** Running evaluation *****
2023-03-27 19:58:51,733   Epoch = 9 iter 2599 step
2023-03-27 19:58:51,733   Num examples = 1043
2023-03-27 19:58:51,733   Batch size = 32
2023-03-27 19:58:51,737 ***** Eval results *****
2023-03-27 19:58:51,737   att_loss = 58777.605707908166
2023-03-27 19:58:51,738   cls_loss = 0.0
2023-03-27 19:58:51,738   global_step = 2599
2023-03-27 19:58:51,738   loss = 58778.28138950893
2023-03-27 19:58:51,738   rep_loss = 0.6755705573120896
2023-03-27 19:58:51,740 ***** Save model *****
 2023-03-27 19:59:07,661 ***** Running evaluation *****
2023-03-27 19:59:07,662   Epoch = 9 iter 2649 step
2023-03-27 19:59:07,662   Num examples = 1043
2023-03-27 19:59:07,662   Batch size = 32
2023-03-27 19:59:07,663 ***** Eval results *****
2023-03-27 19:59:07,663   att_loss = 58655.959857723574
2023-03-27 19:59:07,663   cls_loss = 0.0
2023-03-27 19:59:07,664   global_step = 2649
2023-03-27 19:59:07,664   loss = 58656.634559197155
2023-03-27 19:59:07,664   rep_loss = 0.6746115151459608
2023-03-27 19:59:07,666 ***** Save model *****
2023-03-27 19:59:23,586 ***** Running evaluation *****
2023-03-27 19:59:23,586   Epoch = 10 iter 2699 step
2023-03-27 19:59:23,586   Num examples = 1043
2023-03-27 19:59:23,586   Batch size = 32
2023-03-27 19:59:23,587 ***** Eval results *****
2023-03-27 19:59:23,587   att_loss = 58752.31223060345
2023-03-27 19:59:23,587   cls_loss = 0.0
2023-03-27 19:59:23,587   global_step = 2699
2023-03-27 19:59:23,588   loss = 58752.98141163793
2023-03-27 19:59:23,588   rep_loss = 0.6689566784891589
2023-03-27 19:59:23,595 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2023-03-27 19:59:39,516 ***** Running evaluation *****
2023-03-27 19:59:39,517   Epoch = 10 iter 2749 step
2023-03-27 19:59:39,517   Num examples = 1043
2023-03-27 19:59:39,517   Batch size = 32
2023-03-27 19:59:39,521 ***** Eval results *****
2023-03-27 19:59:39,521   att_loss = 58351.0507318038
2023-03-27 19:59:39,522   cls_loss = 0.0
2023-03-27 19:59:39,522   global_step = 2749
2023-03-27 19:59:39,522   loss = 58351.719640031646
2023-03-27 19:59:39,522   rep_loss = 0.6688004490695422
2023-03-27 19:59:39,529 ***** Save model *****
2023-03-27 19:59:55,446 ***** Running evaluation *****
2023-03-27 19:59:55,446   Epoch = 10 iter 2799 step
2023-03-27 19:59:55,446   Num examples = 1043
2023-03-27 19:59:55,446   Batch size = 32
2023-03-27 19:59:55,447 ***** Eval results *****
2023-03-27 19:59:55,448   att_loss = 58109.57209907946
2023-03-27 19:59:55,448   cls_loss = 0.0
2023-03-27 19:59:55,448   global_step = 2799
2023-03-27 19:59:55,448   loss = 58110.239855862405
2023-03-27 19:59:55,448   rep_loss = 0.6676837880482045
2023-03-27 19:59:55,450 ***** Save model *****
2023-03-27 19:59:33,303 ***** Running evaluation *****
2023-03-27 19:59:33,303   Epoch = 22 iter 5949 step
2023-03-27 19:59:33,303   Num examples = 1043
2023-03-27 19:59:33,303   Batch size = 32
2023-03-27 19:59:33,304 ***** Eval results *****
2023-03-27 19:59:33,305   att_loss = 5.578014055887858
2023-03-27 19:59:33,305   cls_loss = 0.0
2023-03-27 19:59:33,305   global_step = 5949
2023-03-27 19:59:33,305   loss = 6.145613435109456
2023-03-27 19:59:33,306   rep_loss = 0.5675993673006694
2023-03-27 19:59:33,313 ***** Save model *****
2023-03-27 19:59:44,298 ***** Running evaluation *****
2023-03-27 19:59:44,298   Epoch = 22 iter 5999 step
2023-03-27 19:59:44,298   Num examples = 1043
2023-03-27 19:59:44,299   Batch size = 32
2023-03-27 19:59:44,300 ***** Eval results *****
2023-03-27 19:59:44,300   att_loss = 5.588450687408447
2023-03-27 19:59:44,300   cls_loss = 0.0
2023-03-27 19:59:44,300   global_step = 5999
2023-03-27 19:59:44,300   loss = 6.157205261230469
2023-03-27 19:59:44,300   rep_loss = 0.5687545557022095
2023-03-27 19:59:44,307 ***** Save model *****
2023-03-27 19:59:55,281 ***** Running evaluation *****
2023-03-27 19:59:55,281   Epoch = 22 iter 6049 step
2023-03-27 19:59:55,281   Num examples = 1043
2023-03-27 19:59:55,281   Batch size = 32
2023-03-27 19:59:55,282 ***** Eval results *****
2023-03-27 19:59:55,283   att_loss = 5.597875249045236
2023-03-27 19:59:55,283   cls_loss = 0.0
2023-03-27 19:59:55,283   global_step = 6049
2023-03-27 19:59:55,283   loss = 6.167363125937325
2023-03-27 19:59:55,284   rep_loss = 0.5694878687177386
2023-03-27 19:59:55,291 ***** Save model *****
2023-03-27 20:00:06,242 ***** Running evaluation *****
2023-03-27 20:00:06,242   Epoch = 22 iter 6099 step
2023-03-27 20:00:06,242   Num examples = 1043
2023-03-27 20:00:06,242   Batch size = 32
2023-03-27 20:00:06,243 ***** Eval results *****
2023-03-27 20:00:06,243   att_loss = 5.609352277119954
2023-03-27 20:00:06,244   cls_loss = 0.0
2023-03-27 20:00:06,244   global_step = 6099
2023-03-27 20:00:06,244   loss = 6.1791061825222435
2023-03-27 20:00:06,244   rep_loss = 0.5697538918919034
2023-03-27 20:00:06,248 ***** Save model *****
2023-03-27 20:00:17,222 ***** Running evaluation *****
2023-03-27 20:00:17,222   Epoch = 23 iter 6149 step
2023-03-27 20:00:17,222   Num examples = 1043
2023-03-27 20:00:17,223   Batch size = 32
2023-03-27 20:00:17,223 ***** Eval results *****
2023-03-27 20:00:17,224   att_loss = 5.575234055519104
2023-03-27 20:00:17,224   cls_loss = 0.0
2023-03-27 20:00:17,224   global_step = 6149
2023-03-27 20:00:17,224   loss = 6.142765522003174
2023-03-27 20:00:17,225   rep_loss = 0.567531481385231
2023-03-27 20:00:17,231 ***** Save model *****
2023-03-27 20:00:28,197 ***** Running evaluation *****
2023-03-27 20:00:28,197   Epoch = 23 iter 6199 step
2023-03-27 20:00:28,198   Num examples = 1043
2023-03-27 20:00:28,198   Batch size = 32
2023-03-27 20:00:28,199 ***** Eval results *****
2023-03-27 20:00:28,199   att_loss = 5.591453881099306
2023-03-27 20:00:28,200   cls_loss = 0.0
2023-03-27 20:00:28,200   global_step = 6199
2023-03-27 20:00:28,200   loss = 6.161307976163667
2023-03-27 20:00:28,200   rep_loss = 0.5698540888983628
2023-03-27 20:00:28,202 ***** Save model *****
2023-03-27 20:00:39,176 ***** Running evaluation *****
2023-03-27 20:00:39,176   Epoch = 23 iter 6249 step
2023-03-27 20:00:39,176   Num examples = 1043
2023-03-27 20:00:39,177   Batch size = 32
2023-03-27 20:00:39,177 ***** Eval results *****
2023-03-27 20:00:39,178   att_loss = 5.609604990040815
2023-03-27 20:00:39,178   cls_loss = 0.0
2023-03-27 20:00:39,178   global_step = 6249
2023-03-27 20:00:39,179   loss = 6.179203474963153
2023-03-27 20:00:39,179   rep_loss = 0.569598486578023
2023-03-27 20:00:39,180 ***** Save model *****
2023-03-27 20:00:50,139 ***** Running evaluation *****
2023-03-27 20:00:50,139   Epoch = 23 iter 6299 step
2023-03-27 20:00:50,140   Num examples = 1043
2023-03-27 20:00:50,140   Batch size = 32
2023-03-27 20:00:50,141 ***** Eval results *****
2023-03-27 20:00:50,141   att_loss = 5.603661778606946
2023-03-27 20:00:50,141   cls_loss = 0.0
2023-03-27 20:00:50,141   global_step = 6299
2023-03-27 20:00:50,142   loss = 6.173219581193562
2023-03-27 20:00:50,142   rep_loss = 0.5695578033411051
2023-03-27 20:00:50,149 ***** Save model *****
2023-03-27 20:01:01,116 ***** Running evaluation *****
2023-03-27 20:01:01,116   Epoch = 23 iter 6349 step
2023-03-27 20:01:01,116   Num examples = 1043
2023-03-27 20:01:01,117   Batch size = 32
2023-03-27 20:01:01,117 ***** Eval results *****
2023-03-27 20:01:01,117   att_loss = 5.605193355908761
2023-03-27 20:01:01,118   cls_loss = 0.0
2023-03-27 20:01:01,118   global_step = 6349
2023-03-27 20:01:01,118   loss = 6.17435478247129
2023-03-27 20:01:01,118   rep_loss = 0.5691614294281373
2023-03-27 20:01:01,126 ***** Save model *****
2022023-03-27 20:01:30,873 ***** Running evaluation *****
2023-03-27 20:01:30,873   Epoch = 11 iter 3099 step
2023-03-27 20:01:30,873   Num examples = 1043
2023-03-27 20:01:30,873   Batch size = 32
2023-03-27 20:01:30,874 ***** Eval results *****
2023-03-27 20:01:30,874   att_loss = 58150.8138744213
2023-03-27 20:01:30,874   cls_loss = 0.0
2023-03-27 20:01:30,875   global_step = 3099
2023-03-27 20:01:30,875   loss = 58151.4735001929
2023-03-27 20:01:30,875   rep_loss = 0.6595963990246808
2023-03-27 20:01:30,882 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2023-03-27 20:01:46,810 ***** Running evaluation *****
2023-03-27 20:01:46,811   Epoch = 11 iter 3149 step
2023-03-27 20:01:46,811   Num examples = 1043
2023-03-27 20:01:46,811   Batch size = 32
2023-03-27 20:01:46,813 ***** Eval results *****
2023-03-27 20:01:46,813   att_loss = 58131.51811247052
2023-03-27 20:01:46,813   cls_loss = 0.0
2023-03-27 20:01:46,814   global_step = 3149
2023-03-27 20:01:46,814   loss = 58132.17729215802
2023-03-27 20:01:46,814   rep_loss = 0.6590981146074691
2023-03-27 20:01:46,874 ***** Save model *****
2023-03-27 20:02:02,820 ***** Running evaluation *****
2023-03-27 20:02:02,821   Epoch = 11 iter 3199 step
2023-03-27 20:02:02,821   Num examples = 1043
2023-03-27 20:02:02,821   Batch size = 32
2023-03-27 20:02:02,823 ***** Eval results *****
2023-03-27 20:02:02,823   att_loss = 58110.648392771946
2023-03-27 20:02:02,824   cls_loss = 0.0
2023-03-27 20:02:02,824   global_step = 3199
2023-03-27 20:02:02,824   loss = 58111.30714754294
2023-03-27 20:02:02,824   rep_loss = 0.6586664027840127
2023-03-27 20:02:02,828 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2023-03-27 20:02:18,749 ***** Running evaluation *****
2023-03-27 20:02:18,749   Epoch = 12 iter 3249 step
2023-03-27 20:02:18,750   Num examples = 1043
2023-03-27 20:02:18,750   Batch size = 32
2023-03-27 20:02:18,750 ***** Eval results *****
2023-03-27 20:02:18,751   att_loss = 57756.63446180556
2023-03-27 20:02:18,751   cls_loss = 0.0
2023-03-27 20:02:18,751   global_step = 3249
2023-03-27 20:02:18,751   loss = 57757.28723958333
2023-03-27 20:02:18,751   rep_loss = 0.6528308471043904
2023-03-27 20:02:18,753 ***** Save model *****
2023-03-27 20:02:34,642 ***** Running evaluation *****
2023-03-27 20:02:34,642   Epoch = 12 iter 3299 step
2023-03-27 20:02:34,643   Num examples = 1043
2023-03-27 20:02:34,643   Batch size = 32
2023-03-27 20:02:34,643 ***** Eval results *****
2023-03-27 20:02:34,644   att_loss = 58101.07956414474
2023-03-27 20:02:34,644   cls_loss = 0.0
2023-03-27 20:02:34,644   global_step = 3299
2023-03-27 20:02:34,644   loss = 58101.73355263158
2023-03-27 20:02:34,644   rep_loss = 0.6541447965722335
2023-03-27 20:02:34,651 ***** Save model *****
23-03-27 20:02:39,940 ***** Running evaluation *****
2023-03-27 20:02:39,941   Epoch = 25 iter 6799 step
2023-03-27 20:02:39,941   Num examples = 1043
2023-03-27 20:02:39,941   Batch size = 32
2023-03-27 20:02:39,942 ***** Eval results *****
2023-03-27 20:02:39,943   att_loss = 5.553213488671087
2023-03-27 20:02:39,943   cls_loss = 0.0
2023-03-27 20:02:39,943   global_step = 6799
2023-03-27 20:02:39,943   loss = 6.119289244374921
2023-03-27 20:02:39,943   rep_loss = 0.5660757355151638
2023-03-27 20:02:39,951 ***** Save model *****
2023-03-27 20:02:50,926 ***** Running evaluation *****
2023-03-27 20:02:50,926   Epoch = 25 iter 6849 step
2023-03-27 20:02:50,926   Num examples = 1043
2023-03-27 20:02:50,926   Batch size = 32
2023-03-27 20:02:50,927 ***** Eval results *****
2023-03-27 20:02:50,928   att_loss = 5.554388218912585
2023-03-27 20:02:50,928   cls_loss = 0.0
2023-03-27 20:02:50,928   global_step = 6849
2023-03-27 20:02:50,928   loss = 6.120196361651366
2023-03-27 20:02:50,928   rep_loss = 0.565808119787567
2023-03-27 20:02:50,935 ***** Save model *****
2023-03-27 20:03:01,905 ***** Running evaluation *****
2023-03-27 20:03:01,906   Epoch = 25 iter 6899 step
2023-03-27 20:03:01,906   Num examples = 1043
2023-03-27 20:03:01,906   Batch size = 32
2023-03-27 20:03:01,907 ***** Eval results *****
2023-03-27 20:03:01,907   att_loss = 5.5580557861498425
2023-03-27 20:03:01,908   cls_loss = 0.0
2023-03-27 20:03:01,908   global_step = 6899
2023-03-27 20:03:01,908   loss = 6.12360500224999
2023-03-27 20:03:01,908   rep_loss = 0.5655491988041571
2023-03-27 20:03:01,915 ***** Save model *****
2023-03-27 20:03:12,887 ***** Running evaluation *****
2023-03-27 20:03:12,888   Epoch = 26 iter 6949 step
2023-03-27 20:03:12,888   Num examples = 1043
2023-03-27 20:03:12,888   Batch size = 32
2023-03-27 20:03:12,889 ***** Eval results *****
2023-03-27 20:03:12,889   att_loss = 5.515415804726737
2023-03-27 20:03:12,889   cls_loss = 0.0
2023-03-27 20:03:12,889   global_step = 6949
2023-03-27 20:03:12,889   loss = 6.074801104409354
2023-03-27 20:03:12,889   rep_loss = 0.5593853678022113
2023-03-27 20:03:12,891 ***** Save model *****
2023-03-27 20:03:23,851 ***** Running evaluation *****
2023-03-27 20:03:23,852   Epoch = 26 iter 6999 step
2023-03-27 20:03:23,852   Num examples = 1043
2023-03-27 20:03:23,852   Batch size = 32
2023-03-27 20:03:23,854 ***** Eval results *****
2023-03-27 20:03:23,854   att_loss = 5.567864844673558
2023-03-27 20:03:23,854   cls_loss = 0.0
2023-03-27 20:03:23,855   global_step = 6999
2023-03-27 20:03:23,855   loss = 6.133159796396892
2023-03-27 20:03:23,855   rep_loss = 0.5652949433577689
2023-03-27 20:03:23,862 ***** Save model *****
2023-03-27 20:03:34,840 ***** Running evaluation *****
2023-03-27 20:03:34,840   Epoch = 26 iter 7049 step
2023-03-27 20:03:34,840   Num examples = 1043
2023-03-27 20:03:34,840   Batch size = 32
2023-03-27 20:03:34,842 ***** Eval results *****
2023-03-27 20:03:34,842   att_loss = 5.5680177412300464
2023-03-27 20:03:34,842   cls_loss = 0.0
2023-03-27 20:03:34,842   global_step = 7049
2023-03-27 20:03:34,843   loss = 6.132652380756128
2023-03-27 20:03:34,843   rep_loss = 0.5646346450966095
2023-03-27 20:03:34,845 ***** Save model *****
2023-03-27 20:03:54,320 ***** Running evaluation *****
2023-03-27 20:03:54,320   Epoch = 13 iter 3549 step
2023-03-27 20:03:54,320   Num examples = 1043
2023-03-27 20:03:54,320   Batch size = 32
2023-03-27 20:03:54,322 ***** Eval results *****
2023-03-27 20:03:54,322   att_loss = 57607.585286458336
2023-03-27 20:03:54,323   cls_loss = 0.0
2023-03-27 20:03:54,323   global_step = 3549
2023-03-27 20:03:54,323   loss = 57608.232972756414
2023-03-27 20:03:54,323   rep_loss = 0.6478755397674365
2023-03-27 20:03:54,330 ***** Save model *****
2023-03-27 20:04:10,232 ***** Running evaluation *****
2023-03-27 20:04:10,232   Epoch = 13 iter 3599 step
2023-03-27 20:04:10,232   Num examples = 1043
2023-03-27 20:04:10,233   Batch size = 32
2023-03-27 20:04:10,236 ***** Eval results *****
2023-03-27 20:04:10,237   att_loss = 57539.988342285156
2023-03-27 20:04:10,237   cls_loss = 0.0
2023-03-27 20:04:10,237   global_step = 3599
2023-03-27 20:04:10,238   loss = 57540.63427734375
2023-03-27 20:04:10,238   rep_loss = 0.6461333986371756
2023-03-27 20:04:10,245 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     2023-03-27 20:04:26,201 ***** Running evaluation *****
2023-03-27 20:04:26,202   Epoch = 13 iter 3649 step
2023-03-27 20:04:26,202   Num examples = 1043
2023-03-27 20:04:26,202   Batch size = 32
2023-03-27 20:04:26,203 ***** Eval results *****
2023-03-27 20:04:26,203   att_loss = 57438.84572507023
2023-03-27 20:04:26,204   cls_loss = 0.0
2023-03-27 20:04:26,204   global_step = 3649
2023-03-27 20:04:26,204   loss = 57439.491090238764
2023-03-27 20:04:26,204   rep_loss = 0.6455202996730804
2023-03-27 20:04:26,211 ***** Save model *****
2023-03-27 20:04:42,122 ***** Running evaluation *****
2023-03-27 20:04:42,123   Epoch = 13 iter 3699 step
2023-03-27 20:04:42,123   Num examples = 1043
2023-03-27 20:04:42,123   Batch size = 32
2023-03-27 20:04:42,124 ***** Eval results *****
2023-03-27 20:04:42,124   att_loss = 57445.44281112939
2023-03-27 20:04:42,125   cls_loss = 0.0
2023-03-27 20:04:42,125   global_step = 3699
2023-03-27 20:04:42,125   loss = 57446.08809621711
2023-03-27 20:04:42,125   rep_loss = 0.6454012613547476
2023-03-27 20:04:42,133 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2023-03-27 20:04:58,042 ***** Running evaluation *****
2023-03-27 20:04:58,043   Epoch = 14 iter 3749 step
2023-03-27 20:04:58,043   Num examples = 1043
2023-03-27 20:04:58,043   Batch size = 32
2023-03-27 20:04:58,044 ***** Eval results *****
2023-03-27 20:04:58,044   att_loss = 56235.87926136364
2023-03-27 20:04:58,045   cls_loss = 0.0
2023-03-27 20:04:58,045   global_step = 3749
2023-03-27 20:04:58,045   loss = 56236.51953125
2023-03-27 20:04:58,045   rep_loss = 0.6404730677604675
2023-03-27 20:04:58,052 ***** Save model *****
2023-03-27 20:05:13,992 ***** Running evaluation *****
2023-03-27 20:05:13,992   Epoch = 14 iter 3799 step
2023-03-27 20:05:13,992   Num examples = 1043
2023-03-27 20:05:13,993   Batch size = 32
2023-03-27 20:05:13,994 ***** Eval results *****
2023-03-27 20:05:13,994   att_loss = 57247.22508965164
2023-03-27 20:05:13,994   cls_loss = 0.0
2023-03-27 20:05:13,994   global_step = 3799
2023-03-27 20:05:13,994   loss = 57247.86737961065
2023-03-27 20:05:13,994   rep_loss = 0.6422057063853155
2023-03-27 20:05:14,001 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2023-03-27 20:05:29,921 ***** Running evaluation *****
2023-03-27 20:05:29,922   Epoch = 14 iter 3849 step
2023-03-27 20:05:29,922   Num examples = 1043
2023-03-27 20:05:29,922   Batch size = 32
2023-03-27 20:05:29,923 ***** Eval results *****
2023-03-27 20:05:29,923   att_loss = 57060.573198198195
2023-03-27 20:05:29,923   cls_loss = 0.0
2023-03-27 20:05:29,924   global_step = 3849
2023-03-27 20:05:29,924   loss = 57061.21480855856
2023-03-27 20:05:29,924   rep_loss = 0.6415261996758951
2023-03-27 20:05:29,931 ***** Save model *****
2023-03-27 20:05:45,884 ***** Running evaluation *****
2023-03-27 20:05:45,884   Epoch = 14 iter 3899 step
2023-03-27 20:05:45,884   Num examples = 1043
2023-03-27 20:05:45,885   Batch size = 32
2023-03-27 20:05:45,886 ***** Eval results *****
2023-03-27 20:05:45,886   att_loss = 57317.99529309006
2023-03-27 20:05:45,886   cls_loss = 0.0
2023-03-27 20:05:45,886   global_step = 3899
2023-03-27 20:05:45,886   loss = 57318.638392857145
2023-03-27 20:05:45,886   rep_loss = 0.6430355469632593
2023-03-27 20:05:45,894 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2023-03-27 20:06:01,800 ***** Running evaluation *****
2023-03-27 20:06:01,801   Epoch = 14 iter 3949 step
2023-03-27 20:06:01,801   Num examples = 1043
2023-03-27 20:06:01,801   Batch size = 32
2023-03-27 20:06:01,802 ***** Eval results *****
2023-03-27 20:06:01,802   att_loss = 57413.03758145735
2023-03-27 20:06:01,802   cls_loss = 0.0
2023-03-27 20:06:01,802   global_step = 3949
2023-03-27 20:06:01,803   loss = 57413.68020586493
2023-03-27 20:06:01,803   rep_loss = 0.6425544348373232
2023-03-27 20:06:01,805 ***** Save model *****
2023-03-27 20:06:17,735 ***** Running evaluation *****
2023-03-27 20:06:17,735   Epoch = 14 iter 3999 step
2023-03-27 20:06:17,735   Num examples = 1043
2023-03-27 20:06:17,735   Batch size = 32
2023-03-27 20:06:17,737 ***** Eval results *****
2023-03-27 20:06:17,737   att_loss = 57389.29049928161
2023-03-27 20:06:17,737   cls_loss = 0.0
2023-03-27 20:06:17,737   global_step = 3999
2023-03-27 20:06:17,738   loss = 57389.932381465514
2023-03-27 20:06:17,738   rep_loss = 0.6418620820703178
2023-03-27 20:06:17,745 ***** Save model *****
2023-03-27 20:06:33,676 ***** Running evaluation *****
2023-03-27 20:06:33,677   Epoch = 15 iter 4049 step
2023-03-27 20:06:33,677   Num examples = 1043
2023-03-27 20:06:33,677   Batch size = 32
2023-03-27 20:06:33,679 ***** Eval results *****
2023-03-27 20:06:33,680   att_loss = 56648.16610440341
2023-03-27 20:06:33,680   cls_loss = 0.0
2023-03-27 20:06:33,680   global_step = 4049
2023-03-27 20:06:33,681   loss = 56648.802734375
2023-03-27 20:06:33,681   rep_loss = 0.6365350539034064
2023-03-27 20:06:33,689 ***** Save model *****
2023-03-27 20:06:49,618 ***** Running evaluation *****
2023-03-27 20:06:49,618   Epoch = 15 iter 4099 step
2023-03-27 20:06:49,618   Num examples = 1043
2023-03-27 20:06:49,618   Batch size = 32
2023-03-27 20:06:49,619 ***** Eval results *****
2023-03-27 20:06:49,620   att_loss = 56864.40026595745
2023-03-27 20:06:49,620   cls_loss = 0.0
2023-03-27 20:06:49,620   global_step = 4099
2023-03-27 20:06:49,620   loss = 56865.03723404255
2023-03-27 20:06:49,620   rep_loss = 0.636973427331194
2023-03-27 20:06:49,630 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2023-03-27 20:07:05,548 ***** Running evaluation *****
2023-03-27 20:07:05,548   Epoch = 15 iter 4149 step
2023-03-27 20:07:05,548   Num examples = 1043
2023-03-27 20:07:05,549   Batch size = 32
2023-03-27 20:07:05,550 ***** Eval results *****
2023-03-27 20:07:05,550   att_loss = 56828.988742404516
2023-03-27 20:07:05,550   cls_loss = 0.0
2023-03-27 20:07:05,550   global_step = 4149
2023-03-27 20:07:05,551   loss = 56829.625786675344
2023-03-27 20:07:05,551   rep_loss = 0.6370476612614261
2023-03-27 20:07:05,558 ***** Save model *****
2023-03-27 20:07:21,480 ***** Running evaluation *****
2023-03-27 20:07:21,480   Epoch = 15 iter 4199 step
2023-03-27 20:07:21,480   Num examples = 1043
2023-03-27 20:07:21,481   Batch size = 32
2023-03-27 20:07:21,482 ***** Eval results *****
2023-03-27 20:07:21,482   att_loss = 56934.08772954252
2023-03-27 20:07:21,483   cls_loss = 0.0
2023-03-27 20:07:21,483   global_step = 4199
2023-03-27 20:07:21,483   loss = 56934.72462951031
2023-03-27 20:07:21,483   rep_loss = 0.636944845165174
2023-03-27 20:07:21,490 ***** Save model *****
23-03-27 20:07:17,065 device: cuda n_gpu: 1
2023-03-27 20:07:17,115 Writing example 0 of 8551
2023-03-27 20:07:17,115 *** Example ***
2023-03-27 20:07:17,116 guid: train-0
2023-03-27 20:07:17,116 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 20:07:17,116 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:07:17,117 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:07:17,117 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:07:17,117 label: 1
2023-03-27 20:07:17,117 label_id: 1
2023-03-27 20:07:18,134 Writing example 0 of 1043
2023-03-27 20:07:18,134 *** Example ***
2023-03-27 20:07:18,134 guid: dev-0
2023-03-27 20:07:18,135 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 20:07:18,135 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:07:18,135 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:07:18,135 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:07:18,135 label: 1
2023-03-27 20:07:18,135 label_id: 1
2023-03-27 20:07:18,252 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 20:07:18,253 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 20:07:19,868 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 20:07:20,081 loading model...
2023-03-27 20:07:20,131 done!
2023-03-27 20:07:20,132 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 20:07:37,415 ***** Running evaluation *****
2023-03-27 20:07:37,416   Epoch = 15 iter 4249 step
2023-03-27 20:07:37,416   Num examples = 1043
2023-03-27 20:07:37,416   Batch size = 32
2023-03-27 20:07:37,417 ***** Eval results *****
2023-03-27 20:07:37,417   att_loss = 57031.409195696724
2023-03-27 20:07:37,418   cls_loss = 0.0
2023-03-27 20:07:37,418   global_step = 4249
2023-03-27 20:07:37,418   loss = 57032.045914446724
2023-03-27 20:07:37,418   rep_loss = 0.636736313583421
2023-03-27 20:07:37,423 ***** Save model *****
2023-03-27 20:07:53,337 ***** Running evaluation *****
2023-03-27 20:07:53,337   Epoch = 16 iter 4299 step
2023-03-27 20:07:53,337   Num examples = 1043
2023-03-27 20:07:53,337   Batch size = 32
2023-03-27 20:07:53,338 ***** Eval results *****
2023-03-27 20:07:53,339   att_loss = 56302.311631944445
2023-03-27 20:07:53,339   cls_loss = 0.0
2023-03-27 20:07:53,339   global_step = 4299
2023-03-27 20:07:53,339   loss = 56302.942274305555
2023-03-27 20:07:53,339   rep_loss = 0.6309794297924748
2023-03-27 20:07:53,346 ***** Save model *****
ias']
2023-03-27 20:07:20,453 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 20:07:20,464 ***** Running training *****
2023-03-27 20:07:20,464   Num examples = 8551
2023-03-27 20:07:20,464   Batch size = 32
2023-03-27 20:07:20,465   Num steps = 8010
2023-03-27 20:07:20,465 n: bert.embeddings.word_embeddings.weight
2023-03-27 20:07:20,465 n: bert.embeddings.position_embeddings.weight
2023-03-27 20:07:20,465 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 20:07:20,465 n: bert.embeddings.LayerNorm.weight
2023-03-27 20:07:20,465 n: bert.embeddings.LayerNorm.bias
2023-03-27 20:07:20,465 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 20:07:20,466 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 20:07:20,466 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 20:07:20,466 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 20:07:20,466 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 20:07:20,466 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 20:07:20,466 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 20:07:20,466 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 20:07:20,466 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 20:07:20,466 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 20:07:20,467 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 20:07:20,467 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 20:07:20,467 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 20:07:20,467 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 20:07:20,467 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 20:07:20,467 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 20:07:20,467 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 20:07:20,467 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 20:07:20,467 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 20:07:20,467 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 20:07:20,468 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 20:07:20,468 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 20:07:20,468 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 20:07:20,468 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 20:07:20,468 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 20:07:20,468 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 20:07:20,468 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 20:07:20,468 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 20:07:20,468 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 20:07:20,469 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 20:07:20,469 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 20:07:20,469 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 20:07:20,469 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 20:07:20,469 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 20:07:20,469 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 20:07:20,469 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 20:07:20,469 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 20:07:20,469 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 20:07:20,469 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 20:07:20,470 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 20:07:20,470 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 20:07:20,470 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 20:07:20,470 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 20:07:20,470 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 20:07:20,470 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 20:07:20,470 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 20:07:20,470 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 20:07:20,470 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 20:07:20,470 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 20:07:20,470 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 20:07:20,471 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 20:07:20,471 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 20:07:20,471 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 20:07:20,471 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 20:07:20,471 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 20:07:20,471 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 20:07:20,471 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 20:07:20,471 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 20:07:20,471 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 20:07:20,471 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 20:07:20,471 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 20:07:20,472 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 20:07:20,472 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 20:07:20,472 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 20:07:20,472 n: bert.pooler.dense.weight
2023-03-27 20:07:20,472 n: bert.pooler.dense.bias
2023-03-27 20:07:20,472 n: classifier.weight
2023-03-27 20:07:20,472 n: classifier.bias
2023-03-27 20:07:20,472 n: fit_dense.weight
2023-03-27 20:07:20,472 n: fit_dense.bias
2023-03-27 20:07:20,472 Total parameters: 14591258
2023-03-27 20:07:30,289 ***** Running evaluation *****
2023-03-27 20:07:30,289   Epoch = 0 iter 49 step
2023-03-27 20:07:30,290   Num examples = 1043
2023-03-27 20:07:30,290   Batch size = 32
2023-03-27 20:07:30,294 ***** Eval results *****
2023-03-27 20:07:30,294   att_loss = 0.1064056822535943
2023-03-27 20:07:30,294   cls_loss = 0.0
2023-03-27 20:07:30,294   global_step = 49
2023-03-27 20:07:30,294   loss = 1.3551586793393504
2023-03-27 20:07:30,295   rep_loss = 1.248753000278862
2023-03-27 20:07:30,296 ***** Save model *****
2023-03-27 20:07:41,027 ***** Running evaluation *****
2023-03-27 20:07:41,027   Epoch = 0 iter 99 step
2023-03-27 20:07:41,027   Num examples = 1043
2023-03-27 20:07:41,027   Batch size = 32
2023-03-27 20:07:41,028 ***** Eval results *****
2023-03-27 20:07:41,028   att_loss = 0.09678085976176792
2023-03-27 20:07:41,028   cls_loss = 0.0
2023-03-27 20:07:41,028   global_step = 99
2023-03-27 20:07:41,029   loss = 1.1550159665069195
2023-03-27 20:07:41,029   rep_loss = 1.0582351082503194
2023-03-27 20:07:41,035 ***** Save model *****
2023-03-27 20:07:51,866 ***** Running evaluation *****
2023-03-27 20:07:51,866   Epoch = 0 iter 149 step
2023-03-27 20:07:51,866   Num examples = 1043
2023-03-27 20:07:51,867   Batch size = 32
2023-03-27 20:07:51,867 ***** Eval results *****
2023-03-27 20:07:51,868   att_loss = 0.09175092226906911
2023-03-27 20:07:51,868   cls_loss = 0.0
2023-03-27 20:07:51,868   global_step = 149
2023-03-27 20:07:51,868   loss = 1.061883887988609
2023-03-27 20:07:51,868   rep_loss = 0.9701329661695749
2023-03-27 20:07:51,875 ***** Save model *****
2023-03-27 20:08:02,774 ***** Running evaluation *****
2023-03-27 20:08:02,775   Epoch = 0 iter 199 step
2023-03-27 20:08:02,775   Num examples = 1043
2023-03-27 20:08:02,775   Batch size = 32
2023-03-27 20:08:02,777 ***** Eval results *****
2023-03-27 20:08:02,777   att_loss = 0.0887569858500706
2023-03-27 20:08:02,777   cls_loss = 0.0
2023-03-27 20:08:02,777   global_step = 199
2023-03-27 20:08:02,778   loss = 1.0058769592687713
2023-03-27 20:08:02,778   rep_loss = 0.9171199738679818
2023-03-27 20:08:02,785 ***** Save model *****
2023-03-27 20:08:13,712 ***** Running evaluation *****
2023-03-27 20:08:13,712   Epoch = 0 iter 249 step
2023-03-27 20:08:13,713   Num examples = 1043
2023-03-27 20:08:13,713   Batch size = 32
2023-03-27 20:08:13,714 ***** Eval results *****
2023-03-27 20:08:13,714   att_loss = 0.08665501951692574
2023-03-27 20:08:13,714   cls_loss = 0.0
2023-03-27 20:08:13,715   global_step = 249
2023-03-27 20:08:13,715   loss = 0.9678695946333399
2023-03-27 20:08:13,715   rep_loss = 0.881214574637662
2023-03-27 20:08:13,722 ***** Save model *****
2023-03-27 20:08:24,736 ***** Running evaluation *****
2023-03-27 20:08:24,737   Epoch = 1 iter 299 step
2023-03-27 20:08:24,737   Num examples = 1043
2023-03-27 20:08:24,737   Batch size = 32
2023-03-27 20:08:24,738 ***** Eval results *****
2023-03-27 20:08:24,739   att_loss = 0.07622026978060603
2023-03-27 20:08:24,739   cls_loss = 0.0
2023-03-27 20:08:24,739   global_step = 299
2023-03-27 20:08:24,739   loss = 0.7960088066756725
2023-03-27 20:08:24,739   rep_loss = 0.7197885364294052
2023-03-27 20:08:24,747 ***** Save model *****
2023-03-27 20:08:35,760 ***** Running evaluation *****
2023-03-27 20:08:35,761   Epoch = 1 iter 349 step
2023-03-27 20:08:35,761   Num examples = 1043
2023-03-27 20:08:35,761   Batch size = 32
2023-03-27 20:08:35,762 ***** Eval results *****
2023-03-27 20:08:35,763   att_loss = 0.07637313807882913
2023-03-27 20:08:35,763   cls_loss = 0.0
2023-03-27 20:08:35,763   global_step = 349
2023-03-27 20:08:35,763   loss = 0.7913614177122349
2023-03-27 20:08:35,763   rep_loss = 0.7149882796334057
2023-03-27 20:08:35,770 ***** Save model *****
2023-03-27 20:08:46,802 ***** Running evaluation *****
2023-03-27 20:08:46,802   Epoch = 1 iter 399 step
2023-03-27 20:08:46,803   Num examples = 1043
2023-03-27 20:08:46,803   Batch size = 32
2023-03-27 20:08:46,804 ***** Eval results *****
2023-03-27 20:08:46,805   att_loss = 0.07611109734033093
2023-03-27 20:08:46,805   cls_loss = 0.0
2023-03-27 20:08:46,805   global_step = 399
2023-03-27 20:08:46,805   loss = 0.7865391612956019
2023-03-27 20:08:46,805   rep_loss = 0.7104280636166082
2023-03-27 20:08:46,812 ***** Save model *****
2023-03-27 20:08:57,255 ***** Running evaluation *****
2023-03-27 20:08:57,256   Epoch = 16 iter 4499 step
2023-03-27 20:08:57,256   Num examples = 1043
2023-03-27 20:08:57,256   Batch size = 32
2023-03-27 20:08:57,257 ***** Eval results *****
2023-03-27 20:08:57,257   att_loss = 56741.19765280837
2023-03-27 20:08:57,257   cls_loss = 0.0
2023-03-27 20:08:57,258   global_step = 4499
2023-03-27 20:08:57,258   loss = 56741.83000068833
2023-03-27 20:08:57,258   rep_loss = 0.6323656515928092
2023-03-27 20:08:57,263 ***** Save model *****
2023-03-27 20:09:13,169 ***** Running evaluation *****
2023-03-27 20:09:13,169   Epoch = 17 iter 4549 step
2023-03-27 20:09:13,169   Num examples = 1043
2023-03-27 20:09:13,170   Batch size = 32
2023-03-27 20:09:13,171 ***** Eval results *****
2023-03-27 20:09:13,171   att_loss = 57011.206640625
2023-03-27 20:09:13,171   cls_loss = 0.0
2023-03-27 20:09:13,171   global_step = 4549
2023-03-27 20:09:13,172   loss = 57011.83828125
2023-03-27 20:09:13,172   rep_loss = 0.6315541446208954
2023-03-27 20:09:13,180 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2023-03-27 20:09:29,078 ***** Running evaluation *****
2023-03-27 20:09:29,079   Epoch = 17 iter 4599 step
2023-03-27 20:09:29,079   Num examples = 1043
2023-03-27 20:09:29,079   Batch size = 32
2023-03-27 20:09:29,080 ***** Eval results *****
2023-03-27 20:09:29,080   att_loss = 56348.9484375
2023-03-27 20:09:29,080   cls_loss = 0.0
2023-03-27 20:09:29,080   global_step = 4599
2023-03-27 20:09:29,081   loss = 56349.578776041664
2023-03-27 20:09:29,081   rep_loss = 0.630164717634519
2023-03-27 20:09:29,088 ***** Save model *****
2023-03-27 20:09:44,995 ***** Running evaluation *****
2023-03-27 20:09:44,995   Epoch = 17 iter 4649 step
2023-03-27 20:09:44,995   Num examples = 1043
2023-03-27 20:09:44,996   Batch size = 32
2023-03-27 20:09:44,996 ***** Eval results *****
2023-03-27 20:09:44,997   att_loss = 56440.21139914773
2023-03-27 20:09:44,997   cls_loss = 0.0
2023-03-27 20:09:44,997   global_step = 4649
2023-03-27 20:09:44,997   loss = 56440.840767045454
2023-03-27 20:09:44,998   rep_loss = 0.62922781272368
2023-03-27 20:09:45,004 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2023-03-27 20:10:00,918 ***** Running evaluation *****
2023-03-27 20:10:00,918   Epoch = 17 iter 4699 step
2023-03-27 20:10:00,918   Num examples = 1043
2023-03-27 20:10:00,919   Batch size = 32
2023-03-27 20:10:00,920 ***** Eval results *****
2023-03-27 20:10:00,920   att_loss = 56437.571020507814
2023-03-27 20:10:00,920   cls_loss = 0.0
2023-03-27 20:10:00,920   global_step = 4699
2023-03-27 20:10:00,920   loss = 56438.200390625
2023-03-27 20:10:00,920   rep_loss = 0.6292120188474655
2023-03-27 20:10:00,928 ***** Save model *****
2023-03-27 20:10:16,844 ***** Running evaluation *****
2023-03-27 20:10:16,844   Epoch = 17 iter 4749 step
2023-03-27 20:10:16,845   Num examples = 1043
2023-03-27 20:10:16,845   Batch size = 32
2023-03-27 20:10:16,846 ***** Eval results *****
2023-03-27 20:10:16,847   att_loss = 56590.61491815476
2023-03-27 20:10:16,847   cls_loss = 0.0
2023-03-27 20:10:16,847   global_step = 4749
2023-03-27 20:10:16,847   loss = 56591.24436383929
2023-03-27 20:10:16,848   rep_loss = 0.6293586603232793
2023-03-27 20:10:16,851 ***** Save model *****
2023-03-27 20:10:32,762 ***** Running evaluation *****
2023-03-27 20:10:32,762   Epoch = 17 iter 4799 step
2023-03-27 20:10:32,762   Num examples = 1043
2023-03-27 20:10:32,763   Batch size = 32
2023-03-27 20:10:32,764 ***** Eval results *****
2023-03-27 20:10:32,765   att_loss = 56677.689858774036
2023-03-27 20:10:32,765   cls_loss = 0.0
2023-03-27 20:10:32,765   global_step = 4799
2023-03-27 20:10:32,765   loss = 56678.31956129808
2023-03-27 20:10:32,765   rep_loss = 0.6296104192733765
2023-03-27 20:10:32,772 ***** Save model *****
2023-03-27 20:10:48,677 ***** Running evaluation *****
2023-03-27 20:10:48,677   Epoch = 18 iter 4849 step
2023-03-27 20:10:48,677   Num examples = 1043
2023-03-27 20:10:48,678   Batch size = 32
2023-03-27 20:10:48,678 ***** Eval results *****
2023-03-27 20:10:48,679   att_loss = 56533.94431322674
2023-03-27 20:10:48,679   cls_loss = 0.0
2023-03-27 20:10:48,679   global_step = 4849
2023-03-27 20:10:48,679   loss = 56534.57040334302
2023-03-27 20:10:48,679   rep_loss = 0.6262150836545367
2023-03-27 20:10:48,681 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       2023-03-27 20:11:04,609 ***** Running evaluation *****
2023-03-27 20:11:04,609   Epoch = 18 iter 4899 step
2023-03-27 20:11:04,610   Num examples = 1043
2023-03-27 20:11:04,610   Batch size = 32
2023-03-27 20:11:04,611 ***** Eval results *****
2023-03-27 20:11:04,611   att_loss = 56603.943380376346
2023-03-27 20:11:04,611   cls_loss = 0.0
2023-03-27 20:11:04,611   global_step = 4899
2023-03-27 20:11:04,612   loss = 56604.57018649193
2023-03-27 20:11:04,612   rep_loss = 0.6269028007343251
2023-03-27 20:11:04,613 ***** Save model *****
2023-03-27 20:11:20,533 ***** Running evaluation *****
2023-03-27 20:11:20,534   Epoch = 18 iter 4949 step
2023-03-27 20:11:20,534   Num examples = 1043
2023-03-27 20:11:20,534   Batch size = 32
2023-03-27 20:11:20,536 ***** Eval results *****
2023-03-27 20:11:20,536   att_loss = 56331.815914554194
2023-03-27 20:11:20,536   cls_loss = 0.0
2023-03-27 20:11:20,536   global_step = 4949
2023-03-27 20:11:20,536   loss = 56332.44233500874
2023-03-27 20:11:20,536   rep_loss = 0.6263925745770648
2023-03-27 20:11:20,538 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2023-03-27 20:11:36,742 ***** Running evaluation *****
2023-03-27 20:11:36,743   Epoch = 18 iter 4999 step
2023-03-27 20:11:36,743   Num examples = 1043
2023-03-27 20:11:36,743   Batch size = 32
2023-03-27 20:11:36,744 ***** Eval results *****
2023-03-27 20:11:36,745   att_loss = 56320.35158273964
2023-03-27 20:11:36,745   cls_loss = 0.0
2023-03-27 20:11:36,745   global_step = 4999
2023-03-27 20:11:36,745   loss = 56320.977756638604
2023-03-27 20:11:36,745   rep_loss = 0.6261117328633916
2023-03-27 20:11:36,751 ***** Save model *****
2023-03-27 20:11:52,938 ***** Running evaluation *****
2023-03-27 20:11:52,939   Epoch = 18 iter 5049 step
2023-03-27 20:11:52,939   Num examples = 1043
2023-03-27 20:11:52,939   Batch size = 32
2023-03-27 20:11:52,940 ***** Eval results *****
2023-03-27 20:11:52,940   att_loss = 56468.270447530864
2023-03-27 20:11:52,940   cls_loss = 0.0
2023-03-27 20:11:52,940   global_step = 5049
2023-03-27 20:11:52,940   loss = 56468.89679783951
2023-03-27 20:11:52,940   rep_loss = 0.6263373835096634
2023-03-27 20:11:52,942 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2023-03-27 20:12:09,127 ***** Running evaluation *****
2023-03-27 20:12:09,128   Epoch = 19 iter 5099 step
2023-03-27 20:12:09,128   Num examples = 1043
2023-03-27 20:12:09,128   Batch size = 32
2023-03-27 20:12:09,129 ***** Eval results *****
2023-03-27 20:12:09,129   att_loss = 56542.715144230766
2023-03-27 20:12:09,130   cls_loss = 0.0
2023-03-27 20:12:09,130   global_step = 5099
2023-03-27 20:12:09,130   loss = 56543.337890625
2023-03-27 20:12:09,130   rep_loss = 0.6229742077680734
2023-03-27 20:12:09,132 ***** Save model *****
2023-03-27 20:12:25,045 ***** Running evaluation *****
2023-03-27 20:12:25,045   Epoch = 19 iter 5149 step
2023-03-27 20:12:25,045   Num examples = 1043
2023-03-27 20:12:25,045   Batch size = 32
2023-03-27 20:12:25,047 ***** Eval results *****
2023-03-27 20:12:25,047   att_loss = 56335.94500411184
2023-03-27 20:12:25,047   cls_loss = 0.0
2023-03-27 20:12:25,047   global_step = 5149
2023-03-27 20:12:25,048   loss = 56336.56799958881
2023-03-27 20:12:25,048   rep_loss = 0.6230214269537675
2023-03-27 20:12:25,050 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2023-03-27 20:12:40,975 ***** Running evaluation *****
2023-03-27 20:12:40,976   Epoch = 19 iter 5199 step
2023-03-27 20:12:40,976   Num examples = 1043
2023-03-27 20:12:40,976   Batch size = 32
2023-03-27 20:12:40,977 ***** Eval results *****
2023-03-27 20:12:40,978   att_loss = 56314.977058531746
2023-03-27 20:12:40,978   cls_loss = 0.0
2023-03-27 20:12:40,978   global_step = 5199
2023-03-27 20:12:40,978   loss = 56315.600105406746
2023-03-27 20:12:40,978   rep_loss = 0.6229783149938735
2023-03-27 20:12:40,980 ***** Save model *****
2023-03-27 20:12:56,898 ***** Running evaluation *****
2023-03-27 20:12:56,898   Epoch = 19 iter 5249 step
2023-03-27 20:12:56,898   Num examples = 1043
2023-03-27 20:12:56,899   Batch size = 32
2023-03-27 20:12:56,899 ***** Eval results *****
2023-03-27 20:12:56,899   att_loss = 56351.91867897727
2023-03-27 20:12:56,900   cls_loss = 0.0
2023-03-27 20:12:56,900   global_step = 5249
2023-03-27 20:12:56,900   loss = 56352.54221413352
2023-03-27 20:12:56,900   rep_loss = 0.6235122487626292
2023-03-27 20:12:56,902 ***** Save model *****
2023-03-27 20:13:12,825 ***** Running evaluation *****
2023-03-27 20:13:12,825   Epoch = 19 iter 5299 step
2023-03-27 20:13:12,825   Num examples = 1043
2023-03-27 20:13:12,825   Batch size = 32
2023-03-27 20:13:12,826 ***** Eval results *****
2023-03-27 20:13:12,826   att_loss = 56331.553477599555
2023-03-27 20:13:12,826   cls_loss = 0.0
2023-03-27 20:13:12,826   global_step = 5299
2023-03-27 20:13:12,827   loss = 56332.17726769912
2023-03-27 20:13:12,827   rep_loss = 0.6237677852664374
2023-03-27 20:13:12,828 ***** Save model *****
2023-03-27 20:13:28,757 ***** Running evaluation *****
2023-03-27 20:13:28,758   Epoch = 20 iter 5349 step
2023-03-27 20:13:28,758   Num examples = 1043
2023-03-27 20:13:28,758   Batch size = 32
2023-03-27 20:13:28,759 ***** Eval results *****
2023-03-27 20:13:28,760   att_loss = 56258.819010416664
2023-03-27 20:13:28,760   cls_loss = 0.0
2023-03-27 20:13:28,760   global_step = 5349
2023-03-27 20:13:28,760   loss = 56259.43923611111
2023-03-27 20:13:28,760   rep_loss = 0.6201067301962111
2023-03-27 20:13:28,767 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2023-03-27 20:13:44,675 ***** Running evaluation *****
2023-03-27 20:13:44,676   Epoch = 20 iter 5399 step
2023-03-27 20:13:44,676   Num examples = 1043
2023-03-27 20:13:44,676   Batch size = 32
2023-03-27 20:13:44,677 ***** Eval results *****
2023-03-27 20:13:44,677   att_loss = 55994.91657838983
2023-03-27 20:13:44,677   cls_loss = 0.0
2023-03-27 20:13:44,678   global_step = 5399
2023-03-27 20:13:44,678   loss = 55995.536480402545
2023-03-27 20:13:44,678   rep_loss = 0.6200067885851456
2023-03-27 20:13:44,679 ***** Save model *****
2023-03-27 20:14:00,617 ***** Running evaluation *****
2023-03-27 20:14:00,617   Epoch = 20 iter 5449 step
2023-03-27 20:14:00,618   Num examples = 1043
2023-03-27 20:14:00,618   Batch size = 32
2023-03-27 20:14:00,621 ***** Eval results *****
2023-03-27 20:14:00,621   att_loss = 55855.58034690367
2023-03-27 20:14:00,621   cls_loss = 0.0
2023-03-27 20:14:00,622   global_step = 5449
2023-03-27 20:14:00,622   loss = 55856.200007167434
2023-03-27 20:14:00,622   rep_loss = 0.6196823995047753
2023-03-27 20:14:00,624 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2023-03-27 20:14:16,564 ***** Running evaluation *****
2023-03-27 20:14:16,564   Epoch = 20 iter 5499 step
2023-03-27 20:14:16,564   Num examples = 1043
2023-03-27 20:14:16,564   Batch size = 32
2023-03-27 20:14:16,565 ***** Eval results *****
2023-03-27 20:14:16,566   att_loss = 55886.971747248426
2023-03-27 20:14:16,566   cls_loss = 0.0
2023-03-27 20:14:16,566   global_step = 5499
2023-03-27 20:14:16,566   loss = 55887.591710888366
2023-03-27 20:14:16,567   rep_loss = 0.6200616209761901
2023-03-27 20:14:16,568 ***** Save model *****
2023-03-27 20:14:32,489 ***** Running evaluation *****
2023-03-27 20:14:32,490   Epoch = 20 iter 5549 step
2023-03-27 20:14:32,490   Num examples = 1043
2023-03-27 20:14:32,490   Batch size = 32
2023-03-27 20:14:32,492 ***** Eval results *****
2023-03-27 20:14:32,492   att_loss = 56079.55051958732
2023-03-27 20:14:32,492   cls_loss = 0.0
2023-03-27 20:14:32,492   global_step = 5549
2023-03-27 20:14:32,493   loss = 56080.170734898325
2023-03-27 20:14:32,493   rep_loss = 0.6202507150230225
2023-03-27 20:14:32,495 ***** Save model *****

2023-03-27 20:14:27,685 ***** Running evaluation *****
2023-03-27 20:14:27,685   Epoch = 7 iter 1949 step
2023-03-27 20:14:27,685   Num examples = 1043
2023-03-27 20:14:27,685   Batch size = 32
2023-03-27 20:14:27,687 ***** Eval results *****
2023-03-27 20:14:27,688   att_loss = 0.06719264131970704
2023-03-27 20:14:27,688   cls_loss = 0.0
2023-03-27 20:14:27,688   global_step = 1949
2023-03-27 20:14:27,688   loss = 0.6625747501850128
2023-03-27 20:14:27,688   rep_loss = 0.5953821092844009
2023-03-27 20:14:27,691 ***** Save model *****
2023-03-27 20:14:38,697 ***** Running evaluation *****
2023-03-27 20:14:38,697   Epoch = 7 iter 1999 step
2023-03-27 20:14:38,697   Num examples = 1043
2023-03-27 20:14:38,697   Batch size = 32
2023-03-27 20:14:38,701 ***** Eval results *****
2023-03-27 20:14:38,701   att_loss = 0.06735750663165863
2023-03-27 20:14:38,701   cls_loss = 0.0
2023-03-27 20:14:38,702   global_step = 1999
2023-03-27 20:14:38,702   loss = 0.6628093343514663
2023-03-27 20:14:38,702   rep_loss = 0.595451828608146
2023-03-27 20:14:38,709 ***** Save model *****
2023-03-27 20:14:49,683 ***** Running evaluation *****
2023-03-27 20:14:49,684   Epoch = 7 iter 2049 step
2023-03-27 20:14:49,684   Num examples = 1043
2023-03-27 20:14:49,684   Batch size = 32
2023-03-27 20:14:49,686 ***** Eval results *****
2023-03-27 20:14:49,686   att_loss = 0.06734315626737145
2023-03-27 20:14:49,687   cls_loss = 0.0
2023-03-27 20:14:49,687   global_step = 2049
2023-03-27 20:14:49,687   loss = 0.6625040570894877
2023-03-27 20:14:49,687   rep_loss = 0.5951608998907937
2023-03-27 20:14:49,690 ***** Save model *****
2023-03-27 20:15:00,660 ***** Running evaluation *****
2023-03-27 20:15:00,660   Epoch = 7 iter 2099 step
2023-03-27 20:15:00,660   Num examples = 1043
2023-03-27 20:15:00,660   Batch size = 32
2023-03-27 20:15:00,664 ***** Eval results *****
2023-03-27 20:15:00,664   att_loss = 0.06739698577186336
2023-03-27 20:15:00,665   cls_loss = 0.0
2023-03-27 20:15:00,665   global_step = 2099
2023-03-27 20:15:00,665   loss = 0.6627235770225525
2023-03-27 20:15:00,665   rep_loss = 0.595326590538025
2023-03-27 20:15:00,668 ***** Save model *****
2023-03-27 20:15:11,666 ***** Running evaluation *****
2023-03-27 20:15:11,666   Epoch = 8 iter 2149 step
2023-03-27 20:15:11,666   Num examples = 1043
2023-03-27 20:15:11,667   Batch size = 32
2023-03-27 20:15:11,669 ***** Eval results *****
2023-03-27 20:15:11,669   att_loss = 0.06745271384716034
2023-03-27 20:15:11,669   cls_loss = 0.0
2023-03-27 20:15:11,669   global_step = 2149
2023-03-27 20:15:11,669   loss = 0.6641921767821679
2023-03-27 20:15:11,670   rep_loss = 0.5967394663737371
2023-03-27 20:15:11,672 ***** Save model *****
2023-03-27 20:15:22,667 ***** Running evaluation *****
2023-03-27 20:15:22,667   Epoch = 8 iter 2199 step
2023-03-27 20:15:22,667   Num examples = 1043
2023-03-27 20:15:22,667   Batch size = 32
2023-03-27 20:15:22,672 ***** Eval results *****
2023-03-27 20:15:22,672   att_loss = 0.06723112252260012
2023-03-27 20:15:22,673   cls_loss = 0.0
2023-03-27 20:15:22,673   global_step = 2199
2023-03-27 20:15:22,673   loss = 0.6589536014057341
2023-03-27 20:15:22,673   rep_loss = 0.5917224779961601
2023-03-27 20:15:22,676 ***** Save model *****
2023-03-27 20:15:36,258 ***** Running evaluation *****
2023-03-27 20:15:36,258   Epoch = 21 iter 5749 step
2023-03-27 20:15:36,258   Num examples = 1043
2023-03-27 20:15:36,259   Batch size = 32
2023-03-27 20:15:36,261 ***** Eval results *****
2023-03-27 20:15:36,261   att_loss = 55777.15809308979
2023-03-27 20:15:36,261   cls_loss = 0.0
2023-03-27 20:15:36,261   global_step = 5749
2023-03-27 20:15:36,261   loss = 55777.77409771127
2023-03-27 20:15:36,262   rep_loss = 0.6161594533584487
2023-03-27 20:15:36,263 ***** Save model *****
2023-03-27 20:15:52,195 ***** Running evaluation *****
2023-03-27 20:15:52,195   Epoch = 21 iter 5799 step
2023-03-27 20:15:52,195   Num examples = 1043
2023-03-27 20:15:52,196   Batch size = 32
2023-03-27 20:15:52,197 ***** Eval results *****
2023-03-27 20:15:52,197   att_loss = 55840.279296875
2023-03-27 20:15:52,197   cls_loss = 0.0
2023-03-27 20:15:52,197   global_step = 5799
2023-03-27 20:15:52,198   loss = 55840.89585367838
2023-03-27 20:15:52,198   rep_loss = 0.6166863646358252
2023-03-27 20:15:52,200 ***** Save model *****
*****
2023-03-27 20:15:55,656 ***** Running evaluation *****
2023-03-27 20:15:55,656   Epoch = 8 iter 2349 step
2023-03-27 20:15:55,656   Num examples = 1043
2023-03-27 20:15:55,656   Batch size = 32
2023-03-27 20:15:55,658 ***** Eval results *****
2023-03-27 20:15:55,659   att_loss = 0.06679845512123175
2023-03-27 20:15:55,659   cls_loss = 0.0
2023-03-27 20:15:55,659   global_step = 2349
2023-03-27 20:15:55,659   loss = 0.6556259944965023
2023-03-27 20:15:55,659   rep_loss = 0.5888275380985278
2023-03-27 20:15:55,662 ***** Save model *****
2023-03-27 20:16:06,693 ***** Running evaluation *****
2023-03-27 20:16:06,693   Epoch = 8 iter 2399 step
2023-03-27 20:16:06,694   Num examples = 1043
2023-03-27 20:16:06,694   Batch size = 32
2023-03-27 20:16:06,697 ***** Eval results *****
2023-03-27 20:16:06,697   att_loss = 0.06695490606667424
2023-03-27 20:16:06,697   cls_loss = 0.0
2023-03-27 20:16:06,698   global_step = 2399
2023-03-27 20:16:06,698   loss = 0.6556865602391754
2023-03-27 20:16:06,698   rep_loss = 0.5887316533367897
2023-03-27 20:16:06,706 ***** Save model *****
2023-03-27 20:16:17,700 ***** Running evaluation *****
2023-03-27 20:16:17,700   Epoch = 9 iter 2449 step
2023-03-27 20:16:17,700   Num examples = 1043
2023-03-27 20:16:17,700   Batch size = 32
2023-03-27 20:16:17,702 ***** Eval results *****
2023-03-27 20:16:17,703   att_loss = 0.06657638101150161
2023-03-27 20:16:17,703   cls_loss = 0.0
2023-03-27 20:16:17,703   global_step = 2449
2023-03-27 20:16:17,703   loss = 0.6509038598641105
2023-03-27 20:16:17,703   rep_loss = 0.5843274800673776
2023-03-27 20:16:17,706 ***** Save model *****
2023-03-27 20:16:28,669 ***** Running evaluation *****
2023-03-27 20:16:28,670   Epoch = 9 iter 2499 step
2023-03-27 20:16:28,670   Num examples = 1043
2023-03-27 20:16:28,670   Batch size = 32
2023-03-27 20:16:28,675 ***** Eval results *****
2023-03-27 20:16:28,675   att_loss = 0.0667742060419793
2023-03-27 20:16:28,675   cls_loss = 0.0
2023-03-27 20:16:28,675   global_step = 2499
2023-03-27 20:16:28,675   loss = 0.6510139269133409
2023-03-27 20:16:28,676   rep_loss = 0.5842397206773361
2023-03-27 20:16:28,681 ***** Save model *****
2023-03-27 20:16:39,671 ***** Running evaluation *****
2023-03-27 20:16:39,671   Epoch = 9 iter 2549 step
2023-03-27 20:16:39,671   Num examples = 1043
2023-03-27 20:16:39,671   Batch size = 32
2023-03-27 20:16:39,673 ***** Eval results *****
2023-03-27 20:16:39,674   att_loss = 0.06667213991590559
2023-03-27 20:16:39,674   cls_loss = 0.0
2023-03-27 20:16:39,674   global_step = 2549
2023-03-27 20:16:39,674   loss = 0.6505546978075211
2023-03-27 20:16:39,674   rep_loss = 0.5838825563861899
2023-03-27 20:16:39,682 ***** Save model *****
2023-03-27 20:16:50,679 ***** Running evaluation *****
2023-03-27 20:16:50,679   Epoch = 9 iter 2599 step
2023-03-27 20:16:50,679   Num examples = 1043
2023-03-27 20:16:50,679   Batch size = 32
2023-03-27 20:16:50,683 ***** Eval results *****
2023-03-27 20:16:50,683   att_loss = 0.066784104826499
2023-03-27 20:16:50,683   cls_loss = 0.0
2023-03-27 20:16:50,683   global_step = 2599
2023-03-27 20:16:50,684   loss = 0.6507178198318092
2023-03-27 20:16:50,684   rep_loss = 0.5839337143970995
2023-03-27 20:16:50,692 ***** Save model *****
2023-03-27 20:17:01,678 ***** Running evaluation *****
2023-03-27 20:17:01,679   Epoch = 9 iter 2649 step
2023-03-27 20:17:01,679   Num examples = 1043
2023-03-27 20:17:01,679   Batch size = 32
2023-03-27 20:17:01,680 ***** Eval results *****
2023-03-27 20:17:01,680   att_loss = 0.06664597032576557
2023-03-27 20:17:01,680   cls_loss = 0.0
2023-03-27 20:17:01,680   global_step = 2649
2023-03-27 20:17:01,680   loss = 0.6498656280157042
2023-03-27 20:17:01,680   rep_loss = 0.5832196573416392
2023-03-27 20:17:01,682 ***** Save model *****
2023-03-27 20:17:11,864 ***** Running evaluation *****
2023-03-27 20:17:11,864   Epoch = 22 iter 6049 step
2023-03-27 20:17:11,865   Num examples = 1043
2023-03-27 20:17:11,865   Batch size = 32
2023-03-27 20:17:11,866 ***** Eval results *****
2023-03-27 20:17:11,866   att_loss = 55614.33176339286
2023-03-27 20:17:11,866   cls_loss = 0.0
2023-03-27 20:17:11,866   global_step = 6049
2023-03-27 20:17:11,867   loss = 55614.94584821429
2023-03-27 20:17:11,867   rep_loss = 0.6141337544577462
2023-03-27 20:17:11,868 ***** Save model *****
2023-03-27 20:17:27,762 ***** Running evaluation *****
2023-03-27 20:17:27,763   Epoch = 22 iter 6099 step
2023-03-27 20:17:27,763   Num examples = 1043
2023-03-27 20:17:27,763   Batch size = 32
2023-03-27 20:17:27,764 ***** Eval results *****
2023-03-27 20:17:27,764   att_loss = 55714.01303819445
2023-03-27 20:17:27,764   cls_loss = 0.0
2023-03-27 20:17:27,765   global_step = 6099
2023-03-27 20:17:27,765   loss = 55714.627222222225
2023-03-27 20:17:27,765   rep_loss = 0.6142329565684
2023-03-27 20:17:27,766 ***** Save model *****
        2023-03-27 20:17:44,187 ***** Running evaluation *****
2023-03-27 20:17:44,187   Epoch = 23 iter 6149 step
2023-03-27 20:17:44,187   Num examples = 1043
2023-03-27 20:17:44,188   Batch size = 32
2023-03-27 20:17:44,190 ***** Eval results *****
2023-03-27 20:17:44,190   att_loss = 55119.33837890625
2023-03-27 20:17:44,190   cls_loss = 0.0
2023-03-27 20:17:44,190   global_step = 6149
2023-03-27 20:17:44,190   loss = 55119.94921875
2023-03-27 20:17:44,191   rep_loss = 0.6103878393769264
2023-03-27 20:17:44,197 ***** Save model *****
2023-03-27 20:18:00,133 ***** Running evaluation *****
2023-03-27 20:18:00,134   Epoch = 23 iter 6199 step
2023-03-27 20:18:00,134   Num examples = 1043
2023-03-27 20:18:00,134   Batch size = 32
2023-03-27 20:18:00,135 ***** Eval results *****
2023-03-27 20:18:00,135   att_loss = 55618.730603448275
2023-03-27 20:18:00,136   cls_loss = 0.0
2023-03-27 20:18:00,136   global_step = 6199
2023-03-27 20:18:00,136   loss = 55619.345299030174
2023-03-27 20:18:00,136   rep_loss = 0.6146174175985928
2023-03-27 20:18:00,143 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  2023-03-27 20:18:16,053 ***** Running evaluation *****
2023-03-27 20:18:16,053   Epoch = 23 iter 6249 step
2023-03-27 20:18:16,054   Num examples = 1043
2023-03-27 20:18:16,054   Batch size = 32
2023-03-27 20:18:16,055 ***** Eval results *****
2023-03-27 20:18:16,055   att_loss = 55814.651656539354
2023-03-27 20:18:16,055   cls_loss = 0.0
2023-03-27 20:18:16,055   global_step = 6249
2023-03-27 20:18:16,056   loss = 55815.26508246528
2023-03-27 20:18:16,056   rep_loss = 0.6134708352662899
2023-03-27 20:18:16,061 ***** Save model *****
2023-03-27 20:18:31,993 ***** Running evaluation *****
2023-03-27 20:18:31,993   Epoch = 23 iter 6299 step
2023-03-27 20:18:31,993   Num examples = 1043
2023-03-27 20:18:31,993   Batch size = 32
2023-03-27 20:18:31,994 ***** Eval results *****
2023-03-27 20:18:31,994   att_loss = 55737.69125791139
2023-03-27 20:18:31,995   cls_loss = 0.0
2023-03-27 20:18:31,995   global_step = 6299
2023-03-27 20:18:31,995   loss = 55738.3046380538
2023-03-27 20:18:31,995   rep_loss = 0.6133761277681664
2023-03-27 20:18:31,996 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2023-03-27 20:18:47,944 ***** Running evaluation *****
2023-03-27 20:18:47,945   Epoch = 23 iter 6349 step
2023-03-27 20:18:47,945   Num examples = 1043
2023-03-27 20:18:47,945   Batch size = 32
2023-03-27 20:18:47,947 ***** Eval results *****
2023-03-27 20:18:47,947   att_loss = 55763.33745868389
2023-03-27 20:18:47,947   cls_loss = 0.0
2023-03-27 20:18:47,947   global_step = 6349
2023-03-27 20:18:47,948   loss = 55763.95047701322
2023-03-27 20:18:47,948   rep_loss = 0.6129527177948219
2023-03-27 20:18:47,953 ***** Save model *****
2023-03-27 20:19:03,886 ***** Running evaluation *****
2023-03-27 20:19:03,887   Epoch = 23 iter 6399 step
2023-03-27 20:19:03,887   Num examples = 1043
2023-03-27 20:19:03,887   Batch size = 32
2023-03-27 20:19:03,889 ***** Eval results *****
2023-03-27 20:19:03,889   att_loss = 55719.44467659884
2023-03-27 20:19:03,889   cls_loss = 0.0
2023-03-27 20:19:03,889   global_step = 6399
2023-03-27 20:19:03,889   loss = 55720.05773074128
2023-03-27 20:19:03,889   rep_loss = 0.6130489769370057
2023-03-27 20:19:03,895 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2023-03-27 20:19:19,828 ***** Running evaluation *****
2023-03-27 20:19:19,829   Epoch = 24 iter 6449 step
2023-03-27 20:19:19,829   Num examples = 1043
2023-03-27 20:19:19,829   Batch size = 32
2023-03-27 20:19:19,830 ***** Eval results *****
2023-03-27 20:19:19,831   att_loss = 55404.79792301829
2023-03-27 20:19:19,831   cls_loss = 0.0
2023-03-27 20:19:19,831   global_step = 6449
2023-03-27 20:19:19,831   loss = 55405.40958460366
2023-03-27 20:19:19,831   rep_loss = 0.6115307299102225
2023-03-27 20:19:19,833 ***** Save model *****
2023-03-27 20:19:35,741 ***** Running evaluation *****
2023-03-27 20:19:35,742   Epoch = 24 iter 6499 step
2023-03-27 20:19:35,742   Num examples = 1043
2023-03-27 20:19:35,742   Batch size = 32
2023-03-27 20:19:35,743 ***** Eval results *****
2023-03-27 20:19:35,743   att_loss = 55345.90521978022
2023-03-27 20:19:35,743   cls_loss = 0.0
2023-03-27 20:19:35,744   global_step = 6499
2023-03-27 20:19:35,744   loss = 55346.516569368134
2023-03-27 20:19:35,744   rep_loss = 0.6113479818616595
2023-03-27 20:19:35,749 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2023-03-27 20:19:51,666 ***** Running evaluation *****
2023-03-27 20:19:51,666   Epoch = 24 iter 6549 step
2023-03-27 20:19:51,666   Num examples = 1043
2023-03-27 20:19:51,666   Batch size = 32
2023-03-27 20:19:51,668 ***** Eval results *****
2023-03-27 20:19:51,668   att_loss = 55284.16783023049
2023-03-27 20:19:51,668   cls_loss = 0.0
2023-03-27 20:19:51,668   global_step = 6549
2023-03-27 20:19:51,669   loss = 55284.778756648935
2023-03-27 20:19:51,669   rep_loss = 0.6109229279748092
2023-03-27 20:19:51,670 ***** Save model *****

2023-03-27 20:19:57,546 ***** Running evaluation *****
2023-03-27 20:19:57,546   Epoch = 12 iter 3449 step
2023-03-27 20:19:57,547   Num examples = 1043
2023-03-27 20:19:57,547   Batch size = 32
2023-03-27 20:19:57,549 ***** Eval results *****
2023-03-27 20:19:57,549   att_loss = 0.06537359334072289
2023-03-27 20:19:57,549   cls_loss = 0.0
2023-03-27 20:19:57,549   global_step = 3449
2023-03-27 20:19:57,550   loss = 0.634064017023359
2023-03-27 20:19:57,550   rep_loss = 0.5686904228463465
2023-03-27 20:19:57,557 ***** Save model *****
2023-03-27 20:20:08,569 ***** Running evaluation *****
2023-03-27 20:20:08,569   Epoch = 13 iter 3499 step
2023-03-27 20:20:08,569   Num examples = 1043
2023-03-27 20:20:08,570   Batch size = 32
2023-03-27 20:20:08,575 ***** Eval results *****
2023-03-27 20:20:08,575   att_loss = 0.06599012243428401
2023-03-27 20:20:08,575   cls_loss = 0.0
2023-03-27 20:20:08,575   global_step = 3499
2023-03-27 20:20:08,575   loss = 0.6339079162904194
2023-03-27 20:20:08,576   rep_loss = 0.5679177939891815
2023-03-27 20:20:08,579 ***** Save model *****
2023-03-27 20:20:19,571 ***** Running evaluation *****
2023-03-27 20:20:19,571   Epoch = 13 iter 3549 step
2023-03-27 20:20:19,571   Num examples = 1043
2023-03-27 20:20:19,572   Batch size = 32
2023-03-27 20:20:19,574 ***** Eval results *****
2023-03-27 20:20:19,574   att_loss = 0.06518873070868161
2023-03-27 20:20:19,574   cls_loss = 0.0
2023-03-27 20:20:19,574   global_step = 3549
2023-03-27 20:20:19,575   loss = 0.6315772105485965
2023-03-27 20:20:19,575   rep_loss = 0.5663884824667221
2023-03-27 20:20:19,578 ***** Save model *****
2023-03-27 20:20:30,578 ***** Running evaluation *****
2023-03-27 20:20:30,578   Epoch = 13 iter 3599 step
2023-03-27 20:20:30,578   Num examples = 1043
2023-03-27 20:20:30,579   Batch size = 32
2023-03-27 20:20:30,583 ***** Eval results *****
2023-03-27 20:20:30,583   att_loss = 0.06517895980505273
2023-03-27 20:20:30,583   cls_loss = 0.0
2023-03-27 20:20:30,583   global_step = 3599
2023-03-27 20:20:30,584   loss = 0.6310583241283894
2023-03-27 20:20:30,584   rep_loss = 0.5658793658949435
2023-03-27 20:20:30,590 ***** Save model *****
2023-03-27 20:20:41,549 ***** Running evaluation *****
2023-03-27 20:20:41,549   Epoch = 13 iter 3649 step
2023-03-27 20:20:41,549   Num examples = 1043
2023-03-27 20:20:41,550   Batch size = 32
2023-03-27 20:20:41,552 ***** Eval results *****
2023-03-27 20:20:41,552   att_loss = 0.06513845686162456
2023-03-27 20:20:41,553   cls_loss = 0.0
2023-03-27 20:20:41,553   global_step = 3649
2023-03-27 20:20:41,553   loss = 0.6306442584884301
2023-03-27 20:20:41,553   rep_loss = 0.5655058019616631
2023-03-27 20:20:41,558 ***** Save model *****
2023-03-27 20:20:52,539 ***** Running evaluation *****
2023-03-27 20:20:52,539   Epoch = 13 iter 3699 step
2023-03-27 20:20:52,539   Num examples = 1043
2023-03-27 20:20:52,540   Batch size = 32
2023-03-27 20:20:52,545 ***** Eval results *****
2023-03-27 20:20:52,545   att_loss = 0.06519052361775386
2023-03-27 20:20:52,545   cls_loss = 0.0
2023-03-27 20:20:52,546   global_step = 3699
2023-03-27 20:20:52,546   loss = 0.6308448081999495
2023-03-27 20:20:52,546   rep_loss = 0.5656542845985346
2023-03-27 20:20:52,549 ***** Save model *****
2023-03-27 20:21:03,542 ***** Running evaluation *****
2023-03-27 20:21:03,543   Epoch = 14 iter 3749 step
2023-03-27 20:21:03,543   Num examples = 1043
2023-03-27 20:21:03,543   Batch size = 32
2023-03-27 20:21:03,545 ***** Eval results *****
2023-03-27 20:21:03,545   att_loss = 0.06487724930047989
2023-03-27 20:21:03,545   cls_loss = 0.0
2023-03-27 20:21:03,546   global_step = 3749
2023-03-27 20:21:03,546   loss = 0.6247795711864125
2023-03-27 20:21:03,546   rep_loss = 0.5599023266272112
2023-03-27 20:21:03,551 ***** Save model *****
2023-03-27 20:21:27,265 ***** Running evaluation *****
2023-03-27 20:21:27,266   Epoch = 25 iter 6849 step
2023-03-27 20:21:27,266   Num examples = 1043
2023-03-27 20:21:27,266   Batch size = 32
2023-03-27 20:21:27,267 ***** Eval results *****
2023-03-27 20:21:27,267   att_loss = 55566.67021372126
2023-03-27 20:21:27,267   cls_loss = 0.0
2023-03-27 20:21:27,268   global_step = 6849
2023-03-27 20:21:27,268   loss = 55567.28026221264
2023-03-27 20:21:27,268   rep_loss = 0.6100693571156469
2023-03-27 20:21:27,274 ***** Save model *****
2023-03-27 20:21:43,203 ***** Running evaluation *****
2023-03-27 20:21:43,203   Epoch = 25 iter 6899 step
2023-03-27 20:21:43,203   Num examples = 1043
2023-03-27 20:21:43,203   Batch size = 32
2023-03-27 20:21:43,205 ***** Eval results *****
2023-03-27 20:21:43,205   att_loss = 55498.67344447545
2023-03-27 20:21:43,205   cls_loss = 0.0
2023-03-27 20:21:43,205   global_step = 6899
2023-03-27 20:21:43,205   loss = 55499.28226143973
2023-03-27 20:21:43,205   rep_loss = 0.6088380425104073
2023-03-27 20:21:43,207 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2023-03-27 20:21:59,128 ***** Running evaluation *****
2023-03-27 20:21:59,129   Epoch = 26 iter 6949 step
2023-03-27 20:21:59,129   Num examples = 1043
2023-03-27 20:21:59,129   Batch size = 32
2023-03-27 20:21:59,132 ***** Eval results *****
2023-03-27 20:21:59,132   att_loss = 54621.5625
2023-03-27 20:21:59,132   cls_loss = 0.0
2023-03-27 20:21:59,133   global_step = 6949
2023-03-27 20:21:59,133   loss = 54622.162388392855
2023-03-27 20:21:59,133   rep_loss = 0.6008697833333697
2023-03-27 20:21:59,136 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    2023-03-27 20:22:15,055 ***** Running evaluation *****
2023-03-27 20:22:15,056   Epoch = 26 iter 6999 step
2023-03-27 20:22:15,056   Num examples = 1043
2023-03-27 20:22:15,056   Batch size = 32
2023-03-27 20:22:15,058 ***** Eval results *****
2023-03-27 20:22:15,058   att_loss = 55634.79989035088
2023-03-27 20:22:15,058   cls_loss = 0.0
2023-03-27 20:22:15,058   global_step = 6999
2023-03-27 20:22:15,058   loss = 55635.407552083336
2023-03-27 20:22:15,058   rep_loss = 0.6079958980543572
2023-03-27 20:22:15,060 ***** Save model *****
2023-03-27 20:22:30,985 ***** Running evaluation *****
2023-03-27 20:22:30,985   Epoch = 26 iter 7049 step
2023-03-27 20:22:30,985   Num examples = 1043
2023-03-27 20:22:30,985   Batch size = 32
2023-03-27 20:22:30,987 ***** Eval results *****
2023-03-27 20:22:30,987   att_loss = 55546.47769421729
2023-03-27 20:22:30,987   cls_loss = 0.0
2023-03-27 20:22:30,987   global_step = 7049
2023-03-27 20:22:30,988   loss = 55547.08480578271
2023-03-27 20:22:30,988   rep_loss = 0.6072491847466086
2023-03-27 20:22:30,995 ***** Save model *****
     2023-03-27 20:22:46,922 ***** Running evaluation *****
2023-03-27 20:22:46,922   Epoch = 26 iter 7099 step
2023-03-27 20:22:46,922   Num examples = 1043
2023-03-27 20:22:46,922   Batch size = 32
2023-03-27 20:22:46,923 ***** Eval results *****
2023-03-27 20:22:46,923   att_loss = 55419.55137838376
2023-03-27 20:22:46,923   cls_loss = 0.0
2023-03-27 20:22:46,924   global_step = 7099
2023-03-27 20:22:46,924   loss = 55420.15819068471
2023-03-27 20:22:46,924   rep_loss = 0.606904050347152
2023-03-27 20:22:46,926 ***** Save model *****
2023-03-27 20:23:02,852 ***** Running evaluation *****
2023-03-27 20:23:02,853   Epoch = 26 iter 7149 step
2023-03-27 20:23:02,853   Num examples = 1043
2023-03-27 20:23:02,853   Batch size = 32
2023-03-27 20:23:02,854 ***** Eval results *****
2023-03-27 20:23:02,855   att_loss = 55465.45574803744
2023-03-27 20:23:02,855   cls_loss = 0.0
2023-03-27 20:23:02,855   global_step = 7149
2023-03-27 20:23:02,855   loss = 55466.06278306159
2023-03-27 20:23:02,855   rep_loss = 0.6071539233272202
2023-03-27 20:23:02,857 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    2023-03-27 20:23:18,779 ***** Running evaluation *****
2023-03-27 20:23:18,779   Epoch = 26 iter 7199 step
2023-03-27 20:23:18,779   Num examples = 1043
2023-03-27 20:23:18,779   Batch size = 32
2023-03-27 20:23:18,783 ***** Eval results *****
2023-03-27 20:23:18,783   att_loss = 55537.6942941391
2023-03-27 20:23:18,783   cls_loss = 0.0
2023-03-27 20:23:18,783   global_step = 7199
2023-03-27 20:23:18,784   loss = 55538.3016172179
2023-03-27 20:23:18,784   rep_loss = 0.6073975335763122
2023-03-27 20:23:18,793 ***** Save model *****
2023-03-27 20:23:34,718 ***** Running evaluation *****
2023-03-27 20:23:34,719   Epoch = 27 iter 7249 step
2023-03-27 20:23:34,719   Num examples = 1043
2023-03-27 20:23:34,719   Batch size = 32
2023-03-27 20:23:34,722 ***** Eval results *****
2023-03-27 20:23:34,722   att_loss = 55621.03916015625
2023-03-27 20:23:34,722   cls_loss = 0.0
2023-03-27 20:23:34,723   global_step = 7249
2023-03-27 20:23:34,723   loss = 55621.64462890625
2023-03-27 20:23:34,723   rep_loss = 0.6054634764790535
2023-03-27 20:23:34,729 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      2023-03-27 20:23:50,654 ***** Running evaluation *****
2023-03-27 20:23:50,654   Epoch = 27 iter 7299 step
2023-03-27 20:23:50,654   Num examples = 1043
2023-03-27 20:23:50,654   Batch size = 32
2023-03-27 20:23:50,655 ***** Eval results *****
2023-03-27 20:23:50,656   att_loss = 55480.37491319444
2023-03-27 20:23:50,656   cls_loss = 0.0
2023-03-27 20:23:50,656   global_step = 7299
2023-03-27 20:23:50,656   loss = 55480.98125
2023-03-27 20:23:50,657   rep_loss = 0.6063450323210822
2023-03-27 20:23:50,663 ***** Save model *****
2023-03-27 20:24:06,565 ***** Running evaluation *****
2023-03-27 20:24:06,565   Epoch = 27 iter 7349 step
2023-03-27 20:24:06,565   Num examples = 1043
2023-03-27 20:24:06,565   Batch size = 32
2023-03-27 20:24:06,567 ***** Eval results *****
2023-03-27 20:24:06,567   att_loss = 55324.901729910714
2023-03-27 20:24:06,567   cls_loss = 0.0
2023-03-27 20:24:06,567   global_step = 7349
2023-03-27 20:24:06,567   loss = 55325.50717075893
2023-03-27 20:24:06,567   rep_loss = 0.6054126577717918
2023-03-27 20:24:06,574 ***** Save model *****
odel *****
2023-03-27 20:24:10,410 ***** Running evaluation *****
2023-03-27 20:24:10,410   Epoch = 17 iter 4599 step
2023-03-27 20:24:10,411   Num examples = 1043
2023-03-27 20:24:10,411   Batch size = 32
2023-03-27 20:24:10,415 ***** Eval results *****
2023-03-27 20:24:10,415   att_loss = 0.06359978324423234
2023-03-27 20:24:10,416   cls_loss = 0.0
2023-03-27 20:24:10,416   global_step = 4599
2023-03-27 20:24:10,416   loss = 0.6145475606123606
2023-03-27 20:24:10,416   rep_loss = 0.5509477724631627
2023-03-27 20:24:10,419 ***** Save model *****
2023-03-27 20:24:21,419 ***** Running evaluation *****
2023-03-27 20:24:21,420   Epoch = 17 iter 4649 step
2023-03-27 20:24:21,420   Num examples = 1043
2023-03-27 20:24:21,420   Batch size = 32
2023-03-27 20:24:21,422 ***** Eval results *****
2023-03-27 20:24:21,422   att_loss = 0.06378089673817158
2023-03-27 20:24:21,422   cls_loss = 0.0
2023-03-27 20:24:21,422   global_step = 4649
2023-03-27 20:24:21,423   loss = 0.6154994146390395
2023-03-27 20:24:21,423   rep_loss = 0.5517185146158392
2023-03-27 20:24:21,425 ***** Save model *****
2023-03-27 20:24:32,400 ***** Running evaluation *****
2023-03-27 20:24:32,401   Epoch = 17 iter 4699 step
2023-03-27 20:24:32,401   Num examples = 1043
2023-03-27 20:24:32,401   Batch size = 32
2023-03-27 20:24:32,404 ***** Eval results *****
2023-03-27 20:24:32,404   att_loss = 0.0637675077887252
2023-03-27 20:24:32,404   cls_loss = 0.0
2023-03-27 20:24:32,404   global_step = 4699
2023-03-27 20:24:32,404   loss = 0.6153509758412838
2023-03-27 20:24:32,405   rep_loss = 0.5515834659337997
2023-03-27 20:24:32,411 ***** Save model *****
2023-03-27 20:24:43,366 ***** Running evaluation *****
2023-03-27 20:24:43,367   Epoch = 17 iter 4749 step
2023-03-27 20:24:43,367   Num examples = 1043
2023-03-27 20:24:43,367   Batch size = 32
2023-03-27 20:24:43,369 ***** Eval results *****
2023-03-27 20:24:43,369   att_loss = 0.06392089249122711
2023-03-27 20:24:43,369   cls_loss = 0.0
2023-03-27 20:24:43,370   global_step = 4749
2023-03-27 20:24:43,370   loss = 0.6160535003457751
2023-03-27 20:24:43,370   rep_loss = 0.5521326070740109
2023-03-27 20:24:43,377 ***** Save model *****
2023-03-27 20:24:54,367 ***** Running evaluation *****
2023-03-27 20:24:54,367   Epoch = 17 iter 4799 step
2023-03-27 20:24:54,367   Num examples = 1043
2023-03-27 20:24:54,367   Batch size = 32
2023-03-27 20:24:54,372 ***** Eval results *****
2023-03-27 20:24:54,372   att_loss = 0.0640228594868229
2023-03-27 20:24:54,373   cls_loss = 0.0
2023-03-27 20:24:54,373   global_step = 4799
2023-03-27 20:24:54,373   loss = 0.6162902932900649
2023-03-27 20:24:54,373   rep_loss = 0.55226743381757
2023-03-27 20:24:54,376 ***** Save model *****
2023-03-27 20:25:05,344 ***** Running evaluation *****
2023-03-27 20:25:05,344   Epoch = 18 iter 4849 step
2023-03-27 20:25:05,344   Num examples = 1043
2023-03-27 20:25:05,344   Batch size = 32
2023-03-27 20:25:05,346 ***** Eval results *****
2023-03-27 20:25:05,346   att_loss = 0.06411621892868086
2023-03-27 20:25:05,346   cls_loss = 0.0
2023-03-27 20:25:05,346   global_step = 4849
2023-03-27 20:25:05,346   loss = 0.614460873049359
2023-03-27 20:25:05,346   rep_loss = 0.5503446529077929
2023-03-27 20:25:05,353 ***** Save model *****
2023-03-27 20:25:26,217 ***** Running evaluation *****
2023-03-27 20:25:26,218   Epoch = 28 iter 7599 step
2023-03-27 20:25:26,218   Num examples = 1043
2023-03-27 20:25:26,218   Batch size = 32
2023-03-27 20:25:26,220 ***** Eval results *****
2023-03-27 20:25:26,220   att_loss = 55421.560530995936
2023-03-27 20:25:26,220   cls_loss = 0.0
2023-03-27 20:25:26,220   global_step = 7599
2023-03-27 20:25:26,221   loss = 55422.165396341465
2023-03-27 20:25:26,221   rep_loss = 0.6048283707804796
2023-03-27 20:25:26,227 ***** Save model *****
2023-03-27 20:25:42,132 ***** Running evaluation *****
2023-03-27 20:25:42,133   Epoch = 28 iter 7649 step
2023-03-27 20:25:42,133   Num examples = 1043
2023-03-27 20:25:42,133   Batch size = 32
2023-03-27 20:25:42,134 ***** Eval results *****
2023-03-27 20:25:42,134   att_loss = 55470.32724440029
2023-03-27 20:25:42,134   cls_loss = 0.0
2023-03-27 20:25:42,134   global_step = 7649
2023-03-27 20:25:42,134   loss = 55470.93219382226
2023-03-27 20:25:42,135   rep_loss = 0.6050191790382297
2023-03-27 20:25:42,141 ***** Save model *****
**
2023-03-27 20:25:38,283 ***** Running evaluation *****
2023-03-27 20:25:38,283   Epoch = 18 iter 4999 step
2023-03-27 20:25:38,283   Num examples = 1043
2023-03-27 20:25:38,283   Batch size = 32
2023-03-27 20:25:38,287 ***** Eval results *****
2023-03-27 20:25:38,287   att_loss = 0.06367341291950775
2023-03-27 20:25:38,287   cls_loss = 0.0
2023-03-27 20:25:38,287   global_step = 4999
2023-03-27 20:25:38,287   loss = 0.6138143551782005
2023-03-27 20:25:38,287   rep_loss = 0.5501409416989341
2023-03-27 20:25:38,295 ***** Save model *****
2023-03-27 20:25:49,280 ***** Running evaluation *****
2023-03-27 20:25:49,280   Epoch = 18 iter 5049 step
2023-03-27 20:25:49,280   Num examples = 1043
2023-03-27 20:25:49,280   Batch size = 32
2023-03-27 20:25:49,282 ***** Eval results *****
2023-03-27 20:25:49,283   att_loss = 0.06380853507621789
2023-03-27 20:25:49,283   cls_loss = 0.0
2023-03-27 20:25:49,283   global_step = 5049
2023-03-27 20:25:49,283   loss = 0.6142116981278721
2023-03-27 20:25:49,283   rep_loss = 0.5504031630209935
2023-03-27 20:25:49,291 ***** Save model *****
2023-03-27 20:26:00,248 ***** Running evaluation *****
2023-03-27 20:26:00,249   Epoch = 19 iter 5099 step
2023-03-27 20:26:00,249   Num examples = 1043
2023-03-27 20:26:00,249   Batch size = 32
2023-03-27 20:26:00,251 ***** Eval results *****
2023-03-27 20:26:00,251   att_loss = 0.06406578292640355
2023-03-27 20:26:00,251   cls_loss = 0.0
2023-03-27 20:26:00,252   global_step = 5099
2023-03-27 20:26:00,252   loss = 0.60990054332293
2023-03-27 20:26:00,252   rep_loss = 0.5458347591070029
2023-03-27 20:26:00,256 ***** Save model *****
2023-03-27 20:26:11,244 ***** Running evaluation *****
2023-03-27 20:26:11,244   Epoch = 19 iter 5149 step
2023-03-27 20:26:11,244   Num examples = 1043
2023-03-27 20:26:11,245   Batch size = 32
2023-03-27 20:26:11,247 ***** Eval results *****
2023-03-27 20:26:11,247   att_loss = 0.06323010935203026
2023-03-27 20:26:11,247   cls_loss = 0.0
2023-03-27 20:26:11,247   global_step = 5149
2023-03-27 20:26:11,248   loss = 0.6074097125153792
2023-03-27 20:26:11,248   rep_loss = 0.5441796018889076
2023-03-27 20:26:11,251 ***** Save model *****
2023-03-27 20:26:22,232 ***** Running evaluation *****
2023-03-27 20:26:22,232   Epoch = 19 iter 5199 step
2023-03-27 20:26:22,232   Num examples = 1043
2023-03-27 20:26:22,233   Batch size = 32
2023-03-27 20:26:22,237 ***** Eval results *****
2023-03-27 20:26:22,237   att_loss = 0.06331146786373759
2023-03-27 20:26:22,238   cls_loss = 0.0
2023-03-27 20:26:22,238   global_step = 5199
2023-03-27 20:26:22,238   loss = 0.6080478954882849
2023-03-27 20:26:22,238   rep_loss = 0.544736426501047
2023-03-27 20:26:22,241 ***** Save model *****
2023-03-27 20:26:33,207 ***** Running evaluation *****
2023-03-27 20:26:33,208   Epoch = 19 iter 5249 step
2023-03-27 20:26:33,208   Num examples = 1043
2023-03-27 20:26:33,208   Batch size = 32
2023-03-27 20:26:33,210 ***** Eval results *****
2023-03-27 20:26:33,210   att_loss = 0.06348391812802716
2023-03-27 20:26:33,211   cls_loss = 0.0
2023-03-27 20:26:33,211   global_step = 5249
2023-03-27 20:26:33,211   loss = 0.609005447815765
2023-03-27 20:26:33,211   rep_loss = 0.5455215292220766
2023-03-27 20:26:33,214 ***** Save model *****
2023-03-27 20:26:44,181 ***** Running evaluation *****
2023-03-27 20:26:44,182   Epoch = 19 iter 5299 step
2023-03-27 20:26:44,182   Num examples = 1043
2023-03-27 20:26:44,182   Batch size = 32
2023-03-27 20:26:44,185 ***** Eval results *****
2023-03-27 20:26:44,185   att_loss = 0.06352905609307036
2023-03-27 20:26:44,185   cls_loss = 0.0
2023-03-27 20:26:44,186   global_step = 5299
2023-03-27 20:26:44,186   loss = 0.6100363172261061
2023-03-27 20:26:44,186   rep_loss = 0.5465072612319373
2023-03-27 20:26:44,188 ***** Save model *****
2023-03-27 20:27:02,158 ***** Running evaluation *****
2023-03-27 20:27:02,158   Epoch = 29 iter 7899 step
2023-03-27 20:27:02,158   Num examples = 1043
2023-03-27 20:27:02,158   Batch size = 32
2023-03-27 20:27:02,159 ***** Eval results *****
2023-03-27 20:27:02,159   att_loss = 55011.123121995195
2023-03-27 20:27:02,160   cls_loss = 0.0
2023-03-27 20:27:02,160   global_step = 7899
2023-003-27 20:26:55,141   loss = 0.6074110070864359
2023-03-27 20:26:55,141   rep_loss = 0.5441259079509311
2023-03-27 20:26:55,145 ***** Save model *****
2023-03-27 20:27:06,115 ***** Running evaluation *****
2023-03-27 20:27:06,115   Epoch = 20 iter 5399 step
2023-03-27 20:27:06,115   Num examples = 1043
2023-03-27 20:27:06,115   Batch size = 32
2023-03-27 20:27:06,118 ***** Eval results *****
2023-03-27 20:27:06,119   att_loss = 0.06302684596029379
2023-03-27 20:27:06,119   cls_loss = 0.0
2023-03-27 20:27:06,119   global_step = 5399
2023-03-27 20:27:06,120   loss = 0.6073571453660221
2023-03-27 20:27:06,120   rep_loss = 0.5443303019313489
2023-03-27 20:27:06,123 ***** Save model *****
2023-03-27 20:27:17,102 ***** Running evaluation *****
2023-03-27 20:27:17,103   Epoch = 20 iter 5449 step
2023-03-27 20:27:17,103   Num examples = 1043
2023-03-27 20:27:17,103   Batch size = 32
2023-03-27 20:27:17,105 ***** Eval results *****
2023-03-27 20:27:17,106   att_loss = 0.06312898949745598
2023-03-27 20:27:17,106   cls_loss = 0.0
2023-03-27 20:27:17,106   global_step = 5449
2023-03-27 20:27:17,106   loss = 0.6078534902782615
2023-03-27 20:27:17,106   rep_loss = 0.5447245032415478
2023-03-27 20:27:17,110 ***** Save model *****
2023-03-27 20:27:28,072 ***** Running evaluation *****
2023-03-27 20:27:28,072   Epoch = 20 iter 5499 step
2023-03-27 20:27:28,072   Num examples = 1043
2023-03-27 20:27:28,073   Batch size = 32
2023-03-27 20:27:28,077 ***** Eval results *****
2023-03-27 20:27:28,077   att_loss = 0.06306862090743563
2023-03-27 20:27:28,077   cls_loss = 0.0
2023-03-27 20:27:28,078   global_step = 5499
2023-03-27 20:27:28,078   loss = 0.6079519713449778
2023-03-27 20:27:28,078   rep_loss = 0.54488335240562
2023-03-27 20:27:28,083 ***** Save model *****
2023-03-27 20:27:39,043 ***** Running evaluation *****
2023-03-27 20:27:39,044   Epoch = 20 iter 5549 step
2023-03-27 20:27:39,044   Num examples = 1043
2023-03-27 20:27:39,044   Batch size = 32
2023-03-27 20:27:39,046 ***** Eval results *****
2023-03-27 20:27:39,046   att_loss = 0.06322572292964995
2023-03-27 20:27:39,046   cls_loss = 0.0
2023-03-27 20:27:39,047   global_step = 5549
2023-03-27 20:27:39,047   loss = 0.6085361510372618
2023-03-27 20:27:39,047   rep_loss = 0.5453104299791692
2023-03-27 20:27:39,050 ***** Save model *****
2023-03-27 20:27:50,025 ***** Running evaluation *****
2023-03-27 20:27:50,025   Epoch = 20 iter 5599 step
2023-03-27 20:27:50,025   Num examples = 1043
2023-03-27 20:27:50,025   Batch size = 32
2023-03-27 20:27:50,028 ***** Eval results *****
2023-03-27 20:27:50,029   att_loss = 0.06318378209722549
2023-03-27 20:27:50,029   cls_loss = 0.0
2023-03-27 20:27:50,029   global_step = 5599
2023-03-27 20:27:50,029   loss = 0.6083373234078691
2023-03-27 20:27:50,029   rep_loss = 0.5451535423750122
2023-03-27 20:27:50,031 ***** Save model *****
2023-03-27 20:28:00,996 ***** Running evaluation *****
2023-03-27 20:28:00,997   Epoch = 21 iter 5649 step
2023-03-27 20:28:00,997   Num examples = 1043
2023-03-27 20:28:00,997   Batch size = 32
2023-03-27 20:28:00,999 ***** Eval results *****
2023-03-27 20:28:00,999   att_loss = 0.06320288821700074
2023-03-27 20:28:01,000   cls_loss = 0.0
2023-03-27 20:28:01,000   global_step = 5649
2023-03-27 20:28:01,000   loss = 0.6061528438613528
2023-03-27 20:28:01,000   rep_loss = 0.542949956087839
2023-03-27 20:28:01,007 ***** Save model *****
2023-03-27 20:28:11,977 ***** Running evaluation *****
2023-03-27 20:28:11,977   Epoch = 21 iter 5699 step
2023-03-27 20:28:11,977   Num examples = 1043
2023-03-27 20:28:11,977   Batch size = 32
2023-03-27 20:28:11,979 ***** Eval results *****
2023-03-27 20:28:11,979   att_loss = 0.06323753127261349
2023-03-27 20:28:11,980   cls_loss = 0.0
2023-03-27 20:28:11,980   global_step = 5699
2023-03-27 20:28:11,980   loss = 0.6078029767326687
2023-03-27 20:28:11,980   rep_loss = 0.5445654444072557
2023-03-27 20:28:11,987 ***** Save model *****
: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:27:50,985 label: 1
2023-03-27 20:27:50,985 label_id: 1
2023-03-27 20:27:51,105 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 20:27:51,107 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 20:28:22,935 ***** Running evaluation *****
2023-03-27 20:28:22,935   Epoch = 21 iter 5749 step
2023-03-27 20:28:22,936   Num examples = 1043
2023-03-27 20:28:22,936   Batch size = 32
2023-03-27 20:28:22,938 ***** Eval results *****
2023-03-27 20:28:22,938   att_loss = 0.0630748026912481
2023-03-27 20:28:22,938   cls_loss = 0.0
2023-03-27 20:28:22,938   global_step = 5749
2023-03-27 20:28:22,939   loss = 0.6066889683125725
2023-03-27 20:28:22,939   rep_loss = 0.5436141639947891
2023-03-27 20:28:22,946 ***** Save model *****
2023-03-27 20:28:33,914 ***** Running evaluation *****
2023-03-27 20:28:33,915   Epoch = 21 iter 5799 step
2023-03-27 20:28:33,915   Num examples = 1043
2023-03-27 20:28:33,915   Batch size = 32
2023-03-27 20:28:33,920 ***** Eval results *****
2023-03-27 20:28:33,920   att_loss = 0.06314680333404492
2023-03-27 20:28:33,921   cls_loss = 0.0
2023-03-27 20:28:33,921   global_step = 5799
2023-03-27 20:28:33,921   loss = 0.6069601373746991
2023-03-27 20:28:33,922   rep_loss = 0.5438133324496448
2023-03-27 20:28:33,928 ***** Save model *****
2023-03-27 20:28:44,887 ***** Running evaluation *****
2023-03-27 20:28:44,887   Epoch = 21 iter 5849 step
2023-03-27 20:28:44,887   Num examples = 1043
2023-03-27 20:28:44,887   Batch size = 32
2023-03-27 20:28:44,889 ***** Eval results *****
2023-03-27 20:28:44,889   att_loss = 0.06316477965471173
2023-03-27 20:28:44,889   cls_loss = 0.0
2023-03-27 20:28:44,889   global_step = 5849
2023-03-27 20:28:44,889   loss = 0.6069860145572789
2023-03-27 20:28:44,889   rep_loss = 0.543821233486341
2023-03-27 20:28:44,891 ***** Save model *****
.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 20:27:53,191 ***** Running training *****
2023-03-27 20:27:53,192   Num examples = 8551
2023-03-27 20:27:53,192   Batch size = 32
2023-03-27 20:27:53,192   Num steps = 8010
2023-03-27 20:27:53,192 n: bert.embeddings.word_embeddings.weight
2023-03-27 20:27:53,192 n: bert.embeddings.position_embeddings.weight
2023-03-27 20:27:53,192 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 20:27:53,192 n: bert.embeddings.LayerNorm.weight
2023-03-27 20:27:53,193 n: bert.embeddings.LayerNorm.bias
2023-03-27 20:27:53,193 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 20:27:53,193 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 20:27:53,193 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 20:27:53,193 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 20:27:53,193 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 20:27:53,193 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 20:27:53,193 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 20:27:53,193 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 20:27:53,194 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 20:27:53,194 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 20:27:53,194 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 20:27:53,194 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 20:27:53,194 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 20:27:53,194 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 20:27:53,194 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 20:27:53,194 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 20:27:53,194 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 20:27:53,194 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 20:27:53,195 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 20:27:53,195 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 20:27:53,195 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 20:27:53,195 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 20:27:53,195 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 20:27:53,195 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 20:27:53,195 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 20:27:53,195 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 20:27:53,195 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 20:27:53,196 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 20:27:53,196 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 20:27:53,196 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 20:27:53,196 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 20:27:53,196 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 20:27:53,196 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 20:27:53,196 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 20:27:53,196 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 20:27:53,196 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 20:27:53,196 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 20:27:53,197 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 20:27:53,197 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 20:27:53,197 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 20:27:53,197 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 20:27:53,197 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 20:27:53,197 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 20:27:53,197 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 20:27:53,197 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 20:27:53,197 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 20:27:53,197 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 20:27:53,198 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 20:27:53,198 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 20:27:53,198 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 20:27:53,198 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 20:27:53,198 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 20:27:53,198 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 20:27:53,198 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 20:27:53,198 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 20:27:53,198 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 20:27:53,199 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 20:27:53,199 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 20:27:53,199 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 20:27:53,199 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 20:27:53,199 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 20:27:53,199 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 20:27:53,199 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 20:27:53,199 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 20:27:53,199 n: bert.pooler.dense.weight
2023-03-27 20:27:53,199 n: bert.pooler.dense.bias
2023-03-27 20:27:53,200 n: classifier.weight
2023-03-27 20:27:53,200 n: classifier.bias
2023-03-27 20:27:53,200 n: fit_dense.weight
2023-03-27 20:27:53,200 n: fit_dense.bias
2023-03-27 20:27:53,200 Total parameters: 14591258
2023-03-27 20:28:07,999 ***** Running evaluation *****
2023-03-27 20:28:08,000   Epoch = 0 iter 49 step
2023-03-27 20:28:08,000   Num examples = 1043
2023-03-27 20:28:08,000   Batch size = 32
2023-03-27 20:28:08,002 ***** Eval results *****
2023-03-27 20:28:08,003   att_loss = 8772141.010204082
2023-03-27 20:28:08,003   cls_loss = 0.0
2023-03-27 20:28:08,003   global_step = 49
2023-03-27 20:28:08,003   loss = 8772142.663265307
2023-03-27 20:28:08,003   rep_loss = 1.6544393957877646
2023-03-27 20:28:08,005 ***** Save model *****
2023-03-27 20:28:23,769 ***** Running evaluation *****
2023-03-27 20:28:23,770   Epoch = 0 iter 99 step
2023-03-27 20:28:23,770   Num examples = 1043
2023-03-27 20:28:23,770   Batch size = 32
2023-03-27 20:28:23,772 ***** Eval results *****
2023-03-27 20:28:23,772   att_loss = 8095873.777777778
2023-03-27 20:28:23,772   cls_loss = 0.0
2023-03-27 20:28:23,772   global_step = 99
2023-03-27 20:28:23,772   loss = 8095875.313131313
2023-03-27 20:28:23,773   rep_loss = 1.4731342250650579
2023-03-27 20:28:23,774 ***** Save model *****
2023-03-27 20:28:39,611 ***** Running evaluation *****
2023-03-27 20:28:39,612   Epoch = 0 iter 149 step
2023-03-27 20:28:39,612   Num examples = 1043
2023-03-27 20:28:39,612   Batch size = 32
2023-03-27 20:28:39,614 ***** Eval results *****
2023-03-27 20:28:39,614   att_loss = 7731587.533557047
2023-03-27 20:28:39,614   cls_loss = 0.0
2023-03-27 20:28:39,614   global_step = 149
2023-03-27 20:28:39,614   loss = 7731588.889261745
2023-03-27 20:28:39,614   rep_loss = 1.3760941428626143
2023-03-27 20:28:39,616 ***** Save model *****
2023-03-27 20:28:55,438 ***** Running evaluation *****
2023-03-27 20:28:55,438   Epoch = 0 iter 199 step
2023-03-27 20:28:55,439   Num examples = 1043
2023-03-27 20:28:55,439   Batch size = 32
2023-03-27 20:28:55,440 ***** Eval results *****
2023-03-27 20:28:55,440   att_loss = 7513581.600502512
2023-03-27 20:28:55,441   cls_loss = 0.0
2023-03-27 20:28:55,441   global_step = 199
2023-03-27 20:28:55,441   loss = 7513582.866834171
2023-03-27 20:28:55,441   rep_loss = 1.3118175202278635
2023-03-27 20:28:55,448 ***** Save model *****
2023-03-27 20:29:11,316 ***** Running evaluation *****
2023-03-27 20:29:11,316   Epoch = 0 iter 249 step
2023-03-27 20:29:11,316   Num examples = 1043
2023-03-27 20:29:11,317   Batch size = 32
2023-03-27 20:29:11,318 ***** Eval results *****
2023-03-27 20:29:11,318   att_loss = 7373775.008032128
2023-03-27 20:29:11,318   cls_loss = 0.0
2023-03-27 20:29:11,319   global_step = 249
2023-03-27 20:29:11,319   loss = 7373776.220883534
2023-03-27 20:29:11,319   rep_loss = 1.2644298761245236
2023-03-27 20:29:11,326 ***** Save model *****
2023-03-27 20:29:27,180 ***** Running evaluation *****
2023-03-27 20:29:27,181   Epoch = 1 iter 299 step
2023-03-27 20:29:27,181   Num examples = 1043
2023-03-27 20:29:27,181   Batch size = 32
2023-03-27 20:29:27,183 ***** Eval results *****
2023-03-27 20:29:27,183   att_loss = 6672022.328125
2023-03-27 20:29:27,183   cls_loss = 0.0
2023-03-27 20:29:27,183   global_step = 299
2023-03-27 20:29:27,183   loss = 6672023.328125
2023-03-27 20:29:27,184   rep_loss = 1.029799522832036
2023-03-27 20:29:27,185 ***** Save model *****
2023-03-27 20:29:43,053 ***** Running evaluation *****
2023-03-27 20:29:43,053   Epoch = 1 iter 349 step
2023-03-27 20:29:43,054   Num examples = 1043
2023-03-27 20:29:43,054   Batch size = 32
2023-03-27 20:29:43,055 ***** Eval results *****
2023-03-27 20:29:43,055   att_loss = 6660208.4878048785
2023-03-27 20:29:43,056   cls_loss = 0.0
2023-03-27 20:29:43,056   global_step = 349
2023-03-27 20:29:43,056   loss = 6660209.4878048785
2023-03-27 20:29:43,056   rep_loss = 1.0150874576917508
2023-03-27 20:29:43,058 ***** Save model *****
                                         2023-03-27 20:29:58,944 ***** Running evaluation *****
2023-03-27 20:29:58,944   Epoch = 1 iter 399 step
2023-03-27 20:29:58,944   Num examples = 1043
2023-03-27 20:29:58,944   Batch size = 32
2023-03-27 20:29:58,946 ***** Eval results *****
2023-03-27 20:29:58,946   att_loss = 6637574.810606061
2023-03-27 20:29:58,946   cls_loss = 0.0
2023-03-27 20:29:58,946   global_step = 399
2023-03-27 20:29:58,946   loss = 6637575.810606061
2023-03-27 20:29:58,946   rep_loss = 1.0018209223494385
2023-03-27 20:29:58,948 ***** Save model *****
2023-03-27 20:30:14,866 ***** Running evaluation *****
2023-03-27 20:30:14,866   Epoch = 1 iter 449 step
2023-03-27 20:30:14,866   Num examples = 1043
2023-03-27 20:30:14,866   Batch size = 32
2023-03-27 20:30:14,867 ***** Eval results *****
2023-03-27 20:30:14,867   att_loss = 6645768.269230769
2023-03-27 20:30:14,868   cls_loss = 0.0
2023-03-27 20:30:14,868   global_step = 449
2023-03-27 20:30:14,868   loss = 6645769.269230769
2023-03-27 20:30:14,868   rep_loss = 0.9900920106159462
2023-03-27 20:30:14,870 ***** Save model *****
           2023-03-27 20:30:30,786 ***** Running evaluation *****
2023-03-27 20:30:30,786   Epoch = 1 iter 499 step
2023-03-27 20:30:30,786   Num examples = 1043
2023-03-27 20:30:30,786   Batch size = 32
2023-03-27 20:30:30,787 ***** Eval results *****
2023-03-27 20:30:30,787   att_loss = 6609073.810344827
2023-03-27 20:30:30,787   cls_loss = 0.0
2023-03-27 20:30:30,788   global_step = 499
2023-03-27 20:30:30,788   loss = 6609074.810344827
2023-03-27 20:30:30,788   rep_loss = 0.9775924500206421
2023-03-27 20:30:30,790 ***** Save model *****
2023-03-27 20:30:46,695 ***** Running evaluation *****
2023-03-27 20:30:46,695   Epoch = 2 iter 549 step
2023-03-27 20:30:46,696   Num examples = 1043
2023-03-27 20:30:46,696   Batch size = 32
2023-03-27 20:30:46,697 ***** Eval results *****
2023-03-27 20:30:46,698   att_loss = 6229635.6
2023-03-27 20:30:46,698   cls_loss = 0.0
2023-03-27 20:30:46,698   global_step = 549
2023-03-27 20:30:46,698   loss = 6229636.6
2023-03-27 20:30:46,698   rep_loss = 0.8912290930747986
2023-03-27 20:30:46,700 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2023-03-27 20:31:02,593 ***** Running evaluation *****
2023-03-27 20:31:02,593   Epoch = 2 iter 599 step
2023-03-27 20:31:02,593   Num examples = 1043
2023-03-27 20:31:02,594   Batch size = 32
2023-03-27 20:31:02,595 ***** Eval results *****
2023-03-27 20:31:02,595   att_loss = 6386022.876923077
2023-03-27 20:31:02,595   cls_loss = 0.0
2023-03-27 20:31:02,595   global_step = 599
2023-03-27 20:31:02,596   loss = 6386023.876923077
2023-03-27 20:31:02,596   rep_loss = 0.8927238812813392
2023-03-27 20:31:02,602 ***** Save model *****
2023-03-27 20:31:18,518 ***** Running evaluation *****
2023-03-27 20:31:18,518   Epoch = 2 iter 649 step
2023-03-27 20:31:18,519   Num examples = 1043
2023-03-27 20:31:18,519   Batch size = 32
2023-03-27 20:31:18,520 ***** Eval results *****
2023-03-27 20:31:18,520   att_loss = 6368222.195652174
2023-03-27 20:31:18,520   cls_loss = 0.0
2023-03-27 20:31:18,520   global_step = 649
2023-03-27 20:31:18,521   loss = 6368223.195652174
2023-03-27 20:31:18,521   rep_loss = 0.8834083085474761
2023-03-27 20:31:18,527 ***** Save model *****
del *****
2023-03-27 20:31:18,516 ***** Running evaluation *****
2023-03-27 20:31:18,516   Epoch = 24 iter 6549 step
2023-03-27 20:31:18,516   Num examples = 1043
2023-03-27 20:31:18,516   Batch size = 32
2023-03-27 20:31:18,518 ***** Eval results *****
2023-03-27 20:31:18,518   att_loss = 0.06244056029839719
2023-03-27 20:31:18,518   cls_loss = 0.0
2023-03-27 20:31:18,518   global_step = 6549
2023-03-27 20:31:18,519   loss = 0.5988686925975989
2023-03-27 20:31:18,519   rep_loss = 0.5364281349148311
2023-03-27 20:31:18,521 ***** Save model *****
2023-03-27 20:31:29,543 ***** Running evaluation *****
2023-03-27 20:31:29,543   Epoch = 24 iter 6599 step
2023-03-27 20:31:29,544   Num examples = 1043
2023-03-27 20:31:29,544   Batch size = 32
2023-03-27 20:31:29,547 ***** Eval results *****
2023-03-27 20:31:29,547   att_loss = 0.06264134129772636
2023-03-27 20:31:29,547   cls_loss = 0.0
2023-03-27 20:31:29,548   global_step = 6599
2023-03-27 20:31:29,548   loss = 0.5996227342420848
2023-03-27 20:31:29,548   rep_loss = 0.536981395713946
2023-03-27 20:31:29,551 ***** Save model *****
2023-03-27 20:31:40,544 ***** Running evaluation *****
2023-03-27 20:31:40,544   Epoch = 24 iter 6649 step
2023-03-27 20:31:40,544   Num examples = 1043
2023-03-27 20:31:40,545   Batch size = 32
2023-03-27 20:31:40,546 ***** Eval results *****
2023-03-27 20:31:40,547   att_loss = 0.06269260058450006
2023-03-27 20:31:40,547   cls_loss = 0.0
2023-03-27 20:31:40,547   global_step = 6649
2023-03-27 20:31:40,547   loss = 0.5995410532377567
2023-03-27 20:31:40,547   rep_loss = 0.5368484543072237
2023-03-27 20:31:40,550 ***** Save model *****
2023-03-27 20:31:51,543 ***** Running evaluation *****
2023-03-27 20:31:51,543   Epoch = 25 iter 6699 step
2023-03-27 20:31:51,544   Num examples = 1043
2023-03-27 20:31:51,544   Batch size = 32
2023-03-27 20:31:51,548 ***** Eval results *****
2023-03-27 20:31:51,548   att_loss = 0.06219487497583032
2023-03-27 20:31:51,548   cls_loss = 0.0
2023-03-27 20:31:51,549   global_step = 6699
2023-03-27 20:31:51,549   loss = 0.5989450464646021
2023-03-27 20:31:51,549   rep_loss = 0.5367501750588417
2023-03-27 20:31:51,552 ***** Save model *****
2023-03-27 20:32:02,528 ***** Running evaluation *****
2023-03-27 20:32:02,528   Epoch = 25 iter 6749 step
2023-03-27 20:32:02,528   Num examples = 1043
2023-03-27 20:32:02,528   Batch size = 32
2023-03-27 20:32:02,530 ***** Eval results *****
2023-03-27 20:32:02,530   att_loss = 0.06244719582232269
2023-03-27 20:32:02,530   cls_loss = 0.0
2023-03-27 20:32:02,531   global_step = 6749
2023-03-27 20:32:02,531   loss = 0.599640158382622
2023-03-27 20:32:02,531   rep_loss = 0.5371929632650839
2023-03-27 20:32:02,538 ***** Save model *****
2023-03-27 20:32:13,499 ***** Running evaluation *****
2023-03-27 20:32:13,499   Epoch = 25 iter 6799 step
2023-03-27 20:32:13,499   Num examples = 1043
2023-03-27 20:32:13,499   Batch size = 32
2023-03-27 20:32:13,503 ***** Eval results *****
2023-03-27 20:32:13,503   att_loss = 0.06242965456218489
2023-03-27 20:32:13,503   cls_loss = 0.0
2023-03-27 20:32:13,504   global_step = 6799
2023-03-27 20:32:13,504   loss = 0.5982646927718194
2023-03-27 20:32:13,504   rep_loss = 0.535835039231085
2023-03-27 20:32:13,510 ***** Save model *****
2023-03-27 20:32:24,478 ***** Running evaluation *****
2023-03-27 20:32:24,479   Epoch = 25 iter 6849 step
2023-03-27 20:32:24,479   Num examples = 1043
2023-03-27 20:32:24,479   Batch size = 32
2023-03-27 20:32:24,480 ***** Eval results *****
2023-03-27 20:32:24,481   att_loss = 0.06244504098491422
2023-03-27 20:32:24,481   cls_loss = 0.0
2023-03-27 20:32:24,481   global_step = 6849
2023-03-27 20:32:24,481   loss = 0.5981350921351334
2023-03-27 20:32:24,482   rep_loss = 0.5356900517282814
2023-03-27 20:32:24,489 ***** Save model *****
2023-03-27 20:32:38,156 ***** Running evaluation *****
2023-03-27 20:32:38,157   Epoch = 3 iter 899 step
2023-03-27 20:32:38,157   Num examples = 1043
2023-03-27 20:32:38,157   Batch size = 32
2023-03-27 20:32:38,159 ***** Eval results *****
2023-03-27 20:32:38,159   att_loss = 6197056.882653061
2023-03-27 20:32:38,160   cls_loss = 0.0
2023-03-27 20:32:38,160   global_step = 899
2023-03-27 20:32:38,160   loss = 6197057.882653061
2023-03-27 20:32:38,160   rep_loss = 0.8164106169525458
2023-03-27 20:32:38,168 ***** Save model *****
2023-03-27 20:32:54,057 ***** Running evaluation *****
2023-03-27 20:32:54,057   Epoch = 3 iter 949 step
2023-03-27 20:32:54,057   Num examples = 1043
2023-03-27 20:32:54,057   Batch size = 32
2023-03-27 20:32:54,059 ***** Eval results *****
2023-03-27 20:32:54,060   att_loss = 6221389.14527027
2023-03-27 20:32:54,060   cls_loss = 0.0
2023-03-27 20:32:54,060   global_step = 949
2023-03-27 20:32:54,060   loss = 6221390.14527027
2023-03-27 20:32:54,060   rep_loss = 0.8136629570980329
2023-03-27 20:32:54,067 ***** Save model *****
             2023-03-27 20:33:09,955 ***** Running evaluation *****
2023-03-27 20:33:09,955   Epoch = 3 iter 999 step
2023-03-27 20:33:09,955   Num examples = 1043
2023-03-27 20:33:09,955   Batch size = 32
2023-03-27 20:33:09,957 ***** Eval results *****
2023-03-27 20:33:09,957   att_loss = 6223054.1338383835
2023-03-27 20:33:09,957   cls_loss = 0.0
2023-03-27 20:33:09,957   global_step = 999
2023-03-27 20:33:09,958   loss = 6223055.1338383835
2023-03-27 20:33:09,958   rep_loss = 0.8092799466667753
2023-03-27 20:33:09,960 ***** Save model *****
2023-03-27 20:33:25,884 ***** Running evaluation *****
2023-03-27 20:33:25,884   Epoch = 3 iter 1049 step
2023-03-27 20:33:25,884   Num examples = 1043
2023-03-27 20:33:25,884   Batch size = 32
2023-03-27 20:33:25,885 ***** Eval results *****
2023-03-27 20:33:25,886   att_loss = 6227096.895161291
2023-03-27 20:33:25,886   cls_loss = 0.0
2023-03-27 20:33:25,886   global_step = 1049
2023-03-27 20:33:25,886   loss = 6227097.893145162
2023-03-27 20:33:25,886   rep_loss = 0.8048880674665974
2023-03-27 20:33:25,893 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      2023-03-27 20:33:41,794 ***** Running evaluation *****
2023-03-27 20:33:41,794   Epoch = 4 iter 1099 step
2023-03-27 20:33:41,794   Num examples = 1043
2023-03-27 20:33:41,794   Batch size = 32
2023-03-27 20:33:41,796 ***** Eval results *****
2023-03-27 20:33:41,796   att_loss = 6232949.725806451
2023-03-27 20:33:41,796   cls_loss = 0.0
2023-03-27 20:33:41,796   global_step = 1099
2023-03-27 20:33:41,797   loss = 6232950.70967742
2023-03-27 20:33:41,797   rep_loss = 0.7795228054446559
2023-03-27 20:33:41,799 ***** Save model *****
2023-03-27 20:33:57,725 ***** Running evaluation *****
2023-03-27 20:33:57,725   Epoch = 4 iter 1149 step
2023-03-27 20:33:57,725   Num examples = 1043
2023-03-27 20:33:57,726   Batch size = 32
2023-03-27 20:33:57,727 ***** Eval results *****
2023-03-27 20:33:57,727   att_loss = 6191505.209876543
2023-03-27 20:33:57,727   cls_loss = 0.0
2023-03-27 20:33:57,728   global_step = 1149
2023-03-27 20:33:57,728   loss = 6191506.179012346
2023-03-27 20:33:57,728   rep_loss = 0.776390775486275
2023-03-27 20:33:57,735 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2023-03-27 20:34:13,651 ***** Running evaluation *****
2023-03-27 20:34:13,651   Epoch = 4 iter 1199 step
2023-03-27 20:34:13,652   Num examples = 1043
2023-03-27 20:34:13,652   Batch size = 32
2023-03-27 20:34:13,655 ***** Eval results *****
2023-03-27 20:34:13,655   att_loss = 6164221.721374046
2023-03-27 20:34:13,656   cls_loss = 0.0
2023-03-27 20:34:13,656   global_step = 1199
2023-03-27 20:34:13,656   loss = 6164222.652671755
2023-03-27 20:34:13,656   rep_loss = 0.7712424383818648
2023-03-27 20:34:13,658 ***** Save model *****
2023-03-27 20:34:29,555 ***** Running evaluation *****
2023-03-27 20:34:29,555   Epoch = 4 iter 1249 step
2023-03-27 20:34:29,555   Num examples = 1043
2023-03-27 20:34:29,555   Batch size = 32
2023-03-27 20:34:29,556 ***** Eval results *****
2023-03-27 20:34:29,556   att_loss = 6144026.690607735
2023-03-27 20:34:29,556   cls_loss = 0.0
2023-03-27 20:34:29,556   global_step = 1249
2023-03-27 20:34:29,557   loss = 6144027.585635359
2023-03-27 20:34:29,557   rep_loss = 0.7670526402431298
2023-03-27 20:34:29,558 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       2023-03-27 20:34:45,462 ***** Running evaluation *****
2023-03-27 20:34:45,463   Epoch = 4 iter 1299 step
2023-03-27 20:34:45,463   Num examples = 1043
2023-03-27 20:34:45,464   Batch size = 32
2023-03-27 20:34:45,465 ***** Eval results *****
2023-03-27 20:34:45,465   att_loss = 6151419.233766234
2023-03-27 20:34:45,465   cls_loss = 0.0
2023-03-27 20:34:45,466   global_step = 1299
2023-03-27 20:34:45,466   loss = 6151420.106060606
2023-03-27 20:34:45,466   rep_loss = 0.7644575777507964
2023-03-27 20:34:45,467 ***** Save model *****
2023-03-27 20:35:01,378 ***** Running evaluation *****
2023-03-27 20:35:01,379   Epoch = 5 iter 1349 step
2023-03-27 20:35:01,379   Num examples = 1043
2023-03-27 20:35:01,379   Batch size = 32
2023-03-27 20:35:01,380 ***** Eval results *****
2023-03-27 20:35:01,380   att_loss = 6131093.892857143
2023-03-27 20:35:01,380   cls_loss = 0.0
2023-03-27 20:35:01,380   global_step = 1349
2023-03-27 20:35:01,381   loss = 6131094.571428572
2023-03-27 20:35:01,381   rep_loss = 0.7445162492138999
2023-03-27 20:35:01,382 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       2023-03-27 20:35:17,285 ***** Running evaluation *****
2023-03-27 20:35:17,285   Epoch = 5 iter 1399 step
2023-03-27 20:35:17,286   Num examples = 1043
2023-03-27 20:35:17,286   Batch size = 32
2023-03-27 20:35:17,287 ***** Eval results *****
2023-03-27 20:35:17,287   att_loss = 6091730.4375
2023-03-27 20:35:17,287   cls_loss = 0.0
2023-03-27 20:35:17,287   global_step = 1399
2023-03-27 20:35:17,288   loss = 6091731.0859375
2023-03-27 20:35:17,288   rep_loss = 0.7443858087062836
2023-03-27 20:35:17,293 ***** Save model *****
2023-03-27 20:35:33,187 ***** Running evaluation *****
2023-03-27 20:35:33,188   Epoch = 5 iter 1449 step
2023-03-27 20:35:33,188   Num examples = 1043
2023-03-27 20:35:33,188   Batch size = 32
2023-03-27 20:35:33,190 ***** Eval results *****
2023-03-27 20:35:33,191   att_loss = 6075497.206140351
2023-03-27 20:35:33,191   cls_loss = 0.0
2023-03-27 20:35:33,191   global_step = 1449
2023-03-27 20:35:33,191   loss = 6075497.868421053
2023-03-27 20:35:33,192   rep_loss = 0.7421848601416537
2023-03-27 20:35:33,194 ***** Save model *****
                2023-03-27 20:35:49,090 ***** Running evaluation *****
2023-03-27 20:35:49,091   Epoch = 5 iter 1499 step
2023-03-27 20:35:49,091   Num examples = 1043
2023-03-27 20:35:49,091   Batch size = 32
2023-03-27 20:35:49,092 ***** Eval results *****
2023-03-27 20:35:49,092   att_loss = 6078840.320121951
2023-03-27 20:35:49,093   cls_loss = 0.0
2023-03-27 20:35:49,093   global_step = 1499
2023-03-27 20:35:49,093   loss = 6078840.957317073
2023-03-27 20:35:49,093   rep_loss = 0.7397081888303524
2023-03-27 20:35:49,100 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2023-03-27 20:36:05,043 ***** Running evaluation *****
2023-03-27 20:36:05,044   Epoch = 5 iter 1549 step
2023-03-27 20:36:05,044   Num examples = 1043
2023-03-27 20:36:05,044   Batch size = 32
2023-03-27 20:36:05,046 ***** Eval results *****
2023-03-27 20:36:05,046   att_loss = 6074558.289719626
2023-03-27 20:36:05,046   cls_loss = 0.0
2023-03-27 20:36:05,046   global_step = 1549
2023-03-27 20:36:05,047   loss = 6074558.908878504
2023-03-27 20:36:05,047   rep_loss = 0.737111648387998
2023-03-27 20:36:05,049 ***** Save model *****
2023-03-27 20:36:20,959 ***** Running evaluation *****
2023-03-27 20:36:20,960   Epoch = 5 iter 1599 step
2023-03-27 20:36:20,960   Num examples = 1043
2023-03-27 20:36:20,960   Batch size = 32
2023-03-27 20:36:20,961 ***** Eval results *****
2023-03-27 20:36:20,962   att_loss = 6071087.333333333
2023-03-27 20:36:20,962   cls_loss = 0.0
2023-03-27 20:36:20,962   global_step = 1599
2023-03-27 20:36:20,963   loss = 6071087.9375
2023-03-27 20:36:20,963   rep_loss = 0.7351166739156751
2023-03-27 20:36:20,970 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2023-03-27 20:36:36,905 ***** Running evaluation *****
2023-03-27 20:36:36,905   Epoch = 6 iter 1649 step
2023-03-27 20:36:36,906   Num examples = 1043
2023-03-27 20:36:36,906   Batch size = 32
2023-03-27 20:36:36,907 ***** Eval results *****
2023-03-27 20:36:36,907   att_loss = 5978689.872340426
2023-03-27 20:36:36,907   cls_loss = 0.0
2023-03-27 20:36:36,907   global_step = 1649
2023-03-27 20:36:36,907   loss = 5978690.393617021
2023-03-27 20:36:36,907   rep_loss = 0.7182075736370492
2023-03-27 20:36:36,910 ***** Save model *****
2023-03-27 20:36:52,856 ***** Running evaluation *****
2023-03-27 20:36:52,856   Epoch = 6 iter 1699 step
2023-03-27 20:36:52,856   Num examples = 1043
2023-03-27 20:36:52,856   Batch size = 32
2023-03-27 20:36:52,857 ***** Eval results *****
2023-03-27 20:36:52,858   att_loss = 6002773.0051546395
2023-03-27 20:36:52,858   cls_loss = 0.0
2023-03-27 20:36:52,858   global_step = 1699
2023-03-27 20:36:52,860   loss = 6002773.520618557
2023-03-27 20:36:52,860   rep_loss = 0.7192459677912525
2023-03-27 20:36:52,867 ***** Save model *****
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:36:53,178 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:36:53,178 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:36:53,179 label: 1
2023-03-27 20:36:53,179 label_id: 1
2023-03-27 20:36:54,135 Writing example 0 of 1043
2023-03-27 20:36:54,135 *** Example ***
2023-03-27 20:36:54,136 guid: dev-0
2023-03-27 20:36:54,136 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 20:36:54,136 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:36:54,136 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:36:54,136 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 20:36:54,136 label: 1
2023-03-27 20:36:54,136 label_id: 1
2023-03-27 20:36:54,258 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 20:36:54,264 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 20:36:55,881 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 20:36:56,755 loading model...
2023-03-27 20:36:56,806 done!
2023-03-27 20:36:56,806 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 20:36:56,871 loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
2023-03-27 20:36:56,874 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 20:36:57,100 Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
2023-03-27 20:36:57,261 loading model...
2023-03-27 20:36:57,274 done!
2023-03-27 20:36:57,275 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-03-27 20:36:57,276 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2023-03-27 20:36:57,292 ***** Running training *****
2023-03-27 20:36:57,293   Num examples = 8551
2023-03-27 20:36:57,293   Batch size = 32
2023-03-27 20:36:57,294   Num steps = 8010
2023-03-27 20:36:57,294 n: bert.embeddings.word_embeddings.weight
2023-03-27 20:36:57,295 n: bert.embeddings.position_embeddings.weight
2023-03-27 20:36:57,295 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 20:36:57,295 n: bert.embeddings.LayerNorm.weight
2023-03-27 20:36:57,296 n: bert.embeddings.LayerNorm.bias
2023-03-27 20:36:57,296 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 20:36:57,297 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 20:36:57,297 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 20:36:57,297 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 20:36:57,297 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 20:36:57,298 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 20:36:57,298 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 20:36:57,298 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 20:36:57,298 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 20:36:57,298 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 20:36:57,298 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 20:36:57,298 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 20:36:57,299 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 20:36:57,299 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 20:36:57,299 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 20:36:57,299 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 20:36:57,299 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 20:36:57,300 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 20:36:57,300 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 20:36:57,300 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 20:36:57,300 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 20:36:57,300 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 20:36:57,300 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 20:36:57,300 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 20:36:57,301 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 20:36:57,301 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 20:36:57,301 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 20:36:57,301 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 20:36:57,301 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 20:36:57,301 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 20:36:57,301 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 20:36:57,301 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 20:36:57,302 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 20:36:57,302 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 20:36:57,302 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 20:36:57,302 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 20:36:57,302 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 20:36:57,302 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 20:36:57,302 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 20:36:57,302 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 20:36:57,302 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 20:36:57,303 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 20:36:57,303 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 20:36:57,303 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 20:36:57,303 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 20:36:57,303 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 20:36:57,303 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 20:36:57,303 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 20:36:57,303 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 20:36:57,303 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 20:36:57,304 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 20:36:57,304 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 20:36:57,304 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 20:36:57,304 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 20:36:57,304 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 20:36:57,304 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 20:36:57,304 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 20:36:57,304 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 20:36:57,305 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 20:36:57,305 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 20:36:57,305 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 20:36:57,305 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 20:36:57,305 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 20:36:57,305 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 20:36:57,305 n: bert.pooler.dense.weight
2023-03-27 20:36:57,305 n: bert.pooler.dense.bias
2023-03-27 20:36:57,305 n: classifier.weight
2023-03-27 20:36:57,306 n: classifier.bias
2023-03-27 20:36:57,306 n: fit_dense.weight
2023-03-27 20:36:57,306 n: fit_dense.bias
2023-03-27 20:36:57,306 Total parameters: 14591258
2023-03-27 20:37:06,947 ***** Running evaluation *****
2023-03-27 20:37:06,947   Epoch = 0 iter 49 step
2023-03-27 20:37:06,947   Num examples = 1043
2023-03-27 20:37:06,947   Batch size = 32
2023-03-27 20:37:06,955 ***** Eval results *****
2023-03-27 20:37:06,955   att_loss = 0.5624887608751958
2023-03-27 20:37:06,955   cls_loss = 0.0
2023-03-27 20:37:06,956   global_step = 49
2023-03-27 20:37:06,956   loss = 1.885422227334003
2023-03-27 20:37:06,956   rep_loss = 1.3229334549028047
2023-03-27 20:37:06,959 ***** Save model *****
2023-03-27 20:37:17,599 ***** Running evaluation *****
2023-03-27 20:37:17,599   Epoch = 0 iter 99 step
2023-03-27 20:37:17,599   Num examples = 1043
2023-03-27 20:37:17,600   Batch size = 32
2023-03-27 20:37:17,604 ***** Eval results *****
2023-03-27 20:37:17,604   att_loss = 0.5208015110757616
2023-03-27 20:37:17,605   cls_loss = 0.0
2023-03-27 20:37:17,605   global_step = 99
2023-03-27 20:37:17,605   loss = 1.625842166669441
2023-03-27 20:37:17,605   rep_loss = 1.1050406513792095
2023-03-27 20:37:17,613 ***** Save model *****
2023-03-27 20:37:28,266 ***** Running evaluation *****
2023-03-27 20:37:28,267   Epoch = 0 iter 149 step
2023-03-27 20:37:28,267   Num examples = 1043
2023-03-27 20:37:28,267   Batch size = 32
2023-03-27 20:37:28,269 ***** Eval results *****
2023-03-27 20:37:28,269   att_loss = 0.4964984979405499
2023-03-27 20:37:28,270   cls_loss = 0.0
2023-03-27 20:37:28,270   global_step = 149
2023-03-27 20:37:28,270   loss = 1.5029620956254486
2023-03-27 20:37:28,270   rep_loss = 1.0064635928845245
2023-03-27 20:37:28,273 ***** Save model *****
2023-03-27 20:37:41,214 ***** Running evaluation *****
2023-03-27 20:37:41,215   Epoch = 6 iter 1849 step
2023-03-27 20:37:41,215   Num examples = 1043
2023-03-27 20:37:41,215   Batch size = 32
2023-03-27 20:37:41,216 ***** Eval results *****
2023-03-27 20:37:41,216   att_loss = 6006501.995951417
2023-03-27 20:37:41,216   cls_loss = 0.0
2023-03-27 20:37:41,216   global_step = 1849
2023-03-27 20:37:41,217   loss = 6006502.506072874
2023-03-27 20:37:41,217   rep_loss = 0.7144948055869654
2023-03-27 20:37:41,218 ***** Save model *****
2023-03-27 20:37:57,154 ***** Running evaluation *****
2023-03-27 20:37:57,154   Epoch = 7 iter 1899 step
2023-03-27 20:37:57,155   Num examples = 1043
2023-03-27 20:37:57,155   Batch size = 32
2023-03-27 20:37:57,156 ***** Eval results *****
2023-03-27 20:37:57,156   att_loss = 5890957.916666667
2023-03-27 20:37:57,156   cls_loss = 0.0
2023-03-27 20:37:57,156   global_step = 1899
2023-03-27 20:37:57,156   loss = 5890958.416666667
2023-03-27 20:37:57,156   rep_loss = 0.6987526973088583
2023-03-27 20:37:57,158 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2023-03-27 20:38:13,397 ***** Running evaluation *****
2023-03-27 20:38:13,397   Epoch = 7 iter 1949 step
2023-03-27 20:38:13,398   Num examples = 1043
2023-03-27 20:38:13,398   Batch size = 32
2023-03-27 20:38:13,399 ***** Eval results *****
2023-03-27 20:38:13,400   att_loss = 5938219.8625
2023-03-27 20:38:13,400   cls_loss = 0.0
2023-03-27 20:38:13,400   global_step = 1949
2023-03-27 20:38:13,400   loss = 5938220.3625
2023-03-27 20:38:13,400   rep_loss = 0.6995260484516621
2023-03-27 20:38:13,402 ***** Save model *****
2023-03-27 20:38:29,347 ***** Running evaluation *****
2023-03-27 20:38:29,347   Epoch = 7 iter 1999 step
2023-03-27 20:38:29,347   Num examples = 1043
2023-03-27 20:38:29,347   Batch size = 32
2023-03-27 20:38:29,348 ***** Eval results *****
2023-03-27 20:38:29,349   att_loss = 5963562.684615385
2023-03-27 20:38:29,349   cls_loss = 0.0
2023-03-27 20:38:29,349   global_step = 1999
2023-03-27 20:38:29,349   loss = 5963563.184615385
2023-03-27 20:38:29,350   rep_loss = 0.7000927008115329
2023-03-27 20:38:29,354 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      2023-03-27 20:38:45,254 ***** Running evaluation *****
2023-03-27 20:38:45,255   Epoch = 7 iter 2049 step
2023-03-27 20:38:45,255   Num examples = 1043
2023-03-27 20:38:45,255   Batch size = 32
2023-03-27 20:38:45,256 ***** Eval results *****
2023-03-27 20:38:45,256   att_loss = 5970694.780555556
2023-03-27 20:38:45,256   cls_loss = 0.0
2023-03-27 20:38:45,257   global_step = 2049
2023-03-27 20:38:45,257   loss = 5970695.280555556
2023-03-27 20:38:45,257   rep_loss = 0.7003639909956191
2023-03-27 20:38:45,262 ***** Save model *****
2023-03-27 20:39:01,183 ***** Running evaluation *****
2023-03-27 20:39:01,184   Epoch = 7 iter 2099 step
2023-03-27 20:39:01,184   Num examples = 1043
2023-03-27 20:39:01,184   Batch size = 32
2023-03-27 20:39:01,185 ***** Eval results *****
2023-03-27 20:39:01,186   att_loss = 5968826.397826087
2023-03-27 20:39:01,186   cls_loss = 0.0
2023-03-27 20:39:01,186   global_step = 2099
2023-03-27 20:39:01,186   loss = 5968826.897826087
2023-03-27 20:398:55,245   rep_loss = 0.6724788268407186
2023-03-27 20:38:55,247 ***** Save model *****
2023-03-27 20:39:06,165 ***** Running evaluation *****
2023-03-27 20:39:06,165   Epoch = 2 iter 599 step
2023-03-27 20:39:06,165   Num examples = 1043
2023-03-27 20:39:06,165   Batch size = 32
2023-03-27 20:39:06,168 ***** Eval results *****
2023-03-27 20:39:06,168   att_loss = 0.41310284366974465
2023-03-27 20:39:06,168   cls_loss = 0.0
2023-03-27 20:39:06,169   global_step = 599
2023-03-27 20:39:06,169   loss = 1.0961717954048744
2023-03-27 20:39:06,169   rep_loss = 0.6830689430236816
2023-03-27 20:39:06,175 ***** Save model *****
2023-03-27 20:39:17,086 ***** Running evaluation *****
2023-03-27 20:39:17,087   Epoch = 2 iter 649 step
2023-03-27 20:39:17,087   Num examples = 1043
2023-03-27 20:39:17,087   Batch size = 32
2023-03-27 20:39:17,089 ***** Eval results *****
2023-03-27 20:39:17,089   att_loss = 0.40927167768063755
2023-03-27 20:39:17,089   cls_loss = 0.0
2023-03-27 20:39:17,089   global_step = 649
2023-03-27 20:39:17,089   loss = 1.0888966332311216
2023-03-27 20:39:17,090   rep_loss = 0.6796249524406764
2023-03-27 20:39:17,096 ***** Save model *****
2023-03-27 20:39:28,019 ***** Running evaluation *****
2023-03-27 20:39:28,019   Epoch = 2 iter 699 step
2023-03-27 20:39:28,019   Num examples = 1043
2023-03-27 20:39:28,019   Batch size = 32
2023-03-27 20:39:28,023 ***** Eval results *****
2023-03-27 20:39:28,023   att_loss = 0.4119332958351482
2023-03-27 20:39:28,023   cls_loss = 0.0
2023-03-27 20:39:28,023   global_step = 699
2023-03-27 20:39:28,024   loss = 1.0910217075636892
2023-03-27 20:39:28,024   rep_loss = 0.6790884101029598
2023-03-27 20:39:28,026 ***** Save model *****
2023-03-27 20:39:38,903 ***** Running evaluation *****
2023-03-27 20:39:38,903   Epoch = 2 iter 749 step
2023-03-27 20:39:38,903   Num examples = 1043
2023-03-27 20:39:38,904   Batch size = 32
2023-03-27 20:39:38,905 ***** Eval results *****
2023-03-27 20:39:38,905   att_loss = 0.4134378297384395
2023-03-27 20:39:38,905   cls_loss = 0.0
2023-03-27 20:39:38,906   global_step = 749
2023-03-27 20:39:38,906   loss = 1.0912425207537273
2023-03-27 20:39:38,906   rep_loss = 0.6778046899063642
2023-03-27 20:39:38,910 ***** Save model *****
2023-03-27 20:39:49,750 ***** Running evaluation *****
2023-03-27 20:39:49,750   Epoch = 2 iter 799 step
2023-03-27 20:39:49,750   Num examples = 1043
2023-03-27 20:39:49,750   Batch size = 32
2023-03-27 20:39:49,752 ***** Eval results *****
2023-03-27 20:39:49,753   att_loss = 0.4134491615700272
2023-03-27 20:39:49,753   cls_loss = 0.0
2023-03-27 20:39:49,753   global_step = 799
2023-03-27 20:39:49,753   loss = 1.0893800404836547
2023-03-27 20:39:49,754   rep_loss = 0.6759308770017803
2023-03-27 20:39:49,755 ***** Save model *****
2023-03-27 20:40:00,590 ***** Running evaluation *****
2023-03-27 20:40:00,591   Epoch = 3 iter 849 step
2023-03-27 20:40:00,591   Num examples = 1043
2023-03-27 20:40:00,591   Batch size = 32
2023-03-27 20:40:00,592 ***** Eval results *****
2023-03-27 20:40:00,592   att_loss = 0.3948137064774831
2023-03-27 20:40:00,592   cls_loss = 0.0
2023-03-27 20:40:00,592   global_step = 849
2023-03-27 20:40:00,592   loss = 1.0505981830259163
2023-03-27 20:40:00,593   rep_loss = 0.6557844715813795
2023-03-27 20:40:00,599 ***** Save model *****
2023-03-27 20:40:11,433 ***** Running evaluation *****
2023-03-27 20:40:11,434   Epoch = 3 iter 899 step
2023-03-27 20:40:11,434   Num examples = 1043
2023-03-27 20:40:11,434   Batch size = 32
2023-03-27 20:40:11,436 ***** Eval results *****
2023-03-27 20:40:11,437   att_loss = 0.40008256964537564
2023-03-27 20:40:11,437   cls_loss = 0.0
2023-03-27 20:40:11,437   global_step = 899
2023-03-27 20:40:11,438   loss = 1.0546442020912559
2023-03-27 20:40:11,438   rep_loss = 0.6545616345746177
2023-03-27 20:40:11,443 ***** Save model *****
2023-03-27 20:40:22,262 ***** Running evaluation *****
2023-03-27 20:40:22,262   Epoch = 3 iter 949 step
2023-03-27 20:40:22,262   Num examples = 1043
2023-03-27 20:40:22,262   Batch size = 32
2023-03-27 20:40:22,263 ***** Eval results *****
2023-03-27 20:40:36,724   att_loss = 5902264.169201521
2023-03-27 20:40:36,724   cls_loss = 0.0
2023-03-27 20:40:36,724   global_step = 2399
2023-03-27 20:40:36,725   loss = 5902264.669201521
2023-03-27 20:40:36,725   rep_loss = 0.6852334540606452
2023-03-27 20:40:36,731 ***** Save model *****
2023-03-27 20:40:52,659 ***** Running evaluation *****
2023-03-27 20:40:52,660   Epoch = 9 iter 2449 step
2023-03-27 20:40:52,660   Num examples = 1043
2023-03-27 20:40:52,660   Batch size = 32
2023-03-27 20:40:52,661 ***** Eval results *****
2023-03-27 20:40:52,661   att_loss = 5830348.826086956
2023-03-27 20:40:52,661   cls_loss = 0.0
2023-03-27 20:40:52,661   global_step = 2449
2023-03-27 20:40:52,661   loss = 5830349.326086956
2023-03-27 20:40:52,661   rep_loss = 0.6776974304862644
2023-03-27 20:40:52,668 ***** Save model *****
2023-03-27 20:41:08,573 ***** Running evaluation *****
2023-03-27 20:41:08,573   Epoch = 9 iter 2499 step
2023-03-27 20:41:08,574   Num examples = 1043
2023-03-27 20:41:08,574   Batch size = 32
2023-03-27 20:41:08,575 ***** Eval results *****
2023-03-27 20:41:08,576   att_loss = 5875589.192708333
2023-03-27 20:41:08,576   cls_loss = 0.0
2023-03-27 20:41:08,576   global_step = 2499
2023-03-27 20:41:08,576   loss = 5875589.692708333
2023-03-27 20:41:08,577   rep_loss = 0.6772567002723614
2023-03-27 20:41:08,584 ***** Save model *****
2023-03-27 20:41:24,471 ***** Running evaluation *****
2023-03-27 20:41:24,472   Epoch = 9 iter 2549 step
2023-03-27 20:41:24,472   Num examples = 1043
2023-03-27 20:41:24,472   Batch size = 32
2023-03-27 20:41:24,473 ***** Eval results *****
2023-03-27 20:41:24,474   att_loss = 5870948.2397260275
2023-03-27 20:41:24,474   cls_loss = 0.0
2023-03-27 20:41:24,474   global_step = 2549
2023-03-27 20:41:24,474   loss = 5870948.7397260275
2023-03-27 20:41:24,474   rep_loss = 0.6764124938069958
2023-03-27 20:41:24,480 ***** Save model *****
2023-03-27 20:41:40,374 ***** Running evaluation *****
2023-03-27 20:41:40,375   Epoch = 9 iter 2599 step
2023-03-27 20:41:40,375   Num examples = 1043
2023-03-27 20:41:40,375   Batch size = 32
2023-03-27 20:41:40,376 ***** Eval results *****
2023-03-27 20:41:40,377   att_loss = 5877756.484693877
2023-03-27 20:41:40,377   cls_loss = 0.0
2023-03-27 20:41:40,377   global_step = 2599
2023-03-27 20:41:40,377   loss = 5877756.984693877
2023-03-27 20:41:40,377   rep_loss = 0.6756538894711709
2023-03-27 20:41:40,385 ***** Save model *****
 2023-03-27 20:41:56,288 ***** Running evaluation *****
2023-03-27 20:41:56,288   Epoch = 9 iter 2649 step
2023-03-27 20:41:56,288   Num examples = 1043
2023-03-27 20:41:56,289   Batch size = 32
2023-03-27 20:41:56,290 ***** Eval results *****
2023-03-27 20:41:56,291   att_loss = 5865592.215447155
2023-03-27 20:41:56,291   cls_loss = 0.0
2023-03-27 20:41:56,291   global_step = 2649
2023-03-27 20:41:56,292   loss = 5865592.715447155
2023-03-27 20:41:56,292   rep_loss = 0.6746945080718374
2023-03-27 20:41:56,299 ***** Save model *****
2023-03-27 20:42:12,222 ***** Running evaluation *****
2023-03-27 20:42:12,223   Epoch = 10 iter 2699 step
2023-03-27 20:42:12,223   Num examples = 1043
2023-03-27 20:42:12,223   Batch size = 32
2023-03-27 20:42:12,224 ***** Eval results *****
2023-03-27 20:42:12,224   att_loss = 5875227.827586207
2023-03-27 20:42:12,224   cls_loss = 0.0
2023-03-27 20:42:12,224   global_step = 2699
2023-03-27 20:42:12,224   loss = 5875228.327586207
2023-03-27 20:42:12,225   rep_loss = 0.6690344193886066
2023-03-27 20:42:12,231 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2023-03-27 20:42:28,150 ***** Running evaluation *****
2023-03-27 20:42:28,150   Epoch = 10 iter 2749 step
2023-03-27 20:42:28,150   Num examples = 1043
2023-03-27 20:42:28,151   Batch size = 32
2023-03-27 20:42:28,152 ***** Eval results *****
2023-03-27 20:42:28,152   att_loss = 5835101.329113924
2023-03-27 20:42:28,152   cls_loss = 0.0
2023-03-27 20:42:28,152   global_step = 2749
2023-03-27 20:42:28,152   loss = 5835101.829113924
2023-03-27 20:42:28,152   rep_loss = 0.6688782746278787
2023-03-27 20:42:28,154 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2023-03-27 20:42:44,074 ***** Running evaluation *****
2023-03-27 20:42:44,074   Epoch = 10 iter 2799 step
2023-03-27 20:42:44,074   Num examples = 1043
2023-03-27 20:42:44,074   Batch size = 32
2023-03-27 20:42:44,075 ***** Eval results *****
2023-03-27 20:42:44,075   att_loss = 5810953.5852713175
2023-03-27 20:42:44,075   cls_loss = 0.0
2023-03-27 20:42:44,076   global_step = 2799
2023-03-27 20:42:44,076   loss = 5810954.0852713175
2023-03-27 20:42:44,076   rep_loss = 0.6677605616029842
2023-03-27 20:42:44,081 ***** Save model *****
2023-03-27 20:42:59,988 ***** Running evaluation *****
2023-03-27 20:42:59,988   Epoch = 10 iter 2849 step
2023-03-27 20:42:59,989   Num examples = 1043
2023-03-27 20:42:59,989   Batch size = 32
2023-03-27 20:42:59,990 ***** Eval results *****
2023-03-27 20:42:59,990   att_loss = 5838574.48603352
2023-03-27 20:42:59,990   cls_loss = 0.0
2023-03-27 20:42:59,990   global_step = 2849
2023-03-27 20:42:59,991   loss = 5838574.98603352
2023-03-27 20:42:59,991   rep_loss = 0.6675438321502515
2023-03-27 20:42:59,993 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2023-03-27 20:43:15,884 ***** Running evaluation *****
2023-03-27 20:43:15,884   Epoch = 10 iter 2899 step
2023-03-27 20:43:15,884   Num examples = 1043
2023-03-27 20:43:15,885   Batch size = 32
2023-03-27 20:43:15,888 ***** Eval results *****
2023-03-27 20:43:15,888   att_loss = 5835601.519650655
2023-03-27 20:43:15,888   cls_loss = 0.0
2023-03-27 20:43:15,888   global_step = 2899
2023-03-27 20:43:15,888   loss = 5835602.019650655
2023-03-27 20:43:15,888   rep_loss = 0.6664280719632144
2023-03-27 20:43:15,908 ***** Save model *****
2023-03-27 20:43:31,866 ***** Running evaluation *****
2023-03-27 20:43:31,866   Epoch = 11 iter 2949 step
2023-03-27 20:43:31,866   Num examples = 1043
2023-03-27 20:43:31,866   Batch size = 32
2023-03-27 20:43:31,868 ***** Eval results *****
2023-03-27 20:43:31,868   att_loss = 5862490.666666667
2023-03-27 20:43:31,868   cls_loss = 0.0
2023-03-27 20:43:31,868   global_step = 2949
2023-03-27 20:43:31,869   loss = 5862491.166666667
2023-03-27 20:43:31,869   rep_loss = 0.6647147784630457
2023-03-27 20:43:31,875 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                2023-03-27 20:43:47,802 ***** Running evaluation *****
2023-03-27 20:43:47,802   Epoch = 11 iter 2999 step
2023-03-27 20:43:47,803   Num examples = 1043
2023-03-27 20:43:47,803   Batch size = 32
2023-03-27 20:43:47,804 ***** Eval results *****
2023-03-27 20:43:47,804   att_loss = 5800696.008064516
2023-03-27 20:43:47,804   cls_loss = 0.0
2023-03-27 20:43:47,805   global_step = 2999
2023-03-27 20:43:47,805   loss = 5800696.508064516
2023-03-27 20:43:47,805   rep_loss = 0.6595468924891564
2023-03-27 20:43:47,807 ***** Save model *****
2023-03-27 20:44:03,729 ***** Running evaluation *****
2023-03-27 20:44:03,729   Epoch = 11 iter 3049 step
2023-03-27 20:44:03,729   Num examples = 1043
2023-03-27 20:44:03,729   Batch size = 32
2023-03-27 20:44:03,730 ***** Eval results *****
2023-03-27 20:44:03,731   att_loss = 5820427.59375
2023-03-27 20:44:03,731   cls_loss = 0.0
2023-03-27 20:44:03,731   global_step = 3049
2023-03-27 20:44:03,731   loss = 5820428.09375
2023-03-27 20:44:03,731   rep_loss = 0.6605496491704669
2023-03-27 20:44:03,737 ***** Save model *****
         2023-03-27 20:44:19,669 ***** Running evaluation *****
2023-03-27 20:44:19,669   Epoch = 11 iter 3099 step
2023-03-27 20:44:19,670   Num examples = 1043
2023-03-27 20:44:19,670   Batch size = 32
2023-03-27 20:44:19,671 ***** Eval results *****
2023-03-27 20:44:19,671   att_loss = 5815077.651234568
2023-03-27 20:44:19,671   cls_loss = 0.0
2023-03-27 20:44:19,672   global_step = 3099
2023-03-27 20:44:19,672   loss = 5815078.151234568
2023-03-27 20:44:19,672   rep_loss = 0.6596678506445002
2023-03-27 20:44:19,678 ***** Save model *****
2023-03-27 20:44:35,640 ***** Running evaluation *****
2023-03-27 20:44:35,641   Epoch = 11 iter 3149 step
2023-03-27 20:44:35,641   Num examples = 1043
2023-03-27 20:44:35,641   Batch size = 32
2023-03-27 20:44:35,642 ***** Eval results *****
2023-03-27 20:44:35,643   att_loss = 5813148.120283019
2023-03-27 20:44:35,643   cls_loss = 0.0
2023-03-27 20:44:35,643   global_step = 3149
2023-03-27 20:44:35,643   loss = 5813148.620283019
2023-03-27 20:44:35,643   rep_loss = 0.6591691681236591
2023-03-27 20:44:35,645 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2023-03-27 20:44:51,581 ***** Running evaluation *****
2023-03-27 20:44:51,581   Epoch = 11 iter 3199 step
2023-03-27 20:44:51,582   Num examples = 1043
2023-03-27 20:44:51,582   Batch size = 32
2023-03-27 20:44:51,583 ***** Eval results *****
2023-03-27 20:44:51,583   att_loss = 5811061.141221374
2023-03-27 20:44:51,583   cls_loss = 0.0
2023-03-27 20:44:51,583   global_step = 3199
2023-03-27 20:44:51,583   loss = 5811061.641221374
2023-03-27 20:44:51,583   rep_loss = 0.6587372672466831
2023-03-27 20:44:51,585 ***** Save model *****
2023-03-27 20:45:07,511 ***** Running evaluation *****
2023-03-27 20:45:07,512   Epoch = 12 iter 3249 step
2023-03-27 20:45:07,512   Num examples = 1043
2023-03-27 20:45:07,512   Batch size = 32
2023-03-27 20:45:07,513 ***** Eval results *****
2023-03-27 20:45:07,513   att_loss = 5775662.033333333
2023-03-27 20:45:07,514   cls_loss = 0.0
2023-03-27 20:45:07,514   global_step = 3249
2023-03-27 20:45:07,514   loss = 5775662.533333333
2023-03-27 20:45:07,514   rep_loss = 0.6529006375206842
2023-03-27 20:45:07,521 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2023-03-27 20:45:23,444 ***** Running evaluation *****
2023-03-27 20:45:23,445   Epoch = 12 iter 3299 step
2023-03-27 20:45:23,445   Num examples = 1043
2023-03-27 20:45:23,445   Batch size = 32
2023-03-27 20:45:23,447 ***** Eval results *****
2023-03-27 20:45:23,447   att_loss = 5810105.105263158
2023-03-27 20:45:23,447   cls_loss = 0.0
2023-03-27 20:45:23,447   global_step = 3299
2023-03-27 20:45:23,447   loss = 5810105.605263158
2023-03-27 20:45:23,448   rep_loss = 0.6542133212089538
2023-03-27 20:45:23,452 ***** Save model *****
2023-03-27 20:45:39,383 ***** Running evaluation *****
2023-03-27 20:45:39,383   Epoch = 12 iter 3349 step
2023-03-27 20:45:39,383   Num examples = 1043
2023-03-27 20:45:39,384   Batch size = 32
2023-03-27 20:45:39,385 ***** Eval results *****
2023-03-27 20:45:39,385   att_loss = 5804589.4655172415
2023-03-27 20:45:39,385   cls_loss = 0.0
2023-03-27 20:45:39,385   global_step = 3349
2023-03-27 20:45:39,385   loss = 5804589.9655172415
2023-03-27 20:45:39,386   rep_loss = 0.653302997556226
2023-03-27 20:45:39,387 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2023-03-27 20:45:55,321 ***** Running evaluation *****
2023-03-27 20:45:55,321   Epoch = 12 iter 3399 step
2023-03-27 20:45:55,321   Num examples = 1043
2023-03-27 20:45:55,321   Batch size = 32
2023-03-27 20:45:55,323 ***** Eval results *****
2023-03-27 20:45:55,323   att_loss = 5790534.443589743
2023-03-27 20:45:55,323   cls_loss = 0.0
2023-03-27 20:45:55,324   global_step = 3399
2023-03-27 20:45:55,324   loss = 5790534.943589743
2023-03-27 20:45:55,324   rep_loss = 0.6530117954963293
2023-03-27 20:45:55,328 ***** Save model *****
2023-03-27 20:46:11,250 ***** Running evaluation *****
2023-03-27 20:46:11,251   Epoch = 12 iter 3449 step
2023-03-27 20:46:11,251   Num examples = 1043
2023-03-27 20:46:11,251   Batch size = 32
2023-03-27 20:46:11,252 ***** Eval results *****
2023-03-27 20:46:11,252   att_loss = 5784188.348979591
2023-03-27 20:46:11,252   cls_loss = 0.0
2023-03-27 20:46:11,252   global_step = 3449
2023-03-27 20:46:11,252   loss = 5784188.848979591
2023-03-27 20:46:11,252   rep_loss = 0.652948979942166
2023-03-27 20:46:11,258 ***** Save model *****
***
2023-03-27 20:45:46,786 ***** Running evaluation *****
2023-03-27 20:45:46,786   Epoch = 9 iter 2449 step
2023-03-27 20:45:46,787   Num examples = 1043
2023-03-27 20:45:46,787   Batch size = 32
2023-03-27 20:45:46,788 ***** Eval results *****
2023-03-27 20:45:46,788   att_loss = 0.37606995844322705
2023-03-27 20:45:46,788   cls_loss = 0.0
2023-03-27 20:45:46,789   global_step = 2449
2023-03-27 20:45:46,789   loss = 0.970309818568437
2023-03-27 20:45:46,789   rep_loss = 0.5942398607730865
2023-03-27 20:45:46,796 ***** Save model *****
2023-03-27 20:45:57,598 ***** Running evaluation *****
2023-03-27 20:45:57,598   Epoch = 9 iter 2499 step
2023-03-27 20:45:57,598   Num examples = 1043
2023-03-27 20:45:57,598   Batch size = 32
2023-03-27 20:45:57,601 ***** Eval results *****
2023-03-27 20:45:57,602   att_loss = 0.377351771419247
2023-03-27 20:45:57,602   cls_loss = 0.0
2023-03-27 20:45:57,602   global_step = 2499
2023-03-27 20:45:57,602   loss = 0.9706456822653612
2023-03-27 20:45:57,603   rep_loss = 0.5932939114669958
2023-03-27 20:45:57,607 ***** Save model *****
2023-03-27 20:46:08,422 ***** Running evaluation *****
2023-03-27 20:46:08,422   Epoch = 9 iter 2549 step
2023-03-27 20:46:08,422   Num examples = 1043
2023-03-27 20:46:08,422   Batch size = 32
2023-03-27 20:46:08,424 ***** Eval results *****
2023-03-27 20:46:08,424   att_loss = 0.37518155554386035
2023-03-27 20:46:08,424   cls_loss = 0.0
2023-03-27 20:46:08,424   global_step = 2549
2023-03-27 20:46:08,425   loss = 0.967600991872892
2023-03-27 20:46:08,425   rep_loss = 0.5924194369414081
2023-03-27 20:46:08,429 ***** Save model *****
2023-03-27 20:46:19,261 ***** Running evaluation *****
2023-03-27 20:46:19,261   Epoch = 9 iter 2599 step
2023-03-27 20:46:19,262   Num examples = 1043
2023-03-27 20:46:19,262   Batch size = 32
2023-03-27 20:46:19,264 ***** Eval results *****
2023-03-27 20:46:19,264   att_loss = 0.37535735827927685
2023-03-27 20:46:19,265   cls_loss = 0.0
2023-03-27 20:46:19,265   global_step = 2599
2023-03-27 20:46:19,265   loss = 0.9676433208645606
2023-03-27 20:46:19,265   rep_loss = 0.5922859615209152
2023-03-27 20:46:19,269 ***** Save model *****
2023-03-27 20:46:30,089 ***** Running evaluation *****
2023-03-27 20:46:30,089   Epoch = 9 iter 2649 step
2023-03-27 20:46:30,089   Num examples = 1043
2023-03-27 20:46:30,089   Batch size = 32
2023-03-27 20:46:30,091 ***** Eval results *****
2023-03-27 20:46:30,091   att_loss = 0.3735832095873065
2023-03-27 20:46:30,091   cls_loss = 0.0
2023-03-27 20:46:30,092   global_step = 2649
2023-03-27 20:46:30,092   loss = 0.9649801624984276
2023-03-27 20:46:30,092   rep_loss = 0.5913969506093157
2023-03-27 20:46:30,099 ***** Save model *****
2023-03-27 20:46:40,934 ***** Running evaluation *****
2023-03-27 20:46:40,934   Epoch = 10 iter 2699 step
2023-03-27 20:46:40,934   Num examples = 1043
2023-03-27 20:46:40,935   Batch size = 32
2023-03-27 20:46:40,937 ***** Eval results *****
2023-03-27 20:46:40,937   att_loss = 0.3778485018631508
2023-03-27 20:46:40,937   cls_loss = 0.0
2023-03-27 20:46:40,937   global_step = 2699
2023-03-27 20:46:40,937   loss = 0.9691038830526943
2023-03-27 20:46:40,937   rep_loss = 0.5912553709128807
2023-03-27 20:46:40,939 ***** Save model *****
2023-03-27 20:47:30,919 ***** Running evaluation *****
2023-03-27 20:47:30,919   Epoch = 13 iter 3699 step
2023-03-27 20:47:30,919   Num examples = 1043
2023-03-27 20:47:30,919   Batch size = 32
2023-03-27 20:47:30,921 ***** Eval results *****
2023-03-27 20:47:30,921   att_loss = 5744541.10745614
2023-03-27 20:47:30,922   cls_loss = 0.0
2023-03-27 20:47:30,922   global_step = 3699
2023-03-27 20:47:30,922   loss = 5744541.60745614
2023-03-27 20:47:30,922   rep_loss = 0.6454631950248751
2023-03-27 20:47:30,926 ***** Save model *****
2023-03-27 20:47:46,864 ***** Running evaluation *****
2023-03-27 20:47:46,865   Epoch = 14 iter 3749 step
2023-03-27 20:47:46,865   Num examples = 1043
2023-03-27 20:47:46,865   Batch size = 32
2023-03-27 20:47:46,867 ***** Eval results *****
2023-03-27 20:47:46,867   att_loss = 5623583.818181818
2023-03-27 20:47:46,867   cls_loss = 0.0
2023-03-27 20:47:46,868   global_step = 3749
2023-03-27 20:47:46,868   loss = 5623584.318181818
2023-03-27 20:47:46,868   rep_loss = 0.6405332088470459
2023-03-27 20:47:46,875 ***** Save model *****
 *****
2023-03-27 20:47:13,407 ***** Running evaluation *****
2023-03-27 20:47:13,407   Epoch = 10 iter 2849 step
2023-03-27 20:47:13,407   Num examples = 1043
2023-03-27 20:47:13,407   Batch size = 32
2023-03-27 20:47:13,409 ***** Eval results *****
2023-03-27 20:47:13,409   att_loss = 0.37269004746522316
2023-03-27 20:47:13,409   cls_loss = 0.0
2023-03-27 20:47:13,409   global_step = 2849
2023-03-27 20:47:13,409   loss = 0.9606597107215966
2023-03-27 20:47:13,410   rep_loss = 0.5879696627568932
2023-03-27 20:47:13,416 ***** Save model *****
2023-03-27 20:47:24,249 ***** Running evaluation *****
2023-03-27 20:47:24,250   Epoch = 10 iter 2899 step
2023-03-27 20:47:24,250   Num examples = 1043
2023-03-27 20:47:24,250   Batch size = 32
2023-03-27 20:47:24,252 ***** Eval results *****
2023-03-27 20:47:24,252   att_loss = 0.3725128065810974
2023-03-27 20:47:24,252   cls_loss = 0.0
2023-03-27 20:47:24,252   global_step = 2899
2023-03-27 20:47:24,253   loss = 0.9593727932226189
2023-03-27 20:47:24,253   rep_loss = 0.5868599872922272
2023-03-27 20:47:24,260 ***** Save model *****
2023-03-27 20:47:35,106 ***** Running evaluation *****
2023-03-27 20:47:35,106   Epoch = 11 iter 2949 step
2023-03-27 20:47:35,106   Num examples = 1043
2023-03-27 20:47:35,106   Batch size = 32
2023-03-27 20:47:35,107 ***** Eval results *****
2023-03-27 20:47:35,108   att_loss = 0.37356120596329373
2023-03-27 20:47:35,108   cls_loss = 0.0
2023-03-27 20:47:35,108   global_step = 2949
2023-03-27 20:47:35,108   loss = 0.9578048288822174
2023-03-27 20:47:35,108   rep_loss = 0.5842436204353968
2023-03-27 20:47:35,111 ***** Save model *****
2023-03-27 20:47:45,912 ***** Running evaluation *****
2023-03-27 20:47:45,913   Epoch = 11 iter 2999 step
2023-03-27 20:47:45,913   Num examples = 1043
2023-03-27 20:47:45,913   Batch size = 32
2023-03-27 20:47:45,915 ***** Eval results *****
2023-03-27 20:47:45,915   att_loss = 0.3657850844244803
2023-03-27 20:47:45,915   cls_loss = 0.0
2023-03-27 20:47:45,915   global_step = 2999
2023-03-27 20:47:45,916   loss = 0.9461709030212895
2023-03-27 20:47:45,916   rep_loss = 0.5803858195581744
2023-03-27 20:47:45,922 ***** Save model *****
2023-03-27 20:47:56,739 ***** Running evaluation *****
2023-03-27 20:47:56,739   Epoch = 11 iter 3049 step
2023-03-27 20:47:56,739   Num examples = 1043
2023-03-27 20:47:56,739   Batch size = 32
2023-03-27 20:47:56,740 ***** Eval results *****
2023-03-27 20:47:56,740   att_loss = 0.3677858928484576
2023-03-27 20:47:56,740   cls_loss = 0.0
2023-03-27 20:47:56,741   global_step = 3049
2023-03-27 20:47:56,741   loss = 0.9492979012429714
2023-03-27 20:47:56,741   rep_loss = 0.5815120099910668
2023-03-27 20:47:56,743 ***** Save model *****
2023-03-27 20:48:07,585 ***** Running evaluation *****
2023-03-27 20:48:07,586   Epoch = 11 iter 3099 step
2023-03-27 20:48:07,586   Num examples = 1043
2023-03-27 20:48:07,586   Batch size = 32
2023-03-27 20:48:07,588 ***** Eval results *****
2023-03-27 20:48:07,588   att_loss = 0.3670987071078501
2023-03-27 20:48:07,589   cls_loss = 0.0
2023-03-27 20:48:07,589   global_step = 3099
2023-03-27 20:48:07,589   loss = 0.9486219058802099
2023-03-27 20:48:07,589   rep_loss = 0.5815231987723598
2023-03-27 20:48:07,591 ***** Save model *****
2023-03-27 20:48:18,400 ***** Running evaluation *****
2023-03-27 20:48:18,400   Epoch = 11 iter 3149 step
2023-03-27 20:48:18,400   Num examples = 1043
2023-03-27 20:48:18,400   Batch size = 32
2023-03-27 20:48:18,401 ***** Eval results *****
2023-03-27 20:48:18,401   att_loss = 0.3682414715020162
2023-03-27 20:48:18,402   cls_loss = 0.0
2023-03-27 20:48:18,402   global_step = 3149
2023-03-27 20:48:18,402   loss = 0.9490287838117132
2023-03-27 20:48:18,402   rep_loss = 0.5807873125908509
2023-03-27 20:48:18,404 ***** Save model *****
2023-03-27 20:49:06,491 ***** Running evaluation *****
2023-03-27 20:49:06,491   Epoch = 14 iter 3999 step
2023-03-27 20:49:06,492   Num examples = 1043
2023-03-27 20:49:06,492   Batch size = 32
2023-03-27 20:49:06,495 ***** Eval results *****
2023-03-27 20:49:06,495   att_loss = 5738925.231800767
2023-03-27 20:49:06,495   cls_loss = 0.0
2023-03-27 20:49:06,495   global_step = 3999
2023-03-27 20:49:06,495   loss = 5738925.731800767
2023-03-27 20:49:06,496   rep_loss = 0.6419210032028256
2023-03-27 20:49:06,497 ***** Save model *****
2023-03-27 20:49:22,400 ***** Running evaluation *****
2023-03-27 20:49:22,400   Epoch = 15 iter 4049 step
2023-03-27 20:49:22,401   Num examples = 1043
2023-03-27 20:49:22,401   Batch size = 32
2023-03-27 20:49:22,402 ***** Eval results *****
2023-03-27 20:49:22,402   att_loss = 5664813.420454546
2023-03-27 20:49:22,403   cls_loss = 0.0
2023-03-27 20:49:22,403   global_step = 4049
2023-03-27 20:49:22,403   loss = 5664813.920454546
2023-03-27 20:49:22,403   rep_loss = 0.6365930573506788
2023-03-27 20:49:22,405 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2023-03-27 20:49:38,299 ***** Running evaluation *****
2023-03-27 20:49:38,299   Epoch = 15 iter 4099 step
2023-03-27 20:49:38,300   Num examples = 1043
2023-03-27 20:49:38,300   Batch size = 32
2023-03-27 20:49:38,301 ***** Eval results *****
2023-03-27 20:49:38,301   att_loss = 5686436.611702127
2023-03-27 20:49:38,301   cls_loss = 0.0
2023-03-27 20:49:38,301   global_step = 4099
2023-03-27 20:49:38,301   loss = 5686437.111702127
2023-03-27 20:49:38,302   rep_loss = 0.6370309578611496
2023-03-27 20:49:38,304 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2023-03-27 20:49:54,261 ***** Running evaluation *****
2023-03-27 20:49:54,262   Epoch = 15 iter 4149 step
2023-03-27 20:49:54,262   Num examples = 1043
2023-03-27 20:49:54,262   Batch size = 32
2023-03-27 20:49:54,263 ***** Eval results *****
2023-03-27 20:49:54,264   att_loss = 5682894.899305556
2023-03-27 20:49:54,264   cls_loss = 0.0
2023-03-27 20:49:54,264   global_step = 4149
2023-03-27 20:49:54,264   loss = 5682895.399305556
2023-03-27 20:49:54,264   rep_loss = 0.6371049934791194
2023-03-27 20:49:54,266 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2023-03-27 20:50:10,199 ***** Running evaluation *****
2023-03-27 20:50:10,199   Epoch = 15 iter 4199 step
2023-03-27 20:50:10,199   Num examples = 1043
2023-03-27 20:50:10,199   Batch size = 32
2023-03-27 20:50:10,200 ***** Eval results *****
2023-03-27 20:50:10,200   att_loss = 5693404.909793815
2023-03-27 20:50:10,200   cls_loss = 0.0
2023-03-27 20:50:10,201   global_step = 4199
2023-03-27 20:50:10,201   loss = 5693405.409793815
2023-03-27 20:50:10,201   rep_loss = 0.6370018041625465
2023-03-27 20:50:10,207 ***** Save model *****
   2023-03-27 20:50:26,127 ***** Running evaluation *****
2023-03-27 20:50:26,127   Epoch = 15 iter 4249 step
2023-03-27 20:50:26,127   Num examples = 1043
2023-03-27 20:50:26,128   Batch size = 32
2023-03-27 20:50:26,128 ***** Eval results *****
2023-03-27 20:50:26,129   att_loss = 5703137.145491803
2023-03-27 20:50:26,129   cls_loss = 0.0
2023-03-27 20:50:26,129   global_step = 4249
2023-03-27 20:50:26,129   loss = 5703137.645491803
2023-03-27 20:50:26,129   rep_loss = 0.6367929416601775
2023-03-27 20:50:26,131 ***** Save model *****
2023-03-27 20:50:42,032 ***** Running evaluation *****
2023-03-27 20:50:42,032   Epoch = 16 iter 4299 step
2023-03-27 20:50:42,032   Num examples = 1043
2023-03-27 20:50:42,033   Batch size = 32
2023-03-27 20:50:42,034 ***** Eval results *****
2023-03-27 20:50:42,034   att_loss = 5630227.166666667
2023-03-27 20:50:42,034   cls_loss = 0.0
2023-03-27 20:50:42,034   global_step = 4299
2023-03-27 20:50:42,034   loss = 5630227.666666667
2023-03-27 20:50:42,034   rep_loss = 0.631035773842423
2023-03-27 20:50:42,038 ***** Save model *****
       2023-03-27 20:50:57,961 ***** Running evaluation *****
2023-03-27 20:50:57,961   Epoch = 16 iter 4349 step
2023-03-27 20:50:57,961   Num examples = 1043
2023-03-27 20:50:57,961   Batch size = 32
2023-03-27 20:50:57,963 ***** Eval results *****
2023-03-27 20:50:57,964   att_loss = 5622288.720779221
2023-03-27 20:50:57,964   cls_loss = 0.0
2023-03-27 20:50:57,964   global_step = 4349
2023-03-27 20:50:57,964   loss = 5622289.220779221
2023-03-27 20:50:57,964   rep_loss = 0.6313079556861481
2023-03-27 20:50:57,966 ***** Save model *****
2023-03-27 20:51:13,862 ***** Running evaluation *****
2023-03-27 20:51:13,863   Epoch = 16 iter 4399 step
2023-03-27 20:51:13,863   Num examples = 1043
2023-03-27 20:51:13,863   Batch size = 32
2023-03-27 20:51:13,865 ***** Eval results *****
2023-03-27 20:51:13,865   att_loss = 5671017.818897638
2023-03-27 20:51:13,865   cls_loss = 0.0
2023-03-27 20:51:13,865   global_step = 4399
2023-03-27 20:51:13,865   loss = 5671018.318897638
2023-03-27 20:51:13,866   rep_loss = 0.6320519874415059
2023-03-27 20:51:13,868 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                2023-03-27 20:51:29,781 ***** Running evaluation *****
2023-03-27 20:51:29,781   Epoch = 16 iter 4449 step
2023-03-27 20:51:29,781   Num examples = 1043
2023-03-27 20:51:29,781   Batch size = 32
2023-03-27 20:51:29,782 ***** Eval results *****
2023-03-27 20:51:29,783   att_loss = 5678952.5056497175
2023-03-27 20:51:29,783   cls_loss = 0.0
2023-03-27 20:51:29,783   global_step = 4449
2023-03-27 20:51:29,783   loss = 5678953.0056497175
2023-03-27 20:51:29,783   rep_loss = 0.6328206580910979
2023-03-27 20:51:29,789 ***** Save model *****
2023-03-27 20:51:45,709 ***** Running evaluation *****
2023-03-27 20:51:45,709   Epoch = 16 iter 4499 step
2023-03-27 20:51:45,710   Num examples = 1043
2023-03-27 20:51:45,710   Batch size = 32
2023-03-27 20:51:45,713 ***** Eval results *****
2023-03-27 20:51:45,713   att_loss = 5674115.81938326
2023-03-27 20:51:45,714   cls_loss = 0.0
2023-03-27 20:51:45,714   global_step = 4499
2023-03-27 20:51:45,714   loss = 5674116.31938326
2023-03-27 20:51:45,714   rep_loss = 0.632420388087302
2023-03-27 20:51:45,716 ***** Save model *****
*
2023-03-27 20:51:22,492 ***** Running evaluation *****
2023-03-27 20:51:22,492   Epoch = 14 iter 3999 step
2023-03-27 20:51:22,492   Num examples = 1043
2023-03-27 20:51:22,493   Batch size = 32
2023-03-27 20:51:22,496 ***** Eval results *****
2023-03-27 20:51:22,496   att_loss = 0.3648476863272802
2023-03-27 20:51:22,496   cls_loss = 0.0
2023-03-27 20:51:22,497   global_step = 3999
2023-03-27 20:51:22,497   loss = 0.934935813205909
2023-03-27 20:51:22,497   rep_loss = 0.5700881257367774
2023-03-27 20:51:22,501 ***** Save model *****
2023-03-27 20:51:33,331 ***** Running evaluation *****
2023-03-27 20:51:33,332   Epoch = 15 iter 4049 step
2023-03-27 20:51:33,332   Num examples = 1043
2023-03-27 20:51:33,332   Batch size = 32
2023-03-27 20:51:33,333 ***** Eval results *****
2023-03-27 20:51:33,333   att_loss = 0.35664178363301535
2023-03-27 20:51:33,333   cls_loss = 0.0
2023-03-27 20:51:33,333   global_step = 4049
2023-03-27 20:51:33,334   loss = 0.9225755794481798
2023-03-27 20:51:33,334   rep_loss = 0.5659337964924899
2023-03-27 20:51:33,339 ***** Save model *****
2023-03-27 20:51:44,168 ***** Running evaluation *****
2023-03-27 20:51:44,169   Epoch = 15 iter 4099 step
2023-03-27 20:51:44,169   Num examples = 1043
2023-03-27 20:51:44,169   Batch size = 32
2023-03-27 20:51:44,171 ***** Eval results *****
2023-03-27 20:51:44,171   att_loss = 0.35876276994005163
2023-03-27 20:51:44,171   cls_loss = 0.0
2023-03-27 20:51:44,171   global_step = 4099
2023-03-27 20:51:44,171   loss = 0.9252404944693788
2023-03-27 20:51:44,172   rep_loss = 0.5664777248463733
2023-03-27 20:51:44,176 ***** Save model *****
2023-03-27 20:51:55,016 ***** Running evaluation *****
2023-03-27 20:51:55,016   Epoch = 15 iter 4149 step
2023-03-27 20:51:55,017   Num examples = 1043
2023-03-27 20:51:55,017   Batch size = 32
2023-03-27 20:51:55,018 ***** Eval results *****
2023-03-27 20:51:55,018   att_loss = 0.3592638565848271
2023-03-27 20:51:55,018   cls_loss = 0.0
2023-03-27 20:51:55,018   global_step = 4149
2023-03-27 20:51:55,018   loss = 0.9258616541822752
2023-03-27 20:51:55,019   rep_loss = 0.5665977973904874
2023-03-27 20:51:55,025 ***** Save model *****
2023-03-27 20:52:05,831 ***** Running evaluation *****
2023-03-27 20:52:05,832   Epoch = 15 iter 4199 step
2023-03-27 20:52:05,832   Num examples = 1043
2023-03-27 20:52:05,832   Batch size = 32
2023-03-27 20:52:05,833 ***** Eval results *****
2023-03-27 20:52:05,833   att_loss = 0.3603966518775704
2023-03-27 20:52:05,834   cls_loss = 0.0
2023-03-27 20:52:05,834   global_step = 4199
2023-03-27 20:52:05,834   loss = 0.927643975338985
2023-03-27 20:52:05,834   rep_loss = 0.5672473225396933
2023-03-27 20:52:05,841 ***** Save model *****
2023-03-27 20:52:16,680 ***** Running evaluation *****
2023-03-27 20:52:16,680   Epoch = 15 iter 4249 step
2023-03-27 20:52:16,680   Num examples = 1043
2023-03-27 20:52:16,681   Batch size = 32
2023-03-27 20:52:16,682 ***** Eval results *****
2023-03-27 20:52:16,682   att_loss = 0.36146229884175
2023-03-27 20:52:16,682   cls_loss = 0.0
2023-03-27 20:52:16,683   global_step = 4249
2023-03-27 20:52:16,683   loss = 0.9286112028067229
2023-03-27 20:52:16,683   rep_loss = 0.5671489033542696
2023-03-27 20:52:16,688 ***** Save model *****
2023-03-27 20:52:27,528 ***** Running evaluation *****
2023-03-27 20:52:27,528   Epoch = 16 iter 4299 step
2023-03-27 20:52:27,529   Num examples = 1043
2023-03-27 20:52:27,529   Batch size = 32
2023-03-27 20:52:27,531 ***** Eval results *****
2023-03-27 20:52:27,531   att_loss = 0.35486351008768435
2023-03-27 20:52:27,531   cls_loss = 0.0
2023-03-27 20:52:27,531   global_step = 4299
2023-03-27 20:52:27,531   loss = 0.9176156587070889
2023-03-27 20:52:27,532   rep_loss = 0.5627521497231943
2023-03-27 20:52:27,536 ***** Save model *****
2023-03-27 20:53:05,313 ***** Running evaluation *****
2023-03-27 20:53:05,314   Epoch = 17 iter 4749 step
2023-03-27 20:53:05,314   Num examples = 1043
2023-03-27 20:53:05,314   Batch size = 32
2023-03-27 20:53:05,316 ***** Eval results *****
2023-03-27 20:53:05,316   att_loss = 5659057.830952381
2023-03-27 20:53:05,317   cls_loss = 0.0
2023-03-27 20:53:05,317   global_step = 4749
2023-03-27 20:53:05,317   loss = 5659058.330952381
2023-03-27 20:53:05,317   rep_loss = 0.6294124887103126
2023-03-27 20:53:05,324 ***** Save model *****
2023-03-27 20:53:21,235 ***** Running evaluation *****
2023-03-27 20:53:21,236   Epoch = 17 iter 4799 step
2023-03-27 20:53:21,236   Num examples = 1043
2023-03-27 20:53:21,236   Batch size = 32
2023-03-27 20:53:21,237 ***** Eval results *****
2023-03-27 20:53:21,237   att_loss = 5667765.342307692
2023-03-27 20:53:21,237   cls_loss = 0.0
2023-03-27 20:53:21,237   global_step = 4799
2023-03-27 20:53:21,237   loss = 5667765.842307692
2023-03-27 20:53:21,237   rep_loss = 0.6296641604258464
2023-03-27 20:53:21,244 ***** Save model *****
     2023-03-27 20:53:37,196 ***** Running evaluation *****
2023-03-27 20:53:37,196   Epoch = 18 iter 4849 step
2023-03-27 20:53:37,196   Num examples = 1043
2023-03-27 20:53:37,196   Batch size = 32
2023-03-27 20:53:37,197 ***** Eval results *****
2023-03-27 20:53:37,197   att_loss = 5653390.279069767
2023-03-27 20:53:37,198   cls_loss = 0.0
2023-03-27 20:53:37,198   global_step = 4849
2023-03-27 20:53:37,198   loss = 5653390.779069767
2023-03-27 20:53:37,198   rep_loss = 0.6262691256611846
2023-03-27 20:53:37,199 ***** Save model *****
2023-03-27 20:53:53,144 ***** Running evaluation *****
2023-03-27 20:53:53,144   Epoch = 18 iter 4899 step
2023-03-27 20:53:53,144   Num examples = 1043
2023-03-27 20:53:53,145   Batch size = 32
2023-03-27 20:53:53,145 ***** Eval results *****
2023-03-27 20:53:53,146   att_loss = 5660390.311827957
2023-03-27 20:53:53,146   cls_loss = 0.0
2023-03-27 20:53:53,146   global_step = 4899
2023-03-27 20:53:53,146   loss = 5660390.811827957
2023-03-27 20:53:53,146   rep_loss = 0.6269567025605068
2023-03-27 20:53:53,148 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  2023-03-27 20:54:09,065 ***** Running evaluation *****
2023-03-27 20:54:09,065   Epoch = 18 iter 4949 step
2023-03-27 20:54:09,065   Num examples = 1043
2023-03-27 20:54:09,066   Batch size = 32
2023-03-27 20:54:09,067 ***** Eval results *****
2023-03-27 20:54:09,067   att_loss = 5633177.776223776
2023-03-27 20:54:09,067   cls_loss = 0.0
2023-03-27 20:54:09,068   global_step = 4949
2023-03-27 20:54:09,068   loss = 5633178.276223776
2023-03-27 20:54:09,068   rep_loss = 0.6264461558181923
2023-03-27 20:54:09,075 ***** Save model *****
2023-03-27 20:54:24,982 ***** Running evaluation *****
2023-03-27 20:54:24,982   Epoch = 18 iter 4999 step
2023-03-27 20:54:24,983   Num examples = 1043
2023-03-27 20:54:24,983   Batch size = 32
2023-03-27 20:54:24,984 ***** Eval results *****
2023-03-27 20:54:24,984   att_loss = 5632031.430051814
2023-03-27 20:54:24,984   cls_loss = 0.0
2023-03-27 20:54:24,985   global_step = 4999
2023-03-27 20:54:24,985   loss = 5632031.930051814
2023-03-27 20:54:24,985   rep_loss = 0.6261654740170494
2023-03-27 20:54:24,987 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  2023-03-27 20:54:40,888 ***** Running evaluation *****
2023-03-27 20:54:40,888   Epoch = 18 iter 5049 step
2023-03-27 20:54:40,888   Num examples = 1043
2023-03-27 20:54:40,888   Batch size = 32
2023-03-27 20:54:40,889 ***** Eval results *****
2023-03-27 20:54:40,890   att_loss = 5646823.349794239
2023-03-27 20:54:40,890   cls_loss = 0.0
2023-03-27 20:54:40,890   global_step = 5049
2023-03-27 20:54:40,890   loss = 5646823.849794239
2023-03-27 20:54:40,890   rep_loss = 0.6263909202544287
2023-03-27 20:54:40,892 ***** Save model *****
2023-03-27 20:54:56,791 ***** Running evaluation *****
2023-03-27 20:54:56,791   Epoch = 19 iter 5099 step
2023-03-27 20:54:56,791   Num examples = 1043
2023-03-27 20:54:56,792   Batch size = 32
2023-03-27 20:54:56,793 ***** Eval results *****
2023-03-27 20:54:56,794   att_loss = 5654268.076923077
2023-03-27 20:54:56,794   cls_loss = 0.0
2023-03-27 20:54:56,794   global_step = 5099
2023-03-27 20:54:56,794   loss = 5654268.576923077
2023-03-27 20:54:56,794   rep_loss = 0.6230277075217321
2023-03-27 20:54:56,798 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2023-03-27 20:55:12,730 ***** Running evaluation *****
2023-03-27 20:55:12,730   Epoch = 19 iter 5149 step
2023-03-27 20:55:12,731   Num examples = 1043
2023-03-27 20:55:12,731   Batch size = 32
2023-03-27 20:55:12,732 ***** Eval results *****
2023-03-27 20:55:12,733   att_loss = 5633590.4605263155
2023-03-27 20:55:12,733   cls_loss = 0.0
2023-03-27 20:55:12,733   global_step = 5149
2023-03-27 20:55:12,733   loss = 5633590.9605263155
2023-03-27 20:55:12,734   rep_loss = 0.6230745707687578
2023-03-27 20:55:12,740 ***** Save model *****
2023-03-27 20:55:28,645 ***** Running evaluation *****
2023-03-27 20:55:28,646   Epoch = 19 iter 5199 step
2023-03-27 20:55:28,646   Num examples = 1043
2023-03-27 20:55:28,646   Batch size = 32
2023-03-27 20:55:28,647 ***** Eval results *****
2023-03-27 20:55:28,647   att_loss = 5631494.158730159
2023-03-27 20:55:28,648   cls_loss = 0.0
2023-03-27 20:55:28,648   global_step = 5199
2023-03-27 20:55:28,648   loss = 5631494.658730159
2023-03-27 20:55:28,648   rep_loss = 0.6230314175287882
2023-03-27 20:55:28,655 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2023-03-27 20:55:44,593 ***** Running evaluation *****
2023-03-27 20:55:44,594   Epoch = 19 iter 5249 step
2023-03-27 20:55:44,594   Num examples = 1043
2023-03-27 20:55:44,594   Batch size = 32
2023-03-27 20:55:44,595 ***** Eval results *****
2023-03-27 20:55:44,595   att_loss = 5635188.1477272725
2023-03-27 20:55:44,596   cls_loss = 0.0
2023-03-27 20:55:44,596   global_step = 5249
2023-03-27 20:55:44,596   loss = 5635188.6477272725
2023-03-27 20:55:44,596   rep_loss = 0.6235653717409481
2023-03-27 20:55:44,603 ***** Save model *****
2023-03-27 20:56:00,535 ***** Running evaluation *****
2023-03-27 20:56:00,535   Epoch = 19 iter 5299 step
2023-03-27 20:56:00,535   Num examples = 1043
2023-03-27 20:56:00,535   Batch size = 32
2023-03-27 20:56:00,536 ***** Eval results *****
2023-03-27 20:56:00,536   att_loss = 5633151.6283185845
2023-03-27 20:56:00,537   cls_loss = 0.0
2023-03-27 20:56:00,537   global_step = 5299
2023-03-27 20:56:00,537   loss = 5633152.1283185845
2023-03-27 20:56:00,537   rep_loss = 0.6238209879503841
2023-03-27 20:56:00,542 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2023-03-27 20:56:16,449 ***** Running evaluation *****
2023-03-27 20:56:16,449   Epoch = 20 iter 5349 step
2023-03-27 20:56:16,449   Num examples = 1043
2023-03-27 20:56:16,449   Batch size = 32
2023-03-27 20:56:16,451 ***** Eval results *****
2023-03-27 20:56:16,452   att_loss = 5625878.888888889
2023-03-27 20:56:16,452   cls_loss = 0.0
2023-03-27 20:56:16,452   global_step = 5349
2023-03-27 20:56:16,452   loss = 5625879.388888889
2023-03-27 20:56:16,452   rep_loss = 0.6201603213946024
2023-03-27 20:56:16,454 ***** Save model *****
2023-03-27 20:56:32,394 ***** Running evaluation *****
2023-03-27 20:56:32,394   Epoch = 20 iter 5399 step
2023-03-27 20:56:32,394   Num examples = 1043
2023-03-27 20:56:32,395   Batch size = 32
2023-03-27 20:56:32,396 ***** Eval results *****
2023-03-27 20:56:32,396   att_loss = 5599487.483050847
2023-03-27 20:56:32,397   cls_loss = 0.0
2023-03-27 20:56:32,397   global_step = 5399
2023-03-27 20:56:32,397   loss = 5599487.983050847
2023-03-27 20:56:32,397   rep_loss = 0.6200589072906365
2023-03-27 20:56:32,399 ***** Save model *****
    2023-03-27 20:56:48,298 ***** Running evaluation *****
2023-03-27 20:56:48,299   Epoch = 20 iter 5449 step
2023-03-27 20:56:48,299   Num examples = 1043
2023-03-27 20:56:48,299   Batch size = 32
2023-03-27 20:56:48,300 ***** Eval results *****
2023-03-27 20:56:48,300   att_loss = 5585554.036697248
2023-03-27 20:56:48,300   cls_loss = 0.0
2023-03-27 20:56:48,300   global_step = 5449
2023-03-27 20:56:48,301   loss = 5585554.536697248
2023-03-27 20:56:48,301   rep_loss = 0.6197344179547161
2023-03-27 20:56:48,302 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2023-03-27 20:57:04,208 ***** Running evaluation *****
2023-03-27 20:57:04,208   Epoch = 20 iter 5499 step
2023-03-27 20:57:04,208   Num examples = 1043
2023-03-27 20:57:04,208   Batch size = 32
2023-03-27 20:57:04,209 ***** Eval results *****
2023-03-27 20:57:04,210   att_loss = 5588693.421383648
2023-03-27 20:57:04,210   cls_loss = 0.0
2023-03-27 20:57:04,210   global_step = 5499
2023-03-27 20:57:04,211   loss = 5588693.921383648
2023-03-27 20:57:04,211   rep_loss = 0.6201139249891605
2023-03-27 20:57:04,212 ***** Save model *****
2023-03-27 20:57:20,121 ***** Running evaluation *****
2023-03-27 20:57:20,121   Epoch = 20 iter 5549 step
2023-03-27 20:57:20,122   Num examples = 1043
2023-03-27 20:57:20,122   Batch size = 32
2023-03-27 20:57:20,124 ***** Eval results *****
2023-03-27 20:57:20,124   att_loss = 5607949.734449761
2023-03-27 20:57:20,124   cls_loss = 0.0
2023-03-27 20:57:20,124   global_step = 5549
2023-03-27 20:57:20,125   loss = 5607950.234449761
2023-03-27 20:57:20,125   rep_loss = 0.6203027467408249
2023-03-27 20:57:20,127 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  2023-03-27 20:57:36,039 ***** Running evaluation *****
2023-03-27 20:57:36,039   Epoch = 20 iter 5599 step
2023-03-27 20:57:36,039   Num examples = 1043
2023-03-27 20:57:36,040   Batch size = 32
2023-03-27 20:57:36,044 ***** Eval results *****
2023-03-27 20:57:36,044   att_loss = 5610386.972972973
2023-03-27 20:57:36,044   cls_loss = 0.0
2023-03-27 20:57:36,045   global_step = 5599
2023-03-27 20:57:36,045   loss = 5610387.472972973
2023-03-27 20:57:36,045   rep_loss = 0.6205732527846995
2023-03-27 20:57:36,047 ***** Save model *****
2023-03-27 20:57:51,957 ***** Running evaluation *****
2023-03-27 20:57:51,957   Epoch = 21 iter 5649 step
2023-03-27 20:57:51,958   Num examples = 1043
2023-03-27 20:57:51,958   Batch size = 32
2023-03-27 20:57:51,960 ***** Eval results *****
2023-03-27 20:57:51,960   att_loss = 5581282.178571428
2023-03-27 20:57:51,961   cls_loss = 0.0
2023-03-27 20:57:51,961   global_step = 5649
2023-03-27 20:57:51,961   loss = 5581282.678571428
2023-03-27 20:57:51,961   rep_loss = 0.6162691173099336
2023-03-27 20:57:51,965 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2023-03-27 20:58:07,879 ***** Running evaluation *****
2023-03-27 20:58:07,880   Epoch = 21 iter 5699 step
2023-03-27 20:58:07,880   Num examples = 1043
2023-03-27 20:58:07,880   Batch size = 32
2023-03-27 20:58:07,881 ***** Eval results *****
2023-03-27 20:58:07,881   att_loss = 5594357.576086956
2023-03-27 20:58:07,881   cls_loss = 0.0
2023-03-27 20:58:07,882   global_step = 5699
2023-03-27 20:58:07,882   loss = 5594358.076086956
2023-03-27 20:58:07,882   rep_loss = 0.6171779049479443
2023-03-27 20:58:07,883 ***** Save model *****
2023-03-27 20:58:23,788 ***** Running evaluation *****
2023-03-27 20:58:23,789   Epoch = 21 iter 5749 step
2023-03-27 20:58:23,789   Num examples = 1043
2023-03-27 20:58:23,789   Batch size = 32
2023-03-27 20:58:23,791 ***** Eval results *****
2023-03-27 20:58:23,791   att_loss = 5577712.207746479
2023-03-27 20:58:23,791   cls_loss = 0.0
2023-03-27 20:58:23,791   global_step = 5749
2023-03-27 20:58:23,792   loss = 5577712.707746479
2023-03-27 20:58:23,792   rep_loss = 0.6162122143826014
2023-03-27 20:58:23,798 ***** Save model *****
**
2023-03-27 20:58:03,111 ***** Running evaluation *****
2023-03-27 20:58:03,111   Epoch = 21 iter 5849 step
2023-03-27 20:58:03,112   Num examples = 1043
2023-03-27 20:58:03,112   Batch size = 32
2023-03-27 20:58:03,113 ***** Eval results *****
2023-03-27 20:58:03,113   att_loss = 0.35501208063984707
2023-03-27 20:58:03,114   cls_loss = 0.0
2023-03-27 20:58:03,114   global_step = 5849
2023-03-27 20:58:03,114   loss = 0.9084060982731749
2023-03-27 20:58:03,114   rep_loss = 0.5533940154166261
2023-03-27 20:58:03,120 ***** Save model *****
2023-03-27 20:58:13,956 ***** Running evaluation *****
2023-03-27 20:58:13,956   Epoch = 22 iter 5899 step
2023-03-27 20:58:13,956   Num examples = 1043
2023-03-27 20:58:13,956   Batch size = 32
2023-03-27 20:58:13,958 ***** Eval results *****
2023-03-27 20:58:13,958   att_loss = 0.34554816484451295
2023-03-27 20:58:13,958   cls_loss = 0.0
2023-03-27 20:58:13,959   global_step = 5899
2023-03-27 20:58:13,959   loss = 0.8960385489463806
2023-03-27 20:58:13,959   rep_loss = 0.5504903864860534
2023-03-27 20:58:13,961 ***** Save model *****
2023-03-27 20:58:24,791 ***** Running evaluation *****
2023-03-27 20:58:24,792   Epoch = 22 iter 5949 step
2023-03-27 20:58:24,792   Num examples = 1043
2023-03-27 20:58:24,792   Batch size = 32
2023-03-27 20:58:24,793 ***** Eval results *****
2023-03-27 20:58:24,793   att_loss = 0.35059054374694826
2023-03-27 20:58:24,794   cls_loss = 0.0
2023-03-27 20:58:24,794   global_step = 5949
2023-03-27 20:58:24,794   loss = 0.8990005882581075
2023-03-27 20:58:24,794   rep_loss = 0.5484100453058879
2023-03-27 20:58:24,796 ***** Save model *****
2023-03-27 20:58:35,641 ***** Running evaluation *****
2023-03-27 20:58:35,642   Epoch = 22 iter 5999 step
2023-03-27 20:58:35,642   Num examples = 1043
2023-03-27 20:58:35,642   Batch size = 32
2023-03-27 20:58:35,643 ***** Eval results *****
2023-03-27 20:58:35,644   att_loss = 0.35119980430603026
2023-03-27 20:58:35,644   cls_loss = 0.0
2023-03-27 20:58:35,644   global_step = 5999
2023-03-27 20:58:35,644   loss = 0.9007394161224366
2023-03-27 20:58:35,645   rep_loss = 0.5495396127700806
2023-03-27 20:58:35,647 ***** Save model *****
2023-03-27 20:58:46,441 ***** Running evaluation *****
2023-03-27 20:58:46,442   Epoch = 22 iter 6049 step
2023-03-27 20:58:46,442   Num examples = 1043
2023-03-27 20:58:46,442   Batch size = 32
2023-03-27 20:58:46,443 ***** Eval results *****
2023-03-27 20:58:46,443   att_loss = 0.3527162730693817
2023-03-27 20:58:46,443   cls_loss = 0.0
2023-03-27 20:58:46,444   global_step = 6049
2023-03-27 20:58:46,444   loss = 0.9033918823514666
2023-03-27 20:58:46,444   rep_loss = 0.5506756104741778
2023-03-27 20:58:46,446 ***** Save model *****
2023-03-27 20:58:57,231 ***** Running evaluation *****
2023-03-27 20:58:57,231   Epoch = 22 iter 6099 step
2023-03-27 20:58:57,232   Num examples = 1043
2023-03-27 20:58:57,232   Batch size = 32
2023-03-27 20:58:57,233 ***** Eval results *****
2023-03-27 20:58:57,234   att_loss = 0.354451904296875
2023-03-27 20:58:57,234   cls_loss = 0.0
2023-03-27 20:58:57,234   global_step = 6099
2023-03-27 20:58:57,234   loss = 0.9052893543243408
2023-03-27 20:58:57,234   rep_loss = 0.5508374508221944
2023-03-27 20:58:57,240 ***** Save model *****
2023-03-27 20:59:08,026 ***** Running evaluation *****
2023-03-27 20:59:08,026   Epoch = 23 iter 6149 step
2023-03-27 20:59:08,027   Num examples = 1043
2023-03-27 20:59:08,027   Batch size = 32
2023-03-27 20:59:08,028 ***** Eval results *****
2023-03-27 20:59:08,028   att_loss = 0.3573148846626282
2023-03-27 20:59:08,028   cls_loss = 0.0
2023-03-27 20:59:08,028   global_step = 6149
2023-03-27 20:59:08,028   loss = 0.9072972163558006
2023-03-27 20:59:08,028   rep_loss = 0.549982339143753
2023-03-27 20:59:43,327 ***** Running evaluation *****
2023-03-27 20:59:43,327   Epoch = 22 iter 5999 step
2023-03-27 20:59:43,327   Num examples = 1043
2023-03-27 20:59:43,328   Batch size = 32
2023-03-27 20:59:43,329 ***** Eval results *****
2023-03-27 20:59:43,329   att_loss = 5553265.456
2023-03-27 20:59:43,329   cls_loss = 0.0
2023-03-27 20:59:43,329   global_step = 5999
2023-03-27 20:59:43,329   loss = 5553265.956
2023-03-27 20:59:43,330   rep_loss = 0.6134038043022155
2023-03-27 20:59:43,336 ***** Save model *****
2023-03-27 20:59:59,236 ***** Running evaluation *****
2023-03-27 20:59:59,236   Epoch = 22 iter 6049 step
2023-03-27 20:59:59,236   Num examples = 1043
2023-03-27 20:59:59,237   Batch size = 32
2023-03-27 20:59:59,238 ***** Eval results *****
2023-03-27 20:59:59,238   att_loss = 5561429.42
2023-03-27 20:59:59,239   cls_loss = 0.0
2023-03-27 20:59:59,239   global_step = 6049
2023-03-27 20:59:59,239   loss = 5561429.92
2023-03-27 20:59:59,239   rep_loss = 0.6141857777323042
2023-03-27 20:59:59,243 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2023-03-27 21:00:15,161 ***** Running evaluation *****
2023-03-27 21:00:15,162   Epoch = 22 iter 6099 step
2023-03-27 21:00:15,162   Num examples = 1043
2023-03-27 21:00:15,162   Batch size = 32
2023-03-27 21:00:15,166 ***** Eval results *****
2023-03-27 21:00:15,166   att_loss = 5571397.673333333
2023-03-27 21:00:15,166   cls_loss = 0.0
2023-03-27 21:00:15,166   global_step = 6099
2023-03-27 21:00:15,167   loss = 5571398.173333333
2023-03-27 21:00:15,167   rep_loss = 0.6142852171262105
2023-03-27 21:00:15,169 ***** Save model *****
2023-03-27 21:00:31,094 ***** Running evaluation *****
2023-03-27 21:00:31,095   Epoch = 23 iter 6149 step
2023-03-27 21:00:31,095   Num examples = 1043
2023-03-27 21:00:31,095   Batch size = 32
2023-03-27 21:00:31,096 ***** Eval results *****
2023-03-27 21:00:31,096   att_loss = 5511930.875
2023-03-27 21:00:31,096   cls_loss = 0.0
2023-03-27 21:00:31,097   global_step = 6149
2023-03-27 21:00:31,097   loss = 5511931.375
2023-03-27 21:00:31,097   rep_loss = 0.6104413866996765
2023-03-27 21:00:31,098 ***** Save model *****
2023-03-27 21:00:47,042 ***** Running evaluation *****
2023-03-27 21:00:47,042   Epoch = 23 iter 6199 step
2023-03-27 21:00:47,042   Num examples = 1043
2023-03-27 21:00:47,042   Batch size = 32
2023-03-27 21:00:47,044 ***** Eval results *****
2023-03-27 21:00:47,044   att_loss = 5561870.206896552
2023-03-27 21:00:47,044   cls_loss = 0.0
2023-03-27 21:00:47,044   global_step = 6199
2023-03-27 21:00:47,044   loss = 5561870.706896552
2023-03-27 21:00:47,044   rep_loss = 0.614670504783762
2023-03-27 21:00:47,051 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2023-03-27 21:01:02,983 ***** Running evaluation *****
2023-03-27 21:01:02,984   Epoch = 23 iter 6249 step
2023-03-27 21:01:02,984   Num examples = 1043
2023-03-27 21:01:02,984   Batch size = 32
2023-03-27 21:01:02,985 ***** Eval results *****
2023-03-27 21:01:02,985   att_loss = 5581461.574074074
2023-03-27 21:01:02,986   cls_loss = 0.0
2023-03-27 21:01:02,986   global_step = 6249
2023-03-27 21:01:02,986   loss = 5581462.074074074
2023-03-27 21:01:02,986   rep_loss = 0.6135237321809486
2023-03-27 21:01:02,988 ***** Save model *****
2023-03-27 21:01:18,926 ***** Running evaluation *****
2023-03-27 21:01:18,926   Epoch = 23 iter 6299 step
2023-03-27 21:01:18,926   Num examples = 1043
2023-03-27 21:01:18,926   Batch size = 32
2023-03-27 21:01:18,928 ***** Eval results *****
2023-03-27 21:01:18,928   att_loss = 5573765.566455696
2023-03-27 21:01:18,928   cls_loss = 0.0
2023-03-27 21:01:18,928   global_step = 6299
2023-03-27 21:01:18,929   loss = 5573766.066455696
2023-03-27 21:01:18,929   rep_loss = 0.6134288378908664
2023-03-27 21:01:18,931 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  2023-03-27 21:01:34,799 ***** Running evaluation *****
2023-03-27 21:01:34,799   Epoch = 23 iter 6349 step
2023-03-27 21:01:34,800   Num examples = 1043
2023-03-27 21:01:34,800   Batch size = 32
2023-03-27 21:01:34,802 ***** Eval results *****
2023-03-27 21:01:34,802   att_loss = 5576330.189903846
2023-03-27 21:01:34,802   cls_loss = 0.0
2023-03-27 21:01:34,803   global_step = 6349
2023-03-27 21:01:34,803   loss = 5576330.689903846
2023-03-27 21:01:34,803   rep_loss = 0.6130053713344611
2023-03-27 21:01:34,805 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2023-03-27 21:01:50,720 ***** Running evaluation *****
2023-03-27 21:01:50,720   Epoch = 23 iter 6399 step
2023-03-27 21:01:50,721   Num examples = 1043
2023-03-27 21:01:50,721   Batch size = 32
2023-03-27 21:01:50,723 ***** Eval results *****
2023-03-27 21:01:50,723   att_loss = 5571940.8875969
2023-03-27 21:01:50,724   cls_loss = 0.0
2023-03-27 21:01:50,724   global_step = 6399
2023-03-27 21:01:50,724   loss = 5571941.3875969
2023-03-27 21:01:50,724   rep_loss = 0.6131015734155049
2023-03-27 21:01:50,730 ***** Save model *****
2023-03-27 21:02:06,636 ***** Running evaluation *****
2023-03-27 21:02:06,637   Epoch = 24 iter 6449 step
2023-03-27 21:02:06,637   Num examples = 1043
2023-03-27 21:02:06,637   Batch size = 32
2023-03-27 21:02:06,638 ***** Eval results *****
2023-03-27 21:02:06,638   att_loss = 5540476.268292683
2023-03-27 21:02:06,639   cls_loss = 0.0
2023-03-27 21:02:06,639   global_step = 6449
2023-03-27 21:02:06,639   loss = 5540476.768292683
2023-03-27 21:02:06,639   rep_loss = 0.6115828854281727
2023-03-27 21:02:06,646 ***** Save model *****
l *****
2023-03-27 21:02:11,960 ***** Running evaluation *****
2023-03-27 21:02:11,960   Epoch = 26 iter 6999 step
2023-03-27 21:02:11,960   Num examples = 1043
2023-03-27 21:02:11,960   Batch size = 32
2023-03-27 21:02:11,962 ***** Eval results *****
2023-03-27 21:02:11,963   att_loss = 0.3519183512319598
2023-03-27 21:02:11,963   cls_loss = 0.0
2023-03-27 21:02:11,963   global_step = 6999
2023-03-27 21:02:11,963   loss = 0.8969227180146334
2023-03-27 21:02:11,963   rep_loss = 0.5450043657369781
2023-03-27 21:02:11,968 ***** Save model *****
2023-03-27 21:02:22,803 ***** Running evaluation *****
2023-03-27 21:02:22,803   Epoch = 26 iter 7049 step
2023-03-27 21:02:22,803   Num examples = 1043
2023-03-27 21:02:22,803   Batch size = 32
2023-03-27 21:02:22,805 ***** Eval results *****
2023-03-27 21:02:22,805   att_loss = 0.35151463830582447
2023-03-27 21:02:22,805   cls_loss = 0.0
2023-03-27 21:02:22,805   global_step = 7049
2023-03-27 21:02:22,806   loss = 0.895769428984027
2023-03-27 21:02:22,806   rep_loss = 0.5442547903996762
2023-03-27 21:02:22,807 ***** Save model *****
2023-03-27 21:02:33,632 ***** Running evaluation *****
2023-03-27 21:02:33,632   Epoch = 26 iter 7099 step
2023-03-27 21:02:33,632   Num examples = 1043
2023-03-27 21:02:33,632   Batch size = 32
2023-03-27 21:02:33,634 ***** Eval results *****
2023-03-27 21:02:33,634   att_loss = 0.35170709802086947
2023-03-27 21:02:33,634   cls_loss = 0.0
2023-03-27 21:02:33,635   global_step = 7099
2023-03-27 21:02:33,635   loss = 0.8958577980661089
2023-03-27 21:02:33,635   rep_loss = 0.5441506994757682
2023-03-27 21:02:33,640 ***** Save model *****
2023-03-27 21:02:44,454 ***** Running evaluation *****
2023-03-27 21:02:44,454   Epoch = 26 iter 7149 step
2023-03-27 21:02:44,455   Num examples = 1043
2023-03-27 21:02:44,455   Batch size = 32
2023-03-27 21:02:44,456 ***** Eval results *****
2023-03-27 21:02:44,456   att_loss = 0.34997836547197353
2023-03-27 21:02:44,456   cls_loss = 0.0
2023-03-27 21:02:44,457   global_step = 7149
2023-03-27 21:02:44,457   loss = 0.8941319083821946
2023-03-27 21:02:44,457   rep_loss = 0.5441535420463857
2023-03-27 21:02:44,461 ***** Save model *****
2023-03-27 21:02:55,272 ***** Running evaluation *****
2023-03-27 21:02:55,272   Epoch = 26 iter 7199 step
2023-03-27 21:02:55,272   Num examples = 1043
2023-03-27 21:02:55,272   Batch size = 32
2023-03-27 21:02:55,273 ***** Eval results *****
2023-03-27 21:02:55,274   att_loss = 0.35059102743516174
2023-03-27 21:02:55,274   cls_loss = 0.0
2023-03-27 21:02:55,274   global_step = 7199
2023-03-27 21:02:55,274   loss = 0.894721011707291
2023-03-27 21:02:55,274   rep_loss = 0.5441299829965436
2023-03-27 21:02:55,276 ***** Save model *****
2023-03-27 21:03:06,076 ***** Running evaluation *****
2023-03-27 21:03:06,076   Epoch = 27 iter 7249 step
2023-03-27 21:03:06,076   Num examples = 1043
2023-03-27 21:03:06,076   Batch size = 32
2023-03-27 21:03:06,078 ***** Eval results *****
2023-03-27 21:03:06,078   att_loss = 0.35017412155866623
2023-03-27 21:03:06,078   cls_loss = 0.0
2023-03-27 21:03:06,078   global_step = 7249
2023-03-27 21:03:06,078   loss = 0.8936395853757858
2023-03-27 21:03:06,079   rep_loss = 0.5434654653072357
2023-03-27 21:03:06,086 ***** Save model *****
2023-03-27 21:03:26,840 ***** Running evaluation *****
2023-03-27 21:03:26,841   Epoch = 25 iter 6699 step
2023-03-27 21:03:26,841   Num examples = 1043
2023-03-27 21:03:26,841   Batch size = 32
2023-03-27 21:03:26,842 ***** Eval results *****
2023-03-27 21:03:26,843   att_loss = 5541146.0625
2023-03-27 21:03:26,843   cls_loss = 0.0
2023-03-27 21:03:26,843   global_step = 6699
2023-03-27 21:03:26,843   loss = 5541146.5625
2023-03-27 21:03:26,844   rep_loss = 0.6097260117530823
2023-03-27 21:03:26,845 ***** Save model *****
2023-03-27 21:03:42,792 ***** Running evaluation *****
2023-03-27 21:03:42,792   Epoch = 25 iter 6749 step
2023-03-27 21:03:42,793   Num examples = 1043
2023-03-27 21:03:42,793   Batch size = 32
2023-03-27 21:03:42,794 ***** Eval results *****
2023-03-27 21:03:42,794   att_loss = 5553630.790540541
2023-03-27 21:03:42,794   cls_loss = 0.0
2023-03-27 21:03:42,795   global_step = 6749
2023-03-27 21:03:42,795   loss = 5553631.290540541
2023-03-27 21:03:42,795   rep_loss = 0.6110329176928546
2023-03-27 21:03:42,798 ***** Save model *****
ave model *****
2023-03-27 21:03:38,494 ***** Running evaluation *****
2023-03-27 21:03:38,495   Epoch = 27 iter 7399 step
2023-03-27 21:03:38,495   Num examples = 1043
2023-03-27 21:03:38,495   Batch size = 32
2023-03-27 21:03:38,497 ***** Eval results *****
2023-03-27 21:03:38,498   att_loss = 0.35163068096888694
2023-03-27 21:03:38,498   cls_loss = 0.0
2023-03-27 21:03:38,498   global_step = 7399
2023-03-27 21:03:38,498   loss = 0.8952247585120954
2023-03-27 21:03:38,499   rep_loss = 0.5435940798960234
2023-03-27 21:03:38,500 ***** Save model *****
2023-03-27 21:03:49,302 ***** Running evaluation *****
2023-03-27 21:03:49,303   Epoch = 27 iter 7449 step
2023-03-27 21:03:49,303   Num examples = 1043
2023-03-27 21:03:49,303   Batch size = 32
2023-03-27 21:03:49,304 ***** Eval results *****
2023-03-27 21:03:49,304   att_loss = 0.3507948748767376
2023-03-27 21:03:49,304   cls_loss = 0.0
2023-03-27 21:03:49,305   global_step = 7449
2023-03-27 21:03:49,305   loss = 0.8940333264569441
2023-03-27 21:03:49,305   rep_loss = 0.5432384538153807
2023-03-27 21:03:49,306 ***** Save model *****
2023-03-27 21:04:00,086 ***** Running evaluation *****
2023-03-27 21:04:00,086   Epoch = 28 iter 7499 step
2023-03-27 21:04:00,086   Num examples = 1043
2023-03-27 21:04:00,086   Batch size = 32
2023-03-27 21:04:00,087 ***** Eval results *****
2023-03-27 21:04:00,088   att_loss = 0.3523081320783366
2023-03-27 21:04:00,088   cls_loss = 0.0
2023-03-27 21:04:00,088   global_step = 7499
2023-03-27 21:04:00,088   loss = 0.8935459774473439
2023-03-27 21:04:00,088   rep_loss = 0.5412378440732541
2023-03-27 21:04:00,093 ***** Save model *****
2023-03-27 21:04:10,910 ***** Running evaluation *****
2023-03-27 21:04:10,910   Epoch = 28 iter 7549 step
2023-03-27 21:04:10,910   Num examples = 1043
2023-03-27 21:04:10,911   Batch size = 32
2023-03-27 21:04:10,912 ***** Eval results *****
2023-03-27 21:04:10,912   att_loss = 0.35173274392951026
2023-03-27 21:04:10,912   cls_loss = 0.0
2023-03-27 21:04:10,912   global_step = 7549
2023-03-27 21:04:10,913   loss = 0.8945626771613343
2023-03-27 21:04:10,913   rep_loss = 0.542829934048326
2023-03-27 21:04:10,919 ***** Save model *****
2023-03-27 21:04:21,719 ***** Running evaluation *****
2023-03-27 21:04:21,719   Epoch = 28 iter 7599 step
2023-03-27 21:04:21,720   Num examples = 1043
2023-03-27 21:04:21,720   Batch size = 32
2023-03-27 21:04:21,722 ***** Eval results *****
2023-03-27 21:04:21,722   att_loss = 0.3524727230149556
2023-03-27 21:04:21,722   cls_loss = 0.0
2023-03-27 21:04:21,723   global_step = 7599
2023-03-27 21:04:21,723   loss = 0.8957376799932341
2023-03-27 21:04:21,723   rep_loss = 0.5432649569782785
2023-03-27 21:04:21,725 ***** Save model *****
2023-03-27 21:04:32,528 ***** Running evaluation *****
2023-03-27 21:04:32,528   Epoch = 28 iter 7649 step
2023-03-27 21:04:32,529   Num examples = 1043
2023-03-27 21:04:32,529   Batch size = 32
2023-03-27 21:04:32,530 ***** Eval results *****
2023-03-27 21:04:32,530   att_loss = 0.3525202777344367
2023-03-27 21:04:32,530   cls_loss = 0.0
2023-03-27 21:04:32,531   global_step = 7649
2023-03-27 21:04:32,531   loss = 0.8958187430580228
2023-03-27 21:04:32,531   rep_loss = 0.5432984660126571
2023-03-27 21:04:32,537 ***** Save model *****
2023-03-27 21:04:43,340 ***** Running evaluation *****
2023-03-27 21:04:43,341   Epoch = 28 iter 7699 step
2023-03-27 21:04:43,341   Num examples = 1043
2023-03-27 21:04:43,341   Batch size = 32
2023-03-27 21:04:43,342 ***** Eval results *****
2023-03-27 21:04:43,342   att_loss = 0.35001271655741295
2023-03-27 21:04:43,343   cls_loss = 0.0
2023-03-27 21:04:43,343   global_step = 7699
2023-03-27 21:04:43,343   loss = 0.8923889703280188
2023-03-27 21:04:43,343   rep_loss = 0.5423762547061047
2023-03-27 21:04:43,345 ***** Save model *****
2023-03-27 21:05:02,504 ***** Running evaluation *****
2023-03-27 21:05:02,504   Epoch = 26 iter 6999 step
2023-03-27 21:05:02,504   Num examples = 1043
2023-03-27 21:05:02,504   Batch size = 32
2023-03-27 21:05:02,505 ***** Eval results *****
2023-03-27 21:05:02,506   att_loss = 5563476.8771929825
2023-03-27 21:05:02,506   cls_loss = 0.0
2023-03-27 21:05:02,506   global_step = 6999
2023-03-27 21:05:02,506   loss = 5563477.3771929825
2023-03-27 21:05:02,506   rep_loss = 0.6080482821715506
2023-03-27 21:05:02,513 ***** Save model *****
2023-03-27 21:05:18,452 ***** Running evaluation *****
2023-03-27 21:05:18,453   Epoch = 26 iter 7049 step
2023-03-27 21:05:18,453   Num examples = 1043
2023-03-27 21:05:18,453   Batch size = 32
2023-03-27 21:05:18,454 ***** Eval results *****
2023-03-27 21:05:18,454   att_loss = 5554644.504672897
2023-03-27 21:05:18,455   cls_loss = 0.0
2023-03-27 21:05:18,455   global_step = 7049
2023-03-27 21:05:18,455   loss = 5554645.004672897
2023-03-27 21:05:18,455   rep_loss = 0.6073014123417507
2023-03-27 21:05:18,457 ***** Save model *****
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  2023-03-27 21:05:34,379 ***** Running evaluation *****
2023-03-27 21:05:34,379   Epoch = 26 iter 7099 step
2023-03-27 21:05:34,379   Num examples = 1043
2023-03-27 21:05:34,379   Batch size = 32
2023-03-27 21:05:34,380 ***** Eval results *****
2023-03-27 21:05:34,380   att_loss = 5541951.633757962
2023-03-27 21:05:34,381   cls_loss = 0.0
2023-03-27 21:05:34,381   global_step = 7099
2023-03-27 21:05:34,381   loss = 5541952.133757962
2023-03-27 21:05:34,381   rep_loss = 0.6069560738126184
2023-03-27 21:05:34,382 ***** Save model *****
2023-03-27 21:05:50,325 ***** Running evaluation *****
2023-03-27 21:05:50,325   Epoch = 26 iter 7149 step
2023-03-27 21:05:50,326   Num examples = 1043
2023-03-27 21:05:50,326   Batch size = 32
2023-03-27 21:05:50,327 ***** Eval results *****
2023-03-27 21:05:50,328   att_loss = 5546542.012077294
2023-03-27 21:05:50,328   cls_loss = 0.0
2023-03-27 21:05:50,328   global_step = 7149
2023-03-27 21:05:50,328   loss = 5546542.512077294
2023-03-27 21:05:50,328   rep_loss = 0.6072059362982782
2023-03-27 21:05:50,334 ***** Save model *****
   2023-03-27 21:06:06,211 ***** Running evaluation *****
2023-03-27 21:06:06,211   Epoch = 26 iter 7199 step
2023-03-27 21:06:06,211   Num examples = 1043
2023-03-27 21:06:06,211   Batch size = 32
2023-03-27 21:06:06,212 ***** Eval results *****
2023-03-27 21:06:06,213   att_loss = 5553765.875486381
2023-03-27 21:06:06,213   cls_loss = 0.0
2023-03-27 21:06:06,213   global_step = 7199
2023-03-27 21:06:06,213   loss = 5553766.375486381
2023-03-27 21:06:06,213   rep_loss = 0.6074496015964315
2023-03-27 21:06:06,214 ***** Save model *****
2023-03-27 21:06:22,127 ***** Running evaluation *****
2023-03-27 21:06:22,127   Epoch = 27 iter 7249 step
2023-03-27 21:06:22,127   Num examples = 1043
2023-03-27 21:06:22,127   Batch size = 32
2023-03-27 21:06:22,129 ***** Eval results *****
2023-03-27 21:06:22,129   att_loss = 5562100.1875
2023-03-27 21:06:22,129   cls_loss = 0.0
2023-03-27 21:06:22,129   global_step = 7249
2023-03-27 21:06:22,130   loss = 5562100.6875
2023-03-27 21:06:22,130   rep_loss = 0.6055165663361549
2023-03-27 21:06:22,137 ***** Save model *****
2023-03-27 21:06:38,066 ***** Running evaluation *****
2023-03-27 21:06:38,066   Epoch = 27 iter 7299 step
2023-03-27 21:06:38,067   Num examples = 1043
2023-03-27 21:06:38,067   Batch size = 32
2023-03-27 21:06:38,068 ***** Eval results *****
2023-03-27 21:06:38,068   att_loss = 5548034.15
2023-03-27 21:06:38,069   cls_loss = 0.0
2023-03-27 21:06:38,069   global_step = 7299
2023-03-27 21:06:38,069   loss = 5548034.65
2023-03-27 21:06:38,069   rep_loss = 0.6063983069525825
2023-03-27 21:06:38,075 ***** Save model *****
2023-03-27 21:06:54,020 ***** Running evaluation *****
2023-03-27 21:06:54,020   Epoch = 27 iter 7349 step
2023-03-27 21:06:54,020   Num examples = 1043
2023-03-27 21:06:54,020   Batch size = 32
2023-03-27 21:06:54,024 ***** Eval results *****
2023-03-27 21:06:54,024   att_loss = 5532486.732142857
2023-03-27 21:06:54,024   cls_loss = 0.0
2023-03-27 21:06:54,025   global_step = 7349
2023-03-27 21:06:54,025   loss = 5532487.232142857
2023-03-27 21:06:54,025   rep_loss = 0.6054656675883702
2023-03-27 21:06:54,028 ***** Save model *****
2023-03-27 21:07:09,950 ***** Running evaluation *****
2023-03-27 21:07:09,950   Epoch = 27 iter 7399 step
2023-03-27 21:07:09,950   Num examples = 1043
2023-03-27 21:07:09,950   Batch size = 32
2023-03-27 21:07:09,954 ***** Eval results *****
2023-03-27 21:07:09,954   att_loss = 5532083.707894737
2023-03-27 21:07:09,955   cls_loss = 0.0
2023-03-27 21:07:09,955   global_step = 7399
2023-03-27 21:07:09,955   loss = 5532084.207894737
2023-03-27 21:07:09,955   rep_loss = 0.6058588269509767
2023-03-27 21:07:09,964 ***** Save model *****
2023-03-27 21:07:25,887 ***** Running evaluation *****
2023-03-27 21:07:25,887   Epoch = 27 iter 7449 step
2023-03-27 21:07:25,887   Num examples = 1043
2023-03-27 21:07:25,888   Batch size = 32
2023-03-27 21:07:25,890 ***** Eval results *****
2023-03-27 21:07:25,891   att_loss = 5536974.6
2023-03-27 21:07:25,891   cls_loss = 0.0
2023-03-27 21:07:25,891   global_step = 7449
2023-03-27 21:07:25,891   loss = 5536975.1
2023-03-27 21:07:25,891   rep_loss = 0.6062286280095577
2023-03-27 21:07:25,894 ***** Save model *****
2023-03-27 21:07:41,821 ***** Running evaluation *****
2023-03-27 21:07:41,822   Epoch = 28 iter 7499 step
2023-03-27 21:07:41,822   Num examples = 1043
2023-03-27 21:07:41,822   Batch size = 32
2023-03-27 21:07:41,823 ***** Eval results *****
2023-03-27 21:07:41,823   att_loss = 5547991.326086956
2023-03-27 21:07:41,824   cls_loss = 0.0
2023-03-27 21:07:41,824   global_step = 7499
2023-03-27 21:07:41,824   loss = 5547991.826086956
2023-03-27 21:07:41,824   rep_loss = 0.6028277044710906
2023-03-27 21:07:41,826 ***** Save model *****
2023-03-27 21:07:57,752 ***** Running evaluation *****
2023-03-27 21:07:57,752   Epoch = 28 iter 7549 step
2023-03-27 21:07:57,752   Num examples = 1043
2023-03-27 21:07:57,753   Batch size = 32
2023-03-27 21:07:57,754 ***** Eval results *****
2023-03-27 21:07:57,754   att_loss = 5530827.890410959
2023-03-27 21:07:57,754   cls_loss = 0.0
2023-03-27 21:07:57,754   global_step = 7549
2023-03-27 21:07:57,754   loss = 5530828.390410959
2023-03-27 21:07:57,754   rep_loss = 0.604549283034181
2023-03-27 21:07:57,759 ***** Save model *****
2023-03-27 21:08:13,683 ***** Running evaluation *****
2023-03-27 21:08:13,684   Epoch = 28 iter 7599 step
2023-03-27 21:08:13,684   Num examples = 1043
2023-03-27 21:08:13,684   Batch size = 32
2023-03-27 21:08:13,685 ***** Eval results *****
2023-03-27 21:08:13,685   att_loss = 5542152.386178861
2023-03-27 21:08:13,685   cls_loss = 0.0
2023-03-27 21:08:13,685   global_step = 7599
2023-03-27 21:08:13,685   loss = 5542152.886178861
2023-03-27 21:08:13,686   rep_loss = 0.6048809013715605
2023-03-27 21:08:13,690 ***** Save model *****
2023-03-27 21:08:29,553 ***** Running evaluation *****
2023-03-27 21:08:29,553   Epoch = 28 iter 7649 step
2023-03-27 21:08:29,554   Num examples = 1043
2023-03-27 21:08:29,554   Batch size = 32
2023-03-27 21:08:29,555 ***** Eval results *****
2023-03-27 21:08:29,555   att_loss = 5547029.132947977
2023-03-27 21:08:29,555   cls_loss = 0.0
2023-03-27 21:08:29,556   global_step = 7649
2023-03-27 21:08:29,556   loss = 5547029.632947977
2023-03-27 21:08:29,556   rep_loss = 0.6050717258728997
2023-03-27 21:08:29,558 ***** Save model *****
2023-03-27 21:08:45,476 ***** Running evaluation *****
2023-03-27 21:08:45,476   Epoch = 28 iter 7699 step
2023-03-27 21:08:45,476   Num examples = 1043
2023-03-27 21:08:45,477   Batch size = 32
2023-03-27 21:08:45,478 ***** Eval results *****
2023-03-27 21:08:45,478   att_loss = 5536607.49103139
2023-03-27 21:08:45,478   cls_loss = 0.0
2023-03-27 21:08:45,478   global_step = 7699
2023-03-27 21:08:45,478   loss = 5536607.99103139
2023-03-27 21:08:45,479   rep_loss = 0.6042159429580107
2023-03-27 21:08:45,483 ***** Save model *****
2023-03-27 21:09:01,440 ***** Running evaluation *****
2023-03-27 21:09:01,441   Epoch = 29 iter 7749 step
2023-03-27 21:09:01,441   Num examples = 1043
2023-03-27 21:09:01,441   Batch size = 32
2023-03-27 21:09:01,442 ***** Eval results *****
2023-03-27 21:09:01,442   att_loss = 5414491.75
2023-03-27 21:09:01,442   cls_loss = 0.0
2023-03-27 21:09:01,442   global_step = 7749
2023-03-27 21:09:01,443   loss = 5414492.25
2023-03-27 21:09:01,443   rep_loss = 0.5975915094216665
2023-03-27 21:09:01,449 ***** Save model *****
2023-03-27 21:09:17,378 ***** Running evaluation *****
2023-03-27 21:09:17,378   Epoch = 29 iter 7799 step
2023-03-27 21:09:17,378   Num examples = 1043
2023-03-27 21:09:17,379   Batch size = 32
2023-03-27 21:09:17,380 ***** Eval results *****
2023-03-27 21:09:17,380   att_loss = 5450601.053571428
2023-03-27 21:09:17,380   cls_loss = 0.0
2023-03-27 21:09:17,380   global_step = 7799
2023-03-27 21:09:17,380   loss = 5450601.553571428
2023-03-27 21:09:17,380   rep_loss = 0.6014656360660281
2023-03-27 21:09:17,385 ***** Save model *****
2023-03-27 21:09:33,293 ***** Running evaluation *****
2023-03-27 21:09:33,294   Epoch = 29 iter 7849 step
2023-03-27 21:09:33,294   Num examples = 1043
2023-03-27 21:09:33,294   Batch size = 32
2023-03-27 21:09:33,295 ***** Eval results *****
2023-03-27 21:09:33,295   att_loss = 5493960.245283019
2023-03-27 21:09:33,296   cls_loss = 0.0
2023-03-27 21:09:33,296   global_step = 7849
2023-03-27 21:09:33,296   loss = 5493960.745283019
2023-03-27 21:09:33,296   rep_loss = 0.6024453982991992
2023-03-27 21:09:33,298 ***** Save model *****
2023-03-27 21:09:49,210 ***** Running evaluation *****
2023-03-27 21:09:49,210   Epoch = 29 iter 7899 step
2023-03-27 21:09:49,210   Num examples = 1043
2023-03-27 21:09:49,211   Batch size = 32
2023-03-27 21:09:49,212 ***** Eval results *****
2023-03-27 21:09:49,212   att_loss = 5501108.766025641
2023-03-27 21:09:49,212   cls_loss = 0.0
2023-03-27 21:09:49,212   global_step = 7899
2023-03-27 21:09:49,213   loss = 5501109.266025641
2023-03-27 21:09:49,213   rep_loss = 0.6028622744175104
2023-03-27 21:09:49,219 ***** Save model *****
2023-03-27 21:10:05,132 ***** Running evaluation *****
2023-03-27 21:10:05,132   Epoch = 29 iter 7949 step
2023-03-27 21:10:05,132   Num examples = 1043
2023-03-27 21:10:05,133   Batch size = 32
2023-03-27 21:10:05,134 ***** Eval results *****
2023-03-27 21:10:05,135   att_loss = 5499511.762135922
2023-03-27 21:10:05,135   cls_loss = 0.0
2023-03-27 21:10:05,135   global_step = 7949
2023-03-27 21:10:05,135   loss = 5499512.262135922
2023-03-27 21:10:05,136   rep_loss = 0.6031079295190792
2023-03-27 21:10:05,140 ***** Save model *****
2023-03-27 21:10:21,049 ***** Running evaluation *****
2023-03-27 21:10:21,050   Epoch = 29 iter 7999 step
2023-03-27 21:10:21,050   Num examples = 1043
2023-03-27 21:10:21,050   Batch size = 32
2023-03-27 21:10:21,051 ***** Eval results *****
2023-03-27 21:10:21,052   att_loss = 5491728.111328125
2023-03-27 21:10:21,052   cls_loss = 0.0
2023-03-27 21:10:21,052   global_step = 7999
2023-03-27 21:10:21,052   loss = 5491728.611328125
2023-03-27 21:10:21,052   rep_loss = 0.6023421708960086
2023-03-27 21:10:21,059 ***** Save model *****
2023-03-27 23:56:00,848 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TinyBERT_CoLA_4L_312D_kl_weight0.01', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=True, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='KL_ATTN_SWEEP_BATCHMEAN', kl_attn_weight=None)
2023-03-27 23:56:02,835 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 23:59:05,643 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TinyBERT_CoLA_4L_312D_kl_weight0.01', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=True, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='KL_ATTN_SWEEP_BATCHMEAN', kl_attn_weight=None)
2023-03-27 23:59:07,045 Starting sweep agent: entity=None, project=None, count=None
2023-03-27 23:59:09,719 device: cuda n_gpu: 1
2023-03-27 23:59:09,782 Writing example 0 of 8551
2023-03-27 23:59:09,782 *** Example ***
2023-03-27 23:59:09,783 guid: train-0
2023-03-27 23:59:09,783 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-27 23:59:09,783 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 23:59:09,784 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 23:59:09,784 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 23:59:09,784 label: 1
2023-03-27 23:59:09,784 label_id: 1
2023-03-27 23:59:10,803 Writing example 0 of 1043
2023-03-27 23:59:10,803 *** Example ***
2023-03-27 23:59:10,803 guid: dev-0
2023-03-27 23:59:10,803 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-27 23:59:10,804 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 23:59:10,804 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 23:59:10,804 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-27 23:59:10,804 label: 1
2023-03-27 23:59:10,804 label_id: 1
2023-03-27 23:59:10,924 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-27 23:59:10,938 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 23:59:12,723 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-27 23:59:13,789 loading model...
2023-03-27 23:59:13,878 done!
2023-03-27 23:59:13,879 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-27 23:59:15,478 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01
2023-03-27 23:59:15,486 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-27 23:59:15,721 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01/pytorch_model.bin
2023-03-27 23:59:16,303 loading model...
2023-03-27 23:59:16,321 done!
2023-03-27 23:59:16,342 ***** Running training *****
2023-03-27 23:59:16,343   Num examples = 8551
2023-03-27 23:59:16,343   Batch size = 16
2023-03-27 23:59:16,344   Num steps = 1602
2023-03-27 23:59:16,344 n: bert.embeddings.word_embeddings.weight
2023-03-27 23:59:16,345 n: bert.embeddings.position_embeddings.weight
2023-03-27 23:59:16,345 n: bert.embeddings.token_type_embeddings.weight
2023-03-27 23:59:16,345 n: bert.embeddings.LayerNorm.weight
2023-03-27 23:59:16,346 n: bert.embeddings.LayerNorm.bias
2023-03-27 23:59:16,346 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-27 23:59:16,346 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-27 23:59:16,347 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-27 23:59:16,347 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-27 23:59:16,347 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-27 23:59:16,347 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-27 23:59:16,347 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-27 23:59:16,348 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-27 23:59:16,348 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-27 23:59:16,348 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-27 23:59:16,348 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-27 23:59:16,348 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-27 23:59:16,349 n: bert.encoder.layer.0.output.dense.weight
2023-03-27 23:59:16,349 n: bert.encoder.layer.0.output.dense.bias
2023-03-27 23:59:16,349 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-27 23:59:16,349 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-27 23:59:16,349 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-27 23:59:16,349 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-27 23:59:16,350 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-27 23:59:16,350 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-27 23:59:16,350 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-27 23:59:16,350 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-27 23:59:16,350 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-27 23:59:16,350 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-27 23:59:16,351 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-27 23:59:16,351 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-27 23:59:16,351 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-27 23:59:16,351 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-27 23:59:16,351 n: bert.encoder.layer.1.output.dense.weight
2023-03-27 23:59:16,351 n: bert.encoder.layer.1.output.dense.bias
2023-03-27 23:59:16,351 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-27 23:59:16,351 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-27 23:59:16,351 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-27 23:59:16,352 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-27 23:59:16,352 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-27 23:59:16,352 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-27 23:59:16,352 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-27 23:59:16,352 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-27 23:59:16,352 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-27 23:59:16,352 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-27 23:59:16,352 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-27 23:59:16,352 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-27 23:59:16,353 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-27 23:59:16,353 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-27 23:59:16,353 n: bert.encoder.layer.2.output.dense.weight
2023-03-27 23:59:16,353 n: bert.encoder.layer.2.output.dense.bias
2023-03-27 23:59:16,353 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-27 23:59:16,353 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-27 23:59:16,353 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-27 23:59:16,353 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-27 23:59:16,353 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-27 23:59:16,353 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-27 23:59:16,353 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-27 23:59:16,354 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-27 23:59:16,354 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-27 23:59:16,354 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-27 23:59:16,354 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-27 23:59:16,354 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-27 23:59:16,354 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-27 23:59:16,354 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-27 23:59:16,354 n: bert.encoder.layer.3.output.dense.weight
2023-03-27 23:59:16,354 n: bert.encoder.layer.3.output.dense.bias
2023-03-27 23:59:16,354 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-27 23:59:16,354 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-27 23:59:16,355 n: bert.pooler.dense.weight
2023-03-27 23:59:16,355 n: bert.pooler.dense.bias
2023-03-27 23:59:16,355 n: classifier.weight
2023-03-27 23:59:16,355 n: classifier.bias
2023-03-27 23:59:16,355 n: fit_dense.weight
2023-03-27 23:59:16,355 n: fit_dense.bias
2023-03-27 23:59:16,355 Total parameters: 14591258
2023-03-27 23:59:24,707 ***** Running evaluation *****
2023-03-27 23:59:24,708   Epoch = 0 iter 49 step
2023-03-27 23:59:24,708   Num examples = 1043
2023-03-27 23:59:24,708   Batch size = 32
2023-03-27 23:59:25,306 ***** Eval results *****
2023-03-27 23:59:25,306   acc = 0.7372962607861937
2023-03-27 23:59:25,307   att_loss = 0.0
2023-03-27 23:59:25,307   cls_loss = 0.33885330569987393
2023-03-27 23:59:25,307   eval_loss = 0.6491988933447636
2023-03-27 23:59:25,307   global_step = 49
2023-03-27 23:59:25,307   loss = 0.33885330569987393
2023-03-27 23:59:25,307   mcc = 0.30824938608195934
2023-03-27 23:59:25,307   rep_loss = 0.0
2023-03-27 23:59:25,314 ***** Save model *****
2023-03-27 23:59:33,940 ***** Running evaluation *****
2023-03-27 23:59:33,941   Epoch = 0 iter 99 step
2023-03-27 23:59:33,941   Num examples = 1043
2023-03-27 23:59:33,941   Batch size = 32
2023-03-27 23:59:34,534 ***** Eval results *****
2023-03-27 23:59:34,534   acc = 0.7344199424736337
2023-03-27 23:59:34,534   att_loss = 0.0
2023-03-27 23:59:34,534   cls_loss = 0.3238991962538825
2023-03-27 23:59:34,534   eval_loss = 0.5843693261796777
2023-03-27 23:59:34,535   global_step = 99
2023-03-27 23:59:34,535   loss = 0.3238991962538825
2023-03-27 23:59:34,535   mcc = 0.2978314478991522
2023-03-27 23:59:34,535   rep_loss = 0.0
2023-03-27 23:59:42,471 ***** Running evaluation *****
2023-03-27 23:59:42,471   Epoch = 0 iter 149 step
2023-03-27 23:59:42,472   Num examples = 1043
2023-03-27 23:59:42,472   Batch size = 32
2023-03-27 23:59:43,064 ***** Eval results *****
2023-03-27 23:59:43,065   acc = 0.7372962607861937
2023-03-27 23:59:43,065   att_loss = 0.0
2023-03-27 23:59:43,065   cls_loss = 0.31266469303393524
2023-03-27 23:59:43,065   eval_loss = 0.5605835589495572
2023-03-27 23:59:43,065   global_step = 149
2023-03-27 23:59:43,065   loss = 0.31266469303393524
2023-03-27 23:59:43,065   mcc = 0.3237326020252071
2023-03-27 23:59:43,065   rep_loss = 0.0
2023-03-27 23:59:43,073 ***** Save model *****
2023-03-27 23:59:51,725 ***** Running evaluation *****
2023-03-27 23:59:51,726   Epoch = 0 iter 199 step
2023-03-27 23:59:51,726   Num examples = 1043
2023-03-27 23:59:51,726   Batch size = 32
2023-03-27 23:59:52,318 ***** Eval results *****
2023-03-27 23:59:52,319   acc = 0.7238734419942474
2023-03-27 23:59:52,319   att_loss = 0.0
2023-03-27 23:59:52,319   cls_loss = 0.30440946844354944
2023-03-27 23:59:52,319   eval_loss = 0.5606941246625149
2023-03-27 23:59:52,319   global_step = 199
2023-03-27 23:59:52,319   loss = 0.30440946844354944
2023-03-27 23:59:52,319   mcc = 0.316123487599861
2023-03-27 23:59:52,319   rep_loss = 0.0
2023-03-28 00:00:00,282 ***** Running evaluation *****
2023-03-28 00:00:00,282   Epoch = 0 iter 249 step
2023-03-28 00:00:00,283   Num examples = 1043
2023-03-28 00:00:00,283   Batch size = 32
2023-03-28 00:00:00,878 ***** Eval results *****
2023-03-28 00:00:00,879   acc = 0.7363374880153404
2023-03-28 00:00:00,879   att_loss = 0.0
2023-03-28 00:00:00,879   cls_loss = 0.2990471514832064
2023-03-28 00:00:00,879   eval_loss = 0.5547104152766141
2023-03-28 00:00:00,879   global_step = 249
2023-03-28 00:00:00,879   loss = 0.2990471514832064
2023-03-28 00:00:00,879   mcc = 0.32066729696834345
2023-03-28 00:00:00,880   rep_loss = 0.0
2023-03-28 00:00:08,915 ***** Running evaluation *****
2023-03-28 00:00:08,916   Epoch = 0 iter 299 step
2023-03-28 00:00:08,916   Num examples = 1043
2023-03-28 00:00:08,916   Batch size = 32
2023-03-28 00:00:09,511 ***** Eval results *****
2023-03-28 00:00:09,512   acc = 0.7286673058485139
2023-03-28 00:00:09,512   att_loss = 0.0
2023-03-28 00:00:09,512   cls_loss = 0.2953180235664183
2023-03-28 00:00:09,512   eval_loss = 0.5561566036759
2023-03-28 00:00:09,512   global_step = 299
2023-03-28 00:00:09,512   loss = 0.2953180235664183
2023-03-28 00:00:09,512   mcc = 0.332634547752676
2023-03-28 00:00:09,513   rep_loss = 0.0
2023-03-28 00:00:09,514 ***** Save model *****
2023-03-28 00:00:18,169 ***** Running evaluation *****
2023-03-28 00:00:18,169   Epoch = 0 iter 349 step
2023-03-28 00:00:18,169   Num examples = 1043
2023-03-28 00:00:18,169   Batch size = 32
2023-03-28 00:00:18,765 ***** Eval results *****
2023-03-28 00:00:18,765   acc = 0.7315436241610739
2023-03-28 00:00:18,765   att_loss = 0.0
2023-03-28 00:00:18,765   cls_loss = 0.2924519468173598
2023-03-28 00:00:18,765   eval_loss = 0.5577743857195883
2023-03-28 00:00:18,765   global_step = 349
2023-03-28 00:00:18,765   loss = 0.2924519468173598
2023-03-28 00:00:18,765   mcc = 0.2854756779127197
2023-03-28 00:00:18,765   rep_loss = 0.0
2023-03-28 00:00:26,747 ***** Running evaluation *****
2023-03-28 00:00:26,747   Epoch = 0 iter 399 step
2023-03-28 00:00:26,747   Num examples = 1043
2023-03-28 00:00:26,747   Batch size = 32
2023-03-28 00:00:27,343 ***** Eval results *****
2023-03-28 00:00:27,344   acc = 0.7344199424736337
2023-03-28 00:00:27,344   att_loss = 0.0
2023-03-28 00:00:27,344   cls_loss = 0.29057223874524724
2023-03-28 00:00:27,344   eval_loss = 0.5500271139722882
2023-03-28 00:00:27,344   global_step = 399
2023-03-28 00:00:27,344   loss = 0.29057223874524724
2023-03-28 00:00:27,344   mcc = 0.31601258155007883
2023-03-28 00:00:27,344   rep_loss = 0.0
2023-03-28 00:00:35,329 ***** Running evaluation *****
2023-03-28 00:00:35,329   Epoch = 0 iter 449 step
2023-03-28 00:00:35,329   Num examples = 1043
2023-03-28 00:00:35,329   Batch size = 32
2023-03-28 00:00:35,927 ***** Eval results *****
2023-03-28 00:00:35,928   acc = 0.7305848513902206
2023-03-28 00:00:35,928   att_loss = 0.0
2023-03-28 00:00:35,928   cls_loss = 0.2886512992044865
2023-03-28 00:00:35,928   eval_loss = 0.5484534592339487
2023-03-28 00:00:35,928   global_step = 449
2023-03-28 00:00:35,928   loss = 0.2886512992044865
2023-03-28 00:00:35,929   mcc = 0.30932848813727776
2023-03-28 00:00:35,929   rep_loss = 0.0
2023-03-28 00:00:43,930 ***** Running evaluation *****
2023-03-28 00:00:43,930   Epoch = 0 iter 499 step
2023-03-28 00:00:43,931   Num examples = 1043
2023-03-28 00:00:43,931   Batch size = 32
2023-03-28 00:00:44,529 ***** Eval results *****
2023-03-28 00:00:44,530   acc = 0.7334611697027804
2023-03-28 00:00:44,530   att_loss = 0.0
2023-03-28 00:00:44,530   cls_loss = 0.2868491392993258
2023-03-28 00:00:44,530   eval_loss = 0.5457829697565599
2023-03-28 00:00:44,530   global_step = 499
2023-03-28 00:00:44,530   loss = 0.2868491392993258
2023-03-28 00:00:44,531   mcc = 0.31927567720124567
2023-03-28 00:00:44,531   rep_loss = 0.0
2023-03-28 00:00:52,518 ***** Running evaluation *****
2023-03-28 00:00:52,519   Epoch = 1 iter 549 step
2023-03-28 00:00:52,519   Num examples = 1043
2023-03-28 00:00:52,520   Batch size = 32
2023-03-28 00:00:53,119 ***** Eval results *****
2023-03-28 00:00:53,120   acc = 0.7363374880153404
2023-03-28 00:00:53,120   att_loss = 0.0
2023-03-28 00:00:53,120   cls_loss = 0.26884395877520245
2023-03-28 00:00:53,120   eval_loss = 0.5466778233195796
2023-03-28 00:00:53,121   global_step = 549
2023-03-28 00:00:53,121   loss = 0.26884395877520245
2023-03-28 00:00:53,121   mcc = 0.30952433026244985
2023-03-28 00:00:53,121   rep_loss = 0.0
2023-03-28 00:01:01,127 ***** Running evaluation *****
2023-03-28 00:01:01,128   Epoch = 1 iter 599 step
2023-03-28 00:01:01,128   Num examples = 1043
2023-03-28 00:01:01,128   Batch size = 32
2023-03-28 00:01:01,729 ***** Eval results *****
2023-03-28 00:01:01,729   acc = 0.7430488974113135
2023-03-28 00:01:01,729   att_loss = 0.0
2023-03-28 00:01:01,729   cls_loss = 0.27439132309876957
2023-03-28 00:01:01,729   eval_loss = 0.5428797250444238
2023-03-28 00:01:01,729   global_step = 599
2023-03-28 00:01:01,729   loss = 0.27439132309876957
2023-03-28 00:01:01,730   mcc = 0.33993010293244547
2023-03-28 00:01:01,730   rep_loss = 0.0
2023-03-28 00:01:01,736 ***** Save model *****
2023-03-28 00:01:10,416 ***** Running evaluation *****
2023-03-28 00:01:10,417   Epoch = 1 iter 649 step
2023-03-28 00:01:10,417   Num examples = 1043
2023-03-28 00:01:10,417   Batch size = 32
2023-03-28 00:01:11,016 ***** Eval results *****
2023-03-28 00:01:11,017   acc = 0.738255033557047
2023-03-28 00:01:11,017   att_loss = 0.0
2023-03-28 00:01:11,017   cls_loss = 0.2732378377862599
2023-03-28 00:01:11,017   eval_loss = 0.5440557617129702
2023-03-28 00:01:11,017   global_step = 649
2023-03-28 00:01:11,017   loss = 0.2732378377862599
2023-03-28 00:01:11,017   mcc = 0.3366806107924566
2023-03-28 00:01:11,017   rep_loss = 0.0
2023-03-28 00:01:19,017 ***** Running evaluation *****
2023-03-28 00:01:19,017   Epoch = 1 iter 699 step
2023-03-28 00:01:19,018   Num examples = 1043
2023-03-28 00:01:19,018   Batch size = 32
2023-03-28 00:01:19,618 ***** Eval results *****
2023-03-28 00:01:19,618   acc = 0.7440076701821668
2023-03-28 00:01:19,618   att_loss = 0.0
2023-03-28 00:01:19,619   cls_loss = 0.2725823098962957
2023-03-28 00:01:19,619   eval_loss = 0.5451390192364202
2023-03-28 00:01:19,619   global_step = 699
2023-03-28 00:01:19,619   loss = 0.2725823098962957
2023-03-28 00:01:19,619   mcc = 0.334869093311565
2023-03-28 00:01:19,619   rep_loss = 0.0
2023-03-28 00:01:27,627 ***** Running evaluation *****
2023-03-28 00:01:27,627   Epoch = 1 iter 749 step
2023-03-28 00:01:27,627   Num examples = 1043
2023-03-28 00:01:27,627   Batch size = 32
2023-03-28 00:01:28,236 ***** Eval results *****
2023-03-28 00:01:28,236   acc = 0.7459252157238735
2023-03-28 00:01:28,237   att_loss = 0.0
2023-03-28 00:01:28,237   cls_loss = 0.27253518166930174
2023-03-28 00:01:28,237   eval_loss = 0.5452113043178212
2023-03-28 00:01:28,237   global_step = 749
2023-03-28 00:01:28,237   loss = 0.27253518166930174
2023-03-28 00:01:28,238   mcc = 0.3459141688557483
2023-03-28 00:01:28,238   rep_loss = 0.0
2023-03-28 00:01:28,245 ***** Save model *****
2023-03-28 00:01:36,938 ***** Running evaluation *****
2023-03-28 00:01:36,939   Epoch = 1 iter 799 step
2023-03-28 00:01:36,939   Num examples = 1043
2023-03-28 00:01:36,939   Batch size = 32
2023-03-28 00:01:37,540 ***** Eval results *****
2023-03-28 00:01:37,541   acc = 0.738255033557047
2023-03-28 00:01:37,541   att_loss = 0.0
2023-03-28 00:01:37,541   cls_loss = 0.27158926185571924
2023-03-28 00:01:37,541   eval_loss = 0.5469632383548853
2023-03-28 00:01:37,542   global_step = 799
2023-03-28 00:01:37,542   loss = 0.27158926185571924
2023-03-28 00:01:37,542   mcc = 0.33508616900126137
2023-03-28 00:01:37,542   rep_loss = 0.0
2023-03-28 00:01:45,565 ***** Running evaluation *****
2023-03-28 00:01:45,565   Epoch = 1 iter 849 step
2023-03-28 00:01:45,566   Num examples = 1043
2023-03-28 00:01:45,566   Batch size = 32
2023-03-28 00:01:46,169 ***** Eval results *****
2023-03-28 00:01:46,169   acc = 0.7440076701821668
2023-03-28 00:01:46,169   att_loss = 0.0
2023-03-28 00:01:46,169   cls_loss = 0.27176482951830305
2023-03-28 00:01:46,169   eval_loss = 0.5467509537032156
2023-03-28 00:01:46,169   global_step = 849
2023-03-28 00:01:46,170   loss = 0.27176482951830305
2023-03-28 00:01:46,170   mcc = 0.32914451996870747
2023-03-28 00:01:46,170   rep_loss = 0.0
2023-03-28 00:01:54,189 ***** Running evaluation *****
2023-03-28 00:01:54,189   Epoch = 1 iter 899 step
2023-03-28 00:01:54,189   Num examples = 1043
2023-03-28 00:01:54,189   Batch size = 32
2023-03-28 00:01:54,791 ***** Eval results *****
2023-03-28 00:01:54,791   acc = 0.7411313518696069
2023-03-28 00:01:54,791   att_loss = 0.0
2023-03-28 00:01:54,791   cls_loss = 0.27193395005513543
2023-03-28 00:01:54,792   eval_loss = 0.5465984344482422
2023-03-28 00:01:54,792   global_step = 899
2023-03-28 00:01:54,792   loss = 0.27193395005513543
2023-03-28 00:01:54,792   mcc = 0.32940166149822514
2023-03-28 00:01:54,792   rep_loss = 0.0
2023-03-28 00:02:02,821 ***** Running evaluation *****
2023-03-28 00:02:02,821   Epoch = 1 iter 949 step
2023-03-28 00:02:02,821   Num examples = 1043
2023-03-28 00:02:02,821   Batch size = 32
2023-03-28 00:02:03,425 ***** Eval results *****
2023-03-28 00:02:03,425   acc = 0.7392138063279002
2023-03-28 00:02:03,426   att_loss = 0.0
2023-03-28 00:02:03,426   cls_loss = 0.27177377160055094
2023-03-28 00:02:03,426   eval_loss = 0.5478207956660878
2023-03-28 00:02:03,426   global_step = 949
2023-03-28 00:02:03,426   loss = 0.27177377160055094
2023-03-28 00:02:03,427   mcc = 0.33346867625245996
2023-03-28 00:02:03,427   rep_loss = 0.0
2023-03-28 00:02:11,453 ***** Running evaluation *****
2023-03-28 00:02:11,453   Epoch = 1 iter 999 step
2023-03-28 00:02:11,453   Num examples = 1043
2023-03-28 00:02:11,453   Batch size = 32
2023-03-28 00:02:12,059 ***** Eval results *****
2023-03-28 00:02:12,059   acc = 0.7411313518696069
2023-03-28 00:02:12,059   att_loss = 0.0
2023-03-28 00:02:12,060   cls_loss = 0.2714641381976425
2023-03-28 00:02:12,060   eval_loss = 0.5458868010477587
2023-03-28 00:02:12,060   global_step = 999
2023-03-28 00:02:12,060   loss = 0.2714641381976425
2023-03-28 00:02:12,060   mcc = 0.3282176894264508
2023-03-28 00:02:12,060   rep_loss = 0.0
2023-03-28 00:02:20,076 ***** Running evaluation *****
2023-03-28 00:02:20,076   Epoch = 1 iter 1049 step
2023-03-28 00:02:20,076   Num examples = 1043
2023-03-28 00:02:20,077   Batch size = 32
2023-03-28 00:02:20,679 ***** Eval results *****
2023-03-28 00:02:20,680   acc = 0.7420901246404602
2023-03-28 00:02:20,680   att_loss = 0.0
2023-03-28 00:02:20,680   cls_loss = 0.2715712700075316
2023-03-28 00:02:20,680   eval_loss = 0.5444475105314543
2023-03-28 00:02:20,680   global_step = 1049
2023-03-28 00:02:20,680   loss = 0.2715712700075316
2023-03-28 00:02:20,681   mcc = 0.3319045664297391
2023-03-28 00:02:20,681   rep_loss = 0.0
2023-03-28 00:02:28,715 ***** Running evaluation *****
2023-03-28 00:02:28,716   Epoch = 2 iter 1099 step
2023-03-28 00:02:28,716   Num examples = 1043
2023-03-28 00:02:28,716   Batch size = 32
2023-03-28 00:02:29,319 ***** Eval results *****
2023-03-28 00:02:29,319   acc = 0.7267497603068073
2023-03-28 00:02:29,319   att_loss = 0.0
2023-03-28 00:02:29,319   cls_loss = 0.26899741301613467
2023-03-28 00:02:29,319   eval_loss = 0.5450800016070857
2023-03-28 00:02:29,319   global_step = 1099
2023-03-28 00:02:29,319   loss = 0.26899741301613467
2023-03-28 00:02:29,319   mcc = 0.30140571370330893
2023-03-28 00:02:29,320   rep_loss = 0.0
2023-03-28 00:02:37,342 ***** Running evaluation *****
2023-03-28 00:02:37,342   Epoch = 2 iter 1149 step
2023-03-28 00:02:37,343   Num examples = 1043
2023-03-28 00:02:37,343   Batch size = 32
2023-03-28 00:02:37,949 ***** Eval results *****
2023-03-28 00:02:37,949   acc = 0.7401725790987536
2023-03-28 00:02:37,949   att_loss = 0.0
2023-03-28 00:02:37,950   cls_loss = 0.2713141088132505
2023-03-28 00:02:37,950   eval_loss = 0.5428620911005771
2023-03-28 00:02:37,950   global_step = 1149
2023-03-28 00:02:37,950   loss = 0.2713141088132505
2023-03-28 00:02:37,950   mcc = 0.32947359113341773
2023-03-28 00:02:37,950   rep_loss = 0.0
2023-03-28 00:02:45,971 ***** Running evaluation *****
2023-03-28 00:02:45,971   Epoch = 2 iter 1199 step
2023-03-28 00:02:45,971   Num examples = 1043
2023-03-28 00:02:45,971   Batch size = 32
2023-03-28 00:02:46,574 ***** Eval results *****
2023-03-28 00:02:46,574   acc = 0.7392138063279002
2023-03-28 00:02:46,574   att_loss = 0.0
2023-03-28 00:02:46,575   cls_loss = 0.27012854678030235
2023-03-28 00:02:46,575   eval_loss = 0.5425491766496138
2023-03-28 00:02:46,575   global_step = 1199
2023-03-28 00:02:46,575   loss = 0.27012854678030235
2023-03-28 00:02:46,575   mcc = 0.3284361237784734
2023-03-28 00:02:46,575   rep_loss = 0.0
2023-03-28 00:02:54,596 ***** Running evaluation *****
2023-03-28 00:02:54,596   Epoch = 2 iter 1249 step
2023-03-28 00:02:54,596   Num examples = 1043
2023-03-28 00:02:54,596   Batch size = 32
2023-03-28 00:02:55,200 ***** Eval results *****
2023-03-28 00:02:55,200   acc = 0.7401725790987536
2023-03-28 00:02:55,200   att_loss = 0.0
2023-03-28 00:02:55,200   cls_loss = 0.27058869221592474
2023-03-28 00:02:55,201   eval_loss = 0.5433466895060106
2023-03-28 00:02:55,201   global_step = 1249
2023-03-28 00:02:55,201   loss = 0.27058869221592474
2023-03-28 00:02:55,201   mcc = 0.3335779251123227
2023-03-28 00:02:55,201   rep_loss = 0.0
2023-03-28 00:03:03,238 ***** Running evaluation *****
2023-03-28 00:03:03,238   Epoch = 2 iter 1299 step
2023-03-28 00:03:03,238   Num examples = 1043
2023-03-28 00:03:03,239   Batch size = 32
2023-03-28 00:03:03,843 ***** Eval results *****
2023-03-28 00:03:03,843   acc = 0.7315436241610739
2023-03-28 00:03:03,843   att_loss = 0.0
2023-03-28 00:03:03,843   cls_loss = 0.27037943964138694
2023-03-28 00:03:03,843   eval_loss = 0.5428473850091299
2023-03-28 00:03:03,844   global_step = 1299
2023-03-28 00:03:03,844   loss = 0.27037943964138694
2023-03-28 00:03:03,844   mcc = 0.3123814262740551
2023-03-28 00:03:03,844   rep_loss = 0.0
2023-03-28 00:03:11,895 ***** Running evaluation *****
2023-03-28 00:03:11,896   Epoch = 2 iter 1349 step
2023-03-28 00:03:11,896   Num examples = 1043
2023-03-28 00:03:11,896   Batch size = 32
2023-03-28 00:03:12,499 ***** Eval results *****
2023-03-28 00:03:12,500   acc = 0.7420901246404602
2023-03-28 00:03:12,500   att_loss = 0.0
2023-03-28 00:03:12,500   cls_loss = 0.2703264039076096
2023-03-28 00:03:12,500   eval_loss = 0.541332028128884
2023-03-28 00:03:12,500   global_step = 1349
2023-03-28 00:03:12,500   loss = 0.2703264039076096
2023-03-28 00:03:12,500   mcc = 0.3355929137326878
2023-03-28 00:03:12,501   rep_loss = 0.0
2023-03-28 00:03:20,542 ***** Running evaluation *****
2023-03-28 00:03:20,542   Epoch = 2 iter 1399 step
2023-03-28 00:03:20,542   Num examples = 1043
2023-03-28 00:03:20,542   Batch size = 32
2023-03-28 00:03:21,147 ***** Eval results *****
2023-03-28 00:03:21,147   acc = 0.7449664429530202
2023-03-28 00:03:21,147   att_loss = 0.0
2023-03-28 00:03:21,147   cls_loss = 0.27068635866361085
2023-03-28 00:03:21,148   eval_loss = 0.5412637790044149
2023-03-28 00:03:21,148   global_step = 1399
2023-03-28 00:03:21,148   loss = 0.27068635866361085
2023-03-28 00:03:21,148   mcc = 0.34114287267917587
2023-03-28 00:03:21,148   rep_loss = 0.0
2023-03-28 00:03:29,186 ***** Running evaluation *****
2023-03-28 00:03:29,186   Epoch = 2 iter 1449 step
2023-03-28 00:03:29,187   Num examples = 1043
2023-03-28 00:03:29,187   Batch size = 32
2023-03-28 00:03:29,791 ***** Eval results *****
2023-03-28 00:03:29,791   acc = 0.7430488974113135
2023-03-28 00:03:29,791   att_loss = 0.0
2023-03-28 00:03:29,791   cls_loss = 0.27033733684090494
2023-03-28 00:03:29,791   eval_loss = 0.5413770937558376
2023-03-28 00:03:29,792   global_step = 1449
2023-03-28 00:03:29,792   loss = 0.27033733684090494
2023-03-28 00:03:29,792   mcc = 0.3386383515722791
2023-03-28 00:03:29,792   rep_loss = 0.0
2023-03-28 00:03:37,849 ***** Running evaluation *****
2023-03-28 00:03:37,849   Epoch = 2 iter 1499 step
2023-03-28 00:03:37,849   Num examples = 1043
2023-03-28 00:03:37,849   Batch size = 32
2023-03-28 00:03:38,454 ***** Eval results *****
2023-03-28 00:03:38,454   acc = 0.7411313518696069
2023-03-28 00:03:38,455   att_loss = 0.0
2023-03-28 00:03:38,455   cls_loss = 0.27028860369580526
2023-03-28 00:03:38,455   eval_loss = 0.5417198331067057
2023-03-28 00:03:38,455   global_step = 1499
2023-03-28 00:03:38,455   loss = 0.27028860369580526
2023-03-28 00:03:38,455   mcc = 0.3352124333269541
2023-03-28 00:03:38,456   rep_loss = 0.0
2023-03-28 00:03:46,492 ***** Running evaluation *****
2023-03-28 00:03:46,493   Epoch = 2 iter 1549 step
2023-03-28 00:03:46,493   Num examples = 1043
2023-03-28 00:03:46,494   Batch size = 32
2023-03-28 00:03:47,102 ***** Eval results *****
2023-03-28 00:03:47,102   acc = 0.7411313518696069
2023-03-28 00:03:47,102   att_loss = 0.0
2023-03-28 00:03:47,102   cls_loss = 0.2702227036261509
2023-03-28 00:03:47,103   eval_loss = 0.5417696120160999
2023-03-28 00:03:47,103   global_step = 1549
2023-03-28 00:03:47,103   loss = 0.2702227036261509
2023-03-28 00:03:47,103   mcc = 0.3352124333269541
2023-03-28 00:03:47,103   rep_loss = 0.0
2023-03-28 00:03:55,141 ***** Running evaluation *****
2023-03-28 00:03:55,141   Epoch = 2 iter 1599 step
2023-03-28 00:03:55,141   Num examples = 1043
2023-03-28 00:03:55,141   Batch size = 32
2023-03-28 00:03:55,747 ***** Eval results *****
2023-03-28 00:03:55,747   acc = 0.7449664429530202
2023-03-28 00:03:55,747   att_loss = 0.0
2023-03-28 00:03:55,747   cls_loss = 0.2700506564363023
2023-03-28 00:03:55,748   eval_loss = 0.5416369564605482
2023-03-28 00:03:55,748   global_step = 1599
2023-03-28 00:03:55,748   loss = 0.2700506564363023
2023-03-28 00:03:55,748   mcc = 0.34470146250315514
2023-03-28 00:03:55,748   rep_loss = 0.0
2023-03-28 00:04:05,030 device: cuda n_gpu: 1
2023-03-28 00:04:05,151 Writing example 0 of 8551
2023-03-28 00:04:05,151 *** Example ***
2023-03-28 00:04:05,151 guid: train-0
2023-03-28 00:04:05,152 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-28 00:04:05,152 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:04:05,152 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:04:05,153 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:04:05,153 label: 1
2023-03-28 00:04:05,153 label_id: 1
2023-03-28 00:04:06,097 Writing example 0 of 1043
2023-03-28 00:04:06,098 *** Example ***
2023-03-28 00:04:06,098 guid: dev-0
2023-03-28 00:04:06,098 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-28 00:04:06,098 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:04:06,098 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:04:06,099 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:04:06,099 label: 1
2023-03-28 00:04:06,099 label_id: 1
2023-03-28 00:04:06,215 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-28 00:04:06,216 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-28 00:04:07,922 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-28 00:04:08,106 loading model...
2023-03-28 00:04:08,192 done!
2023-03-28 00:04:08,193 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-28 00:04:08,282 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01
2023-03-28 00:04:08,283 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-28 00:04:08,516 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01/pytorch_model.bin
2023-03-28 00:04:08,531 loading model...
2023-03-28 00:04:08,544 done!
2023-03-28 00:04:08,560 ***** Running training *****
2023-03-28 00:04:08,560   Num examples = 8551
2023-03-28 00:04:08,560   Batch size = 16
2023-03-28 00:04:08,560   Num steps = 1602
2023-03-28 00:04:08,560 n: bert.embeddings.word_embeddings.weight
2023-03-28 00:04:08,560 n: bert.embeddings.position_embeddings.weight
2023-03-28 00:04:08,561 n: bert.embeddings.token_type_embeddings.weight
2023-03-28 00:04:08,561 n: bert.embeddings.LayerNorm.weight
2023-03-28 00:04:08,561 n: bert.embeddings.LayerNorm.bias
2023-03-28 00:04:08,561 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-28 00:04:08,561 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-28 00:04:08,561 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-28 00:04:08,561 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-28 00:04:08,561 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-28 00:04:08,561 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-28 00:04:08,562 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-28 00:04:08,562 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-28 00:04:08,562 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-28 00:04:08,562 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-28 00:04:08,562 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-28 00:04:08,562 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-28 00:04:08,562 n: bert.encoder.layer.0.output.dense.weight
2023-03-28 00:04:08,562 n: bert.encoder.layer.0.output.dense.bias
2023-03-28 00:04:08,562 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-28 00:04:08,562 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-28 00:04:08,563 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-28 00:04:08,563 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-28 00:04:08,563 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-28 00:04:08,563 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-28 00:04:08,563 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-28 00:04:08,563 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-28 00:04:08,563 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-28 00:04:08,563 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-28 00:04:08,563 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-28 00:04:08,563 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-28 00:04:08,564 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-28 00:04:08,564 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-28 00:04:08,564 n: bert.encoder.layer.1.output.dense.weight
2023-03-28 00:04:08,564 n: bert.encoder.layer.1.output.dense.bias
2023-03-28 00:04:08,564 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-28 00:04:08,564 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-28 00:04:08,564 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-28 00:04:08,564 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-28 00:04:08,564 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-28 00:04:08,564 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-28 00:04:08,565 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-28 00:04:08,565 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-28 00:04:08,565 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-28 00:04:08,565 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-28 00:04:08,565 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-28 00:04:08,565 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-28 00:04:08,565 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-28 00:04:08,565 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-28 00:04:08,565 n: bert.encoder.layer.2.output.dense.weight
2023-03-28 00:04:08,565 n: bert.encoder.layer.2.output.dense.bias
2023-03-28 00:04:08,566 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-28 00:04:08,566 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-28 00:04:08,566 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-28 00:04:08,566 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-28 00:04:08,566 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-28 00:04:08,566 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-28 00:04:08,566 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-28 00:04:08,566 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-28 00:04:08,566 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-28 00:04:08,566 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-28 00:04:08,567 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-28 00:04:08,567 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-28 00:04:08,567 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-28 00:04:08,567 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-28 00:04:08,567 n: bert.encoder.layer.3.output.dense.weight
2023-03-28 00:04:08,567 n: bert.encoder.layer.3.output.dense.bias
2023-03-28 00:04:08,567 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-28 00:04:08,567 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-28 00:04:08,567 n: bert.pooler.dense.weight
2023-03-28 00:04:08,567 n: bert.pooler.dense.bias
2023-03-28 00:04:08,568 n: classifier.weight
2023-03-28 00:04:08,568 n: classifier.bias
2023-03-28 00:04:08,568 n: fit_dense.weight
2023-03-28 00:04:08,568 n: fit_dense.bias
2023-03-28 00:04:08,568 Total parameters: 14591258
2023-03-28 00:04:16,492 ***** Running evaluation *****
2023-03-28 00:04:16,493   Epoch = 0 iter 49 step
2023-03-28 00:04:16,493   Num examples = 1043
2023-03-28 00:04:16,493   Batch size = 32
2023-03-28 00:04:17,103 ***** Eval results *****
2023-03-28 00:04:17,103   acc = 0.7296260786193672
2023-03-28 00:04:17,103   att_loss = 0.0
2023-03-28 00:04:17,104   cls_loss = 0.33301650808781996
2023-03-28 00:04:17,104   eval_loss = 0.6188767931678079
2023-03-28 00:04:17,104   global_step = 49
2023-03-28 00:04:17,104   loss = 0.33301650808781996
2023-03-28 00:04:17,105   mcc = 0.2958459316236338
2023-03-28 00:04:17,105   rep_loss = 0.0
2023-03-28 00:04:17,112 ***** Save model *****
2023-03-28 00:04:25,802 ***** Running evaluation *****
2023-03-28 00:04:25,803   Epoch = 0 iter 99 step
2023-03-28 00:04:25,803   Num examples = 1043
2023-03-28 00:04:25,803   Batch size = 32
2023-03-28 00:04:26,405 ***** Eval results *****
2023-03-28 00:04:26,406   acc = 0.7238734419942474
2023-03-28 00:04:26,406   att_loss = 0.0
2023-03-28 00:04:26,406   cls_loss = 0.3124097630833135
2023-03-28 00:04:26,406   eval_loss = 0.5630181910413684
2023-03-28 00:04:26,406   global_step = 99
2023-03-28 00:04:26,407   loss = 0.3124097630833135
2023-03-28 00:04:26,407   mcc = 0.30245117663884985
2023-03-28 00:04:26,407   rep_loss = 0.0
2023-03-28 00:04:26,414 ***** Save model *****
2023-03-28 00:04:35,112 ***** Running evaluation *****
2023-03-28 00:04:35,112   Epoch = 0 iter 149 step
2023-03-28 00:04:35,112   Num examples = 1043
2023-03-28 00:04:35,112   Batch size = 32
2023-03-28 00:04:35,716 ***** Eval results *****
2023-03-28 00:04:35,717   acc = 0.7411313518696069
2023-03-28 00:04:35,717   att_loss = 0.0
2023-03-28 00:04:35,717   cls_loss = 0.3044981282429407
2023-03-28 00:04:35,717   eval_loss = 0.5596589861494122
2023-03-28 00:04:35,718   global_step = 149
2023-03-28 00:04:35,718   loss = 0.3044981282429407
2023-03-28 00:04:35,718   mcc = 0.3352124333269541
2023-03-28 00:04:35,718   rep_loss = 0.0
2023-03-28 00:04:35,725 ***** Save model *****
2023-03-28 00:04:44,410 ***** Running evaluation *****
2023-03-28 00:04:44,411   Epoch = 0 iter 199 step
2023-03-28 00:04:44,411   Num examples = 1043
2023-03-28 00:04:44,411   Batch size = 32
2023-03-28 00:04:45,016 ***** Eval results *****
2023-03-28 00:04:45,016   acc = 0.7248322147651006
2023-03-28 00:04:45,016   att_loss = 0.0
2023-03-28 00:04:45,016   cls_loss = 0.2989017496905734
2023-03-28 00:04:45,017   eval_loss = 0.5670564572016398
2023-03-28 00:04:45,017   global_step = 199
2023-03-28 00:04:45,017   loss = 0.2989017496905734
2023-03-28 00:04:45,017   mcc = 0.2782178633384921
2023-03-28 00:04:45,017   rep_loss = 0.0
2023-03-28 00:04:53,079 ***** Running evaluation *****
2023-03-28 00:04:53,079   Epoch = 0 iter 249 step
2023-03-28 00:04:53,080   Num examples = 1043
2023-03-28 00:04:53,080   Batch size = 32
2023-03-28 00:04:53,684 ***** Eval results *****
2023-03-28 00:04:53,684   acc = 0.7363374880153404
2023-03-28 00:04:53,685   att_loss = 0.0
2023-03-28 00:04:53,685   cls_loss = 0.295009752533522
2023-03-28 00:04:53,685   eval_loss = 0.5557529402501655
2023-03-28 00:04:53,685   global_step = 249
2023-03-28 00:04:53,685   loss = 0.295009752533522
2023-03-28 00:04:53,685   mcc = 0.30295408668800067
2023-03-28 00:04:53,685   rep_loss = 0.0
2023-03-28 00:05:01,745 ***** Running evaluation *****
2023-03-28 00:05:01,746   Epoch = 0 iter 299 step
2023-03-28 00:05:01,746   Num examples = 1043
2023-03-28 00:05:01,746   Batch size = 32
2023-03-28 00:05:02,350 ***** Eval results *****
2023-03-28 00:05:02,350   acc = 0.7190795781399808
2023-03-28 00:05:02,350   att_loss = 0.0
2023-03-28 00:05:02,350   cls_loss = 0.2922431619270988
2023-03-28 00:05:02,350   eval_loss = 0.5585006457386594
2023-03-28 00:05:02,350   global_step = 299
2023-03-28 00:05:02,350   loss = 0.2922431619270988
2023-03-28 00:05:02,351   mcc = 0.296334438141132
2023-03-28 00:05:02,351   rep_loss = 0.0
2023-03-28 00:05:10,400 ***** Running evaluation *****
2023-03-28 00:05:10,401   Epoch = 0 iter 349 step
2023-03-28 00:05:10,401   Num examples = 1043
2023-03-28 00:05:10,401   Batch size = 32
2023-03-28 00:05:11,009 ***** Eval results *****
2023-03-28 00:05:11,009   acc = 0.7305848513902206
2023-03-28 00:05:11,009   att_loss = 0.0
2023-03-28 00:05:11,009   cls_loss = 0.29030252271531987
2023-03-28 00:05:11,009   eval_loss = 0.5520122412479285
2023-03-28 00:05:11,010   global_step = 349
2023-03-28 00:05:11,010   loss = 0.29030252271531987
2023-03-28 00:05:11,010   mcc = 0.28754143566748747
2023-03-28 00:05:11,010   rep_loss = 0.0
2023-03-28 00:05:19,054 ***** Running evaluation *****
2023-03-28 00:05:19,054   Epoch = 0 iter 399 step
2023-03-28 00:05:19,054   Num examples = 1043
2023-03-28 00:05:19,054   Batch size = 32
2023-03-28 00:05:19,658 ***** Eval results *****
2023-03-28 00:05:19,659   acc = 0.7248322147651006
2023-03-28 00:05:19,659   att_loss = 0.0
2023-03-28 00:05:19,659   cls_loss = 0.28897675199616224
2023-03-28 00:05:19,659   eval_loss = 0.5535754534331235
2023-03-28 00:05:19,659   global_step = 399
2023-03-28 00:05:19,660   loss = 0.28897675199616224
2023-03-28 00:05:19,660   mcc = 0.29800459598965934
2023-03-28 00:05:19,660   rep_loss = 0.0
2023-03-28 00:05:27,724 ***** Running evaluation *****
2023-03-28 00:05:27,724   Epoch = 0 iter 449 step
2023-03-28 00:05:27,724   Num examples = 1043
2023-03-28 00:05:27,724   Batch size = 32
2023-03-28 00:05:28,329 ***** Eval results *****
2023-03-28 00:05:28,329   acc = 0.7344199424736337
2023-03-28 00:05:28,329   att_loss = 0.0
2023-03-28 00:05:28,330   cls_loss = 0.2874339474468826
2023-03-28 00:05:28,330   eval_loss = 0.5479107426874565
2023-03-28 00:05:28,330   global_step = 449
2023-03-28 00:05:28,330   loss = 0.2874339474468826
2023-03-28 00:05:28,330   mcc = 0.3214888858177521
2023-03-28 00:05:28,330   rep_loss = 0.0
2023-03-28 00:05:36,395 ***** Running evaluation *****
2023-03-28 00:05:36,395   Epoch = 0 iter 499 step
2023-03-28 00:05:36,395   Num examples = 1043
2023-03-28 00:05:36,395   Batch size = 32
2023-03-28 00:05:37,002 ***** Eval results *****
2023-03-28 00:05:37,002   acc = 0.7392138063279002
2023-03-28 00:05:37,002   att_loss = 0.0
2023-03-28 00:05:37,002   cls_loss = 0.28594939225541804
2023-03-28 00:05:37,002   eval_loss = 0.5476396463134072
2023-03-28 00:05:37,002   global_step = 499
2023-03-28 00:05:37,002   loss = 0.28594939225541804
2023-03-28 00:05:37,003   mcc = 0.3180644929516368
2023-03-28 00:05:37,003   rep_loss = 0.0
2023-03-28 00:05:45,134 ***** Running evaluation *****
2023-03-28 00:05:45,134   Epoch = 1 iter 549 step
2023-03-28 00:05:45,134   Num examples = 1043
2023-03-28 00:05:45,135   Batch size = 32
2023-03-28 00:05:45,739 ***** Eval results *****
2023-03-28 00:05:45,739   acc = 0.7353787152444871
2023-03-28 00:05:45,739   att_loss = 0.0
2023-03-28 00:05:45,739   cls_loss = 0.2708614945411682
2023-03-28 00:05:45,740   eval_loss = 0.5441551154310053
2023-03-28 00:05:45,740   global_step = 549
2023-03-28 00:05:45,740   loss = 0.2708614945411682
2023-03-28 00:05:45,740   mcc = 0.3119474321533578
2023-03-28 00:05:45,740   rep_loss = 0.0
2023-03-28 00:05:53,795 ***** Running evaluation *****
2023-03-28 00:05:53,796   Epoch = 1 iter 599 step
2023-03-28 00:05:53,796   Num examples = 1043
2023-03-28 00:05:53,796   Batch size = 32
2023-03-28 00:05:54,402 ***** Eval results *****
2023-03-28 00:05:54,402   acc = 0.738255033557047
2023-03-28 00:05:54,402   att_loss = 0.0
2023-03-28 00:05:54,403   cls_loss = 0.2760019151064066
2023-03-28 00:05:54,403   eval_loss = 0.5436705405061896
2023-03-28 00:05:54,403   global_step = 599
2023-03-28 00:05:54,403   loss = 0.2760019151064066
2023-03-28 00:05:54,403   mcc = 0.321334751513576
2023-03-28 00:05:54,403   rep_loss = 0.0
2023-03-28 00:06:02,471 ***** Running evaluation *****
2023-03-28 00:06:02,471   Epoch = 1 iter 649 step
2023-03-28 00:06:02,472   Num examples = 1043
2023-03-28 00:06:02,472   Batch size = 32
2023-03-28 00:06:03,079 ***** Eval results *****
2023-03-28 00:06:03,079   acc = 0.7353787152444871
2023-03-28 00:06:03,079   att_loss = 0.0
2023-03-28 00:06:03,080   cls_loss = 0.2745593505061191
2023-03-28 00:06:03,080   eval_loss = 0.5443290535247687
2023-03-28 00:06:03,080   global_step = 649
2023-03-28 00:06:03,080   loss = 0.2745593505061191
2023-03-28 00:06:03,080   mcc = 0.3253086972571917
2023-03-28 00:06:03,080   rep_loss = 0.0
2023-03-28 00:06:11,147 ***** Running evaluation *****
2023-03-28 00:06:11,148   Epoch = 1 iter 699 step
2023-03-28 00:06:11,148   Num examples = 1043
2023-03-28 00:06:11,148   Batch size = 32
2023-03-28 00:06:11,752 ***** Eval results *****
2023-03-28 00:06:11,753   acc = 0.7392138063279002
2023-03-28 00:06:11,753   att_loss = 0.0
2023-03-28 00:06:11,753   cls_loss = 0.27349523387172003
2023-03-28 00:06:11,753   eval_loss = 0.5435322839202303
2023-03-28 00:06:11,754   global_step = 699
2023-03-28 00:06:11,754   loss = 0.27349523387172003
2023-03-28 00:06:11,754   mcc = 0.3284361237784734
2023-03-28 00:06:11,754   rep_loss = 0.0
2023-03-28 00:06:19,827 ***** Running evaluation *****
2023-03-28 00:06:19,827   Epoch = 1 iter 749 step
2023-03-28 00:06:19,827   Num examples = 1043
2023-03-28 00:06:19,828   Batch size = 32
2023-03-28 00:06:20,433 ***** Eval results *****
2023-03-28 00:06:20,433   acc = 0.7372962607861937
2023-03-28 00:06:20,433   att_loss = 0.0
2023-03-28 00:06:20,433   cls_loss = 0.27341301406538765
2023-03-28 00:06:20,434   eval_loss = 0.5457523718024745
2023-03-28 00:06:20,434   global_step = 749
2023-03-28 00:06:20,434   loss = 0.27341301406538765
2023-03-28 00:06:20,434   mcc = 0.3230143442035795
2023-03-28 00:06:20,434   rep_loss = 0.0
2023-03-28 00:06:28,482 ***** Running evaluation *****
2023-03-28 00:06:28,483   Epoch = 1 iter 799 step
2023-03-28 00:06:28,483   Num examples = 1043
2023-03-28 00:06:28,483   Batch size = 32
2023-03-28 00:06:29,089 ***** Eval results *****
2023-03-28 00:06:29,089   acc = 0.7315436241610739
2023-03-28 00:06:29,090   att_loss = 0.0
2023-03-28 00:06:29,090   cls_loss = 0.2725147861354756
2023-03-28 00:06:29,090   eval_loss = 0.5479338286500989
2023-03-28 00:06:29,090   global_step = 799
2023-03-28 00:06:29,090   loss = 0.2725147861354756
2023-03-28 00:06:29,090   mcc = 0.32178209105660743
2023-03-28 00:06:29,090   rep_loss = 0.0
2023-03-28 00:06:37,169 ***** Running evaluation *****
2023-03-28 00:06:37,169   Epoch = 1 iter 849 step
2023-03-28 00:06:37,170   Num examples = 1043
2023-03-28 00:06:37,170   Batch size = 32
2023-03-28 00:06:37,775 ***** Eval results *****
2023-03-28 00:06:37,775   acc = 0.7449664429530202
2023-03-28 00:06:37,775   att_loss = 0.0
2023-03-28 00:06:37,775   cls_loss = 0.27274117228530703
2023-03-28 00:06:37,775   eval_loss = 0.5439348058267073
2023-03-28 00:06:37,776   global_step = 849
2023-03-28 00:06:37,776   loss = 0.27274117228530703
2023-03-28 00:06:37,776   mcc = 0.3345270218519135
2023-03-28 00:06:37,776   rep_loss = 0.0
2023-03-28 00:06:45,841 ***** Running evaluation *****
2023-03-28 00:06:45,841   Epoch = 1 iter 899 step
2023-03-28 00:06:45,841   Num examples = 1043
2023-03-28 00:06:45,841   Batch size = 32
2023-03-28 00:06:46,446 ***** Eval results *****
2023-03-28 00:06:46,446   acc = 0.7325023969319271
2023-03-28 00:06:46,446   att_loss = 0.0
2023-03-28 00:06:46,446   cls_loss = 0.2727138049798469
2023-03-28 00:06:46,446   eval_loss = 0.5491433242956797
2023-03-28 00:06:46,447   global_step = 899
2023-03-28 00:06:46,447   loss = 0.2727138049798469
2023-03-28 00:06:46,447   mcc = 0.31790598421745303
2023-03-28 00:06:46,447   rep_loss = 0.0
2023-03-28 00:06:54,500 ***** Running evaluation *****
2023-03-28 00:06:54,500   Epoch = 1 iter 949 step
2023-03-28 00:06:54,501   Num examples = 1043
2023-03-28 00:06:54,501   Batch size = 32
2023-03-28 00:06:55,110 ***** Eval results *****
2023-03-28 00:06:55,110   acc = 0.7363374880153404
2023-03-28 00:06:55,110   att_loss = 0.0
2023-03-28 00:06:55,111   cls_loss = 0.2725446600870914
2023-03-28 00:06:55,111   eval_loss = 0.5451366684653542
2023-03-28 00:06:55,111   global_step = 949
2023-03-28 00:06:55,111   loss = 0.2725446600870914
2023-03-28 00:06:55,111   mcc = 0.32751889624883346
2023-03-28 00:06:55,111   rep_loss = 0.0
2023-03-28 00:07:03,178 ***** Running evaluation *****
2023-03-28 00:07:03,179   Epoch = 1 iter 999 step
2023-03-28 00:07:03,179   Num examples = 1043
2023-03-28 00:07:03,179   Batch size = 32
2023-03-28 00:07:03,783 ***** Eval results *****
2023-03-28 00:07:03,784   acc = 0.7430488974113135
2023-03-28 00:07:03,784   att_loss = 0.0
2023-03-28 00:07:03,784   cls_loss = 0.27214557515677584
2023-03-28 00:07:03,784   eval_loss = 0.5431010804393075
2023-03-28 00:07:03,784   global_step = 999
2023-03-28 00:07:03,784   loss = 0.27214557515677584
2023-03-28 00:07:03,784   mcc = 0.329385550291839
2023-03-28 00:07:03,784   rep_loss = 0.0
2023-03-28 00:07:11,847 ***** Running evaluation *****
2023-03-28 00:07:11,847   Epoch = 1 iter 1049 step
2023-03-28 00:07:11,848   Num examples = 1043
2023-03-28 00:07:11,848   Batch size = 32
2023-03-28 00:07:12,456 ***** Eval results *****
2023-03-28 00:07:12,457   acc = 0.7305848513902206
2023-03-28 00:07:12,457   att_loss = 0.0
2023-03-28 00:07:12,457   cls_loss = 0.2722604475553753
2023-03-28 00:07:12,457   eval_loss = 0.5446351739493284
2023-03-28 00:07:12,457   global_step = 1049
2023-03-28 00:07:12,457   loss = 0.2722604475553753
2023-03-28 00:07:12,457   mcc = 0.3060431881870531
2023-03-28 00:07:12,457   rep_loss = 0.0
2023-03-28 00:07:20,534 ***** Running evaluation *****
2023-03-28 00:07:20,534   Epoch = 2 iter 1099 step
2023-03-28 00:07:20,534   Num examples = 1043
2023-03-28 00:07:20,534   Batch size = 32
2023-03-28 00:07:21,141 ***** Eval results *****
2023-03-28 00:07:21,141   acc = 0.7267497603068073
2023-03-28 00:07:21,141   att_loss = 0.0
2023-03-28 00:07:21,141   cls_loss = 0.2690123074477719
2023-03-28 00:07:21,141   eval_loss = 0.5451868311925367
2023-03-28 00:07:21,141   global_step = 1099
2023-03-28 00:07:21,142   loss = 0.2690123074477719
2023-03-28 00:07:21,142   mcc = 0.30318940271577555
2023-03-28 00:07:21,142   rep_loss = 0.0
2023-03-28 00:07:29,213 ***** Running evaluation *****
2023-03-28 00:07:29,213   Epoch = 2 iter 1149 step
2023-03-28 00:07:29,213   Num examples = 1043
2023-03-28 00:07:29,213   Batch size = 32
2023-03-28 00:07:29,818 ***** Eval results *****
2023-03-28 00:07:29,819   acc = 0.7296260786193672
2023-03-28 00:07:29,819   att_loss = 0.0
2023-03-28 00:07:29,819   cls_loss = 0.2716456850369771
2023-03-28 00:07:29,819   eval_loss = 0.5408475751226599
2023-03-28 00:07:29,819   global_step = 1149
2023-03-28 00:07:29,820   loss = 0.2716456850369771
2023-03-28 00:07:29,820   mcc = 0.3037711783107992
2023-03-28 00:07:29,820   rep_loss = 0.0
2023-03-28 00:07:37,889 ***** Running evaluation *****
2023-03-28 00:07:37,889   Epoch = 2 iter 1199 step
2023-03-28 00:07:37,889   Num examples = 1043
2023-03-28 00:07:37,889   Batch size = 32
2023-03-28 00:07:38,498 ***** Eval results *****
2023-03-28 00:07:38,498   acc = 0.7372962607861937
2023-03-28 00:07:38,499   att_loss = 0.0
2023-03-28 00:07:38,499   cls_loss = 0.2704642380921895
2023-03-28 00:07:38,499   eval_loss = 0.540543050476999
2023-03-28 00:07:38,499   global_step = 1199
2023-03-28 00:07:38,499   loss = 0.2704642380921895
2023-03-28 00:07:38,499   mcc = 0.32743517887814166
2023-03-28 00:07:38,499   rep_loss = 0.0
2023-03-28 00:07:46,551 ***** Running evaluation *****
2023-03-28 00:07:46,551   Epoch = 2 iter 1249 step
2023-03-28 00:07:46,551   Num examples = 1043
2023-03-28 00:07:46,551   Batch size = 32
2023-03-28 00:07:47,157 ***** Eval results *****
2023-03-28 00:07:47,157   acc = 0.7248322147651006
2023-03-28 00:07:47,157   att_loss = 0.0
2023-03-28 00:07:47,157   cls_loss = 0.2709996897871323
2023-03-28 00:07:47,157   eval_loss = 0.5410002226179297
2023-03-28 00:07:47,157   global_step = 1249
2023-03-28 00:07:47,157   loss = 0.2709996897871323
2023-03-28 00:07:47,158   mcc = 0.2943785103528841
2023-03-28 00:07:47,158   rep_loss = 0.0
2023-03-28 00:07:55,232 ***** Running evaluation *****
2023-03-28 00:07:55,233   Epoch = 2 iter 1299 step
2023-03-28 00:07:55,233   Num examples = 1043
2023-03-28 00:07:55,233   Batch size = 32
2023-03-28 00:07:55,839 ***** Eval results *****
2023-03-28 00:07:55,839   acc = 0.7267497603068073
2023-03-28 00:07:55,839   att_loss = 0.0
2023-03-28 00:07:55,840   cls_loss = 0.27076626688370975
2023-03-28 00:07:55,840   eval_loss = 0.5401014225049452
2023-03-28 00:07:55,840   global_step = 1299
2023-03-28 00:07:55,840   loss = 0.27076626688370975
2023-03-28 00:07:55,840   mcc = 0.30140571370330893
2023-03-28 00:07:55,840   rep_loss = 0.0
2023-03-28 00:08:03,921 ***** Running evaluation *****
2023-03-28 00:08:03,921   Epoch = 2 iter 1349 step
2023-03-28 00:08:03,921   Num examples = 1043
2023-03-28 00:08:03,921   Batch size = 32
2023-03-28 00:08:04,527 ***** Eval results *****
2023-03-28 00:08:04,527   acc = 0.7353787152444871
2023-03-28 00:08:04,527   att_loss = 0.0
2023-03-28 00:08:04,527   cls_loss = 0.27069236826005777
2023-03-28 00:08:04,527   eval_loss = 0.5387377711859617
2023-03-28 00:08:04,528   global_step = 1349
2023-03-28 00:08:04,528   loss = 0.27069236826005777
2023-03-28 00:08:04,528   mcc = 0.31400269710663015
2023-03-28 00:08:04,528   rep_loss = 0.0
2023-03-28 00:08:12,593 ***** Running evaluation *****
2023-03-28 00:08:12,594   Epoch = 2 iter 1399 step
2023-03-28 00:08:12,594   Num examples = 1043
2023-03-28 00:08:12,594   Batch size = 32
2023-03-28 00:08:13,201 ***** Eval results *****
2023-03-28 00:08:13,201   acc = 0.7363374880153404
2023-03-28 00:08:13,201   att_loss = 0.0
2023-03-28 00:08:13,201   cls_loss = 0.2710398080395788
2023-03-28 00:08:13,202   eval_loss = 0.5389033700480605
2023-03-28 00:08:13,202   global_step = 1399
2023-03-28 00:08:13,202   loss = 0.2710398080395788
2023-03-28 00:08:13,202   mcc = 0.3178110608461884
2023-03-28 00:08:13,202   rep_loss = 0.0
2023-03-28 00:08:21,279 ***** Running evaluation *****
2023-03-28 00:08:21,279   Epoch = 2 iter 1449 step
2023-03-28 00:08:21,279   Num examples = 1043
2023-03-28 00:08:21,279   Batch size = 32
2023-03-28 00:08:21,884 ***** Eval results *****
2023-03-28 00:08:21,884   acc = 0.7420901246404602
2023-03-28 00:08:21,885   att_loss = 0.0
2023-03-28 00:08:21,885   cls_loss = 0.2705914983990311
2023-03-28 00:08:21,885   eval_loss = 0.5395755939411394
2023-03-28 00:08:21,885   global_step = 1449
2023-03-28 00:08:21,885   loss = 0.2705914983990311
2023-03-28 00:08:21,885   mcc = 0.33495378370752793
2023-03-28 00:08:21,886   rep_loss = 0.0
2023-03-28 00:08:29,955 ***** Running evaluation *****
2023-03-28 00:08:29,955   Epoch = 2 iter 1499 step
2023-03-28 00:08:29,955   Num examples = 1043
2023-03-28 00:08:29,955   Batch size = 32
2023-03-28 00:08:30,561 ***** Eval results *****
2023-03-28 00:08:30,561   acc = 0.738255033557047
2023-03-28 00:08:30,561   att_loss = 0.0
2023-03-28 00:08:30,561   cls_loss = 0.27046371301476196
2023-03-28 00:08:30,562   eval_loss = 0.5400878010374127
2023-03-28 00:08:30,562   global_step = 1499
2023-03-28 00:08:30,562   loss = 0.27046371301476196
2023-03-28 00:08:30,562   mcc = 0.3260777179000933
2023-03-28 00:08:30,562   rep_loss = 0.0
2023-03-28 00:08:38,661 ***** Running evaluation *****
2023-03-28 00:08:38,661   Epoch = 2 iter 1549 step
2023-03-28 00:08:38,662   Num examples = 1043
2023-03-28 00:08:38,662   Batch size = 32
2023-03-28 00:08:39,268 ***** Eval results *****
2023-03-28 00:08:39,268   acc = 0.7372962607861937
2023-03-28 00:08:39,268   att_loss = 0.0
2023-03-28 00:08:39,268   cls_loss = 0.27038438986839725
2023-03-28 00:08:39,268   eval_loss = 0.5400523416923754
2023-03-28 00:08:39,269   global_step = 1549
2023-03-28 00:08:39,269   loss = 0.27038438986839725
2023-03-28 00:08:39,269   mcc = 0.32519186244156545
2023-03-28 00:08:39,269   rep_loss = 0.0
2023-03-28 00:08:47,322 ***** Running evaluation *****
2023-03-28 00:08:47,322   Epoch = 2 iter 1599 step
2023-03-28 00:08:47,322   Num examples = 1043
2023-03-28 00:08:47,322   Batch size = 32
2023-03-28 00:08:47,934 ***** Eval results *****
2023-03-28 00:08:47,934   acc = 0.7401725790987536
2023-03-28 00:08:47,934   att_loss = 0.0
2023-03-28 00:08:47,934   cls_loss = 0.27016970593826023
2023-03-28 00:08:47,935   eval_loss = 0.5397670648314736
2023-03-28 00:08:47,935   global_step = 1599
2023-03-28 00:08:47,935   loss = 0.27016970593826023
2023-03-28 00:08:47,935   mcc = 0.33013641611743566
2023-03-28 00:08:47,935   rep_loss = 0.0
2023-03-28 00:09:00,153 device: cuda n_gpu: 1
2023-03-28 00:09:00,254 Writing example 0 of 8551
2023-03-28 00:09:00,255 *** Example ***
2023-03-28 00:09:00,255 guid: train-0
2023-03-28 00:09:00,255 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-28 00:09:00,256 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:09:00,256 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:09:00,256 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:09:00,256 label: 1
2023-03-28 00:09:00,257 label_id: 1
2023-03-28 00:09:01,202 Writing example 0 of 1043
2023-03-28 00:09:01,203 *** Example ***
2023-03-28 00:09:01,203 guid: dev-0
2023-03-28 00:09:01,203 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-28 00:09:01,203 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:09:01,203 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:09:01,203 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:09:01,203 label: 1
2023-03-28 00:09:01,204 label_id: 1
2023-03-28 00:09:01,321 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-28 00:09:01,322 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-28 00:09:03,026 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-28 00:09:03,209 loading model...
2023-03-28 00:09:03,295 done!
2023-03-28 00:09:03,295 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-28 00:09:03,384 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01
2023-03-28 00:09:03,386 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-28 00:09:03,611 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01/pytorch_model.bin
2023-03-28 00:09:03,636 loading model...
2023-03-28 00:09:03,649 done!
2023-03-28 00:09:03,664 ***** Running training *****
2023-03-28 00:09:03,664   Num examples = 8551
2023-03-28 00:09:03,664   Batch size = 16
2023-03-28 00:09:03,664   Num steps = 1602
2023-03-28 00:09:03,664 n: bert.embeddings.word_embeddings.weight
2023-03-28 00:09:03,665 n: bert.embeddings.position_embeddings.weight
2023-03-28 00:09:03,665 n: bert.embeddings.token_type_embeddings.weight
2023-03-28 00:09:03,665 n: bert.embeddings.LayerNorm.weight
2023-03-28 00:09:03,665 n: bert.embeddings.LayerNorm.bias
2023-03-28 00:09:03,665 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-28 00:09:03,665 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-28 00:09:03,665 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-28 00:09:03,665 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-28 00:09:03,665 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-28 00:09:03,666 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-28 00:09:03,666 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-28 00:09:03,666 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-28 00:09:03,666 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-28 00:09:03,666 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-28 00:09:03,666 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-28 00:09:03,666 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-28 00:09:03,666 n: bert.encoder.layer.0.output.dense.weight
2023-03-28 00:09:03,666 n: bert.encoder.layer.0.output.dense.bias
2023-03-28 00:09:03,667 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-28 00:09:03,667 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-28 00:09:03,667 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-28 00:09:03,667 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-28 00:09:03,667 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-28 00:09:03,667 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-28 00:09:03,667 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-28 00:09:03,667 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-28 00:09:03,667 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-28 00:09:03,667 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-28 00:09:03,668 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-28 00:09:03,668 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-28 00:09:03,668 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-28 00:09:03,668 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-28 00:09:03,668 n: bert.encoder.layer.1.output.dense.weight
2023-03-28 00:09:03,668 n: bert.encoder.layer.1.output.dense.bias
2023-03-28 00:09:03,668 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-28 00:09:03,668 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-28 00:09:03,668 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-28 00:09:03,668 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-28 00:09:03,669 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-28 00:09:03,669 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-28 00:09:03,669 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-28 00:09:03,669 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-28 00:09:03,669 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-28 00:09:03,669 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-28 00:09:03,669 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-28 00:09:03,669 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-28 00:09:03,669 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-28 00:09:03,670 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-28 00:09:03,670 n: bert.encoder.layer.2.output.dense.weight
2023-03-28 00:09:03,670 n: bert.encoder.layer.2.output.dense.bias
2023-03-28 00:09:03,670 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-28 00:09:03,670 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-28 00:09:03,670 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-28 00:09:03,670 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-28 00:09:03,670 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-28 00:09:03,670 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-28 00:09:03,670 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-28 00:09:03,671 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-28 00:09:03,671 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-28 00:09:03,671 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-28 00:09:03,671 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-28 00:09:03,671 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-28 00:09:03,671 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-28 00:09:03,671 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-28 00:09:03,671 n: bert.encoder.layer.3.output.dense.weight
2023-03-28 00:09:03,671 n: bert.encoder.layer.3.output.dense.bias
2023-03-28 00:09:03,671 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-28 00:09:03,672 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-28 00:09:03,672 n: bert.pooler.dense.weight
2023-03-28 00:09:03,672 n: bert.pooler.dense.bias
2023-03-28 00:09:03,672 n: classifier.weight
2023-03-28 00:09:03,672 n: classifier.bias
2023-03-28 00:09:03,672 n: fit_dense.weight
2023-03-28 00:09:03,672 n: fit_dense.bias
2023-03-28 00:09:03,672 Total parameters: 14591258
2023-03-28 00:09:11,557 ***** Running evaluation *****
2023-03-28 00:09:11,557   Epoch = 0 iter 49 step
2023-03-28 00:09:11,557   Num examples = 1043
2023-03-28 00:09:11,557   Batch size = 32
2023-03-28 00:09:12,168 ***** Eval results *****
2023-03-28 00:09:12,168   acc = 0.7123681687440077
2023-03-28 00:09:12,169   att_loss = 0.0
2023-03-28 00:09:12,169   cls_loss = 0.3283280663344325
2023-03-28 00:09:12,169   eval_loss = 0.59939587838722
2023-03-28 00:09:12,169   global_step = 49
2023-03-28 00:09:12,169   loss = 0.3283280663344325
2023-03-28 00:09:12,169   mcc = 0.27740658764302706
2023-03-28 00:09:12,169   rep_loss = 0.0
2023-03-28 00:09:12,177 ***** Save model *****
2023-03-28 00:09:20,894 ***** Running evaluation *****
2023-03-28 00:09:20,894   Epoch = 0 iter 99 step
2023-03-28 00:09:20,894   Num examples = 1043
2023-03-28 00:09:20,895   Batch size = 32
2023-03-28 00:09:21,499 ***** Eval results *****
2023-03-28 00:09:21,499   acc = 0.7046979865771812
2023-03-28 00:09:21,499   att_loss = 0.0
2023-03-28 00:09:21,500   cls_loss = 0.31092522752405416
2023-03-28 00:09:21,500   eval_loss = 0.5854144096374512
2023-03-28 00:09:21,500   global_step = 99
2023-03-28 00:09:21,500   loss = 0.31092522752405416
2023-03-28 00:09:21,501   mcc = 0.3237953631930207
2023-03-28 00:09:21,501   rep_loss = 0.0
2023-03-28 00:09:21,509 ***** Save model *****
2023-03-28 00:09:30,243 ***** Running evaluation *****
2023-03-28 00:09:30,243   Epoch = 0 iter 149 step
2023-03-28 00:09:30,243   Num examples = 1043
2023-03-28 00:09:30,243   Batch size = 32
2023-03-28 00:09:30,848 ***** Eval results *****
2023-03-28 00:09:30,848   acc = 0.7238734419942474
2023-03-28 00:09:30,848   att_loss = 0.0
2023-03-28 00:09:30,848   cls_loss = 0.3030981437471889
2023-03-28 00:09:30,848   eval_loss = 0.5736978315945828
2023-03-28 00:09:30,849   global_step = 149
2023-03-28 00:09:30,849   loss = 0.3030981437471889
2023-03-28 00:09:30,849   mcc = 0.32422602944663587
2023-03-28 00:09:30,849   rep_loss = 0.0
2023-03-28 00:09:30,852 ***** Save model *****
2023-03-28 00:09:39,601 ***** Running evaluation *****
2023-03-28 00:09:39,601   Epoch = 0 iter 199 step
2023-03-28 00:09:39,601   Num examples = 1043
2023-03-28 00:09:39,601   Batch size = 32
2023-03-28 00:09:40,206 ***** Eval results *****
2023-03-28 00:09:40,206   acc = 0.7305848513902206
2023-03-28 00:09:40,206   att_loss = 0.0
2023-03-28 00:09:40,206   cls_loss = 0.29842364930327814
2023-03-28 00:09:40,206   eval_loss = 0.5557437882278905
2023-03-28 00:09:40,207   global_step = 199
2023-03-28 00:09:40,207   loss = 0.29842364930327814
2023-03-28 00:09:40,207   mcc = 0.28696235374396895
2023-03-28 00:09:40,207   rep_loss = 0.0
2023-03-28 00:09:48,266 ***** Running evaluation *****
2023-03-28 00:09:48,266   Epoch = 0 iter 249 step
2023-03-28 00:09:48,266   Num examples = 1043
2023-03-28 00:09:48,266   Batch size = 32
2023-03-28 00:09:48,871 ***** Eval results *****
2023-03-28 00:09:48,871   acc = 0.7344199424736337
2023-03-28 00:09:48,872   att_loss = 0.0
2023-03-28 00:09:48,872   cls_loss = 0.2946288312774107
2023-03-28 00:09:48,872   eval_loss = 0.5637156439549995
2023-03-28 00:09:48,872   global_step = 249
2023-03-28 00:09:48,872   loss = 0.2946288312774107
2023-03-28 00:09:48,872   mcc = 0.2940905497972125
2023-03-28 00:09:48,873   rep_loss = 0.0
2023-03-28 00:09:56,927 ***** Running evaluation *****
2023-03-28 00:09:56,927   Epoch = 0 iter 299 step
2023-03-28 00:09:56,927   Num examples = 1043
2023-03-28 00:09:56,928   Batch size = 32
2023-03-28 00:09:57,531 ***** Eval results *****
2023-03-28 00:09:57,532   acc = 0.7171620325982742
2023-03-28 00:09:57,532   att_loss = 0.0
2023-03-28 00:09:57,532   cls_loss = 0.29266029337178107
2023-03-28 00:09:57,532   eval_loss = 0.5612032178676489
2023-03-28 00:09:57,532   global_step = 299
2023-03-28 00:09:57,532   loss = 0.29266029337178107
2023-03-28 00:09:57,532   mcc = 0.2852679092106396
2023-03-28 00:09:57,532   rep_loss = 0.0
2023-03-28 00:10:05,603 ***** Running evaluation *****
2023-03-28 00:10:05,604   Epoch = 0 iter 349 step
2023-03-28 00:10:05,604   Num examples = 1043
2023-03-28 00:10:05,604   Batch size = 32
2023-03-28 00:10:06,212 ***** Eval results *****
2023-03-28 00:10:06,212   acc = 0.7219558964525408
2023-03-28 00:10:06,212   att_loss = 0.0
2023-03-28 00:10:06,212   cls_loss = 0.29105806196999756
2023-03-28 00:10:06,212   eval_loss = 0.5500552166591991
2023-03-28 00:10:06,212   global_step = 349
2023-03-28 00:10:06,212   loss = 0.29105806196999756
2023-03-28 00:10:06,213   mcc = 0.2683934054786649
2023-03-28 00:10:06,213   rep_loss = 0.0
2023-03-28 00:10:14,255 ***** Running evaluation *****
2023-03-28 00:10:14,255   Epoch = 0 iter 399 step
2023-03-28 00:10:14,256   Num examples = 1043
2023-03-28 00:10:14,256   Batch size = 32
2023-03-28 00:10:14,860 ***** Eval results *****
2023-03-28 00:10:14,860   acc = 0.7296260786193672
2023-03-28 00:10:14,860   att_loss = 0.0
2023-03-28 00:10:14,861   cls_loss = 0.2900835150913487
2023-03-28 00:10:14,861   eval_loss = 0.5486904825225021
2023-03-28 00:10:14,861   global_step = 399
2023-03-28 00:10:14,861   loss = 0.2900835150913487
2023-03-28 00:10:14,861   mcc = 0.287324023093052
2023-03-28 00:10:14,861   rep_loss = 0.0
2023-03-28 00:10:22,920 ***** Running evaluation *****
2023-03-28 00:10:22,921   Epoch = 0 iter 449 step
2023-03-28 00:10:22,921   Num examples = 1043
2023-03-28 00:10:22,921   Batch size = 32
2023-03-28 00:10:23,526 ***** Eval results *****
2023-03-28 00:10:23,526   acc = 0.7277085330776606
2023-03-28 00:10:23,526   att_loss = 0.0
2023-03-28 00:10:23,526   cls_loss = 0.28854837771380665
2023-03-28 00:10:23,527   eval_loss = 0.5578452294523065
2023-03-28 00:10:23,527   global_step = 449
2023-03-28 00:10:23,527   loss = 0.28854837771380665
2023-03-28 00:10:23,527   mcc = 0.320997827186018
2023-03-28 00:10:23,527   rep_loss = 0.0
2023-03-28 00:10:31,584 ***** Running evaluation *****
2023-03-28 00:10:31,584   Epoch = 0 iter 499 step
2023-03-28 00:10:31,584   Num examples = 1043
2023-03-28 00:10:31,584   Batch size = 32
2023-03-28 00:10:32,191 ***** Eval results *****
2023-03-28 00:10:32,191   acc = 0.7430488974113135
2023-03-28 00:10:32,191   att_loss = 0.0
2023-03-28 00:10:32,192   cls_loss = 0.2871336294618064
2023-03-28 00:10:32,192   eval_loss = 0.5510263226249001
2023-03-28 00:10:32,192   global_step = 499
2023-03-28 00:10:32,192   loss = 0.2871336294618064
2023-03-28 00:10:32,192   mcc = 0.3270645042567984
2023-03-28 00:10:32,192   rep_loss = 0.0
2023-03-28 00:10:32,199 ***** Save model *****
2023-03-28 00:10:40,963 ***** Running evaluation *****
2023-03-28 00:10:40,963   Epoch = 1 iter 549 step
2023-03-28 00:10:40,963   Num examples = 1043
2023-03-28 00:10:40,963   Batch size = 32
2023-03-28 00:10:41,569 ***** Eval results *****
2023-03-28 00:10:41,569   acc = 0.7344199424736337
2023-03-28 00:10:41,569   att_loss = 0.0
2023-03-28 00:10:41,570   cls_loss = 0.2716089516878128
2023-03-28 00:10:41,570   eval_loss = 0.5500055361877788
2023-03-28 00:10:41,570   global_step = 549
2023-03-28 00:10:41,570   loss = 0.2716089516878128
2023-03-28 00:10:41,570   mcc = 0.3214888858177521
2023-03-28 00:10:41,570   rep_loss = 0.0
2023-03-28 00:10:49,639 ***** Running evaluation *****
2023-03-28 00:10:49,639   Epoch = 1 iter 599 step
2023-03-28 00:10:49,639   Num examples = 1043
2023-03-28 00:10:49,639   Batch size = 32
2023-03-28 00:10:50,244 ***** Eval results *****
2023-03-28 00:10:50,245   acc = 0.7344199424736337
2023-03-28 00:10:50,245   att_loss = 0.0
2023-03-28 00:10:50,245   cls_loss = 0.2765833685031304
2023-03-28 00:10:50,245   eval_loss = 0.5430646845788667
2023-03-28 00:10:50,245   global_step = 599
2023-03-28 00:10:50,246   loss = 0.2765833685031304
2023-03-28 00:10:50,246   mcc = 0.31087774676686925
2023-03-28 00:10:50,246   rep_loss = 0.0
2023-03-28 00:10:58,321 ***** Running evaluation *****
2023-03-28 00:10:58,321   Epoch = 1 iter 649 step
2023-03-28 00:10:58,321   Num examples = 1043
2023-03-28 00:10:58,321   Batch size = 32
2023-03-28 00:10:58,926 ***** Eval results *****
2023-03-28 00:10:58,927   acc = 0.7334611697027804
2023-03-28 00:10:58,927   att_loss = 0.0
2023-03-28 00:10:58,927   cls_loss = 0.27576247790585395
2023-03-28 00:10:58,927   eval_loss = 0.5451278948422634
2023-03-28 00:10:58,927   global_step = 649
2023-03-28 00:10:58,927   loss = 0.27576247790585395
2023-03-28 00:10:58,927   mcc = 0.3225892878730812
2023-03-28 00:10:58,927   rep_loss = 0.0
2023-03-28 00:11:06,998 ***** Running evaluation *****
2023-03-28 00:11:06,998   Epoch = 1 iter 699 step
2023-03-28 00:11:06,999   Num examples = 1043
2023-03-28 00:11:06,999   Batch size = 32
2023-03-28 00:11:07,607 ***** Eval results *****
2023-03-28 00:11:07,608   acc = 0.7372962607861937
2023-03-28 00:11:07,608   att_loss = 0.0
2023-03-28 00:11:07,608   cls_loss = 0.27461838090058527
2023-03-28 00:11:07,608   eval_loss = 0.5426828527089321
2023-03-28 00:11:07,609   global_step = 699
2023-03-28 00:11:07,609   loss = 0.27461838090058527
2023-03-28 00:11:07,609   mcc = 0.3195441399431679
2023-03-28 00:11:07,609   rep_loss = 0.0
2023-03-28 00:11:15,670 ***** Running evaluation *****
2023-03-28 00:11:15,671   Epoch = 1 iter 749 step
2023-03-28 00:11:15,671   Num examples = 1043
2023-03-28 00:11:15,671   Batch size = 32
2023-03-28 00:11:16,276 ***** Eval results *****
2023-03-28 00:11:16,277   acc = 0.7372962607861937
2023-03-28 00:11:16,277   att_loss = 0.0
2023-03-28 00:11:16,277   cls_loss = 0.274618001316869
2023-03-28 00:11:16,277   eval_loss = 0.5490935417738828
2023-03-28 00:11:16,277   global_step = 749
2023-03-28 00:11:16,277   loss = 0.274618001316869
2023-03-28 00:11:16,277   mcc = 0.32743517887814166
2023-03-28 00:11:16,277   rep_loss = 0.0
2023-03-28 00:11:16,279 ***** Save model *****
2023-03-28 00:11:25,012 ***** Running evaluation *****
2023-03-28 00:11:25,013   Epoch = 1 iter 799 step
2023-03-28 00:11:25,013   Num examples = 1043
2023-03-28 00:11:25,013   Batch size = 32
2023-03-28 00:11:25,618 ***** Eval results *****
2023-03-28 00:11:25,618   acc = 0.7315436241610739
2023-03-28 00:11:25,619   att_loss = 0.0
2023-03-28 00:11:25,619   cls_loss = 0.2738011497371602
2023-03-28 00:11:25,619   eval_loss = 0.5542273033748973
2023-03-28 00:11:25,619   global_step = 799
2023-03-28 00:11:25,619   loss = 0.2738011497371602
2023-03-28 00:11:25,619   mcc = 0.3364667090764656
2023-03-28 00:11:25,619   rep_loss = 0.0
2023-03-28 00:11:25,626 ***** Save model *****
2023-03-28 00:11:34,375 ***** Running evaluation *****
2023-03-28 00:11:34,376   Epoch = 1 iter 849 step
2023-03-28 00:11:34,376   Num examples = 1043
2023-03-28 00:11:34,376   Batch size = 32
2023-03-28 00:11:34,983 ***** Eval results *****
2023-03-28 00:11:34,983   acc = 0.7430488974113135
2023-03-28 00:11:34,984   att_loss = 0.0
2023-03-28 00:11:34,984   cls_loss = 0.2740212007174416
2023-03-28 00:11:34,984   eval_loss = 0.5429370918057181
2023-03-28 00:11:34,984   global_step = 849
2023-03-28 00:11:34,984   loss = 0.2740212007174416
2023-03-28 00:11:34,984   mcc = 0.3380063289865065
2023-03-28 00:11:34,984   rep_loss = 0.0
2023-03-28 00:11:35,007 ***** Save model *****
2023-03-28 00:11:43,746 ***** Running evaluation *****
2023-03-28 00:11:43,747   Epoch = 1 iter 899 step
2023-03-28 00:11:43,747   Num examples = 1043
2023-03-28 00:11:43,747   Batch size = 32
2023-03-28 00:11:44,353 ***** Eval results *****
2023-03-28 00:11:44,353   acc = 0.7305848513902206
2023-03-28 00:11:44,354   att_loss = 0.0
2023-03-28 00:11:44,354   cls_loss = 0.27381703400448576
2023-03-28 00:11:44,354   eval_loss = 0.5531656868530043
2023-03-28 00:11:44,354   global_step = 899
2023-03-28 00:11:44,354   loss = 0.27381703400448576
2023-03-28 00:11:44,354   mcc = 0.32605098190985565
2023-03-28 00:11:44,354   rep_loss = 0.0
2023-03-28 00:11:52,419 ***** Running evaluation *****
2023-03-28 00:11:52,420   Epoch = 1 iter 949 step
2023-03-28 00:11:52,420   Num examples = 1043
2023-03-28 00:11:52,420   Batch size = 32
2023-03-28 00:11:53,025 ***** Eval results *****
2023-03-28 00:11:53,026   acc = 0.7372962607861937
2023-03-28 00:11:53,026   att_loss = 0.0
2023-03-28 00:11:53,026   cls_loss = 0.27364335850060706
2023-03-28 00:11:53,026   eval_loss = 0.5432972311973572
2023-03-28 00:11:53,026   global_step = 949
2023-03-28 00:11:53,027   loss = 0.27364335850060706
2023-03-28 00:11:53,027   mcc = 0.3259325330443582
2023-03-28 00:11:53,027   rep_loss = 0.0
2023-03-28 00:12:01,095 ***** Running evaluation *****
2023-03-28 00:12:01,096   Epoch = 1 iter 999 step
2023-03-28 00:12:01,096   Num examples = 1043
2023-03-28 00:12:01,096   Batch size = 32
2023-03-28 00:12:01,703 ***** Eval results *****
2023-03-28 00:12:01,703   acc = 0.7440076701821668
2023-03-28 00:12:01,703   att_loss = 0.0
2023-03-28 00:12:01,703   cls_loss = 0.2731752650712126
2023-03-28 00:12:01,703   eval_loss = 0.5413061940308773
2023-03-28 00:12:01,703   global_step = 999
2023-03-28 00:12:01,703   loss = 0.2731752650712126
2023-03-28 00:12:01,704   mcc = 0.3306219515641091
2023-03-28 00:12:01,704   rep_loss = 0.0
2023-03-28 00:12:09,778 ***** Running evaluation *****
2023-03-28 00:12:09,778   Epoch = 1 iter 1049 step
2023-03-28 00:12:09,779   Num examples = 1043
2023-03-28 00:12:09,779   Batch size = 32
2023-03-28 00:12:10,385 ***** Eval results *****
2023-03-28 00:12:10,385   acc = 0.738255033557047
2023-03-28 00:12:10,385   att_loss = 0.0
2023-03-28 00:12:10,385   cls_loss = 0.27321593871394406
2023-03-28 00:12:10,385   eval_loss = 0.5413706736131148
2023-03-28 00:12:10,385   global_step = 1049
2023-03-28 00:12:10,386   loss = 0.27321593871394406
2023-03-28 00:12:10,386   mcc = 0.32467991850218353
2023-03-28 00:12:10,386   rep_loss = 0.0
2023-03-28 00:12:18,456 ***** Running evaluation *****
2023-03-28 00:12:18,456   Epoch = 2 iter 1099 step
2023-03-28 00:12:18,457   Num examples = 1043
2023-03-28 00:12:18,457   Batch size = 32
2023-03-28 00:12:19,062 ***** Eval results *****
2023-03-28 00:12:19,063   acc = 0.7305848513902206
2023-03-28 00:12:19,063   att_loss = 0.0
2023-03-28 00:12:19,063   cls_loss = 0.2685939659995417
2023-03-28 00:12:19,063   eval_loss = 0.5417754930077177
2023-03-28 00:12:19,063   global_step = 1099
2023-03-28 00:12:19,063   loss = 0.2685939659995417
2023-03-28 00:12:19,063   mcc = 0.31185098672151873
2023-03-28 00:12:19,063   rep_loss = 0.0
2023-03-28 00:12:27,143 ***** Running evaluation *****
2023-03-28 00:12:27,143   Epoch = 2 iter 1149 step
2023-03-28 00:12:27,144   Num examples = 1043
2023-03-28 00:12:27,144   Batch size = 32
2023-03-28 00:12:27,752 ***** Eval results *****
2023-03-28 00:12:27,752   acc = 0.7401725790987536
2023-03-28 00:12:27,752   att_loss = 0.0
2023-03-28 00:12:27,753   cls_loss = 0.2715171111954583
2023-03-28 00:12:27,753   eval_loss = 0.5379424600890188
2023-03-28 00:12:27,753   global_step = 1149
2023-03-28 00:12:27,753   loss = 0.2715171111954583
2023-03-28 00:12:27,753   mcc = 0.32753989938135253
2023-03-28 00:12:27,753   rep_loss = 0.0
2023-03-28 00:12:35,821 ***** Running evaluation *****
2023-03-28 00:12:35,822   Epoch = 2 iter 1199 step
2023-03-28 00:12:35,822   Num examples = 1043
2023-03-28 00:12:35,822   Batch size = 32
2023-03-28 00:12:36,427 ***** Eval results *****
2023-03-28 00:12:36,428   acc = 0.7401725790987536
2023-03-28 00:12:36,428   att_loss = 0.0
2023-03-28 00:12:36,428   cls_loss = 0.27032440184181883
2023-03-28 00:12:36,428   eval_loss = 0.5379777433294238
2023-03-28 00:12:36,428   global_step = 1199
2023-03-28 00:12:36,429   loss = 0.27032440184181883
2023-03-28 00:12:36,429   mcc = 0.33796416926704326
2023-03-28 00:12:36,429   rep_loss = 0.0
2023-03-28 00:12:44,481 ***** Running evaluation *****
2023-03-28 00:12:44,482   Epoch = 2 iter 1249 step
2023-03-28 00:12:44,482   Num examples = 1043
2023-03-28 00:12:44,482   Batch size = 32
2023-03-28 00:12:45,087 ***** Eval results *****
2023-03-28 00:12:45,087   acc = 0.7363374880153404
2023-03-28 00:12:45,087   att_loss = 0.0
2023-03-28 00:12:45,087   cls_loss = 0.27106529004995333
2023-03-28 00:12:45,088   eval_loss = 0.537276872179725
2023-03-28 00:12:45,088   global_step = 1249
2023-03-28 00:12:45,088   loss = 0.27106529004995333
2023-03-28 00:12:45,088   mcc = 0.3244057129208282
2023-03-28 00:12:45,088   rep_loss = 0.0
2023-03-28 00:12:53,149 ***** Running evaluation *****
2023-03-28 00:12:53,149   Epoch = 2 iter 1299 step
2023-03-28 00:12:53,149   Num examples = 1043
2023-03-28 00:12:53,149   Batch size = 32
2023-03-28 00:12:53,758 ***** Eval results *****
2023-03-28 00:12:53,759   acc = 0.738255033557047
2023-03-28 00:12:53,759   att_loss = 0.0
2023-03-28 00:12:53,759   cls_loss = 0.27078252288944277
2023-03-28 00:12:53,759   eval_loss = 0.5372213332942037
2023-03-28 00:12:53,759   global_step = 1299
2023-03-28 00:12:53,759   loss = 0.27078252288944277
2023-03-28 00:12:53,759   mcc = 0.3342982430724728
2023-03-28 00:12:53,759   rep_loss = 0.0
2023-03-28 00:13:01,829 ***** Running evaluation *****
2023-03-28 00:13:01,829   Epoch = 2 iter 1349 step
2023-03-28 00:13:01,829   Num examples = 1043
2023-03-28 00:13:01,830   Batch size = 32
2023-03-28 00:13:02,437 ***** Eval results *****
2023-03-28 00:13:02,438   acc = 0.7449664429530202
2023-03-28 00:13:02,438   att_loss = 0.0
2023-03-28 00:13:02,438   cls_loss = 0.2707341301695732
2023-03-28 00:13:02,438   eval_loss = 0.5376597435185404
2023-03-28 00:13:02,438   global_step = 1349
2023-03-28 00:13:02,438   loss = 0.2707341301695732
2023-03-28 00:13:02,439   mcc = 0.34114287267917587
2023-03-28 00:13:02,439   rep_loss = 0.0
2023-03-28 00:13:02,441 ***** Save model *****
2023-03-28 00:13:11,185 ***** Running evaluation *****
2023-03-28 00:13:11,186   Epoch = 2 iter 1399 step
2023-03-28 00:13:11,186   Num examples = 1043
2023-03-28 00:13:11,186   Batch size = 32
2023-03-28 00:13:11,792 ***** Eval results *****
2023-03-28 00:13:11,792   acc = 0.7430488974113135
2023-03-28 00:13:11,792   att_loss = 0.0
2023-03-28 00:13:11,792   cls_loss = 0.27104338211235324
2023-03-28 00:13:11,793   eval_loss = 0.5376112000508741
2023-03-28 00:13:11,793   global_step = 1399
2023-03-28 00:13:11,793   loss = 0.27104338211235324
2023-03-28 00:13:11,793   mcc = 0.3392796829282613
2023-03-28 00:13:11,793   rep_loss = 0.0
2023-03-28 00:13:19,877 ***** Running evaluation *****
2023-03-28 00:13:19,878   Epoch = 2 iter 1449 step
2023-03-28 00:13:19,878   Num examples = 1043
2023-03-28 00:13:19,878   Batch size = 32
2023-03-28 00:13:20,487 ***** Eval results *****
2023-03-28 00:13:20,487   acc = 0.7420901246404602
2023-03-28 00:13:20,488   att_loss = 0.0
2023-03-28 00:13:20,488   cls_loss = 0.27060766799712743
2023-03-28 00:13:20,488   eval_loss = 0.5387037857012316
2023-03-28 00:13:20,488   global_step = 1449
2023-03-28 00:13:20,488   loss = 0.27060766799712743
2023-03-28 00:13:20,489   mcc = 0.33689855976359306
2023-03-28 00:13:20,489   rep_loss = 0.0
2023-03-28 00:13:28,564 ***** Running evaluation *****
2023-03-28 00:13:28,564   Epoch = 2 iter 1499 step
2023-03-28 00:13:28,564   Num examples = 1043
2023-03-28 00:13:28,564   Batch size = 32
2023-03-28 00:13:29,170 ***** Eval results *****
2023-03-28 00:13:29,170   acc = 0.7392138063279002
2023-03-28 00:13:29,170   att_loss = 0.0
2023-03-28 00:13:29,171   cls_loss = 0.2705608833486132
2023-03-28 00:13:29,171   eval_loss = 0.538345832716335
2023-03-28 00:13:29,171   global_step = 1499
2023-03-28 00:13:29,171   loss = 0.2705608833486132
2023-03-28 00:13:29,171   mcc = 0.3327272758141264
2023-03-28 00:13:29,171   rep_loss = 0.0
2023-03-28 00:13:37,255 ***** Running evaluation *****
2023-03-28 00:13:37,255   Epoch = 2 iter 1549 step
2023-03-28 00:13:37,256   Num examples = 1043
2023-03-28 00:13:37,256   Batch size = 32
2023-03-28 00:13:37,861 ***** Eval results *****
2023-03-28 00:13:37,862   acc = 0.738255033557047
2023-03-28 00:13:37,862   att_loss = 0.0
2023-03-28 00:13:37,862   cls_loss = 0.27041230711644504
2023-03-28 00:13:37,862   eval_loss = 0.5390039411458102
2023-03-28 00:13:37,862   global_step = 1549
2023-03-28 00:13:37,862   loss = 0.27041230711644504
2023-03-28 00:13:37,862   mcc = 0.3335166592636138
2023-03-28 00:13:37,862   rep_loss = 0.0
2023-03-28 00:13:45,923 ***** Running evaluation *****
2023-03-28 00:13:45,923   Epoch = 2 iter 1599 step
2023-03-28 00:13:45,923   Num examples = 1043
2023-03-28 00:13:45,924   Batch size = 32
2023-03-28 00:13:46,533 ***** Eval results *****
2023-03-28 00:13:46,534   acc = 0.7420901246404602
2023-03-28 00:13:46,534   att_loss = 0.0
2023-03-28 00:13:46,534   cls_loss = 0.27021937930988055
2023-03-28 00:13:46,534   eval_loss = 0.5384474920504021
2023-03-28 00:13:46,534   global_step = 1599
2023-03-28 00:13:46,534   loss = 0.27021937930988055
2023-03-28 00:13:46,535   mcc = 0.3382393061253004
2023-03-28 00:13:46,535   rep_loss = 0.0
2023-03-28 00:13:59,176 device: cuda n_gpu: 1
2023-03-28 00:13:59,226 Writing example 0 of 8551
2023-03-28 00:13:59,226 *** Example ***
2023-03-28 00:13:59,227 guid: train-0
2023-03-28 00:13:59,227 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-28 00:13:59,227 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:13:59,227 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:13:59,228 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:13:59,228 label: 1
2023-03-28 00:13:59,228 label_id: 1
2023-03-28 00:14:00,179 Writing example 0 of 1043
2023-03-28 00:14:00,180 *** Example ***
2023-03-28 00:14:00,180 guid: dev-0
2023-03-28 00:14:00,180 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-28 00:14:00,180 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:14:00,180 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:14:00,180 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:14:00,181 label: 1
2023-03-28 00:14:00,181 label_id: 1
2023-03-28 00:14:00,299 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-28 00:14:00,301 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-28 00:14:01,927 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-28 00:14:02,399 loading model...
2023-03-28 00:14:02,487 done!
2023-03-28 00:14:02,488 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-28 00:14:02,581 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01
2023-03-28 00:14:02,583 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-28 00:14:02,800 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01/pytorch_model.bin
2023-03-28 00:14:02,815 loading model...
2023-03-28 00:14:02,830 done!
2023-03-28 00:14:02,844 ***** Running training *****
2023-03-28 00:14:02,844   Num examples = 8551
2023-03-28 00:14:02,844   Batch size = 32
2023-03-28 00:14:02,844   Num steps = 801
2023-03-28 00:14:02,845 n: bert.embeddings.word_embeddings.weight
2023-03-28 00:14:02,845 n: bert.embeddings.position_embeddings.weight
2023-03-28 00:14:02,845 n: bert.embeddings.token_type_embeddings.weight
2023-03-28 00:14:02,845 n: bert.embeddings.LayerNorm.weight
2023-03-28 00:14:02,845 n: bert.embeddings.LayerNorm.bias
2023-03-28 00:14:02,845 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-28 00:14:02,845 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-28 00:14:02,845 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-28 00:14:02,846 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-28 00:14:02,846 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-28 00:14:02,846 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-28 00:14:02,846 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-28 00:14:02,846 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-28 00:14:02,846 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-28 00:14:02,846 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-28 00:14:02,846 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-28 00:14:02,846 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-28 00:14:02,846 n: bert.encoder.layer.0.output.dense.weight
2023-03-28 00:14:02,847 n: bert.encoder.layer.0.output.dense.bias
2023-03-28 00:14:02,847 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-28 00:14:02,847 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-28 00:14:02,847 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-28 00:14:02,847 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-28 00:14:02,847 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-28 00:14:02,847 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-28 00:14:02,847 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-28 00:14:02,847 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-28 00:14:02,848 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-28 00:14:02,848 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-28 00:14:02,848 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-28 00:14:02,848 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-28 00:14:02,848 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-28 00:14:02,848 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-28 00:14:02,848 n: bert.encoder.layer.1.output.dense.weight
2023-03-28 00:14:02,848 n: bert.encoder.layer.1.output.dense.bias
2023-03-28 00:14:02,848 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-28 00:14:02,849 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-28 00:14:02,849 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-28 00:14:02,849 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-28 00:14:02,849 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-28 00:14:02,849 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-28 00:14:02,849 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-28 00:14:02,849 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-28 00:14:02,849 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-28 00:14:02,849 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-28 00:14:02,849 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-28 00:14:02,850 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-28 00:14:02,850 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-28 00:14:02,850 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-28 00:14:02,850 n: bert.encoder.layer.2.output.dense.weight
2023-03-28 00:14:02,850 n: bert.encoder.layer.2.output.dense.bias
2023-03-28 00:14:02,850 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-28 00:14:02,850 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-28 00:14:02,850 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-28 00:14:02,850 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-28 00:14:02,851 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-28 00:14:02,851 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-28 00:14:02,851 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-28 00:14:02,851 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-28 00:14:02,851 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-28 00:14:02,851 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-28 00:14:02,851 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-28 00:14:02,851 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-28 00:14:02,851 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-28 00:14:02,851 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-28 00:14:02,852 n: bert.encoder.layer.3.output.dense.weight
2023-03-28 00:14:02,852 n: bert.encoder.layer.3.output.dense.bias
2023-03-28 00:14:02,852 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-28 00:14:02,852 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-28 00:14:02,852 n: bert.pooler.dense.weight
2023-03-28 00:14:02,852 n: bert.pooler.dense.bias
2023-03-28 00:14:02,852 n: classifier.weight
2023-03-28 00:14:02,852 n: classifier.bias
2023-03-28 00:14:02,852 n: fit_dense.weight
2023-03-28 00:14:02,853 n: fit_dense.bias
2023-03-28 00:14:02,853 Total parameters: 14591258
2023-03-28 00:14:15,917 ***** Running evaluation *****
2023-03-28 00:14:15,917   Epoch = 0 iter 49 step
2023-03-28 00:14:15,917   Num examples = 1043
2023-03-28 00:14:15,917   Batch size = 32
2023-03-28 00:14:16,529 ***** Eval results *****
2023-03-28 00:14:16,529   acc = 0.7363374880153404
2023-03-28 00:14:16,529   att_loss = 0.0
2023-03-28 00:14:16,529   cls_loss = 0.3319362408044387
2023-03-28 00:14:16,530   eval_loss = 0.6128379991560271
2023-03-28 00:14:16,530   global_step = 49
2023-03-28 00:14:16,530   loss = 0.3319362408044387
2023-03-28 00:14:16,530   mcc = 0.3053949118878911
2023-03-28 00:14:16,531   rep_loss = 0.0
2023-03-28 00:14:16,538 ***** Save model *****
2023-03-28 00:14:30,528 ***** Running evaluation *****
2023-03-28 00:14:30,529   Epoch = 0 iter 99 step
2023-03-28 00:14:30,529   Num examples = 1043
2023-03-28 00:14:30,529   Batch size = 32
2023-03-28 00:14:31,134 ***** Eval results *****
2023-03-28 00:14:31,135   acc = 0.7142857142857143
2023-03-28 00:14:31,135   att_loss = 0.0
2023-03-28 00:14:31,135   cls_loss = 0.3111466241605354
2023-03-28 00:14:31,135   eval_loss = 0.5713456471761068
2023-03-28 00:14:31,135   global_step = 99
2023-03-28 00:14:31,136   loss = 0.3111466241605354
2023-03-28 00:14:31,136   mcc = 0.2961613097391884
2023-03-28 00:14:31,136   rep_loss = 0.0
2023-03-28 00:14:44,478 ***** Running evaluation *****
2023-03-28 00:14:44,478   Epoch = 0 iter 149 step
2023-03-28 00:14:44,478   Num examples = 1043
2023-03-28 00:14:44,478   Batch size = 32
2023-03-28 00:14:45,085 ***** Eval results *****
2023-03-28 00:14:45,086   acc = 0.7305848513902206
2023-03-28 00:14:45,086   att_loss = 0.0
2023-03-28 00:14:45,086   cls_loss = 0.3001451808334197
2023-03-28 00:14:45,086   eval_loss = 0.562409188711282
2023-03-28 00:14:45,086   global_step = 149
2023-03-28 00:14:45,086   loss = 0.3001451808334197
2023-03-28 00:14:45,086   mcc = 0.2912670481507531
2023-03-28 00:14:45,087   rep_loss = 0.0
2023-03-28 00:14:58,426 ***** Running evaluation *****
2023-03-28 00:14:58,426   Epoch = 0 iter 199 step
2023-03-28 00:14:58,427   Num examples = 1043
2023-03-28 00:14:58,427   Batch size = 32
2023-03-28 00:14:59,033 ***** Eval results *****
2023-03-28 00:14:59,033   acc = 0.7238734419942474
2023-03-28 00:14:59,033   att_loss = 0.0
2023-03-28 00:14:59,033   cls_loss = 0.2953660648372305
2023-03-28 00:14:59,034   eval_loss = 0.5577722715609001
2023-03-28 00:14:59,034   global_step = 199
2023-03-28 00:14:59,034   loss = 0.2953660648372305
2023-03-28 00:14:59,034   mcc = 0.31413008856684815
2023-03-28 00:14:59,034   rep_loss = 0.0
2023-03-28 00:14:59,041 ***** Save model *****
2023-03-28 00:15:13,051 ***** Running evaluation *****
2023-03-28 00:15:13,051   Epoch = 0 iter 249 step
2023-03-28 00:15:13,052   Num examples = 1043
2023-03-28 00:15:13,052   Batch size = 32
2023-03-28 00:15:13,658 ***** Eval results *****
2023-03-28 00:15:13,658   acc = 0.7248322147651006
2023-03-28 00:15:13,658   att_loss = 0.0
2023-03-28 00:15:13,658   cls_loss = 0.2910160808917509
2023-03-28 00:15:13,659   eval_loss = 0.5529284097931602
2023-03-28 00:15:13,659   global_step = 249
2023-03-28 00:15:13,659   loss = 0.2910160808917509
2023-03-28 00:15:13,659   mcc = 0.2952786406947246
2023-03-28 00:15:13,659   rep_loss = 0.0
2023-03-28 00:15:27,027 ***** Running evaluation *****
2023-03-28 00:15:27,028   Epoch = 1 iter 299 step
2023-03-28 00:15:27,028   Num examples = 1043
2023-03-28 00:15:27,028   Batch size = 32
2023-03-28 00:15:27,637 ***** Eval results *****
2023-03-28 00:15:27,637   acc = 0.7286673058485139
2023-03-28 00:15:27,637   att_loss = 0.0
2023-03-28 00:15:27,637   cls_loss = 0.27492258278653026
2023-03-28 00:15:27,638   eval_loss = 0.5523554664669614
2023-03-28 00:15:27,638   global_step = 299
2023-03-28 00:15:27,638   loss = 0.27492258278653026
2023-03-28 00:15:27,638   mcc = 0.28676039766283073
2023-03-28 00:15:27,638   rep_loss = 0.0
2023-03-28 00:15:40,997 ***** Running evaluation *****
2023-03-28 00:15:40,997   Epoch = 1 iter 349 step
2023-03-28 00:15:40,997   Num examples = 1043
2023-03-28 00:15:40,998   Batch size = 32
2023-03-28 00:15:41,603 ***** Eval results *****
2023-03-28 00:15:41,603   acc = 0.7267497603068073
2023-03-28 00:15:41,603   att_loss = 0.0
2023-03-28 00:15:41,603   cls_loss = 0.2734830326786855
2023-03-28 00:15:41,603   eval_loss = 0.5514040210030295
2023-03-28 00:15:41,604   global_step = 349
2023-03-28 00:15:41,604   loss = 0.2734830326786855
2023-03-28 00:15:41,604   mcc = 0.28092572618232253
2023-03-28 00:15:41,604   rep_loss = 0.0
2023-03-28 00:15:54,979 ***** Running evaluation *****
2023-03-28 00:15:54,979   Epoch = 1 iter 399 step
2023-03-28 00:15:54,980   Num examples = 1043
2023-03-28 00:15:54,980   Batch size = 32
2023-03-28 00:15:55,586 ***** Eval results *****
2023-03-28 00:15:55,586   acc = 0.7334611697027804
2023-03-28 00:15:55,586   att_loss = 0.0
2023-03-28 00:15:55,586   cls_loss = 0.27316116643222893
2023-03-28 00:15:55,586   eval_loss = 0.5503851084998159
2023-03-28 00:15:55,587   global_step = 399
2023-03-28 00:15:55,587   loss = 0.27316116643222893
2023-03-28 00:15:55,587   mcc = 0.31765323876865625
2023-03-28 00:15:55,587   rep_loss = 0.0
2023-03-28 00:15:55,594 ***** Save model *****
2023-03-28 00:16:09,636 ***** Running evaluation *****
2023-03-28 00:16:09,636   Epoch = 1 iter 449 step
2023-03-28 00:16:09,637   Num examples = 1043
2023-03-28 00:16:09,637   Batch size = 32
2023-03-28 00:16:10,244 ***** Eval results *****
2023-03-28 00:16:10,245   acc = 0.7363374880153404
2023-03-28 00:16:10,245   att_loss = 0.0
2023-03-28 00:16:10,245   cls_loss = 0.2726224245769637
2023-03-28 00:16:10,245   eval_loss = 0.5475269321239356
2023-03-28 00:16:10,245   global_step = 449
2023-03-28 00:16:10,246   loss = 0.2726224245769637
2023-03-28 00:16:10,246   mcc = 0.3164317976383637
2023-03-28 00:16:10,246   rep_loss = 0.0
2023-03-28 00:16:23,610 ***** Running evaluation *****
2023-03-28 00:16:23,611   Epoch = 1 iter 499 step
2023-03-28 00:16:23,611   Num examples = 1043
2023-03-28 00:16:23,611   Batch size = 32
2023-03-28 00:16:24,218 ***** Eval results *****
2023-03-28 00:16:24,218   acc = 0.7315436241610739
2023-03-28 00:16:24,218   att_loss = 0.0
2023-03-28 00:16:24,218   cls_loss = 0.27252123819599894
2023-03-28 00:16:24,218   eval_loss = 0.5455325345198313
2023-03-28 00:16:24,218   global_step = 499
2023-03-28 00:16:24,218   loss = 0.27252123819599894
2023-03-28 00:16:24,219   mcc = 0.3021753974624
2023-03-28 00:16:24,219   rep_loss = 0.0
2023-03-28 00:16:37,627 ***** Running evaluation *****
2023-03-28 00:16:37,627   Epoch = 2 iter 549 step
2023-03-28 00:16:37,627   Num examples = 1043
2023-03-28 00:16:37,627   Batch size = 32
2023-03-28 00:16:38,235 ***** Eval results *****
2023-03-28 00:16:38,235   acc = 0.7286673058485139
2023-03-28 00:16:38,235   att_loss = 0.0
2023-03-28 00:16:38,235   cls_loss = 0.26776098807652793
2023-03-28 00:16:38,235   eval_loss = 0.5500001446767286
2023-03-28 00:16:38,235   global_step = 549
2023-03-28 00:16:38,236   loss = 0.26776098807652793
2023-03-28 00:16:38,236   mcc = 0.31013906111435474
2023-03-28 00:16:38,236   rep_loss = 0.0
2023-03-28 00:16:51,639 ***** Running evaluation *****
2023-03-28 00:16:51,639   Epoch = 2 iter 599 step
2023-03-28 00:16:51,640   Num examples = 1043
2023-03-28 00:16:51,640   Batch size = 32
2023-03-28 00:16:52,247 ***** Eval results *****
2023-03-28 00:16:52,247   acc = 0.7296260786193672
2023-03-28 00:16:52,247   att_loss = 0.0
2023-03-28 00:16:52,248   cls_loss = 0.2701574343901414
2023-03-28 00:16:52,248   eval_loss = 0.5488958855470022
2023-03-28 00:16:52,248   global_step = 599
2023-03-28 00:16:52,248   loss = 0.2701574343901414
2023-03-28 00:16:52,248   mcc = 0.30710961377945384
2023-03-28 00:16:52,248   rep_loss = 0.0
2023-03-28 00:17:05,659 ***** Running evaluation *****
2023-03-28 00:17:05,659   Epoch = 2 iter 649 step
2023-03-28 00:17:05,660   Num examples = 1043
2023-03-28 00:17:05,660   Batch size = 32
2023-03-28 00:17:06,267 ***** Eval results *****
2023-03-28 00:17:06,268   acc = 0.7315436241610739
2023-03-28 00:17:06,268   att_loss = 0.0
2023-03-28 00:17:06,268   cls_loss = 0.27024195557055264
2023-03-28 00:17:06,268   eval_loss = 0.5470097137219978
2023-03-28 00:17:06,268   global_step = 649
2023-03-28 00:17:06,269   loss = 0.27024195557055264
2023-03-28 00:17:06,269   mcc = 0.3059694526507708
2023-03-28 00:17:06,269   rep_loss = 0.0
2023-03-28 00:17:19,704 ***** Running evaluation *****
2023-03-28 00:17:19,704   Epoch = 2 iter 699 step
2023-03-28 00:17:19,704   Num examples = 1043
2023-03-28 00:17:19,705   Batch size = 32
2023-03-28 00:17:20,312 ***** Eval results *****
2023-03-28 00:17:20,312   acc = 0.7325023969319271
2023-03-28 00:17:20,312   att_loss = 0.0
2023-03-28 00:17:20,312   cls_loss = 0.27114079829418297
2023-03-28 00:17:20,312   eval_loss = 0.5471868253115452
2023-03-28 00:17:20,312   global_step = 699
2023-03-28 00:17:20,312   loss = 0.27114079829418297
2023-03-28 00:17:20,313   mcc = 0.31220024302985555
2023-03-28 00:17:20,313   rep_loss = 0.0
2023-03-28 00:17:33,719 ***** Running evaluation *****
2023-03-28 00:17:33,720   Epoch = 2 iter 749 step
2023-03-28 00:17:33,720   Num examples = 1043
2023-03-28 00:17:33,720   Batch size = 32
2023-03-28 00:17:34,327 ***** Eval results *****
2023-03-28 00:17:34,328   acc = 0.7296260786193672
2023-03-28 00:17:34,328   att_loss = 0.0
2023-03-28 00:17:34,328   cls_loss = 0.27083225368067276
2023-03-28 00:17:34,328   eval_loss = 0.5469547898480387
2023-03-28 00:17:34,329   global_step = 749
2023-03-28 00:17:34,329   loss = 0.27083225368067276
2023-03-28 00:17:34,329   mcc = 0.3029506273850554
2023-03-28 00:17:34,329   rep_loss = 0.0
2023-03-28 00:17:47,747 ***** Running evaluation *****
2023-03-28 00:17:47,748   Epoch = 2 iter 799 step
2023-03-28 00:17:47,748   Num examples = 1043
2023-03-28 00:17:47,748   Batch size = 32
2023-03-28 00:17:48,357 ***** Eval results *****
2023-03-28 00:17:48,358   acc = 0.7315436241610739
2023-03-28 00:17:48,358   att_loss = 0.0
2023-03-28 00:17:48,358   cls_loss = 0.2704735637273429
2023-03-28 00:17:48,358   eval_loss = 0.5468856872934283
2023-03-28 00:17:48,359   global_step = 799
2023-03-28 00:17:48,359   loss = 0.2704735637273429
2023-03-28 00:17:48,359   mcc = 0.3067490054788977
2023-03-28 00:17:48,359   rep_loss = 0.0
2023-03-28 00:17:59,923 device: cuda n_gpu: 1
2023-03-28 00:17:59,972 Writing example 0 of 8551
2023-03-28 00:17:59,972 *** Example ***
2023-03-28 00:17:59,972 guid: train-0
2023-03-28 00:17:59,973 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-28 00:17:59,973 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:17:59,973 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:17:59,973 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:17:59,973 label: 1
2023-03-28 00:17:59,974 label_id: 1
2023-03-28 00:18:00,935 Writing example 0 of 1043
2023-03-28 00:18:00,935 *** Example ***
2023-03-28 00:18:00,935 guid: dev-0
2023-03-28 00:18:00,935 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-28 00:18:00,936 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:18:00,936 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:18:00,936 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:18:00,936 label: 1
2023-03-28 00:18:00,936 label_id: 1
2023-03-28 00:18:01,057 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-28 00:18:01,058 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-28 00:18:02,840 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-28 00:18:03,324 loading model...
2023-03-28 00:18:03,413 done!
2023-03-28 00:18:03,414 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-28 00:18:03,503 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01
2023-03-28 00:18:03,506 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-28 00:18:03,732 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01/pytorch_model.bin
2023-03-28 00:18:03,822 loading model...
2023-03-28 00:18:03,835 done!
2023-03-28 00:18:03,850 ***** Running training *****
2023-03-28 00:18:03,850   Num examples = 8551
2023-03-28 00:18:03,850   Batch size = 32
2023-03-28 00:18:03,851   Num steps = 801
2023-03-28 00:18:03,851 n: bert.embeddings.word_embeddings.weight
2023-03-28 00:18:03,851 n: bert.embeddings.position_embeddings.weight
2023-03-28 00:18:03,851 n: bert.embeddings.token_type_embeddings.weight
2023-03-28 00:18:03,851 n: bert.embeddings.LayerNorm.weight
2023-03-28 00:18:03,851 n: bert.embeddings.LayerNorm.bias
2023-03-28 00:18:03,852 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-28 00:18:03,852 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-28 00:18:03,852 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-28 00:18:03,852 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-28 00:18:03,852 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-28 00:18:03,852 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-28 00:18:03,852 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-28 00:18:03,852 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-28 00:18:03,852 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-28 00:18:03,852 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-28 00:18:03,853 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-28 00:18:03,853 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-28 00:18:03,853 n: bert.encoder.layer.0.output.dense.weight
2023-03-28 00:18:03,853 n: bert.encoder.layer.0.output.dense.bias
2023-03-28 00:18:03,853 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-28 00:18:03,853 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-28 00:18:03,853 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-28 00:18:03,853 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-28 00:18:03,853 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-28 00:18:03,853 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-28 00:18:03,854 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-28 00:18:03,854 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-28 00:18:03,854 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-28 00:18:03,854 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-28 00:18:03,854 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-28 00:18:03,854 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-28 00:18:03,854 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-28 00:18:03,854 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-28 00:18:03,854 n: bert.encoder.layer.1.output.dense.weight
2023-03-28 00:18:03,855 n: bert.encoder.layer.1.output.dense.bias
2023-03-28 00:18:03,855 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-28 00:18:03,855 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-28 00:18:03,855 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-28 00:18:03,855 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-28 00:18:03,855 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-28 00:18:03,855 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-28 00:18:03,855 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-28 00:18:03,855 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-28 00:18:03,855 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-28 00:18:03,856 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-28 00:18:03,856 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-28 00:18:03,856 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-28 00:18:03,856 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-28 00:18:03,856 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-28 00:18:03,856 n: bert.encoder.layer.2.output.dense.weight
2023-03-28 00:18:03,856 n: bert.encoder.layer.2.output.dense.bias
2023-03-28 00:18:03,856 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-28 00:18:03,856 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-28 00:18:03,856 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-28 00:18:03,857 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-28 00:18:03,857 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-28 00:18:03,857 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-28 00:18:03,857 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-28 00:18:03,857 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-28 00:18:03,857 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-28 00:18:03,857 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-28 00:18:03,857 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-28 00:18:03,857 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-28 00:18:03,857 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-28 00:18:03,858 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-28 00:18:03,858 n: bert.encoder.layer.3.output.dense.weight
2023-03-28 00:18:03,858 n: bert.encoder.layer.3.output.dense.bias
2023-03-28 00:18:03,858 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-28 00:18:03,858 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-28 00:18:03,858 n: bert.pooler.dense.weight
2023-03-28 00:18:03,858 n: bert.pooler.dense.bias
2023-03-28 00:18:03,858 n: classifier.weight
2023-03-28 00:18:03,858 n: classifier.bias
2023-03-28 00:18:03,858 n: fit_dense.weight
2023-03-28 00:18:03,859 n: fit_dense.bias
2023-03-28 00:18:03,859 Total parameters: 14591258
2023-03-28 00:18:16,931 ***** Running evaluation *****
2023-03-28 00:18:16,931   Epoch = 0 iter 49 step
2023-03-28 00:18:16,931   Num examples = 1043
2023-03-28 00:18:16,931   Batch size = 32
2023-03-28 00:18:17,740 ***** Eval results *****
2023-03-28 00:18:17,740   acc = 0.7315436241610739
2023-03-28 00:18:17,741   att_loss = 0.0
2023-03-28 00:18:17,741   cls_loss = 0.32147838448991584
2023-03-28 00:18:17,741   eval_loss = 0.5749412106745171
2023-03-28 00:18:17,741   global_step = 49
2023-03-28 00:18:17,742   loss = 0.32147838448991584
2023-03-28 00:18:17,742   mcc = 0.2920517715695529
2023-03-28 00:18:17,742   rep_loss = 0.0
2023-03-28 00:18:17,753 ***** Save model *****
2023-03-28 00:18:31,733 ***** Running evaluation *****
2023-03-28 00:18:31,733   Epoch = 0 iter 99 step
2023-03-28 00:18:31,733   Num examples = 1043
2023-03-28 00:18:31,734   Batch size = 32
2023-03-28 00:18:32,339 ***** Eval results *****
2023-03-28 00:18:32,339   acc = 0.6970278044103547
2023-03-28 00:18:32,339   att_loss = 0.0
2023-03-28 00:18:32,339   cls_loss = 0.30451243634175773
2023-03-28 00:18:32,339   eval_loss = 0.600251628593965
2023-03-28 00:18:32,340   global_step = 99
2023-03-28 00:18:32,340   loss = 0.30451243634175773
2023-03-28 00:18:32,340   mcc = 0.282885835108455
2023-03-28 00:18:32,340   rep_loss = 0.0
2023-03-28 00:18:45,689 ***** Running evaluation *****
2023-03-28 00:18:45,689   Epoch = 0 iter 149 step
2023-03-28 00:18:45,689   Num examples = 1043
2023-03-28 00:18:45,689   Batch size = 32
2023-03-28 00:18:46,296 ***** Eval results *****
2023-03-28 00:18:46,296   acc = 0.7181208053691275
2023-03-28 00:18:46,296   att_loss = 0.0
2023-03-28 00:18:46,296   cls_loss = 0.29717853685353424
2023-03-28 00:18:46,296   eval_loss = 0.5778075601115371
2023-03-28 00:18:46,296   global_step = 149
2023-03-28 00:18:46,296   loss = 0.29717853685353424
2023-03-28 00:18:46,297   mcc = 0.2764464416592447
2023-03-28 00:18:46,297   rep_loss = 0.0
2023-03-28 00:18:59,644 ***** Running evaluation *****
2023-03-28 00:18:59,645   Epoch = 0 iter 199 step
2023-03-28 00:18:59,645   Num examples = 1043
2023-03-28 00:18:59,645   Batch size = 32
2023-03-28 00:19:00,260 ***** Eval results *****
2023-03-28 00:19:00,260   acc = 0.7085330776605945
2023-03-28 00:19:00,261   att_loss = 0.0
2023-03-28 00:19:00,261   cls_loss = 0.29376217243659436
2023-03-28 00:19:00,261   eval_loss = 0.5722114741802216
2023-03-28 00:19:00,261   global_step = 199
2023-03-28 00:19:00,261   loss = 0.29376217243659436
2023-03-28 00:19:00,261   mcc = 0.29410053711898326
2023-03-28 00:19:00,261   rep_loss = 0.0
2023-03-28 00:19:00,264 ***** Save model *****
2023-03-28 00:19:14,287 ***** Running evaluation *****
2023-03-28 00:19:14,288   Epoch = 0 iter 249 step
2023-03-28 00:19:14,288   Num examples = 1043
2023-03-28 00:19:14,288   Batch size = 32
2023-03-28 00:19:14,893 ***** Eval results *****
2023-03-28 00:19:14,893   acc = 0.7286673058485139
2023-03-28 00:19:14,893   att_loss = 0.0
2023-03-28 00:19:14,893   cls_loss = 0.2902013989217789
2023-03-28 00:19:14,893   eval_loss = 0.5595149632656213
2023-03-28 00:19:14,893   global_step = 249
2023-03-28 00:19:14,894   loss = 0.2902013989217789
2023-03-28 00:19:14,894   mcc = 0.2911416522578436
2023-03-28 00:19:14,894   rep_loss = 0.0
2023-03-28 00:19:28,263 ***** Running evaluation *****
2023-03-28 00:19:28,264   Epoch = 1 iter 299 step
2023-03-28 00:19:28,264   Num examples = 1043
2023-03-28 00:19:28,264   Batch size = 32
2023-03-28 00:19:28,869 ***** Eval results *****
2023-03-28 00:19:28,870   acc = 0.7219558964525408
2023-03-28 00:19:28,870   att_loss = 0.0
2023-03-28 00:19:28,870   cls_loss = 0.2755206823348999
2023-03-28 00:19:28,870   eval_loss = 0.5602690478165945
2023-03-28 00:19:28,870   global_step = 299
2023-03-28 00:19:28,870   loss = 0.2755206823348999
2023-03-28 00:19:28,870   mcc = 0.25820812984395297
2023-03-28 00:19:28,871   rep_loss = 0.0
2023-03-28 00:19:42,237 ***** Running evaluation *****
2023-03-28 00:19:42,238   Epoch = 1 iter 349 step
2023-03-28 00:19:42,238   Num examples = 1043
2023-03-28 00:19:42,238   Batch size = 32
2023-03-28 00:19:42,843 ***** Eval results *****
2023-03-28 00:19:42,843   acc = 0.725790987535954
2023-03-28 00:19:42,843   att_loss = 0.0
2023-03-28 00:19:42,844   cls_loss = 0.27432840008561205
2023-03-28 00:19:42,844   eval_loss = 0.5527312385313439
2023-03-28 00:19:42,844   global_step = 349
2023-03-28 00:19:42,844   loss = 0.27432840008561205
2023-03-28 00:19:42,844   mcc = 0.28146722461472
2023-03-28 00:19:42,844   rep_loss = 0.0
2023-03-28 00:19:56,239 ***** Running evaluation *****
2023-03-28 00:19:56,239   Epoch = 1 iter 399 step
2023-03-28 00:19:56,240   Num examples = 1043
2023-03-28 00:19:56,240   Batch size = 32
2023-03-28 00:19:56,846 ***** Eval results *****
2023-03-28 00:19:56,846   acc = 0.7219558964525408
2023-03-28 00:19:56,846   att_loss = 0.0
2023-03-28 00:19:56,847   cls_loss = 0.2739170069495837
2023-03-28 00:19:56,847   eval_loss = 0.5542624394098917
2023-03-28 00:19:56,847   global_step = 399
2023-03-28 00:19:56,847   loss = 0.2739170069495837
2023-03-28 00:19:56,847   mcc = 0.29739732160697946
2023-03-28 00:19:56,847   rep_loss = 0.0
2023-03-28 00:19:56,855 ***** Save model *****
2023-03-28 00:20:11,386 ***** Running evaluation *****
2023-03-28 00:20:11,386   Epoch = 1 iter 449 step
2023-03-28 00:20:11,386   Num examples = 1043
2023-03-28 00:20:11,386   Batch size = 32
2023-03-28 00:20:11,993 ***** Eval results *****
2023-03-28 00:20:11,993   acc = 0.7325023969319271
2023-03-28 00:20:11,993   att_loss = 0.0
2023-03-28 00:20:11,993   cls_loss = 0.2733109984751586
2023-03-28 00:20:11,994   eval_loss = 0.5481378941824941
2023-03-28 00:20:11,994   global_step = 449
2023-03-28 00:20:11,994   loss = 0.2733109984751586
2023-03-28 00:20:11,994   mcc = 0.3090750240568001
2023-03-28 00:20:11,994   rep_loss = 0.0
2023-03-28 00:20:12,001 ***** Save model *****
2023-03-28 00:20:26,062 ***** Running evaluation *****
2023-03-28 00:20:26,063   Epoch = 1 iter 499 step
2023-03-28 00:20:26,063   Num examples = 1043
2023-03-28 00:20:26,063   Batch size = 32
2023-03-28 00:20:26,669 ***** Eval results *****
2023-03-28 00:20:26,669   acc = 0.738255033557047
2023-03-28 00:20:26,669   att_loss = 0.0
2023-03-28 00:20:26,670   cls_loss = 0.27311007884042016
2023-03-28 00:20:26,670   eval_loss = 0.5470819247491432
2023-03-28 00:20:26,670   global_step = 499
2023-03-28 00:20:26,670   loss = 0.27311007884042016
2023-03-28 00:20:26,670   mcc = 0.3200611511804038
2023-03-28 00:20:26,670   rep_loss = 0.0
2023-03-28 00:20:26,677 ***** Save model *****
2023-03-28 00:20:40,740 ***** Running evaluation *****
2023-03-28 00:20:40,740   Epoch = 2 iter 549 step
2023-03-28 00:20:40,740   Num examples = 1043
2023-03-28 00:20:40,740   Batch size = 32
2023-03-28 00:20:41,347 ***** Eval results *****
2023-03-28 00:20:41,347   acc = 0.7248322147651006
2023-03-28 00:20:41,348   att_loss = 0.0
2023-03-28 00:20:41,348   cls_loss = 0.2675975014766057
2023-03-28 00:20:41,348   eval_loss = 0.5533715098193197
2023-03-28 00:20:41,348   global_step = 549
2023-03-28 00:20:41,348   loss = 0.2675975014766057
2023-03-28 00:20:41,348   mcc = 0.30639861601277335
2023-03-28 00:20:41,349   rep_loss = 0.0
2023-03-28 00:20:54,751 ***** Running evaluation *****
2023-03-28 00:20:54,751   Epoch = 2 iter 599 step
2023-03-28 00:20:54,751   Num examples = 1043
2023-03-28 00:20:54,751   Batch size = 32
2023-03-28 00:20:55,358 ***** Eval results *****
2023-03-28 00:20:55,358   acc = 0.7315436241610739
2023-03-28 00:20:55,359   att_loss = 0.0
2023-03-28 00:20:55,359   cls_loss = 0.2702183349774434
2023-03-28 00:20:55,359   eval_loss = 0.5498513705802687
2023-03-28 00:20:55,359   global_step = 599
2023-03-28 00:20:55,359   loss = 0.2702183349774434
2023-03-28 00:20:55,359   mcc = 0.3067490054788977
2023-03-28 00:20:55,359   rep_loss = 0.0
2023-03-28 00:21:08,781 ***** Running evaluation *****
2023-03-28 00:21:08,782   Epoch = 2 iter 649 step
2023-03-28 00:21:08,782   Num examples = 1043
2023-03-28 00:21:08,782   Batch size = 32
2023-03-28 00:21:09,388 ***** Eval results *****
2023-03-28 00:21:09,389   acc = 0.7353787152444871
2023-03-28 00:21:09,389   att_loss = 0.0
2023-03-28 00:21:09,389   cls_loss = 0.2702715846507446
2023-03-28 00:21:09,389   eval_loss = 0.5479927171360363
2023-03-28 00:21:09,389   global_step = 649
2023-03-28 00:21:09,389   loss = 0.2702715846507446
2023-03-28 00:21:09,389   mcc = 0.3147047823493449
2023-03-28 00:21:09,389   rep_loss = 0.0
2023-03-28 00:21:22,818 ***** Running evaluation *****
2023-03-28 00:21:22,819   Epoch = 2 iter 699 step
2023-03-28 00:21:22,819   Num examples = 1043
2023-03-28 00:21:22,819   Batch size = 32
2023-03-28 00:21:23,427 ***** Eval results *****
2023-03-28 00:21:23,427   acc = 0.7315436241610739
2023-03-28 00:21:23,427   att_loss = 0.0
2023-03-28 00:21:23,428   cls_loss = 0.271251658688892
2023-03-28 00:21:23,428   eval_loss = 0.5483483130281622
2023-03-28 00:21:23,428   global_step = 699
2023-03-28 00:21:23,428   loss = 0.271251658688892
2023-03-28 00:21:23,428   mcc = 0.30993123241054954
2023-03-28 00:21:23,428   rep_loss = 0.0
2023-03-28 00:21:36,851 ***** Running evaluation *****
2023-03-28 00:21:36,851   Epoch = 2 iter 749 step
2023-03-28 00:21:36,851   Num examples = 1043
2023-03-28 00:21:36,851   Batch size = 32
2023-03-28 00:21:37,461 ***** Eval results *****
2023-03-28 00:21:37,461   acc = 0.7372962607861937
2023-03-28 00:21:37,461   att_loss = 0.0
2023-03-28 00:21:37,462   cls_loss = 0.27094306010146474
2023-03-28 00:21:37,462   eval_loss = 0.5483153633999102
2023-03-28 00:21:37,462   global_step = 749
2023-03-28 00:21:37,462   loss = 0.27094306010146474
2023-03-28 00:21:37,462   mcc = 0.322303902111525
2023-03-28 00:21:37,462   rep_loss = 0.0
2023-03-28 00:21:37,470 ***** Save model *****
2023-03-28 00:21:51,557 ***** Running evaluation *****
2023-03-28 00:21:51,558   Epoch = 2 iter 799 step
2023-03-28 00:21:51,558   Num examples = 1043
2023-03-28 00:21:51,558   Batch size = 32
2023-03-28 00:21:52,165 ***** Eval results *****
2023-03-28 00:21:52,166   acc = 0.7392138063279002
2023-03-28 00:21:52,166   att_loss = 0.0
2023-03-28 00:21:52,166   cls_loss = 0.27052306617205996
2023-03-28 00:21:52,166   eval_loss = 0.5482751835476268
2023-03-28 00:21:52,167   global_step = 799
2023-03-28 00:21:52,167   loss = 0.27052306617205996
2023-03-28 00:21:52,167   mcc = 0.3257379617775925
2023-03-28 00:21:52,167   rep_loss = 0.0
2023-03-28 00:21:52,174 ***** Save model *****
2023-03-28 00:22:07,289 device: cuda n_gpu: 1
2023-03-28 00:22:07,341 Writing example 0 of 8551
2023-03-28 00:22:07,342 *** Example ***
2023-03-28 00:22:07,342 guid: train-0
2023-03-28 00:22:07,343 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2023-03-28 00:22:07,343 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:22:07,343 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:22:07,343 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:22:07,344 label: 1
2023-03-28 00:22:07,344 label_id: 1
2023-03-28 00:22:08,302 Writing example 0 of 1043
2023-03-28 00:22:08,302 *** Example ***
2023-03-28 00:22:08,303 guid: dev-0
2023-03-28 00:22:08,303 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2023-03-28 00:22:08,303 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:22:08,303 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:22:08,303 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-03-28 00:22:08,304 label: 1
2023-03-28 00:22:08,304 label_id: 1
2023-03-28 00:22:08,430 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
2023-03-28 00:22:08,431 Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-28 00:22:10,131 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
2023-03-28 00:22:10,244 loading model...
2023-03-28 00:22:10,330 done!
2023-03-28 00:22:10,330 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-03-28 00:22:10,436 loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01
2023-03-28 00:22:10,436 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2023-03-28 00:22:10,662 Loading model /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01/pytorch_model.bin
2023-03-28 00:22:10,684 loading model...
2023-03-28 00:22:10,697 done!
2023-03-28 00:22:10,712 ***** Running training *****
2023-03-28 00:22:10,713   Num examples = 8551
2023-03-28 00:22:10,713   Batch size = 32
2023-03-28 00:22:10,713   Num steps = 801
2023-03-28 00:22:10,713 n: bert.embeddings.word_embeddings.weight
2023-03-28 00:22:10,713 n: bert.embeddings.position_embeddings.weight
2023-03-28 00:22:10,713 n: bert.embeddings.token_type_embeddings.weight
2023-03-28 00:22:10,713 n: bert.embeddings.LayerNorm.weight
2023-03-28 00:22:10,714 n: bert.embeddings.LayerNorm.bias
2023-03-28 00:22:10,714 n: bert.encoder.layer.0.attention.self.query.weight
2023-03-28 00:22:10,714 n: bert.encoder.layer.0.attention.self.query.bias
2023-03-28 00:22:10,714 n: bert.encoder.layer.0.attention.self.key.weight
2023-03-28 00:22:10,714 n: bert.encoder.layer.0.attention.self.key.bias
2023-03-28 00:22:10,714 n: bert.encoder.layer.0.attention.self.value.weight
2023-03-28 00:22:10,714 n: bert.encoder.layer.0.attention.self.value.bias
2023-03-28 00:22:10,714 n: bert.encoder.layer.0.attention.output.dense.weight
2023-03-28 00:22:10,715 n: bert.encoder.layer.0.attention.output.dense.bias
2023-03-28 00:22:10,715 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-03-28 00:22:10,715 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-03-28 00:22:10,715 n: bert.encoder.layer.0.intermediate.dense.weight
2023-03-28 00:22:10,715 n: bert.encoder.layer.0.intermediate.dense.bias
2023-03-28 00:22:10,715 n: bert.encoder.layer.0.output.dense.weight
2023-03-28 00:22:10,715 n: bert.encoder.layer.0.output.dense.bias
2023-03-28 00:22:10,715 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-03-28 00:22:10,716 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-03-28 00:22:10,716 n: bert.encoder.layer.1.attention.self.query.weight
2023-03-28 00:22:10,716 n: bert.encoder.layer.1.attention.self.query.bias
2023-03-28 00:22:10,716 n: bert.encoder.layer.1.attention.self.key.weight
2023-03-28 00:22:10,716 n: bert.encoder.layer.1.attention.self.key.bias
2023-03-28 00:22:10,716 n: bert.encoder.layer.1.attention.self.value.weight
2023-03-28 00:22:10,716 n: bert.encoder.layer.1.attention.self.value.bias
2023-03-28 00:22:10,716 n: bert.encoder.layer.1.attention.output.dense.weight
2023-03-28 00:22:10,717 n: bert.encoder.layer.1.attention.output.dense.bias
2023-03-28 00:22:10,717 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-03-28 00:22:10,717 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-03-28 00:22:10,717 n: bert.encoder.layer.1.intermediate.dense.weight
2023-03-28 00:22:10,717 n: bert.encoder.layer.1.intermediate.dense.bias
2023-03-28 00:22:10,717 n: bert.encoder.layer.1.output.dense.weight
2023-03-28 00:22:10,717 n: bert.encoder.layer.1.output.dense.bias
2023-03-28 00:22:10,717 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-03-28 00:22:10,717 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-03-28 00:22:10,717 n: bert.encoder.layer.2.attention.self.query.weight
2023-03-28 00:22:10,718 n: bert.encoder.layer.2.attention.self.query.bias
2023-03-28 00:22:10,718 n: bert.encoder.layer.2.attention.self.key.weight
2023-03-28 00:22:10,718 n: bert.encoder.layer.2.attention.self.key.bias
2023-03-28 00:22:10,718 n: bert.encoder.layer.2.attention.self.value.weight
2023-03-28 00:22:10,718 n: bert.encoder.layer.2.attention.self.value.bias
2023-03-28 00:22:10,718 n: bert.encoder.layer.2.attention.output.dense.weight
2023-03-28 00:22:10,718 n: bert.encoder.layer.2.attention.output.dense.bias
2023-03-28 00:22:10,718 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-03-28 00:22:10,718 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-03-28 00:22:10,718 n: bert.encoder.layer.2.intermediate.dense.weight
2023-03-28 00:22:10,719 n: bert.encoder.layer.2.intermediate.dense.bias
2023-03-28 00:22:10,719 n: bert.encoder.layer.2.output.dense.weight
2023-03-28 00:22:10,719 n: bert.encoder.layer.2.output.dense.bias
2023-03-28 00:22:10,719 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-03-28 00:22:10,719 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-03-28 00:22:10,719 n: bert.encoder.layer.3.attention.self.query.weight
2023-03-28 00:22:10,719 n: bert.encoder.layer.3.attention.self.query.bias
2023-03-28 00:22:10,719 n: bert.encoder.layer.3.attention.self.key.weight
2023-03-28 00:22:10,719 n: bert.encoder.layer.3.attention.self.key.bias
2023-03-28 00:22:10,719 n: bert.encoder.layer.3.attention.self.value.weight
2023-03-28 00:22:10,720 n: bert.encoder.layer.3.attention.self.value.bias
2023-03-28 00:22:10,720 n: bert.encoder.layer.3.attention.output.dense.weight
2023-03-28 00:22:10,720 n: bert.encoder.layer.3.attention.output.dense.bias
2023-03-28 00:22:10,720 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-03-28 00:22:10,720 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-03-28 00:22:10,720 n: bert.encoder.layer.3.intermediate.dense.weight
2023-03-28 00:22:10,720 n: bert.encoder.layer.3.intermediate.dense.bias
2023-03-28 00:22:10,720 n: bert.encoder.layer.3.output.dense.weight
2023-03-28 00:22:10,720 n: bert.encoder.layer.3.output.dense.bias
2023-03-28 00:22:10,720 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-03-28 00:22:10,721 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-03-28 00:22:10,721 n: bert.pooler.dense.weight
2023-03-28 00:22:10,721 n: bert.pooler.dense.bias
2023-03-28 00:22:10,721 n: classifier.weight
2023-03-28 00:22:10,721 n: classifier.bias
2023-03-28 00:22:10,721 n: fit_dense.weight
2023-03-28 00:22:10,721 n: fit_dense.bias
2023-03-28 00:22:10,721 Total parameters: 14591258
2023-03-28 00:22:23,768 ***** Running evaluation *****
2023-03-28 00:22:23,769   Epoch = 0 iter 49 step
2023-03-28 00:22:23,769   Num examples = 1043
2023-03-28 00:22:23,769   Batch size = 32
2023-03-28 00:22:24,381 ***** Eval results *****
2023-03-28 00:22:24,381   acc = 0.725790987535954
2023-03-28 00:22:24,381   att_loss = 0.0
2023-03-28 00:22:24,381   cls_loss = 0.3152639306321436
2023-03-28 00:22:24,382   eval_loss = 0.5738055543466047
2023-03-28 00:22:24,382   global_step = 49
2023-03-28 00:22:24,382   loss = 0.3152639306321436
2023-03-28 00:22:24,382   mcc = 0.3084429860027673
2023-03-28 00:22:24,382   rep_loss = 0.0
2023-03-28 00:22:24,389 ***** Save model *****
2023-03-28 00:22:38,400 ***** Running evaluation *****
2023-03-28 00:22:38,401   Epoch = 0 iter 99 step
2023-03-28 00:22:38,401   Num examples = 1043
2023-03-28 00:22:38,401   Batch size = 32
2023-03-28 00:22:39,006 ***** Eval results *****
2023-03-28 00:22:39,006   acc = 0.713326941514861
2023-03-28 00:22:39,006   att_loss = 0.0
2023-03-28 00:22:39,007   cls_loss = 0.30111954338622815
2023-03-28 00:22:39,007   eval_loss = 0.5789847310745355
2023-03-28 00:22:39,007   global_step = 99
2023-03-28 00:22:39,007   loss = 0.30111954338622815
2023-03-28 00:22:39,007   mcc = 0.27309350211119415
2023-03-28 00:22:39,007   rep_loss = 0.0
2023-03-28 00:22:52,366 ***** Running evaluation *****
2023-03-28 00:22:52,366   Epoch = 0 iter 149 step
2023-03-28 00:22:52,366   Num examples = 1043
2023-03-28 00:22:52,366   Batch size = 32
2023-03-28 00:22:52,973 ***** Eval results *****
2023-03-28 00:22:52,973   acc = 0.7085330776605945
2023-03-28 00:22:52,973   att_loss = 0.0
2023-03-28 00:22:52,973   cls_loss = 0.2951192441802697
2023-03-28 00:22:52,974   eval_loss = 0.5864884347626658
2023-03-28 00:22:52,974   global_step = 149
2023-03-28 00:22:52,974   loss = 0.2951192441802697
2023-03-28 00:22:52,974   mcc = 0.29297036702151047
2023-03-28 00:22:52,974   rep_loss = 0.0
2023-03-28 00:23:06,344 ***** Running evaluation *****
2023-03-28 00:23:06,344   Epoch = 0 iter 199 step
2023-03-28 00:23:06,344   Num examples = 1043
2023-03-28 00:23:06,344   Batch size = 32
2023-03-28 00:23:06,950 ***** Eval results *****
2023-03-28 00:23:06,950   acc = 0.716203259827421
2023-03-28 00:23:06,951   att_loss = 0.0
2023-03-28 00:23:06,951   cls_loss = 0.29252139302953406
2023-03-28 00:23:06,951   eval_loss = 0.5719913271340457
2023-03-28 00:23:06,951   global_step = 199
2023-03-28 00:23:06,951   loss = 0.29252139302953406
2023-03-28 00:23:06,951   mcc = 0.32158850135585637
2023-03-28 00:23:06,952   rep_loss = 0.0
2023-03-28 00:23:06,959 ***** Save model *****
2023-03-28 00:23:20,982 ***** Running evaluation *****
2023-03-28 00:23:20,983   Epoch = 0 iter 249 step
2023-03-28 00:23:20,983   Num examples = 1043
2023-03-28 00:23:20,983   Batch size = 32
2023-03-28 00:23:21,588 ***** Eval results *****
2023-03-28 00:23:21,588   acc = 0.7267497603068073
2023-03-28 00:23:21,588   att_loss = 0.0
2023-03-28 00:23:21,588   cls_loss = 0.2895129847239299
2023-03-28 00:23:21,588   eval_loss = 0.5594473381837209
2023-03-28 00:23:21,589   global_step = 249
2023-03-28 00:23:21,589   loss = 0.2895129847239299
2023-03-28 00:23:21,589   mcc = 0.29277529119302126
2023-03-28 00:23:21,589   rep_loss = 0.0
2023-03-28 00:23:34,953 ***** Running evaluation *****
2023-03-28 00:23:34,954   Epoch = 1 iter 299 step
2023-03-28 00:23:34,954   Num examples = 1043
2023-03-28 00:23:34,954   Batch size = 32
2023-03-28 00:23:35,560 ***** Eval results *****
2023-03-28 00:23:35,560   acc = 0.7267497603068073
2023-03-28 00:23:35,561   att_loss = 0.0
2023-03-28 00:23:35,561   cls_loss = 0.275757216848433
2023-03-28 00:23:35,561   eval_loss = 0.554865634802616
2023-03-28 00:23:35,561   global_step = 299
2023-03-28 00:23:35,561   loss = 0.275757216848433
2023-03-28 00:23:35,561   mcc = 0.28019392756014444
2023-03-28 00:23:35,562   rep_loss = 0.0
2023-03-28 00:23:48,925 ***** Running evaluation *****
2023-03-28 00:23:48,925   Epoch = 1 iter 349 step
2023-03-28 00:23:48,925   Num examples = 1043
2023-03-28 00:23:48,926   Batch size = 32
2023-03-28 00:23:49,531 ***** Eval results *****
2023-03-28 00:23:49,531   acc = 0.7209971236816874
2023-03-28 00:23:49,531   att_loss = 0.0
2023-03-28 00:23:49,532   cls_loss = 0.27462983694745274
2023-03-28 00:23:49,532   eval_loss = 0.5555411613348759
2023-03-28 00:23:49,532   global_step = 349
2023-03-28 00:23:49,532   loss = 0.27462983694745274
2023-03-28 00:23:49,532   mcc = 0.25326269928606765
2023-03-28 00:23:49,532   rep_loss = 0.0
2023-03-28 00:24:02,904 ***** Running evaluation *****
2023-03-28 00:24:02,905   Epoch = 1 iter 399 step
2023-03-28 00:24:02,905   Num examples = 1043
2023-03-28 00:24:02,905   Batch size = 32
2023-03-28 00:24:03,512 ***** Eval results *****
2023-03-28 00:24:03,513   acc = 0.7325023969319271
2023-03-28 00:24:03,513   att_loss = 0.0
2023-03-28 00:24:03,513   cls_loss = 0.27455509623343294
2023-03-28 00:24:03,513   eval_loss = 0.5527565054821245
2023-03-28 00:24:03,513   global_step = 399
2023-03-28 00:24:03,513   loss = 0.27455509623343294
2023-03-28 00:24:03,513   mcc = 0.3162468367931466
2023-03-28 00:24:03,514   rep_loss = 0.0
2023-03-28 00:24:16,881 ***** Running evaluation *****
2023-03-28 00:24:16,881   Epoch = 1 iter 449 step
2023-03-28 00:24:16,881   Num examples = 1043
2023-03-28 00:24:16,881   Batch size = 32
2023-03-28 00:24:17,489 ***** Eval results *****
2023-03-28 00:24:17,489   acc = 0.7344199424736337
2023-03-28 00:24:17,489   att_loss = 0.0
2023-03-28 00:24:17,490   cls_loss = 0.27395274539242737
2023-03-28 00:24:17,490   eval_loss = 0.5459111134211222
2023-03-28 00:24:17,490   global_step = 449
2023-03-28 00:24:17,490   loss = 0.27395274539242737
2023-03-28 00:24:17,490   mcc = 0.3137669232716793
2023-03-28 00:24:17,490   rep_loss = 0.0
2023-03-28 00:24:30,894 ***** Running evaluation *****
2023-03-28 00:24:30,894   Epoch = 1 iter 499 step
2023-03-28 00:24:30,894   Num examples = 1043
2023-03-28 00:24:30,894   Batch size = 32
2023-03-28 00:24:31,501 ***** Eval results *****
2023-03-28 00:24:31,501   acc = 0.7305848513902206
2023-03-28 00:24:31,501   att_loss = 0.0
2023-03-28 00:24:31,501   cls_loss = 0.2739337133564826
2023-03-28 00:24:31,502   eval_loss = 0.5476396508289106
2023-03-28 00:24:31,502   global_step = 499
2023-03-28 00:24:31,502   loss = 0.2739337133564826
2023-03-28 00:24:31,502   mcc = 0.29396160878340133
2023-03-28 00:24:31,502   rep_loss = 0.0
2023-03-28 00:24:44,925 ***** Running evaluation *****
2023-03-28 00:24:44,926   Epoch = 2 iter 549 step
2023-03-28 00:24:44,926   Num examples = 1043
2023-03-28 00:24:44,927   Batch size = 32
2023-03-28 00:24:45,537 ***** Eval results *****
2023-03-28 00:24:45,537   acc = 0.7171620325982742
2023-03-28 00:24:45,537   att_loss = 0.0
2023-03-28 00:24:45,538   cls_loss = 0.26767777403195697
2023-03-28 00:24:45,538   eval_loss = 0.5522422077077808
2023-03-28 00:24:45,538   global_step = 549
2023-03-28 00:24:45,538   loss = 0.26767777403195697
2023-03-28 00:24:45,538   mcc = 0.2903562313160724
2023-03-28 00:24:45,538   rep_loss = 0.0
2023-03-28 00:24:58,947 ***** Running evaluation *****
2023-03-28 00:24:58,947   Epoch = 2 iter 599 step
2023-03-28 00:24:58,948   Num examples = 1043
2023-03-28 00:24:58,948   Batch size = 32
2023-03-28 00:24:59,555 ***** Eval results *****
2023-03-28 00:24:59,556   acc = 0.7363374880153404
2023-03-28 00:24:59,556   att_loss = 0.0
2023-03-28 00:24:59,556   cls_loss = 0.27029584371126614
2023-03-28 00:24:59,556   eval_loss = 0.5473969542618954
2023-03-28 00:24:59,556   global_step = 599
2023-03-28 00:24:59,557   loss = 0.27029584371126614
2023-03-28 00:24:59,557   mcc = 0.31922342088693145
2023-03-28 00:24:59,557   rep_loss = 0.0
2023-03-28 00:25:12,973 ***** Running evaluation *****
2023-03-28 00:25:12,973   Epoch = 2 iter 649 step
2023-03-28 00:25:12,973   Num examples = 1043
2023-03-28 00:25:12,974   Batch size = 32
2023-03-28 00:25:13,581 ***** Eval results *****
2023-03-28 00:25:13,581   acc = 0.7353787152444871
2023-03-28 00:25:13,581   att_loss = 0.0
2023-03-28 00:25:13,581   cls_loss = 0.27055747897728627
2023-03-28 00:25:13,581   eval_loss = 0.546215663353602
2023-03-28 00:25:13,582   global_step = 649
2023-03-28 00:25:13,582   loss = 0.27055747897728627
2023-03-28 00:25:13,582   mcc = 0.3161331724093124
2023-03-28 00:25:13,582   rep_loss = 0.0
2023-03-28 00:25:27,001 ***** Running evaluation *****
2023-03-28 00:25:27,002   Epoch = 2 iter 699 step
2023-03-28 00:25:27,002   Num examples = 1043
2023-03-28 00:25:27,002   Batch size = 32
2023-03-28 00:25:27,610 ***** Eval results *****
2023-03-28 00:25:27,610   acc = 0.7392138063279002
2023-03-28 00:25:27,611   att_loss = 0.0
2023-03-28 00:25:27,611   cls_loss = 0.2714384933312734
2023-03-28 00:25:27,611   eval_loss = 0.5453668724406849
2023-03-28 00:25:27,611   global_step = 699
2023-03-28 00:25:27,611   loss = 0.2714384933312734
2023-03-28 00:25:27,611   mcc = 0.32706975069790944
2023-03-28 00:25:27,612   rep_loss = 0.0
2023-03-28 00:25:27,614 ***** Save model *****
2023-03-28 00:25:41,683 ***** Running evaluation *****
2023-03-28 00:25:41,684   Epoch = 2 iter 749 step
2023-03-28 00:25:41,684   Num examples = 1043
2023-03-28 00:25:41,684   Batch size = 32
2023-03-28 00:25:42,291 ***** Eval results *****
2023-03-28 00:25:42,292   acc = 0.738255033557047
2023-03-28 00:25:42,292   att_loss = 0.0
2023-03-28 00:25:42,292   cls_loss = 0.27106603515702626
2023-03-28 00:25:42,292   eval_loss = 0.5455463781501307
2023-03-28 00:25:42,293   global_step = 749
2023-03-28 00:25:42,293   loss = 0.27106603515702626
2023-03-28 00:25:42,293   mcc = 0.32467991850218353
2023-03-28 00:25:42,293   rep_loss = 0.0
2023-03-28 00:25:55,714 ***** Running evaluation *****
2023-03-28 00:25:55,715   Epoch = 2 iter 799 step
2023-03-28 00:25:55,715   Num examples = 1043
2023-03-28 00:25:55,715   Batch size = 32
2023-03-28 00:25:56,323 ***** Eval results *****
2023-03-28 00:25:56,323   acc = 0.7401725790987536
2023-03-28 00:25:56,324   att_loss = 0.0
2023-03-28 00:25:56,324   cls_loss = 0.27068503555261864
2023-03-28 00:25:56,324   eval_loss = 0.5451791792204885
2023-03-28 00:25:56,324   global_step = 799
2023-03-28 00:25:56,324   loss = 0.27068503555261864
2023-03-28 00:25:56,324   mcc = 0.325693501661788
2023-03-28 00:25:56,325   rep_loss = 0.0
2023-03-28 00:38:15,453 The args: Namespace(data_dir='/w/331/adeemj/csc2516_proj/data/CoLA', teacher_model='/w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/', student_model='/w/331/adeemj/csc2516_proj/models/CoLA/BASE_COMPARE/TempTinyBERT_CoLA_4L_312D', task_name='CoLA', output_dir_original='/w/331/adeemj/csc2516_proj/models/CoLA/BASE_COMPARE/TinyBERT_CoLA_4L_312D', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=True, data_url='', temperature=1.0, use_wandb=True, wandb_username='99adeem', wandb_runname='BASE_COMPARE', kl_attn_weight=None)
2023-03-28 00:38:16,892 Starting sweep agent: entity=None, project=None, count=None
2023-03-28 00:38:19,384 device: cuda n_gpu: 1
2023-03-28 00:38:30,515 device: cuda n_gpu: 1
2023-03-28 00:38:40,541 device: cuda n_gpu: 1
