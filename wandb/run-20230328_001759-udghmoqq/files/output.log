03/28 12:17:59 AM device: cuda n_gpu: 1
03/28 12:17:59 AM Writing example 0 of 8551
03/28 12:17:59 AM *** Example ***
03/28 12:17:59 AM guid: train-0
03/28 12:17:59 AM tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
03/28 12:17:59 AM input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/28 12:17:59 AM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/28 12:17:59 AM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/28 12:17:59 AM label: 1
03/28 12:17:59 AM label_id: 1
03/28 12:18:00 AM Writing example 0 of 1043
03/28 12:18:00 AM *** Example ***
03/28 12:18:00 AM guid: dev-0
03/28 12:18:00 AM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
03/28 12:18:00 AM input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/28 12:18:00 AM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/28 12:18:00 AM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/28 12:18:00 AM label: 1
03/28 12:18:00 AM label_id: 1
03/28 12:18:01 AM loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
03/28 12:18:01 AM Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}
03/28 12:18:02 AM Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
03/28 12:18:03 AM loading model...
03/28 12:18:03 AM done!
03/28 12:18:03 AM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
03/28 12:18:03 AM loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01
03/28 12:18:03 AM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}
03/28 12:18:03 AM Loading model /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_SWEEP_BATCHMEAN/TempTinyBERT_CoLA_4L_312D_kl_weight0.01/pytorch_model.bin
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]
Iteration:   2%|1         | 5/268 [00:01<01:10,  3.73it/s]
03/28 12:18:03 AM loading model...
03/28 12:18:03 AM done!
03/28 12:18:03 AM ***** Running training *****
03/28 12:18:03 AM   Num examples = 8551
03/28 12:18:03 AM   Batch size = 32
03/28 12:18:03 AM   Num steps = 801
03/28 12:18:03 AM n: bert.embeddings.word_embeddings.weight
03/28 12:18:03 AM n: bert.embeddings.position_embeddings.weight
03/28 12:18:03 AM n: bert.embeddings.token_type_embeddings.weight
03/28 12:18:03 AM n: bert.embeddings.LayerNorm.weight
03/28 12:18:03 AM n: bert.embeddings.LayerNorm.bias
03/28 12:18:03 AM n: bert.encoder.layer.0.attention.self.query.weight
03/28 12:18:03 AM n: bert.encoder.layer.0.attention.self.query.bias
03/28 12:18:03 AM n: bert.encoder.layer.0.attention.self.key.weight
03/28 12:18:03 AM n: bert.encoder.layer.0.attention.self.key.bias
03/28 12:18:03 AM n: bert.encoder.layer.0.attention.self.value.weight
03/28 12:18:03 AM n: bert.encoder.layer.0.attention.self.value.bias
03/28 12:18:03 AM n: bert.encoder.layer.0.attention.output.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.0.attention.output.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
03/28 12:18:03 AM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
03/28 12:18:03 AM n: bert.encoder.layer.0.intermediate.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.0.intermediate.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.0.output.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.0.output.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.0.output.LayerNorm.weight
03/28 12:18:03 AM n: bert.encoder.layer.0.output.LayerNorm.bias
03/28 12:18:03 AM n: bert.encoder.layer.1.attention.self.query.weight
03/28 12:18:03 AM n: bert.encoder.layer.1.attention.self.query.bias
03/28 12:18:03 AM n: bert.encoder.layer.1.attention.self.key.weight
03/28 12:18:03 AM n: bert.encoder.layer.1.attention.self.key.bias
03/28 12:18:03 AM n: bert.encoder.layer.1.attention.self.value.weight
03/28 12:18:03 AM n: bert.encoder.layer.1.attention.self.value.bias
03/28 12:18:03 AM n: bert.encoder.layer.1.attention.output.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.1.attention.output.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
03/28 12:18:03 AM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
03/28 12:18:03 AM n: bert.encoder.layer.1.intermediate.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.1.intermediate.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.1.output.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.1.output.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.1.output.LayerNorm.weight
03/28 12:18:03 AM n: bert.encoder.layer.1.output.LayerNorm.bias
03/28 12:18:03 AM n: bert.encoder.layer.2.attention.self.query.weight
03/28 12:18:03 AM n: bert.encoder.layer.2.attention.self.query.bias
03/28 12:18:03 AM n: bert.encoder.layer.2.attention.self.key.weight
03/28 12:18:03 AM n: bert.encoder.layer.2.attention.self.key.bias
03/28 12:18:03 AM n: bert.encoder.layer.2.attention.self.value.weight
03/28 12:18:03 AM n: bert.encoder.layer.2.attention.self.value.bias
03/28 12:18:03 AM n: bert.encoder.layer.2.attention.output.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.2.attention.output.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
03/28 12:18:03 AM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
03/28 12:18:03 AM n: bert.encoder.layer.2.intermediate.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.2.intermediate.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.2.output.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.2.output.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.2.output.LayerNorm.weight
03/28 12:18:03 AM n: bert.encoder.layer.2.output.LayerNorm.bias
03/28 12:18:03 AM n: bert.encoder.layer.3.attention.self.query.weight
03/28 12:18:03 AM n: bert.encoder.layer.3.attention.self.query.bias
03/28 12:18:03 AM n: bert.encoder.layer.3.attention.self.key.weight
03/28 12:18:03 AM n: bert.encoder.layer.3.attention.self.key.bias
03/28 12:18:03 AM n: bert.encoder.layer.3.attention.self.value.weight
03/28 12:18:03 AM n: bert.encoder.layer.3.attention.self.value.bias
03/28 12:18:03 AM n: bert.encoder.layer.3.attention.output.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.3.attention.output.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
03/28 12:18:03 AM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
03/28 12:18:03 AM n: bert.encoder.layer.3.intermediate.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.3.intermediate.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.3.output.dense.weight
03/28 12:18:03 AM n: bert.encoder.layer.3.output.dense.bias
03/28 12:18:03 AM n: bert.encoder.layer.3.output.LayerNorm.weight
03/28 12:18:03 AM n: bert.encoder.layer.3.output.LayerNorm.bias
03/28 12:18:03 AM n: bert.pooler.dense.weight
03/28 12:18:03 AM n: bert.pooler.dense.bias
03/28 12:18:03 AM n: classifier.weight
03/28 12:18:03 AM n: classifier.bias
03/28 12:18:03 AM n: fit_dense.weight
03/28 12:18:03 AM n: fit_dense.bias





Iteration:  18%|#7        | 48/268 [00:12<00:58,  3.76it/s]
Evaluating:  73%|███████▎  | 24/33 [00:00<00:00, 54.06it/s]
03/28 12:18:16 AM ***** Running evaluation *****
03/28 12:18:16 AM   Epoch = 0 iter 49 step
03/28 12:18:16 AM   Num examples = 1043
03/28 12:18:16 AM   Batch size = 32
03/28 12:18:17 AM ***** Eval results *****
03/28 12:18:17 AM   acc = 0.7315436241610739
03/28 12:18:17 AM   att_loss = 0.0
03/28 12:18:17 AM   cls_loss = 0.32147838448991584
03/28 12:18:17 AM   eval_loss = 0.5749412106745171
03/28 12:18:17 AM   global_step = 49
03/28 12:18:17 AM   loss = 0.32147838448991584
03/28 12:18:17 AM   mcc = 0.2920517715695529
03/28 12:18:17 AM   rep_loss = 0.0







Iteration:  36%|###6      | 97/268 [00:27<00:45,  3.75it/s]
03/28 12:18:31 AM ***** Running evaluation *****
03/28 12:18:31 AM   Epoch = 0 iter 99 step
03/28 12:18:31 AM   Num examples = 1043
Iteration:  37%|###6      | 98/268 [00:27<00:45,  3.75it/s]
Iteration:  38%|###8      | 102/268 [00:29<00:54,  3.03it/s]
03/28 12:18:32 AM ***** Eval results *****
03/28 12:18:32 AM   acc = 0.6970278044103547
03/28 12:18:32 AM   att_loss = 0.0
03/28 12:18:32 AM   cls_loss = 0.30451243634175773
03/28 12:18:32 AM   eval_loss = 0.600251628593965
03/28 12:18:32 AM   global_step = 99
03/28 12:18:32 AM   loss = 0.30451243634175773
03/28 12:18:32 AM   mcc = 0.282885835108455






Iteration:  55%|#####5    | 148/268 [00:41<00:32,  3.74it/s]
03/28 12:18:45 AM ***** Running evaluation *****
03/28 12:18:45 AM   Epoch = 0 iter 149 step
03/28 12:18:45 AM   Num examples = 1043
03/28 12:18:45 AM   Batch size = 32
Iteration:  57%|#####7    | 153/268 [00:43<00:35,  3.22it/s]
03/28 12:18:46 AM ***** Eval results *****
03/28 12:18:46 AM   acc = 0.7181208053691275
03/28 12:18:46 AM   att_loss = 0.0
03/28 12:18:46 AM   cls_loss = 0.29717853685353424
03/28 12:18:46 AM   eval_loss = 0.5778075601115371
03/28 12:18:46 AM   global_step = 149
03/28 12:18:46 AM   loss = 0.29717853685353424
03/28 12:18:46 AM   mcc = 0.2764464416592447






Iteration:  74%|#######3  | 198/268 [00:55<00:18,  3.75it/s]
03/28 12:18:59 AM ***** Running evaluation *****
03/28 12:18:59 AM   Epoch = 0 iter 199 step
03/28 12:18:59 AM   Num examples = 1043
03/28 12:18:59 AM   Batch size = 32
Iteration:  75%|#######4  | 200/268 [00:57<00:36,  1.86it/s]
03/28 12:19:00 AM ***** Eval results *****
03/28 12:19:00 AM   acc = 0.7085330776605945
03/28 12:19:00 AM   att_loss = 0.0
03/28 12:19:00 AM   cls_loss = 0.29376217243659436
03/28 12:19:00 AM   eval_loss = 0.5722114741802216
03/28 12:19:00 AM   global_step = 199
03/28 12:19:00 AM   loss = 0.29376217243659436
03/28 12:19:00 AM   mcc = 0.29410053711898326
03/28 12:19:00 AM   rep_loss = 0.0






Iteration:  93%|#########2| 248/268 [01:10<00:05,  3.75it/s]
Iteration:  94%|#########3| 251/268 [01:11<00:06,  2.80it/s]
03/28 12:19:14 AM ***** Running evaluation *****
03/28 12:19:14 AM   Epoch = 0 iter 249 step
03/28 12:19:14 AM   Num examples = 1043
03/28 12:19:14 AM   Batch size = 32
03/28 12:19:14 AM ***** Eval results *****
03/28 12:19:14 AM   acc = 0.7286673058485139
03/28 12:19:14 AM   att_loss = 0.0
03/28 12:19:14 AM   cls_loss = 0.2902013989217789
03/28 12:19:14 AM   eval_loss = 0.5595149632656213
03/28 12:19:14 AM   global_step = 249
03/28 12:19:14 AM   loss = 0.2902013989217789
03/28 12:19:14 AM   mcc = 0.2911416522578436


Epoch:  33%|███▎      | 1/3 [01:15<02:31, 75.85s/it].74it/s]



Iteration:  12%|#1        | 31/268 [00:08<01:03,  3.74it/s]
Iteration:  13%|#2        | 34/268 [00:09<01:23,  2.79it/s]
03/28 12:19:28 AM ***** Running evaluation *****
03/28 12:19:28 AM   Epoch = 1 iter 299 step
03/28 12:19:28 AM   Num examples = 1043
03/28 12:19:28 AM   Batch size = 32
03/28 12:19:28 AM ***** Eval results *****
03/28 12:19:28 AM   acc = 0.7219558964525408
03/28 12:19:28 AM   att_loss = 0.0
03/28 12:19:28 AM   cls_loss = 0.2755206823348999
03/28 12:19:28 AM   eval_loss = 0.5602690478165945
03/28 12:19:28 AM   global_step = 299
03/28 12:19:28 AM   loss = 0.2755206823348999
03/28 12:19:28 AM   mcc = 0.25820812984395297






Iteration:  30%|###       | 81/268 [00:22<00:49,  3.75it/s]
Iteration:  31%|###1      | 84/268 [00:23<01:05,  2.80it/s]
03/28 12:19:42 AM ***** Running evaluation *****
03/28 12:19:42 AM   Epoch = 1 iter 349 step
03/28 12:19:42 AM   Num examples = 1043
03/28 12:19:42 AM   Batch size = 32
03/28 12:19:42 AM ***** Eval results *****
03/28 12:19:42 AM   acc = 0.725790987535954
03/28 12:19:42 AM   att_loss = 0.0
03/28 12:19:42 AM   cls_loss = 0.27432840008561205
03/28 12:19:42 AM   eval_loss = 0.5527312385313439
03/28 12:19:42 AM   global_step = 349
03/28 12:19:42 AM   loss = 0.27432840008561205
03/28 12:19:42 AM   mcc = 0.28146722461472






Iteration:  49%|####8     | 131/268 [00:36<00:36,  3.73it/s]
Evaluating:  91%|█████████ | 30/33 [00:00<00:00, 54.10it/s]
03/28 12:19:56 AM ***** Running evaluation *****
03/28 12:19:56 AM   Epoch = 1 iter 399 step
03/28 12:19:56 AM   Num examples = 1043
03/28 12:19:56 AM   Batch size = 32
03/28 12:19:56 AM ***** Eval results *****
03/28 12:19:56 AM   acc = 0.7219558964525408
03/28 12:19:56 AM   att_loss = 0.0
03/28 12:19:56 AM   cls_loss = 0.2739170069495837
03/28 12:19:56 AM   eval_loss = 0.5542624394098917
03/28 12:19:56 AM   global_step = 399
03/28 12:19:56 AM   loss = 0.2739170069495837
03/28 12:19:56 AM   mcc = 0.29739732160697946
03/28 12:19:56 AM   rep_loss = 0.0






Iteration:  68%|######7   | 181/268 [00:51<00:23,  3.72it/s]
Evaluating:  18%|█▊        | 6/33 [00:00<00:00, 54.38it/s]
03/28 12:20:11 AM ***** Running evaluation *****
03/28 12:20:11 AM   Epoch = 1 iter 449 step
03/28 12:20:11 AM   Num examples = 1043

Iteration:  69%|######9   | 185/268 [00:53<00:33,  2.49it/s]
03/28 12:20:11 AM ***** Eval results *****
03/28 12:20:11 AM   acc = 0.7325023969319271
03/28 12:20:11 AM   att_loss = 0.0
03/28 12:20:11 AM   cls_loss = 0.2733109984751586
03/28 12:20:11 AM   eval_loss = 0.5481378941824941
03/28 12:20:11 AM   global_step = 449
03/28 12:20:11 AM   loss = 0.2733109984751586
03/28 12:20:11 AM   mcc = 0.3090750240568001
03/28 12:20:11 AM   rep_loss = 0.0






Iteration:  86%|########6 | 231/268 [01:06<00:09,  3.75it/s]
Iteration:  87%|########6 | 232/268 [01:07<00:23,  1.52it/s]
03/28 12:20:26 AM ***** Running evaluation *****
03/28 12:20:26 AM   Epoch = 1 iter 499 step
03/28 12:20:26 AM   Num examples = 1043
03/28 12:20:26 AM   Batch size = 32
03/28 12:20:26 AM ***** Eval results *****
03/28 12:20:26 AM   acc = 0.738255033557047
03/28 12:20:26 AM   att_loss = 0.0
03/28 12:20:26 AM   cls_loss = 0.27311007884042016
03/28 12:20:26 AM   eval_loss = 0.5470819247491432
03/28 12:20:26 AM   global_step = 499
03/28 12:20:26 AM   loss = 0.27311007884042016
03/28 12:20:26 AM   mcc = 0.3200611511804038
03/28 12:20:26 AM   rep_loss = 0.0




Epoch:  67%|██████▋   | 2/3 [02:32<01:16, 76.54s/it].75it/s]

Iteration:   5%|5         | 14/268 [00:03<01:07,  3.74it/s]
Iteration:   6%|5         | 16/268 [00:04<01:40,  2.51it/s]
03/28 12:20:40 AM ***** Running evaluation *****
03/28 12:20:40 AM   Epoch = 2 iter 549 step
03/28 12:20:40 AM   Num examples = 1043
03/28 12:20:40 AM   Batch size = 32
03/28 12:20:41 AM ***** Eval results *****
03/28 12:20:41 AM   acc = 0.7248322147651006
03/28 12:20:41 AM   att_loss = 0.0
03/28 12:20:41 AM   cls_loss = 0.2675975014766057
03/28 12:20:41 AM   eval_loss = 0.5533715098193197
03/28 12:20:41 AM   global_step = 549
03/28 12:20:41 AM   loss = 0.2675975014766057
03/28 12:20:41 AM   mcc = 0.30639861601277335






Iteration:  24%|##3       | 64/268 [00:17<00:54,  3.72it/s]
Iteration:  25%|##4       | 66/268 [00:18<01:20,  2.51it/s]
03/28 12:20:54 AM ***** Running evaluation *****
03/28 12:20:54 AM   Epoch = 2 iter 599 step
03/28 12:20:54 AM   Num examples = 1043
03/28 12:20:54 AM   Batch size = 32
03/28 12:20:55 AM ***** Eval results *****
03/28 12:20:55 AM   acc = 0.7315436241610739
03/28 12:20:55 AM   att_loss = 0.0
03/28 12:20:55 AM   cls_loss = 0.2702183349774434
03/28 12:20:55 AM   eval_loss = 0.5498513705802687
03/28 12:20:55 AM   global_step = 599
03/28 12:20:55 AM   loss = 0.2702183349774434
03/28 12:20:55 AM   mcc = 0.3067490054788977






Iteration:  43%|####2     | 114/268 [00:31<00:41,  3.73it/s]
Iteration:  43%|####3     | 116/268 [00:32<01:00,  2.52it/s]
03/28 12:21:08 AM ***** Running evaluation *****
03/28 12:21:08 AM   Epoch = 2 iter 649 step
03/28 12:21:08 AM   Num examples = 1043
03/28 12:21:08 AM   Batch size = 32
03/28 12:21:09 AM ***** Eval results *****
03/28 12:21:09 AM   acc = 0.7353787152444871
03/28 12:21:09 AM   att_loss = 0.0
03/28 12:21:09 AM   cls_loss = 0.2702715846507446
03/28 12:21:09 AM   eval_loss = 0.5479927171360363
03/28 12:21:09 AM   global_step = 649
03/28 12:21:09 AM   loss = 0.2702715846507446
03/28 12:21:09 AM   mcc = 0.3147047823493449






Iteration:  61%|######1   | 164/268 [00:45<00:27,  3.73it/s]
Iteration:  62%|######1   | 166/268 [00:46<00:40,  2.51it/s]
03/28 12:21:22 AM ***** Running evaluation *****
03/28 12:21:22 AM   Epoch = 2 iter 699 step
03/28 12:21:22 AM   Num examples = 1043
03/28 12:21:22 AM   Batch size = 32
03/28 12:21:23 AM ***** Eval results *****
03/28 12:21:23 AM   acc = 0.7315436241610739
03/28 12:21:23 AM   att_loss = 0.0
03/28 12:21:23 AM   cls_loss = 0.271251658688892
03/28 12:21:23 AM   eval_loss = 0.5483483130281622
03/28 12:21:23 AM   global_step = 699
03/28 12:21:23 AM   loss = 0.271251658688892
03/28 12:21:23 AM   mcc = 0.30993123241054954






Iteration:  80%|#######9  | 214/268 [00:59<00:14,  3.73it/s]
Evaluating:  91%|█████████ | 30/33 [00:00<00:00, 53.70it/s]
03/28 12:21:36 AM ***** Running evaluation *****
03/28 12:21:36 AM   Epoch = 2 iter 749 step
03/28 12:21:36 AM   Num examples = 1043
03/28 12:21:36 AM   Batch size = 32
03/28 12:21:37 AM ***** Eval results *****
03/28 12:21:37 AM   acc = 0.7372962607861937
03/28 12:21:37 AM   att_loss = 0.0
03/28 12:21:37 AM   cls_loss = 0.27094306010146474
03/28 12:21:37 AM   eval_loss = 0.5483153633999102
03/28 12:21:37 AM   global_step = 749
03/28 12:21:37 AM   loss = 0.27094306010146474
03/28 12:21:37 AM   mcc = 0.322303902111525
03/28 12:21:37 AM   rep_loss = 0.0






Iteration:  99%|#########8| 264/268 [01:14<00:01,  3.74it/s]
Evaluating:  18%|█▊        | 6/33 [00:00<00:00, 54.33it/s]
03/28 12:21:51 AM ***** Running evaluation *****
03/28 12:21:51 AM   Epoch = 2 iter 799 step
03/28 12:21:51 AM   Num examples = 1043

Epoch: 100%|██████████| 3/3 [03:49<00:00, 76.53s/it].14it/s]
03/28 12:21:52 AM ***** Eval results *****
03/28 12:21:52 AM   acc = 0.7392138063279002
03/28 12:21:52 AM   att_loss = 0.0
03/28 12:21:52 AM   cls_loss = 0.27052306617205996
03/28 12:21:52 AM   eval_loss = 0.5482751835476268
03/28 12:21:52 AM   global_step = 799
03/28 12:21:52 AM   loss = 0.27052306617205996
03/28 12:21:52 AM   mcc = 0.3257379617775925
03/28 12:21:52 AM   rep_loss = 0.0
03/28 12:21:52 AM ***** Save model *****