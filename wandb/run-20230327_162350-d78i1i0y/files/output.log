03/27 04:23:51 PM device: cuda n_gpu: 1
03/27 04:23:52 PM Writing example 0 of 8551
03/27 04:23:52 PM *** Example ***
03/27 04:23:52 PM guid: train-0
03/27 04:23:52 PM tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
03/27 04:23:52 PM input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 04:23:52 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 04:23:52 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 04:23:52 PM label: 1
03/27 04:23:52 PM label_id: 1
03/27 04:23:53 PM Writing example 0 of 1043
03/27 04:23:53 PM *** Example ***
03/27 04:23:53 PM guid: dev-0
03/27 04:23:53 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
03/27 04:23:53 PM input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 04:23:53 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 04:23:53 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 04:23:53 PM label: 1
03/27 04:23:53 PM label_id: 1
03/27 04:23:53 PM loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
03/27 04:23:53 PM Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}
03/27 04:23:55 PM Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
03/27 04:23:59 PM loading model...
03/27 04:23:59 PM done!
03/27 04:23:59 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
03/27 04:24:05 PM loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_BASE/TempTinyBERT_CoLA_4L_312D_5e-05_32
03/27 04:24:05 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}
03/27 04:24:05 PM Loading model /w/331/adeemj/csc2516_proj/models/CoLA/KL_ATTN_BASE/TempTinyBERT_CoLA_4L_312D_5e-05_32/pytorch_model.bin
03/27 04:24:05 PM loading model...
03/27 04:24:06 PM done!
03/27 04:24:06 PM ***** Running training *****
03/27 04:24:06 PM   Num examples = 8551
03/27 04:24:06 PM   Batch size = 32
03/27 04:24:06 PM   Num steps = 801
03/27 04:24:06 PM n: bert.embeddings.word_embeddings.weight
03/27 04:24:06 PM n: bert.embeddings.position_embeddings.weight
03/27 04:24:06 PM n: bert.embeddings.token_type_embeddings.weight
03/27 04:24:06 PM n: bert.embeddings.LayerNorm.weight
03/27 04:24:06 PM n: bert.embeddings.LayerNorm.bias
03/27 04:24:06 PM n: bert.encoder.layer.0.attention.self.query.weight
03/27 04:24:06 PM n: bert.encoder.layer.0.attention.self.query.bias
03/27 04:24:06 PM n: bert.encoder.layer.0.attention.self.key.weight
03/27 04:24:06 PM n: bert.encoder.layer.0.attention.self.key.bias
03/27 04:24:06 PM n: bert.encoder.layer.0.attention.self.value.weight
03/27 04:24:06 PM n: bert.encoder.layer.0.attention.self.value.bias
03/27 04:24:06 PM n: bert.encoder.layer.0.attention.output.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.0.attention.output.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
03/27 04:24:06 PM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
03/27 04:24:06 PM n: bert.encoder.layer.0.intermediate.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.0.intermediate.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.0.output.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.0.output.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.0.output.LayerNorm.weight
03/27 04:24:06 PM n: bert.encoder.layer.0.output.LayerNorm.bias
03/27 04:24:06 PM n: bert.encoder.layer.1.attention.self.query.weight
03/27 04:24:06 PM n: bert.encoder.layer.1.attention.self.query.bias
03/27 04:24:06 PM n: bert.encoder.layer.1.attention.self.key.weight
03/27 04:24:06 PM n: bert.encoder.layer.1.attention.self.key.bias
03/27 04:24:06 PM n: bert.encoder.layer.1.attention.self.value.weight
03/27 04:24:06 PM n: bert.encoder.layer.1.attention.self.value.bias
03/27 04:24:06 PM n: bert.encoder.layer.1.attention.output.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.1.attention.output.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
03/27 04:24:06 PM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
03/27 04:24:06 PM n: bert.encoder.layer.1.intermediate.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.1.intermediate.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.1.output.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.1.output.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.1.output.LayerNorm.weight
03/27 04:24:06 PM n: bert.encoder.layer.1.output.LayerNorm.bias
03/27 04:24:06 PM n: bert.encoder.layer.2.attention.self.query.weight
03/27 04:24:06 PM n: bert.encoder.layer.2.attention.self.query.bias
03/27 04:24:06 PM n: bert.encoder.layer.2.attention.self.key.weight
03/27 04:24:06 PM n: bert.encoder.layer.2.attention.self.key.bias
03/27 04:24:06 PM n: bert.encoder.layer.2.attention.self.value.weight
03/27 04:24:06 PM n: bert.encoder.layer.2.attention.self.value.bias
03/27 04:24:06 PM n: bert.encoder.layer.2.attention.output.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.2.attention.output.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
03/27 04:24:06 PM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
03/27 04:24:06 PM n: bert.encoder.layer.2.intermediate.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.2.intermediate.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.2.output.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.2.output.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.2.output.LayerNorm.weight
03/27 04:24:06 PM n: bert.encoder.layer.2.output.LayerNorm.bias
03/27 04:24:06 PM n: bert.encoder.layer.3.attention.self.query.weight
03/27 04:24:06 PM n: bert.encoder.layer.3.attention.self.query.bias
03/27 04:24:06 PM n: bert.encoder.layer.3.attention.self.key.weight
03/27 04:24:06 PM n: bert.encoder.layer.3.attention.self.key.bias
03/27 04:24:06 PM n: bert.encoder.layer.3.attention.self.value.weight
03/27 04:24:06 PM n: bert.encoder.layer.3.attention.self.value.bias
03/27 04:24:06 PM n: bert.encoder.layer.3.attention.output.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.3.attention.output.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
03/27 04:24:06 PM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
03/27 04:24:06 PM n: bert.encoder.layer.3.intermediate.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.3.intermediate.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.3.output.dense.weight
03/27 04:24:06 PM n: bert.encoder.layer.3.output.dense.bias
03/27 04:24:06 PM n: bert.encoder.layer.3.output.LayerNorm.weight
03/27 04:24:06 PM n: bert.encoder.layer.3.output.LayerNorm.bias
03/27 04:24:06 PM n: bert.pooler.dense.weight
03/27 04:24:06 PM n: bert.pooler.dense.bias
03/27 04:24:06 PM n: classifier.weight
03/27 04:24:06 PM n: classifier.bias
03/27 04:24:06 PM n: fit_dense.weight
03/27 04:24:06 PM n: fit_dense.bias
03/27 04:24:06 PM Total parameters: 14591258
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]
	add_(Number alpha, Tensor other) [00:00<?, ?it/s]
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  next_m.mul_(beta1).add_(1 - beta1, grad)





Iteration:  16%|#5        | 42/268 [00:15<00:59,  3.81it/s]
03/27 04:24:23 PM ***** Running evaluation *****
03/27 04:24:23 PM   Epoch = 0 iter 49 step
03/27 04:24:23 PM   Num examples = 1043
Iteration:  18%|#7        | 48/268 [00:16<00:57,  3.86it/s]
Evaluating:  55%|█████▍    | 18/33 [00:00<00:00, 55.04it/s]
03/27 04:24:23 PM ***** Eval results *****
03/27 04:24:23 PM   acc = 0.7066155321188878
03/27 04:24:23 PM   att_loss = 0.0
03/27 04:24:23 PM   cls_loss = 0.3103802331856319
03/27 04:24:23 PM   eval_loss = 0.5863235791524252
03/27 04:24:23 PM   global_step = 49
03/27 04:24:23 PM   loss = 0.3103802331856319
03/27 04:24:23 PM   mcc = 0.22552275265291963
03/27 04:24:23 PM   rep_loss = 0.0






Iteration:  37%|###6      | 98/268 [00:31<00:44,  3.80it/s]
Evaluating:   0%|          | 0/33 [00:00<?, ?it/s]
03/27 04:24:37 PM ***** Running evaluation *****
03/27 04:24:37 PM   Epoch = 0 iter 99 step
03/27 04:24:37 PM   Num examples = 1043
03/27 04:24:37 PM   Batch size = 32
03/27 04:24:38 PM ***** Eval results *****
03/27 04:24:38 PM   acc = 0.7114093959731543
03/27 04:24:38 PM   att_loss = 0.0
03/27 04:24:38 PM   cls_loss = 0.2980859806441297
03/27 04:24:38 PM   eval_loss = 0.5904002794713685
03/27 04:24:38 PM   global_step = 99
03/27 04:24:38 PM   loss = 0.2980859806441297
03/27 04:24:38 PM   mcc = 0.23920647786365987
03/27 04:24:38 PM   rep_loss = 0.0







Iteration:  55%|#####5    | 148/268 [00:45<00:31,  3.83it/s]
03/27 04:24:51 PM ***** Running evaluation *****
03/27 04:24:51 PM   Epoch = 0 iter 149 step
03/27 04:24:51 PM   Num examples = 1043
03/27 04:24:51 PM   Batch size = 32
03/27 04:24:52 PM ***** Eval results *****
03/27 04:24:52 PM   acc = 0.6788111217641419
03/27 04:24:52 PM   att_loss = 0.0
03/27 04:24:52 PM   cls_loss = 0.2918725621780293
03/27 04:24:52 PM   eval_loss = 0.601570596297582
03/27 04:24:52 PM   global_step = 149
03/27 04:24:52 PM   loss = 0.2918725621780293
03/27 04:24:52 PM   mcc = 0.21325587021553824
03/27 04:24:52 PM   rep_loss = 0.0





Iteration:  71%|#######1  | 191/268 [00:57<00:20,  3.82it/s]
03/27 04:25:05 PM ***** Running evaluation *****
03/27 04:25:05 PM   Epoch = 0 iter 199 step
03/27 04:25:05 PM   Num examples = 1043
Iteration:  74%|#######3  | 198/268 [00:59<00:18,  3.82it/s]
Evaluating:  18%|█▊        | 6/33 [00:00<00:00, 54.68it/s]
03/27 04:25:06 PM ***** Eval results *****
03/27 04:25:06 PM   acc = 0.6931927133269415
03/27 04:25:06 PM   att_loss = 0.0
03/27 04:25:06 PM   cls_loss = 0.29033733821993496
03/27 04:25:06 PM   eval_loss = 0.5990417825453209
03/27 04:25:06 PM   global_step = 199
03/27 04:25:06 PM   loss = 0.29033733821993496
03/27 04:25:06 PM   mcc = 0.2530776397478349
03/27 04:25:06 PM   rep_loss = 0.0







Iteration:  93%|#########2| 248/268 [01:13<00:05,  3.80it/s]
03/27 04:25:19 PM ***** Running evaluation *****
03/27 04:25:19 PM   Epoch = 0 iter 249 step
03/27 04:25:19 PM   Num examples = 1043
03/27 04:25:19 PM   Batch size = 32
03/27 04:25:20 PM ***** Eval results *****
03/27 04:25:20 PM   acc = 0.7056567593480345
03/27 04:25:20 PM   att_loss = 0.0
03/27 04:25:20 PM   cls_loss = 0.2881632454424019
03/27 04:25:20 PM   eval_loss = 0.595067585959579
03/27 04:25:20 PM   global_step = 249
03/27 04:25:20 PM   loss = 0.2881632454424019
03/27 04:25:20 PM   mcc = 0.24505616733536942
03/27 04:25:20 PM   rep_loss = 0.0

Epoch:  33%|███▎      | 1/3 [01:19<02:38, 79.23s/it].80it/s]



Iteration:  12%|#1        | 31/268 [00:08<01:02,  3.79it/s]
Evaluating:   0%|          | 0/33 [00:00<?, ?it/s]
03/27 04:25:33 PM ***** Running evaluation *****
03/27 04:25:33 PM   Epoch = 1 iter 299 step
03/27 04:25:33 PM   Num examples = 1043
03/27 04:25:33 PM   Batch size = 32
03/27 04:25:34 PM ***** Eval results *****
03/27 04:25:34 PM   acc = 0.6864813039309684
03/27 04:25:34 PM   att_loss = 0.0
03/27 04:25:34 PM   cls_loss = 0.2769400039687753
03/27 04:25:34 PM   eval_loss = 0.60657240134297
03/27 04:25:34 PM   global_step = 299
03/27 04:25:34 PM   loss = 0.2769400039687753
03/27 04:25:34 PM   mcc = 0.23473441073354398






Iteration:  28%|##7       | 75/268 [00:20<00:50,  3.79it/s]
03/27 04:25:47 PM ***** Running evaluation *****
03/27 04:25:47 PM   Epoch = 1 iter 349 step
03/27 04:25:47 PM   Num examples = 1043
Iteration:  30%|###       | 81/268 [00:21<00:49,  3.78it/s]
Evaluating:  36%|███▋      | 12/33 [00:00<00:00, 54.65it/s]
03/27 04:25:48 PM ***** Eval results *****
03/27 04:25:48 PM   acc = 0.6931927133269415
03/27 04:25:48 PM   att_loss = 0.0
03/27 04:25:48 PM   cls_loss = 0.2753661449362592
03/27 04:25:48 PM   eval_loss = 0.5987993063348712
03/27 04:25:48 PM   global_step = 349
03/27 04:25:48 PM   loss = 0.2753661449362592
03/27 04:25:48 PM   mcc = 0.22280382305801125






Iteration:  47%|####7     | 126/268 [00:34<00:37,  3.78it/s]
03/27 04:26:01 PM ***** Running evaluation *****
03/27 04:26:01 PM   Epoch = 1 iter 399 step
03/27 04:26:01 PM   Num examples = 1043
Iteration:  49%|####8     | 131/268 [00:35<00:36,  3.78it/s]
Evaluating:  55%|█████▍    | 18/33 [00:00<00:00, 54.63it/s]
03/27 04:26:01 PM ***** Eval results *****
03/27 04:26:01 PM   acc = 0.716203259827421
03/27 04:26:01 PM   att_loss = 0.0
03/27 04:26:01 PM   cls_loss = 0.27552773103569494
03/27 04:26:01 PM   eval_loss = 0.5817531934290221
03/27 04:26:01 PM   global_step = 399
03/27 04:26:01 PM   loss = 0.27552773103569494
03/27 04:26:01 PM   mcc = 0.2711945584758675
03/27 04:26:01 PM   rep_loss = 0.0







Iteration:  68%|######7   | 181/268 [00:50<00:23,  3.78it/s]
03/27 04:26:15 PM ***** Running evaluation *****
03/27 04:26:15 PM   Epoch = 1 iter 449 step
03/27 04:26:15 PM   Num examples = 1043
03/27 04:26:15 PM   Batch size = 32
03/27 04:26:16 PM ***** Eval results *****
03/27 04:26:16 PM   acc = 0.7085330776605945
03/27 04:26:16 PM   att_loss = 0.0
03/27 04:26:16 PM   cls_loss = 0.2751357512814658
03/27 04:26:16 PM   eval_loss = 0.5813823613253507
03/27 04:26:16 PM   global_step = 449
03/27 04:26:16 PM   loss = 0.2751357512814658
03/27 04:26:16 PM   mcc = 0.26854789956063607
03/27 04:26:16 PM   rep_loss = 0.0





Iteration:  86%|########6 | 231/268 [01:04<00:09,  3.77it/s]
Evaluating:   0%|          | 0/33 [00:00<?, ?it/s]
03/27 04:26:29 PM ***** Running evaluation *****
03/27 04:26:29 PM   Epoch = 1 iter 499 step
03/27 04:26:29 PM   Num examples = 1043
03/27 04:26:29 PM   Batch size = 32
03/27 04:26:30 PM ***** Eval results *****
03/27 04:26:30 PM   acc = 0.7085330776605945
03/27 04:26:30 PM   att_loss = 0.0
03/27 04:26:30 PM   cls_loss = 0.2749976789386108
03/27 04:26:30 PM   eval_loss = 0.5832070356065576
03/27 04:26:30 PM   global_step = 499
03/27 04:26:30 PM   loss = 0.2749976789386108
03/27 04:26:30 PM   mcc = 0.26202033638388644




Epoch:  67%|██████▋   | 2/3 [02:33<01:16, 76.38s/it].72it/s]

Iteration:   3%|2         | 8/268 [00:02<01:08,  3.77it/s]
03/27 04:26:43 PM ***** Running evaluation *****
03/27 04:26:43 PM   Epoch = 2 iter 549 step
03/27 04:26:43 PM   Num examples = 1043
Iteration:   5%|5         | 14/268 [00:03<01:07,  3.77it/s]
Evaluating:  18%|█▊        | 6/33 [00:00<00:00, 54.61it/s]
03/27 04:26:44 PM ***** Eval results *****
03/27 04:26:44 PM   acc = 0.6941514860977949
03/27 04:26:44 PM   att_loss = 0.0
03/27 04:26:44 PM   cls_loss = 0.27057949006557463
03/27 04:26:44 PM   eval_loss = 0.5882140262560411
03/27 04:26:44 PM   global_step = 549
03/27 04:26:44 PM   loss = 0.27057949006557463
03/27 04:26:44 PM   mcc = 0.25120764646171945






Iteration:  22%|##1       | 58/268 [00:16<00:56,  3.74it/s]
03/27 04:26:57 PM ***** Running evaluation *****
03/27 04:26:57 PM   Epoch = 2 iter 599 step
03/27 04:26:57 PM   Num examples = 1043
Iteration:  24%|##3       | 64/268 [00:17<00:54,  3.76it/s]
Evaluating:  36%|███▋      | 12/33 [00:00<00:00, 54.49it/s]
03/27 04:26:58 PM ***** Eval results *****
03/27 04:26:58 PM   acc = 0.7229146692233941
03/27 04:26:58 PM   att_loss = 0.0
03/27 04:26:58 PM   cls_loss = 0.27090857235284954
03/27 04:26:58 PM   eval_loss = 0.5756063145218473
03/27 04:26:58 PM   global_step = 599
03/27 04:26:58 PM   loss = 0.27090857235284954
03/27 04:26:58 PM   mcc = 0.29469430172062516
03/27 04:26:58 PM   rep_loss = 0.0







Iteration:  43%|####2     | 114/268 [00:32<00:40,  3.76it/s]
03/27 04:27:12 PM ***** Running evaluation *****
03/27 04:27:12 PM   Epoch = 2 iter 649 step
03/27 04:27:12 PM   Num examples = 1043
03/27 04:27:12 PM   Batch size = 32
03/27 04:27:12 PM ***** Eval results *****
03/27 04:27:12 PM   acc = 0.7152444870565676
03/27 04:27:12 PM   att_loss = 0.0
03/27 04:27:12 PM   cls_loss = 0.27064377497071807
03/27 04:27:12 PM   eval_loss = 0.57960219726418
03/27 04:27:12 PM   global_step = 649
03/27 04:27:12 PM   loss = 0.27064377497071807
03/27 04:27:12 PM   mcc = 0.28022221613462045
03/27 04:27:12 PM   rep_loss = 0.0






Iteration:  61%|######1   | 164/268 [00:46<00:27,  3.76it/s]
03/27 04:27:26 PM ***** Running evaluation *****
03/27 04:27:26 PM   Epoch = 2 iter 699 step
03/27 04:27:26 PM   Num examples = 1043
03/27 04:27:26 PM   Batch size = 32
03/27 04:27:26 PM ***** Eval results *****
03/27 04:27:26 PM   acc = 0.7181208053691275
03/27 04:27:26 PM   att_loss = 0.0
03/27 04:27:26 PM   cls_loss = 0.27110419517213646
03/27 04:27:26 PM   eval_loss = 0.577468196551005
03/27 04:27:26 PM   global_step = 699
03/27 04:27:26 PM   loss = 0.27110419517213646
03/27 04:27:26 PM   mcc = 0.28133108710648114
03/27 04:27:26 PM   rep_loss = 0.0






Iteration:  80%|#######9  | 214/268 [01:00<00:14,  3.75it/s]
03/27 04:27:39 PM ***** Running evaluation *****
03/27 04:27:39 PM   Epoch = 2 iter 749 step
03/27 04:27:39 PM   Num examples = 1043
03/27 04:27:39 PM   Batch size = 32
03/27 04:27:40 PM ***** Eval results *****
03/27 04:27:40 PM   acc = 0.7181208053691275
03/27 04:27:40 PM   att_loss = 0.0
03/27 04:27:40 PM   cls_loss = 0.27059034857639047
03/27 04:27:40 PM   eval_loss = 0.5778703039342706
03/27 04:27:40 PM   global_step = 749
03/27 04:27:40 PM   loss = 0.27059034857639047
03/27 04:27:40 PM   mcc = 0.28529203384052887
03/27 04:27:40 PM   rep_loss = 0.0





Iteration:  99%|#########8| 264/268 [01:13<00:01,  3.76it/s]
Evaluating:   0%|          | 0/33 [00:00<?, ?it/s]
03/27 04:27:53 PM ***** Running evaluation *****
03/27 04:27:53 PM   Epoch = 2 iter 799 step
03/27 04:27:53 PM   Num examples = 1043
03/27 04:27:53 PM   Batch size = 32
03/27 04:27:54 PM ***** Eval results *****
03/27 04:27:54 PM   acc = 0.7181208053691275
03/27 04:27:54 PM   att_loss = 0.0
03/27 04:27:54 PM   cls_loss = 0.27029852096764545
03/27 04:27:54 PM   eval_loss = 0.5788202285766602
03/27 04:27:54 PM   global_step = 799
03/27 04:27:54 PM   loss = 0.27029852096764545
03/27 04:27:54 PM   mcc = 0.28829259472164825

Epoch: 100%|██████████| 3/3 [03:48<00:00, 76.33s/it].80it/s]