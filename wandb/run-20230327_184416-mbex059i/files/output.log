03/27 06:44:16 PM device: cuda n_gpu: 1
USING KL ATTN LOSS WITH WEIGHT =  10000
03/27 06:44:16 PM Writing example 0 of 8551
03/27 06:44:16 PM *** Example ***
03/27 06:44:16 PM guid: train-0
03/27 06:44:16 PM tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
03/27 06:44:16 PM input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 06:44:16 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 06:44:16 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 06:44:16 PM label: 1
03/27 06:44:16 PM label_id: 1
03/27 06:44:17 PM Writing example 0 of 1043
03/27 06:44:17 PM *** Example ***
03/27 06:44:17 PM guid: dev-0
03/27 06:44:17 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
03/27 06:44:17 PM input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 06:44:17 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 06:44:17 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 06:44:17 PM label: 1
03/27 06:44:17 PM label_id: 1
03/27 06:44:17 PM loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
03/27 06:44:17 PM Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}
03/27 06:44:19 PM Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
03/27 06:44:19 PM loading model...
03/27 06:44:19 PM done!
03/27 06:44:19 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
03/27 06:44:20 PM loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
03/27 06:44:20 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}
03/27 06:44:20 PM Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
03/27 06:44:20 PM loading model...
03/27 06:44:20 PM done!
03/27 06:44:20 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
03/27 06:44:20 PM Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
03/27 06:44:20 PM ***** Running training *****
03/27 06:44:20 PM   Num examples = 8551
03/27 06:44:20 PM   Batch size = 32
03/27 06:44:20 PM   Num steps = 8010
03/27 06:44:20 PM n: bert.embeddings.word_embeddings.weight
03/27 06:44:20 PM n: bert.embeddings.position_embeddings.weight
03/27 06:44:20 PM n: bert.embeddings.token_type_embeddings.weight
03/27 06:44:20 PM n: bert.embeddings.LayerNorm.weight
03/27 06:44:20 PM n: bert.embeddings.LayerNorm.bias
03/27 06:44:20 PM n: bert.encoder.layer.0.attention.self.query.weight
03/27 06:44:20 PM n: bert.encoder.layer.0.attention.self.query.bias
03/27 06:44:20 PM n: bert.encoder.layer.0.attention.self.key.weight
03/27 06:44:20 PM n: bert.encoder.layer.0.attention.self.key.bias
03/27 06:44:20 PM n: bert.encoder.layer.0.attention.self.value.weight
03/27 06:44:20 PM n: bert.encoder.layer.0.attention.self.value.bias
03/27 06:44:20 PM n: bert.encoder.layer.0.attention.output.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.0.attention.output.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
03/27 06:44:20 PM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
03/27 06:44:20 PM n: bert.encoder.layer.0.intermediate.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.0.intermediate.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.0.output.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.0.output.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.0.output.LayerNorm.weight
03/27 06:44:20 PM n: bert.encoder.layer.0.output.LayerNorm.bias
03/27 06:44:20 PM n: bert.encoder.layer.1.attention.self.query.weight
03/27 06:44:20 PM n: bert.encoder.layer.1.attention.self.query.bias
03/27 06:44:20 PM n: bert.encoder.layer.1.attention.self.key.weight
03/27 06:44:20 PM n: bert.encoder.layer.1.attention.self.key.bias
03/27 06:44:20 PM n: bert.encoder.layer.1.attention.self.value.weight
03/27 06:44:20 PM n: bert.encoder.layer.1.attention.self.value.bias
03/27 06:44:20 PM n: bert.encoder.layer.1.attention.output.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.1.attention.output.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
03/27 06:44:20 PM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
03/27 06:44:20 PM n: bert.encoder.layer.1.intermediate.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.1.intermediate.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.1.output.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.1.output.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.1.output.LayerNorm.weight
03/27 06:44:20 PM n: bert.encoder.layer.1.output.LayerNorm.bias
03/27 06:44:20 PM n: bert.encoder.layer.2.attention.self.query.weight
03/27 06:44:20 PM n: bert.encoder.layer.2.attention.self.query.bias
03/27 06:44:20 PM n: bert.encoder.layer.2.attention.self.key.weight
03/27 06:44:20 PM n: bert.encoder.layer.2.attention.self.key.bias
03/27 06:44:20 PM n: bert.encoder.layer.2.attention.self.value.weight
03/27 06:44:20 PM n: bert.encoder.layer.2.attention.self.value.bias
03/27 06:44:20 PM n: bert.encoder.layer.2.attention.output.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.2.attention.output.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
03/27 06:44:20 PM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
03/27 06:44:20 PM n: bert.encoder.layer.2.intermediate.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.2.intermediate.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.2.output.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.2.output.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.2.output.LayerNorm.weight
03/27 06:44:20 PM n: bert.encoder.layer.2.output.LayerNorm.bias
03/27 06:44:20 PM n: bert.encoder.layer.3.attention.self.query.weight
03/27 06:44:20 PM n: bert.encoder.layer.3.attention.self.query.bias
03/27 06:44:20 PM n: bert.encoder.layer.3.attention.self.key.weight
03/27 06:44:20 PM n: bert.encoder.layer.3.attention.self.key.bias
03/27 06:44:20 PM n: bert.encoder.layer.3.attention.self.value.weight
03/27 06:44:20 PM n: bert.encoder.layer.3.attention.self.value.bias
03/27 06:44:20 PM n: bert.encoder.layer.3.attention.output.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.3.attention.output.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
03/27 06:44:20 PM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
03/27 06:44:20 PM n: bert.encoder.layer.3.intermediate.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.3.intermediate.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.3.output.dense.weight
03/27 06:44:20 PM n: bert.encoder.layer.3.output.dense.bias
03/27 06:44:20 PM n: bert.encoder.layer.3.output.LayerNorm.weight
03/27 06:44:20 PM n: bert.encoder.layer.3.output.LayerNorm.bias
03/27 06:44:20 PM n: bert.pooler.dense.weight
03/27 06:44:20 PM n: bert.pooler.dense.bias
03/27 06:44:20 PM n: classifier.weight
03/27 06:44:20 PM n: classifier.bias
03/27 06:44:20 PM n: fit_dense.weight
03/27 06:44:20 PM n: fit_dense.bias
03/27 06:44:20 PM Total parameters: 14591258
Epoch:   0%|          | 0/30 [00:00<?, ?it/s]







Iteration:  18%|#8        | 49/268 [00:15<01:52,  1.94it/s]
03/27 06:44:35 PM ***** Running evaluation *****
03/27 06:44:35 PM   Epoch = 0 iter 49 step
03/27 06:44:35 PM   Num examples = 1043
03/27 06:44:35 PM   Batch size = 32
03/27 06:44:35 PM ***** Eval results *****
03/27 06:44:35 PM   att_loss = 8772141.010204082
03/27 06:44:35 PM   cls_loss = 0.0
03/27 06:44:35 PM   global_step = 49
03/27 06:44:35 PM   loss = 8772142.663265307
03/27 06:44:35 PM   rep_loss = 1.6544393957877646








Iteration:  37%|###7      | 100/268 [00:31<01:14,  2.26it/s]
03/27 06:44:50 PM ***** Running evaluation *****
03/27 06:44:50 PM   Epoch = 0 iter 99 step
03/27 06:44:50 PM   Num examples = 1043
03/27 06:44:50 PM   Batch size = 32
03/27 06:44:50 PM ***** Eval results *****
03/27 06:44:50 PM   att_loss = 8095873.777777778
03/27 06:44:50 PM   cls_loss = 0.0
03/27 06:44:50 PM   global_step = 99
03/27 06:44:50 PM   loss = 8095875.313131313
03/27 06:44:50 PM   rep_loss = 1.4731342250650579







Iteration:  54%|#####4    | 146/268 [00:45<00:36,  3.30it/s]
03/27 06:45:06 PM ***** Running evaluation *****
03/27 06:45:06 PM   Epoch = 0 iter 149 step
03/27 06:45:06 PM   Num examples = 1043
03/27 06:45:06 PM   Batch size = 32
03/27 06:45:06 PM ***** Eval results *****
03/27 06:45:06 PM   att_loss = 7731587.533557047
03/27 06:45:06 PM   cls_loss = 0.0
03/27 06:45:06 PM   global_step = 149
03/27 06:45:06 PM   loss = 7731588.889261745
03/27 06:45:06 PM   rep_loss = 1.3760941428626143








Iteration:  73%|#######3  | 196/268 [01:01<00:21,  3.30it/s]
03/27 06:45:22 PM ***** Running evaluation *****
03/27 06:45:22 PM   Epoch = 0 iter 199 step
03/27 06:45:22 PM   Num examples = 1043
03/27 06:45:22 PM   Batch size = 32
03/27 06:45:22 PM ***** Eval results *****
03/27 06:45:22 PM   att_loss = 7513581.600502512
03/27 06:45:22 PM   cls_loss = 0.0
03/27 06:45:22 PM   global_step = 199
03/27 06:45:22 PM   loss = 7513582.866834171
03/27 06:45:22 PM   rep_loss = 1.3118175202278635








Iteration:  92%|#########2| 247/268 [01:17<00:06,  3.30it/s]
03/27 06:45:38 PM ***** Running evaluation *****
03/27 06:45:38 PM   Epoch = 0 iter 249 step
03/27 06:45:38 PM   Num examples = 1043
03/27 06:45:38 PM   Batch size = 32
03/27 06:45:38 PM ***** Eval results *****
03/27 06:45:38 PM   att_loss = 7373775.008032128
03/27 06:45:38 PM   cls_loss = 0.0
03/27 06:45:38 PM   global_step = 249
03/27 06:45:38 PM   loss = 7373776.220883534
03/27 06:45:38 PM   rep_loss = 1.2644298761245236



Epoch:   3%|▎         | 1/30 [01:24<40:41, 84.21s/it]30it/s]




Iteration:  12%|#1        | 31/268 [00:09<01:11,  3.30it/s]
03/27 06:45:54 PM ***** Running evaluation *****
03/27 06:45:54 PM   Epoch = 1 iter 299 step
03/27 06:45:54 PM   Num examples = 1043
03/27 06:45:54 PM   Batch size = 32
03/27 06:45:54 PM ***** Eval results *****
03/27 06:45:54 PM   att_loss = 6672022.328125
03/27 06:45:54 PM   cls_loss = 0.0
03/27 06:45:54 PM   global_step = 299
03/27 06:45:54 PM   loss = 6672023.328125
03/27 06:45:54 PM   rep_loss = 1.029799522832036








Iteration:  30%|###       | 81/268 [00:25<00:56,  3.30it/s]
03/27 06:46:10 PM ***** Running evaluation *****
03/27 06:46:10 PM   Epoch = 1 iter 349 step
03/27 06:46:10 PM   Num examples = 1043
03/27 06:46:10 PM   Batch size = 32
03/27 06:46:10 PM ***** Eval results *****
03/27 06:46:10 PM   att_loss = 6660208.4878048785
03/27 06:46:10 PM   cls_loss = 0.0
03/27 06:46:10 PM   global_step = 349
03/27 06:46:10 PM   loss = 6660209.4878048785
03/27 06:46:10 PM   rep_loss = 1.0150874576917508








Iteration:  49%|####8     | 131/268 [00:41<00:41,  3.29it/s]
03/27 06:46:25 PM ***** Running evaluation *****
03/27 06:46:25 PM   Epoch = 1 iter 399 step
03/27 06:46:25 PM   Num examples = 1043
03/27 06:46:25 PM   Batch size = 32
03/27 06:46:25 PM ***** Eval results *****
03/27 06:46:25 PM   att_loss = 6637574.810606061
03/27 06:46:25 PM   cls_loss = 0.0
03/27 06:46:25 PM   global_step = 399
03/27 06:46:25 PM   loss = 6637575.810606061
03/27 06:46:25 PM   rep_loss = 1.0018209223494385








Iteration:  68%|######7   | 181/268 [00:57<00:26,  3.29it/s]
03/27 06:46:41 PM ***** Running evaluation *****
03/27 06:46:41 PM   Epoch = 1 iter 449 step
03/27 06:46:41 PM   Num examples = 1043
03/27 06:46:41 PM   Batch size = 32
03/27 06:46:41 PM ***** Eval results *****
03/27 06:46:41 PM   att_loss = 6645768.269230769
03/27 06:46:41 PM   cls_loss = 0.0
03/27 06:46:41 PM   global_step = 449
03/27 06:46:41 PM   loss = 6645769.269230769
03/27 06:46:41 PM   rep_loss = 0.9900920106159462








Iteration:  86%|########6 | 231/268 [01:12<00:11,  3.28it/s]
03/27 06:46:57 PM ***** Running evaluation *****
03/27 06:46:57 PM   Epoch = 1 iter 499 step
03/27 06:46:57 PM   Num examples = 1043
03/27 06:46:57 PM   Batch size = 32
03/27 06:46:57 PM ***** Eval results *****
03/27 06:46:57 PM   att_loss = 6609073.810344827
03/27 06:46:57 PM   cls_loss = 0.0
03/27 06:46:57 PM   global_step = 499
03/27 06:46:57 PM   loss = 6609074.810344827
03/27 06:46:57 PM   rep_loss = 0.9775924500206421





Epoch:   7%|▋         | 2/30 [02:48<39:23, 84.42s/it]29it/s]


Iteration:   5%|5         | 14/268 [00:04<01:17,  3.28it/s]
03/27 06:47:13 PM ***** Running evaluation *****
03/27 06:47:13 PM   Epoch = 2 iter 549 step
03/27 06:47:13 PM   Num examples = 1043
03/27 06:47:13 PM   Batch size = 32
03/27 06:47:13 PM ***** Eval results *****
03/27 06:47:13 PM   att_loss = 6229635.6
03/27 06:47:13 PM   cls_loss = 0.0
03/27 06:47:13 PM   global_step = 549
03/27 06:47:13 PM   loss = 6229636.6
03/27 06:47:13 PM   rep_loss = 0.8912290930747986








Iteration:  24%|##3       | 64/268 [00:20<01:02,  3.28it/s]
03/27 06:47:29 PM ***** Running evaluation *****
03/27 06:47:29 PM   Epoch = 2 iter 599 step
03/27 06:47:29 PM   Num examples = 1043
03/27 06:47:29 PM   Batch size = 32
03/27 06:47:29 PM ***** Eval results *****
03/27 06:47:29 PM   att_loss = 6386022.876923077
03/27 06:47:29 PM   cls_loss = 0.0
03/27 06:47:29 PM   global_step = 599
03/27 06:47:29 PM   loss = 6386023.876923077
03/27 06:47:29 PM   rep_loss = 0.8927238812813392








Iteration:  43%|####2     | 115/268 [00:37<01:18,  1.95it/s]
03/27 06:47:45 PM ***** Running evaluation *****
03/27 06:47:45 PM   Epoch = 2 iter 649 step
03/27 06:47:45 PM   Num examples = 1043
03/27 06:47:45 PM   Batch size = 32
03/27 06:47:45 PM ***** Eval results *****
03/27 06:47:45 PM   att_loss = 6368222.195652174
03/27 06:47:45 PM   cls_loss = 0.0
03/27 06:47:45 PM   global_step = 649
03/27 06:47:45 PM   loss = 6368223.195652174
03/27 06:47:45 PM   rep_loss = 0.8834083085474761








Iteration:  62%|######1   | 165/268 [00:52<00:52,  1.97it/s]
03/27 06:48:01 PM ***** Running evaluation *****
03/27 06:48:01 PM   Epoch = 2 iter 699 step
03/27 06:48:01 PM   Num examples = 1043
03/27 06:48:01 PM   Batch size = 32
03/27 06:48:01 PM ***** Eval results *****
03/27 06:48:01 PM   att_loss = 6382634.796969697
03/27 06:48:01 PM   cls_loss = 0.0
03/27 06:48:01 PM   global_step = 699
03/27 06:48:01 PM   loss = 6382635.796969697
03/27 06:48:01 PM   rep_loss = 0.8774340831872188








Iteration:  80%|########  | 215/268 [01:08<00:27,  1.96it/s]
03/27 06:48:17 PM ***** Running evaluation *****
03/27 06:48:17 PM   Epoch = 2 iter 749 step
03/27 06:48:17 PM   Num examples = 1043
03/27 06:48:17 PM   Batch size = 32
03/27 06:48:17 PM ***** Eval results *****
03/27 06:48:17 PM   att_loss = 6387473.902325582
03/27 06:48:17 PM   cls_loss = 0.0
03/27 06:48:17 PM   global_step = 749
03/27 06:48:17 PM   loss = 6387474.902325582
03/27 06:48:17 PM   rep_loss = 0.8715457830318185








Iteration:  99%|#########9| 266/268 [01:25<00:00,  2.24it/s]
03/27 06:48:33 PM ***** Running evaluation *****
03/27 06:48:33 PM   Epoch = 2 iter 799 step
03/27 06:48:33 PM   Num examples = 1043
03/27 06:48:33 PM   Batch size = 32
03/27 06:48:33 PM ***** Eval results *****
03/27 06:48:33 PM   att_loss = 6375219.01509434
03/27 06:48:33 PM   cls_loss = 0.0
03/27 06:48:33 PM   global_step = 799
03/27 06:48:33 PM   loss = 6375220.01509434
03/27 06:48:33 PM   rep_loss = 0.8653223440332233
Epoch:  10%|█         | 3/30 [04:14<38:11, 84.87s/it]47it/s]







Iteration:  18%|#8        | 49/268 [00:15<01:39,  2.19it/s]
03/27 06:48:49 PM ***** Running evaluation *****
03/27 06:48:49 PM   Epoch = 3 iter 849 step
03/27 06:48:49 PM   Num examples = 1043
03/27 06:48:49 PM   Batch size = 32
03/27 06:48:49 PM ***** Eval results *****
03/27 06:48:49 PM   att_loss = 6165580.25
03/27 06:48:49 PM   cls_loss = 0.0
03/27 06:48:49 PM   global_step = 849
03/27 06:48:49 PM   loss = 6165581.25
03/27 06:48:49 PM   rep_loss = 0.8207402353485426








Iteration:  37%|###6      | 99/268 [00:31<01:16,  2.21it/s]
03/27 06:49:05 PM ***** Running evaluation *****
03/27 06:49:05 PM   Epoch = 3 iter 899 step
03/27 06:49:05 PM   Num examples = 1043
03/27 06:49:05 PM   Batch size = 32
03/27 06:49:05 PM ***** Eval results *****
03/27 06:49:05 PM   att_loss = 6197056.882653061
03/27 06:49:05 PM   cls_loss = 0.0
03/27 06:49:05 PM   global_step = 899
03/27 06:49:05 PM   loss = 6197057.882653061
03/27 06:49:05 PM   rep_loss = 0.8164106169525458







Iteration:  54%|#####4    | 145/268 [00:45<00:37,  3.28it/s]
03/27 06:49:21 PM ***** Running evaluation *****
03/27 06:49:21 PM   Epoch = 3 iter 949 step
03/27 06:49:21 PM   Num examples = 1043
03/27 06:49:21 PM   Batch size = 32
03/27 06:49:21 PM ***** Eval results *****
03/27 06:49:21 PM   att_loss = 6221389.14527027
03/27 06:49:21 PM   cls_loss = 0.0
03/27 06:49:21 PM   global_step = 949
03/27 06:49:21 PM   loss = 6221390.14527027
03/27 06:49:21 PM   rep_loss = 0.8136629570980329








Iteration:  73%|#######2  | 195/268 [01:01<00:22,  3.28it/s]
03/27 06:49:36 PM ***** Running evaluation *****
03/27 06:49:36 PM   Epoch = 3 iter 999 step
03/27 06:49:36 PM   Num examples = 1043
03/27 06:49:36 PM   Batch size = 32
03/27 06:49:36 PM ***** Eval results *****
03/27 06:49:36 PM   att_loss = 6223054.1338383835
03/27 06:49:36 PM   cls_loss = 0.0
03/27 06:49:36 PM   global_step = 999
03/27 06:49:36 PM   loss = 6223055.1338383835
03/27 06:49:36 PM   rep_loss = 0.8092799466667753








Iteration:  92%|#########1| 246/268 [01:17<00:06,  3.28it/s]
03/27 06:49:52 PM ***** Running evaluation *****
03/27 06:49:52 PM   Epoch = 3 iter 1049 step
03/27 06:49:52 PM   Num examples = 1043
03/27 06:49:52 PM   Batch size = 32
03/27 06:49:52 PM ***** Eval results *****
03/27 06:49:52 PM   att_loss = 6227096.895161291
03/27 06:49:52 PM   cls_loss = 0.0
03/27 06:49:52 PM   global_step = 1049
03/27 06:49:52 PM   loss = 6227097.893145162
03/27 06:49:52 PM   rep_loss = 0.8048880674665974



Epoch:  13%|█▎        | 4/30 [05:39<36:47, 84.89s/it]28it/s]




Iteration:  11%|#         | 29/268 [00:08<01:12,  3.28it/s]
03/27 06:50:08 PM ***** Running evaluation *****
03/27 06:50:08 PM   Epoch = 4 iter 1099 step
03/27 06:50:08 PM   Num examples = 1043
03/27 06:50:08 PM   Batch size = 32
03/27 06:50:08 PM ***** Eval results *****
03/27 06:50:08 PM   att_loss = 6232949.725806451
03/27 06:50:08 PM   cls_loss = 0.0
03/27 06:50:08 PM   global_step = 1099
03/27 06:50:08 PM   loss = 6232950.70967742
03/27 06:50:08 PM   rep_loss = 0.7795228054446559








Iteration:  29%|##9       | 79/268 [00:24<00:57,  3.28it/s]
03/27 06:50:24 PM ***** Running evaluation *****
03/27 06:50:24 PM   Epoch = 4 iter 1149 step
03/27 06:50:24 PM   Num examples = 1043
03/27 06:50:24 PM   Batch size = 32
03/27 06:50:24 PM ***** Eval results *****
03/27 06:50:24 PM   att_loss = 6191505.209876543
03/27 06:50:24 PM   cls_loss = 0.0
03/27 06:50:24 PM   global_step = 1149
03/27 06:50:24 PM   loss = 6191506.179012346
03/27 06:50:24 PM   rep_loss = 0.776390775486275








Iteration:  49%|####8     | 130/268 [00:41<00:42,  3.27it/s]
03/27 06:50:40 PM ***** Running evaluation *****
03/27 06:50:40 PM   Epoch = 4 iter 1199 step
03/27 06:50:40 PM   Num examples = 1043
03/27 06:50:40 PM   Batch size = 32
03/27 06:50:40 PM ***** Eval results *****
03/27 06:50:40 PM   att_loss = 6164221.721374046
03/27 06:50:40 PM   cls_loss = 0.0
03/27 06:50:40 PM   global_step = 1199
03/27 06:50:40 PM   loss = 6164222.652671755
03/27 06:50:40 PM   rep_loss = 0.7712424383818648








Iteration:  67%|######7   | 180/268 [00:56<00:26,  3.28it/s]
03/27 06:50:56 PM ***** Running evaluation *****
03/27 06:50:56 PM   Epoch = 4 iter 1249 step
03/27 06:50:56 PM   Num examples = 1043
03/27 06:50:56 PM   Batch size = 32
03/27 06:50:56 PM ***** Eval results *****
03/27 06:50:56 PM   att_loss = 6144026.690607735
03/27 06:50:56 PM   cls_loss = 0.0
03/27 06:50:56 PM   global_step = 1249
03/27 06:50:56 PM   loss = 6144027.585635359
03/27 06:50:56 PM   rep_loss = 0.7670526402431298








Iteration:  86%|########5 | 230/268 [01:12<00:11,  3.28it/s]
03/27 06:51:12 PM ***** Running evaluation *****
03/27 06:51:12 PM   Epoch = 4 iter 1299 step
03/27 06:51:12 PM   Num examples = 1043
03/27 06:51:12 PM   Batch size = 32
03/27 06:51:12 PM ***** Eval results *****
03/27 06:51:12 PM   att_loss = 6151419.233766234
03/27 06:51:12 PM   cls_loss = 0.0
03/27 06:51:12 PM   global_step = 1299
03/27 06:51:12 PM   loss = 6151420.106060606
03/27 06:51:12 PM   rep_loss = 0.7644575777507964





Epoch:  17%|█▋        | 5/30 [07:03<35:22, 84.88s/it]27it/s]


Iteration:   5%|4         | 13/268 [00:03<01:17,  3.28it/s]
03/27 06:51:28 PM ***** Running evaluation *****
03/27 06:51:28 PM   Epoch = 5 iter 1349 step
03/27 06:51:28 PM   Num examples = 1043
03/27 06:51:28 PM   Batch size = 32
03/27 06:51:28 PM ***** Eval results *****
03/27 06:51:28 PM   att_loss = 6131093.892857143
03/27 06:51:28 PM   cls_loss = 0.0
03/27 06:51:28 PM   global_step = 1349
03/27 06:51:28 PM   loss = 6131094.571428572
03/27 06:51:28 PM   rep_loss = 0.7445162492138999








Iteration:  24%|##3       | 63/268 [00:19<01:02,  3.28it/s]
03/27 06:51:44 PM ***** Running evaluation *****
03/27 06:51:44 PM   Epoch = 5 iter 1399 step
03/27 06:51:44 PM   Num examples = 1043
03/27 06:51:44 PM   Batch size = 32
03/27 06:51:44 PM ***** Eval results *****
03/27 06:51:44 PM   att_loss = 6091730.4375
03/27 06:51:44 PM   cls_loss = 0.0
03/27 06:51:44 PM   global_step = 1399
03/27 06:51:44 PM   loss = 6091731.0859375
03/27 06:51:44 PM   rep_loss = 0.7443858087062836








Iteration:  42%|####2     | 113/268 [00:35<00:47,  3.28it/s]
03/27 06:52:00 PM ***** Running evaluation *****
03/27 06:52:00 PM   Epoch = 5 iter 1449 step
03/27 06:52:00 PM   Num examples = 1043
03/27 06:52:00 PM   Batch size = 32
03/27 06:52:00 PM ***** Eval results *****
03/27 06:52:00 PM   att_loss = 6075497.206140351
03/27 06:52:00 PM   cls_loss = 0.0
03/27 06:52:00 PM   global_step = 1449
03/27 06:52:00 PM   loss = 6075497.868421053
03/27 06:52:00 PM   rep_loss = 0.7421848601416537








Iteration:  61%|######    | 163/268 [00:51<00:32,  3.28it/s]
03/27 06:52:16 PM ***** Running evaluation *****
03/27 06:52:16 PM   Epoch = 5 iter 1499 step
03/27 06:52:16 PM   Num examples = 1043
03/27 06:52:16 PM   Batch size = 32
03/27 06:52:16 PM ***** Eval results *****
03/27 06:52:16 PM   att_loss = 6078840.320121951
03/27 06:52:16 PM   cls_loss = 0.0
03/27 06:52:16 PM   global_step = 1499
03/27 06:52:16 PM   loss = 6078840.957317073
03/27 06:52:16 PM   rep_loss = 0.7397081888303524








Iteration:  79%|#######9  | 213/268 [01:07<00:16,  3.28it/s]
03/27 06:52:32 PM ***** Running evaluation *****
03/27 06:52:32 PM   Epoch = 5 iter 1549 step
03/27 06:52:32 PM   Num examples = 1043
03/27 06:52:32 PM   Batch size = 32
03/27 06:52:32 PM ***** Eval results *****
03/27 06:52:32 PM   att_loss = 6074558.289719626
03/27 06:52:32 PM   cls_loss = 0.0
03/27 06:52:32 PM   global_step = 1549
03/27 06:52:32 PM   loss = 6074558.908878504
03/27 06:52:32 PM   rep_loss = 0.737111648387998








Iteration:  98%|#########8| 263/268 [01:23<00:01,  3.28it/s]
03/27 06:52:48 PM ***** Running evaluation *****
03/27 06:52:48 PM   Epoch = 5 iter 1599 step
03/27 06:52:48 PM   Num examples = 1043
03/27 06:52:48 PM   Batch size = 32
03/27 06:52:48 PM ***** Eval results *****
03/27 06:52:48 PM   att_loss = 6071087.333333333
03/27 06:52:48 PM   cls_loss = 0.0
03/27 06:52:48 PM   global_step = 1599
03/27 06:52:48 PM   loss = 6071087.9375
03/27 06:52:48 PM   rep_loss = 0.7351166739156751
Epoch:  20%|██        | 6/30 [08:29<34:03, 85.13s/it]67it/s]







Iteration:  17%|#7        | 46/268 [00:14<01:07,  3.29it/s]
03/27 06:53:04 PM ***** Running evaluation *****
03/27 06:53:04 PM   Epoch = 6 iter 1649 step
03/27 06:53:04 PM   Num examples = 1043
03/27 06:53:04 PM   Batch size = 32
03/27 06:53:04 PM ***** Eval results *****
03/27 06:53:04 PM   att_loss = 5978689.872340426
03/27 06:53:04 PM   cls_loss = 0.0
03/27 06:53:04 PM   global_step = 1649
03/27 06:53:04 PM   loss = 5978690.393617021
03/27 06:53:04 PM   rep_loss = 0.7182075736370492








Iteration:  36%|###5      | 96/268 [00:29<00:52,  3.28it/s]
03/27 06:53:20 PM ***** Running evaluation *****
03/27 06:53:20 PM   Epoch = 6 iter 1699 step
03/27 06:53:20 PM   Num examples = 1043
03/27 06:53:20 PM   Batch size = 32
03/27 06:53:20 PM ***** Eval results *****
03/27 06:53:20 PM   att_loss = 6002773.0051546395
03/27 06:53:20 PM   cls_loss = 0.0
03/27 06:53:20 PM   global_step = 1699
03/27 06:53:20 PM   loss = 6002773.520618557
03/27 06:53:20 PM   rep_loss = 0.7192459677912525








Iteration:  54%|#####4    | 146/268 [00:45<00:37,  3.28it/s]
03/27 06:53:36 PM ***** Running evaluation *****
03/27 06:53:36 PM   Epoch = 6 iter 1749 step
03/27 06:53:36 PM   Num examples = 1043
03/27 06:53:36 PM   Batch size = 32
03/27 06:53:36 PM ***** Eval results *****
03/27 06:53:36 PM   att_loss = 6004321.452380952
03/27 06:53:36 PM   cls_loss = 0.0
03/27 06:53:36 PM   global_step = 1749
03/27 06:53:36 PM   loss = 6004321.969387755
03/27 06:53:36 PM   rep_loss = 0.7184097787149909








Iteration:  74%|#######3  | 197/268 [01:02<00:36,  1.93it/s]
03/27 06:53:52 PM ***** Running evaluation *****
03/27 06:53:52 PM   Epoch = 6 iter 1799 step
03/27 06:53:52 PM   Num examples = 1043
03/27 06:53:52 PM   Batch size = 32
03/27 06:53:52 PM ***** Eval results *****
03/27 06:53:52 PM   att_loss = 6000767.175126904
03/27 06:53:52 PM   cls_loss = 0.0
03/27 06:53:52 PM   global_step = 1799
03/27 06:53:52 PM   loss = 6000767.687817259
03/27 06:53:52 PM   rep_loss = 0.7168312142343085








Iteration:  92%|#########2| 247/268 [01:18<00:10,  1.94it/s]
03/27 06:54:07 PM ***** Running evaluation *****
03/27 06:54:07 PM   Epoch = 6 iter 1849 step
03/27 06:54:07 PM   Num examples = 1043
03/27 06:54:07 PM   Batch size = 32
03/27 06:54:07 PM ***** Eval results *****
03/27 06:54:07 PM   att_loss = 6006501.995951417
03/27 06:54:07 PM   cls_loss = 0.0
03/27 06:54:07 PM   global_step = 1849
03/27 06:54:07 PM   loss = 6006502.506072874
03/27 06:54:07 PM   rep_loss = 0.7144948055869654


Epoch:  23%|██▎       | 7/30 [09:54<32:36, 85.05s/it]28it/s]





Iteration:  11%|#1        | 30/268 [00:09<02:00,  1.97it/s]
03/27 06:54:23 PM ***** Running evaluation *****
03/27 06:54:23 PM   Epoch = 7 iter 1899 step
03/27 06:54:23 PM   Num examples = 1043
03/27 06:54:23 PM   Batch size = 32
03/27 06:54:23 PM ***** Eval results *****
03/27 06:54:23 PM   att_loss = 5890957.916666667
03/27 06:54:23 PM   cls_loss = 0.0
03/27 06:54:23 PM   global_step = 1899
03/27 06:54:23 PM   loss = 5890958.416666667
03/27 06:54:23 PM   rep_loss = 0.6987526973088583








Iteration:  30%|##9       | 80/268 [00:25<01:37,  1.93it/s]
03/27 06:54:39 PM ***** Running evaluation *****
03/27 06:54:39 PM   Epoch = 7 iter 1949 step
03/27 06:54:39 PM   Num examples = 1043
03/27 06:54:39 PM   Batch size = 32
03/27 06:54:39 PM ***** Eval results *****
03/27 06:54:39 PM   att_loss = 5938219.8625
03/27 06:54:39 PM   cls_loss = 0.0
03/27 06:54:39 PM   global_step = 1949
03/27 06:54:39 PM   loss = 5938220.3625
03/27 06:54:39 PM   rep_loss = 0.6995260484516621








Iteration:  49%|####8     | 131/268 [00:41<01:01,  2.24it/s]
03/27 06:54:55 PM ***** Running evaluation *****
03/27 06:54:55 PM   Epoch = 7 iter 1999 step
03/27 06:54:55 PM   Num examples = 1043
03/27 06:54:55 PM   Batch size = 32
03/27 06:54:55 PM ***** Eval results *****
03/27 06:54:55 PM   att_loss = 5963562.684615385
03/27 06:54:55 PM   cls_loss = 0.0
03/27 06:54:55 PM   global_step = 1999
03/27 06:54:55 PM   loss = 5963563.184615385
03/27 06:54:55 PM   rep_loss = 0.7000927008115329








Iteration:  68%|######7   | 181/268 [00:57<00:38,  2.26it/s]
03/27 06:55:11 PM ***** Running evaluation *****
03/27 06:55:11 PM   Epoch = 7 iter 2049 step
03/27 06:55:11 PM   Num examples = 1043
03/27 06:55:11 PM   Batch size = 32
03/27 06:55:11 PM ***** Eval results *****
03/27 06:55:11 PM   att_loss = 5970694.780555556
03/27 06:55:11 PM   cls_loss = 0.0
03/27 06:55:11 PM   global_step = 2049
03/27 06:55:11 PM   loss = 5970695.280555556
03/27 06:55:11 PM   rep_loss = 0.7003639909956191








Iteration:  87%|########6 | 232/268 [01:14<00:14,  2.47it/s]
03/27 06:55:27 PM ***** Running evaluation *****
03/27 06:55:27 PM   Epoch = 7 iter 2099 step
03/27 06:55:27 PM   Num examples = 1043
03/27 06:55:27 PM   Batch size = 32
03/27 06:55:27 PM ***** Eval results *****
03/27 06:55:27 PM   att_loss = 5968826.397826087
03/27 06:55:27 PM   cls_loss = 0.0
03/27 06:55:27 PM   global_step = 2099
03/27 06:55:27 PM   loss = 5968826.897826087
03/27 06:55:27 PM   rep_loss = 0.699311658869619





Epoch:  27%|██▋       | 8/30 [11:19<31:08, 84.95s/it]28it/s]

Iteration:   4%|4         | 11/268 [00:03<01:18,  3.28it/s]
03/27 06:55:43 PM ***** Running evaluation *****
03/27 06:55:43 PM   Epoch = 8 iter 2149 step
03/27 06:55:43 PM   Num examples = 1043
03/27 06:55:43 PM   Batch size = 32
03/27 06:55:43 PM ***** Eval results *****
03/27 06:55:43 PM   att_loss = 5901230.0
03/27 06:55:43 PM   cls_loss = 0.0
03/27 06:55:43 PM   global_step = 2149
03/27 06:55:43 PM   loss = 5901230.5
03/27 06:55:43 PM   rep_loss = 0.6904450150636526








Iteration:  23%|##2       | 61/268 [00:19<01:03,  3.28it/s]
03/27 06:55:59 PM ***** Running evaluation *****
03/27 06:55:59 PM   Epoch = 8 iter 2199 step
03/27 06:55:59 PM   Num examples = 1043
03/27 06:55:59 PM   Batch size = 32
03/27 06:55:59 PM ***** Eval results *****
03/27 06:55:59 PM   att_loss = 5892827.849206349
03/27 06:55:59 PM   cls_loss = 0.0
03/27 06:55:59 PM   global_step = 2199
03/27 06:55:59 PM   loss = 5892828.349206349
03/27 06:55:59 PM   rep_loss = 0.6874580061624921








Iteration:  41%|####1     | 111/268 [00:35<00:47,  3.28it/s]
03/27 06:56:15 PM ***** Running evaluation *****
03/27 06:56:15 PM   Epoch = 8 iter 2249 step
03/27 06:56:15 PM   Num examples = 1043
03/27 06:56:15 PM   Batch size = 32
03/27 06:56:15 PM ***** Eval results *****
03/27 06:56:15 PM   att_loss = 5901845.898230089
03/27 06:56:15 PM   cls_loss = 0.0
03/27 06:56:15 PM   global_step = 2249
03/27 06:56:15 PM   loss = 5901846.398230089
03/27 06:56:15 PM   rep_loss = 0.6886715023918489








Iteration:  60%|######    | 161/268 [00:51<00:32,  3.27it/s]
03/27 06:56:31 PM ***** Running evaluation *****
03/27 06:56:31 PM   Epoch = 8 iter 2299 step
03/27 06:56:31 PM   Num examples = 1043
03/27 06:56:31 PM   Batch size = 32
03/27 06:56:31 PM ***** Eval results *****
03/27 06:56:31 PM   att_loss = 5883479.54601227
03/27 06:56:31 PM   cls_loss = 0.0
03/27 06:56:31 PM   global_step = 2299
03/27 06:56:31 PM   loss = 5883480.04601227
03/27 06:56:31 PM   rep_loss = 0.6871369663923065








Iteration:  79%|#######9  | 212/268 [01:07<00:17,  3.28it/s]
03/27 06:56:47 PM ***** Running evaluation *****
03/27 06:56:47 PM   Epoch = 8 iter 2349 step
03/27 06:56:47 PM   Num examples = 1043
03/27 06:56:47 PM   Batch size = 32
03/27 06:56:47 PM ***** Eval results *****
03/27 06:56:47 PM   att_loss = 5885289.692488263
03/27 06:56:47 PM   cls_loss = 0.0
03/27 06:56:47 PM   global_step = 2349
03/27 06:56:47 PM   loss = 5885290.192488263
03/27 06:56:47 PM   rep_loss = 0.6855912368062517








Iteration:  98%|#########7| 262/268 [01:23<00:01,  3.28it/s]
03/27 06:57:03 PM ***** Running evaluation *****
03/27 06:57:03 PM   Epoch = 8 iter 2399 step
03/27 06:57:03 PM   Num examples = 1043
03/27 06:57:03 PM   Batch size = 32
03/27 06:57:03 PM ***** Eval results *****
03/27 06:57:03 PM   att_loss = 5902264.169201521
03/27 06:57:03 PM   cls_loss = 0.0
03/27 06:57:03 PM   global_step = 2399
03/27 06:57:03 PM   loss = 5902264.669201521
03/27 06:57:03 PM   rep_loss = 0.6852334540606452

Epoch:  30%|███       | 9/30 [12:44<29:48, 85.16s/it]80it/s]






Iteration:  17%|#6        | 45/268 [00:13<01:08,  3.24it/s]
03/27 06:57:19 PM ***** Running evaluation *****
03/27 06:57:19 PM   Epoch = 9 iter 2449 step
03/27 06:57:19 PM   Num examples = 1043
03/27 06:57:19 PM   Batch size = 32
03/27 06:57:19 PM ***** Eval results *****
03/27 06:57:19 PM   att_loss = 5830348.826086956
03/27 06:57:19 PM   cls_loss = 0.0
03/27 06:57:19 PM   global_step = 2449
03/27 06:57:19 PM   loss = 5830349.326086956
03/27 06:57:19 PM   rep_loss = 0.6776974304862644








Iteration:  35%|###5      | 94/268 [00:29<00:52,  3.28it/s]
03/27 06:57:35 PM ***** Running evaluation *****
03/27 06:57:35 PM   Epoch = 9 iter 2499 step
03/27 06:57:35 PM   Num examples = 1043
03/27 06:57:35 PM   Batch size = 32
03/27 06:57:35 PM ***** Eval results *****
03/27 06:57:35 PM   att_loss = 5875589.192708333
03/27 06:57:35 PM   cls_loss = 0.0
03/27 06:57:35 PM   global_step = 2499
03/27 06:57:35 PM   loss = 5875589.692708333
03/27 06:57:35 PM   rep_loss = 0.6772567002723614








Iteration:  53%|#####2    | 142/268 [00:44<00:38,  3.28it/s]
03/27 06:57:50 PM ***** Running evaluation *****
03/27 06:57:50 PM   Epoch = 9 iter 2549 step
03/27 06:57:50 PM   Num examples = 1043
03/27 06:57:50 PM   Batch size = 32
03/27 06:57:50 PM ***** Eval results *****
03/27 06:57:50 PM   att_loss = 5870948.2397260275
03/27 06:57:50 PM   cls_loss = 0.0
03/27 06:57:50 PM   global_step = 2549
03/27 06:57:50 PM   loss = 5870948.7397260275
03/27 06:57:50 PM   rep_loss = 0.6764124938069958








Iteration:  73%|#######2  | 195/268 [01:01<00:22,  3.28it/s]
03/27 06:58:06 PM ***** Running evaluation *****
03/27 06:58:06 PM   Epoch = 9 iter 2599 step
03/27 06:58:06 PM   Num examples = 1043
03/27 06:58:06 PM   Batch size = 32
03/27 06:58:06 PM ***** Eval results *****
03/27 06:58:06 PM   att_loss = 5877756.484693877
03/27 06:58:06 PM   cls_loss = 0.0
03/27 06:58:06 PM   global_step = 2599
03/27 06:58:06 PM   loss = 5877756.984693877
03/27 06:58:06 PM   rep_loss = 0.6756538894711709








Iteration:  91%|#########1| 245/268 [01:17<00:07,  3.28it/s]
03/27 06:58:22 PM ***** Running evaluation *****
03/27 06:58:22 PM   Epoch = 9 iter 2649 step
03/27 06:58:22 PM   Num examples = 1043
03/27 06:58:22 PM   Batch size = 32
03/27 06:58:22 PM ***** Eval results *****
03/27 06:58:22 PM   att_loss = 5865592.215447155
03/27 06:58:22 PM   cls_loss = 0.0
03/27 06:58:22 PM   global_step = 2649
03/27 06:58:22 PM   loss = 5865592.715447155
03/27 06:58:22 PM   rep_loss = 0.6746945080718374



Epoch:  33%|███▎      | 10/30 [14:09<28:21, 85.05s/it]8it/s]




Iteration:  10%|#         | 28/268 [00:08<01:13,  3.28it/s]
03/27 06:58:38 PM ***** Running evaluation *****
03/27 06:58:38 PM   Epoch = 10 iter 2699 step
03/27 06:58:38 PM   Num examples = 1043
03/27 06:58:38 PM   Batch size = 32
03/27 06:58:38 PM ***** Eval results *****
03/27 06:58:38 PM   att_loss = 5875227.827586207
03/27 06:58:38 PM   cls_loss = 0.0
03/27 06:58:38 PM   global_step = 2699
03/27 06:58:38 PM   loss = 5875228.327586207
03/27 06:58:38 PM   rep_loss = 0.6690344193886066








Iteration:  29%|##9       | 78/268 [00:24<00:57,  3.28it/s]
03/27 06:58:54 PM ***** Running evaluation *****
03/27 06:58:54 PM   Epoch = 10 iter 2749 step
03/27 06:58:54 PM   Num examples = 1043
03/27 06:58:54 PM   Batch size = 32
03/27 06:58:54 PM ***** Eval results *****
03/27 06:58:54 PM   att_loss = 5835101.329113924
03/27 06:58:54 PM   cls_loss = 0.0
03/27 06:58:54 PM   global_step = 2749
03/27 06:58:54 PM   loss = 5835101.829113924
03/27 06:58:54 PM   rep_loss = 0.6688782746278787








Iteration:  48%|####7     | 128/268 [00:40<00:42,  3.28it/s]
03/27 06:59:10 PM ***** Running evaluation *****
03/27 06:59:10 PM   Epoch = 10 iter 2799 step
03/27 06:59:10 PM   Num examples = 1043
03/27 06:59:10 PM   Batch size = 32
03/27 06:59:10 PM ***** Eval results *****
03/27 06:59:10 PM   att_loss = 5810953.5852713175
03/27 06:59:10 PM   cls_loss = 0.0
03/27 06:59:10 PM   global_step = 2799
03/27 06:59:10 PM   loss = 5810954.0852713175
03/27 06:59:10 PM   rep_loss = 0.6677605616029842








Iteration:  66%|######6   | 178/268 [00:56<00:27,  3.28it/s]
03/27 06:59:26 PM ***** Running evaluation *****
03/27 06:59:26 PM   Epoch = 10 iter 2849 step
03/27 06:59:26 PM   Num examples = 1043
03/27 06:59:26 PM   Batch size = 32
03/27 06:59:26 PM ***** Eval results *****
03/27 06:59:26 PM   att_loss = 5838574.48603352
03/27 06:59:26 PM   cls_loss = 0.0
03/27 06:59:26 PM   global_step = 2849
03/27 06:59:26 PM   loss = 5838574.98603352
03/27 06:59:26 PM   rep_loss = 0.6675438321502515








Iteration:  85%|########5 | 228/268 [01:12<00:12,  3.28it/s]
03/27 06:59:42 PM ***** Running evaluation *****
03/27 06:59:42 PM   Epoch = 10 iter 2899 step
03/27 06:59:42 PM   Num examples = 1043
03/27 06:59:42 PM   Batch size = 32
03/27 06:59:42 PM ***** Eval results *****
03/27 06:59:42 PM   att_loss = 5835601.519650655
03/27 06:59:42 PM   cls_loss = 0.0
03/27 06:59:42 PM   global_step = 2899
03/27 06:59:42 PM   loss = 5835602.019650655
03/27 06:59:42 PM   rep_loss = 0.6664280719632144





Epoch:  37%|███▋      | 11/30 [15:34<26:54, 85.00s/it]8it/s]


Iteration:   4%|4         | 11/268 [00:03<01:18,  3.28it/s]
03/27 06:59:58 PM ***** Running evaluation *****
03/27 06:59:58 PM   Epoch = 11 iter 2949 step
03/27 06:59:58 PM   Num examples = 1043
03/27 06:59:58 PM   Batch size = 32
03/27 06:59:58 PM ***** Eval results *****
03/27 06:59:58 PM   att_loss = 5862490.666666667
03/27 06:59:58 PM   cls_loss = 0.0
03/27 06:59:58 PM   global_step = 2949
03/27 06:59:58 PM   loss = 5862491.166666667
03/27 06:59:58 PM   rep_loss = 0.6647147784630457








Iteration:  23%|##2       | 61/268 [00:19<01:03,  3.28it/s]
03/27 07:00:14 PM ***** Running evaluation *****
03/27 07:00:14 PM   Epoch = 11 iter 2999 step
03/27 07:00:14 PM   Num examples = 1043
03/27 07:00:14 PM   Batch size = 32
03/27 07:00:14 PM ***** Eval results *****
03/27 07:00:14 PM   att_loss = 5800696.008064516
03/27 07:00:14 PM   cls_loss = 0.0
03/27 07:00:14 PM   global_step = 2999
03/27 07:00:14 PM   loss = 5800696.508064516
03/27 07:00:14 PM   rep_loss = 0.6595468924891564








Iteration:  41%|####1     | 111/268 [00:35<00:47,  3.28it/s]
03/27 07:00:30 PM ***** Running evaluation *****
03/27 07:00:30 PM   Epoch = 11 iter 3049 step
03/27 07:00:30 PM   Num examples = 1043
03/27 07:00:30 PM   Batch size = 32
03/27 07:00:30 PM ***** Eval results *****
03/27 07:00:30 PM   att_loss = 5820427.59375
03/27 07:00:30 PM   cls_loss = 0.0
03/27 07:00:30 PM   global_step = 3049
03/27 07:00:30 PM   loss = 5820428.09375
03/27 07:00:30 PM   rep_loss = 0.6605496491704669








