03/27 05:11:27 PM device: cuda n_gpu: 1
USING KL ATTN LOSS WITH WEIGHT =  100.0
03/27 05:11:27 PM Writing example 0 of 8551
03/27 05:11:27 PM *** Example ***
03/27 05:11:27 PM guid: train-0
03/27 05:11:27 PM tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
03/27 05:11:27 PM input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:11:27 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:11:27 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:11:27 PM label: 1
03/27 05:11:27 PM label_id: 1
03/27 05:11:28 PM Writing example 0 of 1043
03/27 05:11:28 PM *** Example ***
03/27 05:11:28 PM guid: dev-0
03/27 05:11:28 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
03/27 05:11:28 PM input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:11:28 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:11:28 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:11:28 PM label: 1
03/27 05:11:28 PM label_id: 1
03/27 05:11:28 PM loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
03/27 05:11:28 PM Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}
03/27 05:11:30 PM Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
03/27 05:11:30 PM loading model...
03/27 05:11:30 PM done!
03/27 05:11:30 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
03/27 05:11:31 PM loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
03/27 05:11:31 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}
03/27 05:11:31 PM Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
03/27 05:11:32 PM loading model...
03/27 05:11:32 PM done!
03/27 05:11:32 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
03/27 05:11:32 PM Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
03/27 05:11:32 PM ***** Running training *****
03/27 05:11:32 PM   Num examples = 8551
03/27 05:11:32 PM   Batch size = 32
03/27 05:11:32 PM   Num steps = 8010
03/27 05:11:32 PM n: bert.embeddings.word_embeddings.weight
03/27 05:11:32 PM n: bert.embeddings.position_embeddings.weight
03/27 05:11:32 PM n: bert.embeddings.token_type_embeddings.weight
03/27 05:11:32 PM n: bert.embeddings.LayerNorm.weight
03/27 05:11:32 PM n: bert.embeddings.LayerNorm.bias
03/27 05:11:32 PM n: bert.encoder.layer.0.attention.self.query.weight
03/27 05:11:32 PM n: bert.encoder.layer.0.attention.self.query.bias
03/27 05:11:32 PM n: bert.encoder.layer.0.attention.self.key.weight
03/27 05:11:32 PM n: bert.encoder.layer.0.attention.self.key.bias
03/27 05:11:32 PM n: bert.encoder.layer.0.attention.self.value.weight
03/27 05:11:32 PM n: bert.encoder.layer.0.attention.self.value.bias
03/27 05:11:32 PM n: bert.encoder.layer.0.attention.output.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.0.attention.output.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
03/27 05:11:32 PM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
03/27 05:11:32 PM n: bert.encoder.layer.0.intermediate.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.0.intermediate.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.0.output.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.0.output.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.0.output.LayerNorm.weight
03/27 05:11:32 PM n: bert.encoder.layer.0.output.LayerNorm.bias
03/27 05:11:32 PM n: bert.encoder.layer.1.attention.self.query.weight
03/27 05:11:32 PM n: bert.encoder.layer.1.attention.self.query.bias
03/27 05:11:32 PM n: bert.encoder.layer.1.attention.self.key.weight
03/27 05:11:32 PM n: bert.encoder.layer.1.attention.self.key.bias
03/27 05:11:32 PM n: bert.encoder.layer.1.attention.self.value.weight
03/27 05:11:32 PM n: bert.encoder.layer.1.attention.self.value.bias
03/27 05:11:32 PM n: bert.encoder.layer.1.attention.output.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.1.attention.output.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
03/27 05:11:32 PM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
03/27 05:11:32 PM n: bert.encoder.layer.1.intermediate.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.1.intermediate.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.1.output.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.1.output.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.1.output.LayerNorm.weight
03/27 05:11:32 PM n: bert.encoder.layer.1.output.LayerNorm.bias
03/27 05:11:32 PM n: bert.encoder.layer.2.attention.self.query.weight
03/27 05:11:32 PM n: bert.encoder.layer.2.attention.self.query.bias
03/27 05:11:32 PM n: bert.encoder.layer.2.attention.self.key.weight
03/27 05:11:32 PM n: bert.encoder.layer.2.attention.self.key.bias
03/27 05:11:32 PM n: bert.encoder.layer.2.attention.self.value.weight
03/27 05:11:32 PM n: bert.encoder.layer.2.attention.self.value.bias
03/27 05:11:32 PM n: bert.encoder.layer.2.attention.output.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.2.attention.output.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
03/27 05:11:32 PM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
03/27 05:11:32 PM n: bert.encoder.layer.2.intermediate.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.2.intermediate.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.2.output.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.2.output.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.2.output.LayerNorm.weight
03/27 05:11:32 PM n: bert.encoder.layer.2.output.LayerNorm.bias
03/27 05:11:32 PM n: bert.encoder.layer.3.attention.self.query.weight
03/27 05:11:32 PM n: bert.encoder.layer.3.attention.self.query.bias
03/27 05:11:32 PM n: bert.encoder.layer.3.attention.self.key.weight
03/27 05:11:32 PM n: bert.encoder.layer.3.attention.self.key.bias
03/27 05:11:32 PM n: bert.encoder.layer.3.attention.self.value.weight
03/27 05:11:32 PM n: bert.encoder.layer.3.attention.self.value.bias
03/27 05:11:32 PM n: bert.encoder.layer.3.attention.output.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.3.attention.output.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
03/27 05:11:32 PM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
03/27 05:11:32 PM n: bert.encoder.layer.3.intermediate.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.3.intermediate.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.3.output.dense.weight
03/27 05:11:32 PM n: bert.encoder.layer.3.output.dense.bias
03/27 05:11:32 PM n: bert.encoder.layer.3.output.LayerNorm.weight
03/27 05:11:32 PM n: bert.encoder.layer.3.output.LayerNorm.bias
03/27 05:11:32 PM n: bert.pooler.dense.weight
03/27 05:11:32 PM n: bert.pooler.dense.bias
03/27 05:11:32 PM n: classifier.weight
03/27 05:11:32 PM n: classifier.bias
03/27 05:11:32 PM n: fit_dense.weight
03/27 05:11:32 PM n: fit_dense.bias
03/27 05:11:32 PM Total parameters: 14591258
Iteration:   0%|          | 0/268 [00:00<?, ?it/s]
Epoch:   0%|          | 0/30 [00:00<?, ?it/s]it/s]