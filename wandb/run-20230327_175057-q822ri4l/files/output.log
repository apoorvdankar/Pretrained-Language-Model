03/27 05:50:58 PM device: cuda n_gpu: 1
USING KL ATTN LOSS WITH WEIGHT =  10000
03/27 05:50:58 PM Writing example 0 of 8551
03/27 05:50:58 PM *** Example ***
03/27 05:50:58 PM guid: train-0
03/27 05:50:58 PM tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
03/27 05:50:58 PM input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:50:58 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:50:58 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:50:58 PM label: 1
03/27 05:50:58 PM label_id: 1
03/27 05:50:59 PM Writing example 0 of 1043
03/27 05:50:59 PM *** Example ***
03/27 05:50:59 PM guid: dev-0
03/27 05:50:59 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
03/27 05:50:59 PM input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:50:59 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:50:59 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
03/27 05:50:59 PM label: 1
03/27 05:50:59 PM label_id: 1
03/27 05:50:59 PM loading archive file /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/
03/27 05:50:59 PM Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}
03/27 05:51:01 PM Loading model /w/331/adeemj/csc2516_proj/models/CoLA/models--textattack--bert-base-uncased-CoLA/snapshots/5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391/pytorch_model.bin
03/27 05:51:05 PM loading model...
03/27 05:51:05 PM done!
03/27 05:51:05 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
03/27 05:51:10 PM loading archive file /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/
03/27 05:51:10 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}
03/27 05:51:10 PM Loading model /w/331/adeemj/csc2516_proj/models/models--huawei-noah--TinyBERT_General_4L_312D/snapshots/34707a33cd59a94ecde241ac209bf35103691b43/pytorch_model.bin
03/27 05:51:11 PM loading model...
03/27 05:51:11 PM done!
03/27 05:51:11 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
03/27 05:51:11 PM Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
03/27 05:51:11 PM ***** Running training *****
03/27 05:51:11 PM   Num examples = 8551
03/27 05:51:11 PM   Batch size = 32
03/27 05:51:11 PM   Num steps = 8010
03/27 05:51:11 PM n: bert.embeddings.word_embeddings.weight
03/27 05:51:11 PM n: bert.embeddings.position_embeddings.weight
03/27 05:51:11 PM n: bert.embeddings.token_type_embeddings.weight
03/27 05:51:11 PM n: bert.embeddings.LayerNorm.weight
03/27 05:51:11 PM n: bert.embeddings.LayerNorm.bias
03/27 05:51:11 PM n: bert.encoder.layer.0.attention.self.query.weight
03/27 05:51:11 PM n: bert.encoder.layer.0.attention.self.query.bias
03/27 05:51:11 PM n: bert.encoder.layer.0.attention.self.key.weight
03/27 05:51:11 PM n: bert.encoder.layer.0.attention.self.key.bias
03/27 05:51:11 PM n: bert.encoder.layer.0.attention.self.value.weight
03/27 05:51:11 PM n: bert.encoder.layer.0.attention.self.value.bias
03/27 05:51:11 PM n: bert.encoder.layer.0.attention.output.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.0.attention.output.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
03/27 05:51:11 PM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
03/27 05:51:11 PM n: bert.encoder.layer.0.intermediate.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.0.intermediate.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.0.output.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.0.output.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.0.output.LayerNorm.weight
03/27 05:51:11 PM n: bert.encoder.layer.0.output.LayerNorm.bias
03/27 05:51:11 PM n: bert.encoder.layer.1.attention.self.query.weight
03/27 05:51:11 PM n: bert.encoder.layer.1.attention.self.query.bias
03/27 05:51:11 PM n: bert.encoder.layer.1.attention.self.key.weight
03/27 05:51:11 PM n: bert.encoder.layer.1.attention.self.key.bias
03/27 05:51:11 PM n: bert.encoder.layer.1.attention.self.value.weight
03/27 05:51:11 PM n: bert.encoder.layer.1.attention.self.value.bias
03/27 05:51:11 PM n: bert.encoder.layer.1.attention.output.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.1.attention.output.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
03/27 05:51:11 PM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
03/27 05:51:11 PM n: bert.encoder.layer.1.intermediate.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.1.intermediate.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.1.output.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.1.output.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.1.output.LayerNorm.weight
03/27 05:51:11 PM n: bert.encoder.layer.1.output.LayerNorm.bias
03/27 05:51:11 PM n: bert.encoder.layer.2.attention.self.query.weight
03/27 05:51:11 PM n: bert.encoder.layer.2.attention.self.query.bias
03/27 05:51:11 PM n: bert.encoder.layer.2.attention.self.key.weight
03/27 05:51:11 PM n: bert.encoder.layer.2.attention.self.key.bias
03/27 05:51:11 PM n: bert.encoder.layer.2.attention.self.value.weight
03/27 05:51:11 PM n: bert.encoder.layer.2.attention.self.value.bias
03/27 05:51:11 PM n: bert.encoder.layer.2.attention.output.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.2.attention.output.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
03/27 05:51:11 PM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
03/27 05:51:11 PM n: bert.encoder.layer.2.intermediate.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.2.intermediate.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.2.output.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.2.output.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.2.output.LayerNorm.weight
03/27 05:51:11 PM n: bert.encoder.layer.2.output.LayerNorm.bias
03/27 05:51:11 PM n: bert.encoder.layer.3.attention.self.query.weight
03/27 05:51:11 PM n: bert.encoder.layer.3.attention.self.query.bias
03/27 05:51:11 PM n: bert.encoder.layer.3.attention.self.key.weight
03/27 05:51:11 PM n: bert.encoder.layer.3.attention.self.key.bias
03/27 05:51:11 PM n: bert.encoder.layer.3.attention.self.value.weight
03/27 05:51:11 PM n: bert.encoder.layer.3.attention.self.value.bias
03/27 05:51:11 PM n: bert.encoder.layer.3.attention.output.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.3.attention.output.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
03/27 05:51:11 PM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
03/27 05:51:11 PM n: bert.encoder.layer.3.intermediate.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.3.intermediate.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.3.output.dense.weight
03/27 05:51:11 PM n: bert.encoder.layer.3.output.dense.bias
03/27 05:51:11 PM n: bert.encoder.layer.3.output.LayerNorm.weight
03/27 05:51:11 PM n: bert.encoder.layer.3.output.LayerNorm.bias
03/27 05:51:11 PM n: bert.pooler.dense.weight
03/27 05:51:11 PM n: bert.pooler.dense.bias
03/27 05:51:11 PM n: classifier.weight
03/27 05:51:11 PM n: classifier.bias
03/27 05:51:11 PM n: fit_dense.weight
03/27 05:51:11 PM n: fit_dense.bias
03/27 05:51:11 PM Total parameters: 14591258
Epoch:   0%|          | 0/30 [00:00<?, ?it/s]
	add_(Number alpha, Tensor other) [00:00<?, ?it/s]
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  next_m.mul_(beta1).add_(1 - beta1, grad)






Iteration:  17%|#7        | 46/268 [00:17<01:02,  3.54it/s]
03/27 05:51:29 PM ***** Running evaluation *****
03/27 05:51:29 PM   Epoch = 0 iter 49 step
03/27 05:51:29 PM   Num examples = 1043
03/27 05:51:29 PM   Batch size = 32
03/27 05:51:29 PM ***** Eval results *****
03/27 05:51:29 PM   att_loss = 8772141.010204082
03/27 05:51:29 PM   cls_loss = 0.0
03/27 05:51:29 PM   global_step = 49
03/27 05:51:29 PM   loss = 8772142.663265307

Iteration:  19%|#9        | 51/268 [00:19<01:23,  2.60it/s]







Iteration:  37%|###6      | 98/268 [00:32<00:48,  3.50it/s]
03/27 05:51:44 PM ***** Running evaluation *****
03/27 05:51:44 PM   Epoch = 0 iter 99 step
03/27 05:51:44 PM   Num examples = 1043
03/27 05:51:44 PM   Batch size = 32
03/27 05:51:44 PM ***** Eval results *****
03/27 05:51:44 PM   att_loss = 8095873.777777778
03/27 05:51:44 PM   cls_loss = 0.0
03/27 05:51:44 PM   global_step = 99
03/27 05:51:44 PM   loss = 8095875.313131313
03/27 05:51:44 PM   rep_loss = 1.4731342250650579







Iteration:  55%|#####4    | 147/268 [00:47<00:34,  3.50it/s]
03/27 05:51:59 PM ***** Running evaluation *****
03/27 05:51:59 PM   Epoch = 0 iter 149 step
03/27 05:51:59 PM   Num examples = 1043
03/27 05:51:59 PM   Batch size = 32
03/27 05:51:59 PM ***** Eval results *****
03/27 05:51:59 PM   att_loss = 7731587.533557047
03/27 05:51:59 PM   cls_loss = 0.0
03/27 05:51:59 PM   global_step = 149
03/27 05:51:59 PM   loss = 7731588.889261745
03/27 05:51:59 PM   rep_loss = 1.3760941428626143








Iteration:  74%|#######3  | 198/268 [01:02<00:20,  3.49it/s]
03/27 05:52:14 PM ***** Running evaluation *****
03/27 05:52:14 PM   Epoch = 0 iter 199 step
03/27 05:52:14 PM   Num examples = 1043
03/27 05:52:14 PM   Batch size = 32
03/27 05:52:14 PM ***** Eval results *****
03/27 05:52:14 PM   att_loss = 7513581.600502512
03/27 05:52:14 PM   cls_loss = 0.0
03/27 05:52:14 PM   global_step = 199
03/27 05:52:14 PM   loss = 7513582.866834171
03/27 05:52:14 PM   rep_loss = 1.3118175202278635







Iteration:  92%|#########2| 247/268 [01:17<00:06,  3.48it/s]
03/27 05:52:29 PM ***** Running evaluation *****
03/27 05:52:29 PM   Epoch = 0 iter 249 step
03/27 05:52:29 PM   Num examples = 1043
03/27 05:52:29 PM   Batch size = 32
03/27 05:52:29 PM ***** Eval results *****
03/27 05:52:29 PM   att_loss = 7373775.008032128
03/27 05:52:29 PM   cls_loss = 0.0
03/27 05:52:29 PM   global_step = 249
03/27 05:52:29 PM   loss = 7373776.220883534
03/27 05:52:29 PM   rep_loss = 1.2644298761245236



Epoch:   3%|▎         | 1/30 [01:23<40:32, 83.89s/it]47it/s]




Iteration:  12%|#1        | 31/268 [00:08<01:08,  3.46it/s]
03/27 05:52:44 PM ***** Running evaluation *****
03/27 05:52:44 PM   Epoch = 1 iter 299 step
03/27 05:52:44 PM   Num examples = 1043
03/27 05:52:44 PM   Batch size = 32
03/27 05:52:44 PM ***** Eval results *****
03/27 05:52:44 PM   att_loss = 6672022.328125
03/27 05:52:44 PM   cls_loss = 0.0
03/27 05:52:44 PM   global_step = 299
03/27 05:52:44 PM   loss = 6672023.328125
03/27 05:52:44 PM   rep_loss = 1.029799522832036







Iteration:  29%|##9       | 79/268 [00:23<00:55,  3.44it/s]
03/27 05:52:59 PM ***** Running evaluation *****
03/27 05:52:59 PM   Epoch = 1 iter 349 step
03/27 05:52:59 PM   Num examples = 1043
03/27 05:52:59 PM   Batch size = 32
03/27 05:52:59 PM ***** Eval results *****
03/27 05:52:59 PM   att_loss = 6660208.4878048785
03/27 05:52:59 PM   cls_loss = 0.0
03/27 05:52:59 PM   global_step = 349
03/27 05:52:59 PM   loss = 6660209.4878048785
03/27 05:52:59 PM   rep_loss = 1.0150874576917508








Iteration:  49%|####8     | 131/268 [00:39<00:39,  3.43it/s]
03/27 05:53:15 PM ***** Running evaluation *****
03/27 05:53:15 PM   Epoch = 1 iter 399 step
03/27 05:53:15 PM   Num examples = 1043
03/27 05:53:15 PM   Batch size = 32
03/27 05:53:15 PM ***** Eval results *****
03/27 05:53:15 PM   att_loss = 6637574.810606061
03/27 05:53:15 PM   cls_loss = 0.0
03/27 05:53:15 PM   global_step = 399
03/27 05:53:15 PM   loss = 6637575.810606061
03/27 05:53:15 PM   rep_loss = 1.0018209223494385








Iteration:  68%|######7   | 182/268 [00:55<00:42,  2.02it/s]
03/27 05:53:30 PM ***** Running evaluation *****
03/27 05:53:30 PM   Epoch = 1 iter 449 step
03/27 05:53:30 PM   Num examples = 1043
03/27 05:53:30 PM   Batch size = 32
03/27 05:53:30 PM ***** Eval results *****
03/27 05:53:30 PM   att_loss = 6645768.269230769
03/27 05:53:30 PM   cls_loss = 0.0
03/27 05:53:30 PM   global_step = 449
03/27 05:53:30 PM   loss = 6645769.269230769
03/27 05:53:30 PM   rep_loss = 0.9900920106159462







Iteration:  86%|########5 | 230/268 [01:09<00:11,  3.42it/s]
03/27 05:53:45 PM ***** Running evaluation *****
03/27 05:53:45 PM   Epoch = 1 iter 499 step
03/27 05:53:45 PM   Num examples = 1043
03/27 05:53:45 PM   Batch size = 32
03/27 05:53:45 PM ***** Eval results *****
03/27 05:53:45 PM   att_loss = 6609073.810344827
03/27 05:53:45 PM   cls_loss = 0.0
03/27 05:53:45 PM   global_step = 499
03/27 05:53:45 PM   loss = 6609074.810344827
03/27 05:53:45 PM   rep_loss = 0.9775924500206421





Epoch:   7%|▋         | 2/30 [02:45<38:24, 82.29s/it]41it/s]


Iteration:   5%|5         | 14/268 [00:04<01:14,  3.40it/s]
03/27 05:54:01 PM ***** Running evaluation *****
03/27 05:54:01 PM   Epoch = 2 iter 549 step
03/27 05:54:01 PM   Num examples = 1043
03/27 05:54:01 PM   Batch size = 32
03/27 05:54:01 PM ***** Eval results *****
03/27 05:54:01 PM   att_loss = 6229635.6
03/27 05:54:01 PM   cls_loss = 0.0
03/27 05:54:01 PM   global_step = 549
03/27 05:54:01 PM   loss = 6229636.6
03/27 05:54:01 PM   rep_loss = 0.8912290930747986








Iteration:  24%|##4       | 65/268 [00:20<01:40,  2.01it/s]
03/27 05:54:16 PM ***** Running evaluation *****
03/27 05:54:16 PM   Epoch = 2 iter 599 step
03/27 05:54:16 PM   Num examples = 1043
03/27 05:54:16 PM   Batch size = 32
03/27 05:54:16 PM ***** Eval results *****
03/27 05:54:16 PM   att_loss = 6386022.876923077
03/27 05:54:16 PM   cls_loss = 0.0
03/27 05:54:16 PM   global_step = 599
03/27 05:54:16 PM   loss = 6386023.876923077
03/27 05:54:16 PM   rep_loss = 0.8927238812813392







Iteration:  42%|####2     | 113/268 [00:34<00:45,  3.37it/s]
03/27 05:54:32 PM ***** Running evaluation *****
03/27 05:54:32 PM   Epoch = 2 iter 649 step
03/27 05:54:32 PM   Num examples = 1043
03/27 05:54:32 PM   Batch size = 32
03/27 05:54:32 PM ***** Eval results *****
03/27 05:54:32 PM   att_loss = 6368222.195652174
03/27 05:54:32 PM   cls_loss = 0.0
03/27 05:54:32 PM   global_step = 649
03/27 05:54:32 PM   loss = 6368223.195652174
03/27 05:54:32 PM   rep_loss = 0.8834083085474761








Iteration:  61%|######1   | 164/268 [00:50<00:30,  3.38it/s]
03/27 05:54:47 PM ***** Running evaluation *****
03/27 05:54:47 PM   Epoch = 2 iter 699 step
03/27 05:54:47 PM   Num examples = 1043
03/27 05:54:47 PM   Batch size = 32
03/27 05:54:47 PM ***** Eval results *****
03/27 05:54:47 PM   att_loss = 6382634.796969697
03/27 05:54:47 PM   cls_loss = 0.0
03/27 05:54:47 PM   global_step = 699
03/27 05:54:47 PM   loss = 6382635.796969697
03/27 05:54:47 PM   rep_loss = 0.8774340831872188








Iteration:  80%|#######9  | 214/268 [01:05<00:16,  3.36it/s]
03/27 05:55:02 PM ***** Running evaluation *****
03/27 05:55:02 PM   Epoch = 2 iter 749 step
03/27 05:55:02 PM   Num examples = 1043
03/27 05:55:02 PM   Batch size = 32
03/27 05:55:02 PM ***** Eval results *****
03/27 05:55:02 PM   att_loss = 6387473.902325582
03/27 05:55:02 PM   cls_loss = 0.0
03/27 05:55:02 PM   global_step = 749
03/27 05:55:02 PM   loss = 6387474.902325582
03/27 05:55:02 PM   rep_loss = 0.8715457830318185








Iteration:  99%|#########9| 266/268 [01:22<00:00,  2.27it/s]
03/27 05:55:18 PM ***** Running evaluation *****
03/27 05:55:18 PM   Epoch = 2 iter 799 step
03/27 05:55:18 PM   Num examples = 1043
03/27 05:55:18 PM   Batch size = 32
03/27 05:55:18 PM ***** Eval results *****
03/27 05:55:18 PM   att_loss = 6375219.01509434
03/27 05:55:18 PM   cls_loss = 0.0
03/27 05:55:18 PM   global_step = 799
03/27 05:55:18 PM   loss = 6375220.01509434
03/27 05:55:18 PM   rep_loss = 0.8653223440332233
Epoch:  10%|█         | 3/30 [04:08<37:11, 82.63s/it]51it/s]






Iteration:  17%|#7        | 46/268 [00:13<01:06,  3.36it/s]
03/27 05:55:34 PM ***** Running evaluation *****
03/27 05:55:34 PM   Epoch = 3 iter 849 step
03/27 05:55:34 PM   Num examples = 1043
03/27 05:55:34 PM   Batch size = 32
03/27 05:55:34 PM ***** Eval results *****
03/27 05:55:34 PM   att_loss = 6165580.25
03/27 05:55:34 PM   cls_loss = 0.0
03/27 05:55:34 PM   global_step = 849
03/27 05:55:34 PM   loss = 6165581.25
03/27 05:55:34 PM   rep_loss = 0.8207402353485426








Iteration:  36%|###6      | 97/268 [00:29<00:50,  3.36it/s]
03/27 05:55:49 PM ***** Running evaluation *****
03/27 05:55:49 PM   Epoch = 3 iter 899 step
03/27 05:55:49 PM   Num examples = 1043
03/27 05:55:49 PM   Batch size = 32
03/27 05:55:49 PM ***** Eval results *****
03/27 05:55:49 PM   att_loss = 6197056.882653061
03/27 05:55:49 PM   cls_loss = 0.0
03/27 05:55:49 PM   global_step = 899
03/27 05:55:49 PM   loss = 6197057.882653061
03/27 05:55:49 PM   rep_loss = 0.8164106169525458








Iteration:  55%|#####4    | 147/268 [00:45<00:36,  3.35it/s]
03/27 05:56:05 PM ***** Running evaluation *****
03/27 05:56:05 PM   Epoch = 3 iter 949 step
03/27 05:56:05 PM   Num examples = 1043
03/27 05:56:05 PM   Batch size = 32
03/27 05:56:05 PM ***** Eval results *****
03/27 05:56:05 PM   att_loss = 6221389.14527027
03/27 05:56:05 PM   cls_loss = 0.0
03/27 05:56:05 PM   global_step = 949
03/27 05:56:05 PM   loss = 6221390.14527027
03/27 05:56:05 PM   rep_loss = 0.8136629570980329








Iteration:  74%|#######3  | 198/268 [01:01<00:35,  1.98it/s]
03/27 05:56:20 PM ***** Running evaluation *****
03/27 05:56:20 PM   Epoch = 3 iter 999 step
03/27 05:56:20 PM   Num examples = 1043
03/27 05:56:20 PM   Batch size = 32
03/27 05:56:20 PM ***** Eval results *****
03/27 05:56:20 PM   att_loss = 6223054.1338383835
03/27 05:56:20 PM   cls_loss = 0.0
03/27 05:56:20 PM   global_step = 999
03/27 05:56:20 PM   loss = 6223055.1338383835
03/27 05:56:20 PM   rep_loss = 0.8092799466667753








Iteration:  93%|#########2| 249/268 [01:17<00:08,  2.27it/s]
03/27 05:56:36 PM ***** Running evaluation *****
03/27 05:56:36 PM   Epoch = 3 iter 1049 step
03/27 05:56:36 PM   Num examples = 1043
03/27 05:56:36 PM   Batch size = 32
03/27 05:56:36 PM ***** Eval results *****
03/27 05:56:36 PM   att_loss = 6227096.895161291
03/27 05:56:36 PM   cls_loss = 0.0
03/27 05:56:36 PM   global_step = 1049
03/27 05:56:36 PM   loss = 6227097.893145162
03/27 05:56:36 PM   rep_loss = 0.8048880674665974


Epoch:  13%|█▎        | 4/30 [05:31<35:54, 82.85s/it]33it/s]




Iteration:  11%|#         | 29/268 [00:08<01:11,  3.34it/s]
03/27 05:56:52 PM ***** Running evaluation *****
03/27 05:56:52 PM   Epoch = 4 iter 1099 step
03/27 05:56:52 PM   Num examples = 1043
03/27 05:56:52 PM   Batch size = 32
03/27 05:56:52 PM ***** Eval results *****
03/27 05:56:52 PM   att_loss = 6232949.725806451
03/27 05:56:52 PM   cls_loss = 0.0
03/27 05:56:52 PM   global_step = 1099
03/27 05:56:52 PM   loss = 6232950.70967742
03/27 05:56:52 PM   rep_loss = 0.7795228054446559








Iteration:  30%|##9       | 80/268 [00:24<00:56,  3.33it/s]
03/27 05:57:07 PM ***** Running evaluation *****
03/27 05:57:07 PM   Epoch = 4 iter 1149 step
03/27 05:57:07 PM   Num examples = 1043
03/27 05:57:07 PM   Batch size = 32
03/27 05:57:07 PM ***** Eval results *****
03/27 05:57:07 PM   att_loss = 6191505.209876543
03/27 05:57:07 PM   cls_loss = 0.0
03/27 05:57:07 PM   global_step = 1149
03/27 05:57:07 PM   loss = 6191506.179012346
03/27 05:57:07 PM   rep_loss = 0.776390775486275








Iteration:  49%|####8     | 130/268 [00:40<00:41,  3.33it/s]
03/27 05:57:23 PM ***** Running evaluation *****
03/27 05:57:23 PM   Epoch = 4 iter 1199 step
03/27 05:57:23 PM   Num examples = 1043
03/27 05:57:23 PM   Batch size = 32
03/27 05:57:23 PM ***** Eval results *****
03/27 05:57:23 PM   att_loss = 6164221.721374046
03/27 05:57:23 PM   cls_loss = 0.0
03/27 05:57:23 PM   global_step = 1199
03/27 05:57:23 PM   loss = 6164222.652671755
03/27 05:57:23 PM   rep_loss = 0.7712424383818648








Iteration:  67%|######7   | 180/268 [00:56<00:26,  3.33it/s]
03/27 05:57:39 PM ***** Running evaluation *****
03/27 05:57:39 PM   Epoch = 4 iter 1249 step
03/27 05:57:39 PM   Num examples = 1043
03/27 05:57:39 PM   Batch size = 32
03/27 05:57:39 PM ***** Eval results *****
03/27 05:57:39 PM   att_loss = 6144026.690607735
03/27 05:57:39 PM   cls_loss = 0.0
03/27 05:57:39 PM   global_step = 1249
03/27 05:57:39 PM   loss = 6144027.585635359
03/27 05:57:39 PM   rep_loss = 0.7670526402431298








Iteration:  86%|########6 | 231/268 [01:12<00:18,  1.96it/s]
03/27 05:57:55 PM ***** Running evaluation *****
03/27 05:57:55 PM   Epoch = 4 iter 1299 step
03/27 05:57:55 PM   Num examples = 1043
03/27 05:57:55 PM   Batch size = 32
03/27 05:57:55 PM ***** Eval results *****
03/27 05:57:55 PM   att_loss = 6151419.233766234
03/27 05:57:55 PM   cls_loss = 0.0
03/27 05:57:55 PM   global_step = 1299
03/27 05:57:55 PM   loss = 6151420.106060606
03/27 05:57:55 PM   rep_loss = 0.7644575777507964





Epoch:  17%|█▋        | 5/30 [06:54<34:38, 83.15s/it]28it/s]


Iteration:   6%|5         | 15/268 [00:05<01:51,  2.27it/s]
03/27 05:58:10 PM ***** Running evaluation *****
03/27 05:58:10 PM   Epoch = 5 iter 1349 step
03/27 05:58:10 PM   Num examples = 1043
03/27 05:58:10 PM   Batch size = 32
03/27 05:58:10 PM ***** Eval results *****
03/27 05:58:10 PM   att_loss = 6131093.892857143
03/27 05:58:10 PM   cls_loss = 0.0
03/27 05:58:10 PM   global_step = 1349
03/27 05:58:10 PM   loss = 6131094.571428572
03/27 05:58:10 PM   rep_loss = 0.7445162492138999







Iteration:  23%|##2       | 61/268 [00:19<01:02,  3.32it/s]
03/27 05:58:26 PM ***** Running evaluation *****
03/27 05:58:26 PM   Epoch = 5 iter 1399 step
03/27 05:58:26 PM   Num examples = 1043
03/27 05:58:26 PM   Batch size = 32
03/27 05:58:26 PM ***** Eval results *****
03/27 05:58:26 PM   att_loss = 6091730.4375
03/27 05:58:26 PM   cls_loss = 0.0
03/27 05:58:26 PM   global_step = 1399
03/27 05:58:26 PM   loss = 6091731.0859375
03/27 05:58:26 PM   rep_loss = 0.7443858087062836








Iteration:  42%|####1     | 112/268 [00:35<00:47,  3.32it/s]
03/27 05:58:42 PM ***** Running evaluation *****
03/27 05:58:42 PM   Epoch = 5 iter 1449 step
03/27 05:58:42 PM   Num examples = 1043
03/27 05:58:42 PM   Batch size = 32
03/27 05:58:42 PM ***** Eval results *****
03/27 05:58:42 PM   att_loss = 6075497.206140351
03/27 05:58:42 PM   cls_loss = 0.0
03/27 05:58:42 PM   global_step = 1449
03/27 05:58:42 PM   loss = 6075497.868421053
03/27 05:58:42 PM   rep_loss = 0.7421848601416537








Iteration:  61%|######    | 163/268 [00:51<00:31,  3.31it/s]
03/27 05:58:58 PM ***** Running evaluation *****
03/27 05:58:58 PM   Epoch = 5 iter 1499 step
03/27 05:58:58 PM   Num examples = 1043
03/27 05:58:58 PM   Batch size = 32
03/27 05:58:58 PM ***** Eval results *****
03/27 05:58:58 PM   att_loss = 6078840.320121951
03/27 05:58:58 PM   cls_loss = 0.0
03/27 05:58:58 PM   global_step = 1499
03/27 05:58:58 PM   loss = 6078840.957317073
03/27 05:58:58 PM   rep_loss = 0.7397081888303524








Iteration:  79%|#######9  | 213/268 [01:07<00:16,  3.31it/s]
03/27 05:59:13 PM ***** Running evaluation *****
03/27 05:59:13 PM   Epoch = 5 iter 1549 step
03/27 05:59:13 PM   Num examples = 1043
03/27 05:59:13 PM   Batch size = 32
03/27 05:59:13 PM ***** Eval results *****
03/27 05:59:13 PM   att_loss = 6074558.289719626
03/27 05:59:13 PM   cls_loss = 0.0
03/27 05:59:13 PM   global_step = 1549
03/27 05:59:13 PM   loss = 6074558.908878504
03/27 05:59:13 PM   rep_loss = 0.737111648387998








Iteration:  98%|#########8| 263/268 [01:22<00:01,  3.31it/s]
03/27 05:59:29 PM ***** Running evaluation *****
03/27 05:59:29 PM   Epoch = 5 iter 1599 step
03/27 05:59:29 PM   Num examples = 1043
03/27 05:59:29 PM   Batch size = 32
03/27 05:59:29 PM ***** Eval results *****
03/27 05:59:29 PM   att_loss = 6071087.333333333
03/27 05:59:29 PM   cls_loss = 0.0
03/27 05:59:29 PM   global_step = 1599
03/27 05:59:29 PM   loss = 6071087.9375
03/27 05:59:29 PM   rep_loss = 0.7351166739156751
Epoch:  20%|██        | 6/30 [08:19<33:28, 83.70s/it]67it/s]







Iteration:  17%|#7        | 46/268 [00:13<01:07,  3.30it/s]
03/27 05:59:45 PM ***** Running evaluation *****
03/27 05:59:45 PM   Epoch = 6 iter 1649 step
03/27 05:59:45 PM   Num examples = 1043
03/27 05:59:45 PM   Batch size = 32
03/27 05:59:45 PM ***** Eval results *****
03/27 05:59:45 PM   att_loss = 5978689.872340426
03/27 05:59:45 PM   cls_loss = 0.0
03/27 05:59:45 PM   global_step = 1649
03/27 05:59:45 PM   loss = 5978690.393617021
03/27 05:59:45 PM   rep_loss = 0.7182075736370492








Iteration:  36%|###5      | 96/268 [00:29<00:52,  3.30it/s]
03/27 06:00:01 PM ***** Running evaluation *****
03/27 06:00:01 PM   Epoch = 6 iter 1699 step
03/27 06:00:01 PM   Num examples = 1043
03/27 06:00:01 PM   Batch size = 32
03/27 06:00:01 PM ***** Eval results *****
03/27 06:00:01 PM   att_loss = 6002773.0051546395
03/27 06:00:01 PM   cls_loss = 0.0
03/27 06:00:01 PM   global_step = 1699
03/27 06:00:01 PM   loss = 6002773.520618557
03/27 06:00:01 PM   rep_loss = 0.7192459677912525








Iteration:  55%|#####4    | 147/268 [00:46<01:02,  1.94it/s]
03/27 06:00:17 PM ***** Running evaluation *****
03/27 06:00:17 PM   Epoch = 6 iter 1749 step
03/27 06:00:17 PM   Num examples = 1043
03/27 06:00:17 PM   Batch size = 32
03/27 06:00:17 PM ***** Eval results *****
03/27 06:00:17 PM   att_loss = 6004321.452380952
03/27 06:00:17 PM   cls_loss = 0.0
03/27 06:00:17 PM   global_step = 1749
03/27 06:00:17 PM   loss = 6004321.969387755
03/27 06:00:17 PM   rep_loss = 0.7184097787149909








Iteration:  74%|#######3  | 197/268 [01:02<00:36,  1.95it/s]
03/27 06:00:33 PM ***** Running evaluation *****
03/27 06:00:33 PM   Epoch = 6 iter 1799 step
03/27 06:00:33 PM   Num examples = 1043
03/27 06:00:33 PM   Batch size = 32
03/27 06:00:33 PM ***** Eval results *****
03/27 06:00:33 PM   att_loss = 6000767.175126904
03/27 06:00:33 PM   cls_loss = 0.0
03/27 06:00:33 PM   global_step = 1799
03/27 06:00:33 PM   loss = 6000767.687817259
03/27 06:00:33 PM   rep_loss = 0.7168312142343085




